[
  {
    "path": "posts/2022-03-25-run-course/",
    "title": "코스 템플리트 실행하기",
    "description": "이네스 코스 개발 환경에서 코스 템플리트 앱을 실행합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-25",
    "categories": [
      "R Markdown",
      "Reproducible Research",
      "Development"
    ],
    "contents": "\n\nContents\n코스 템플리트 둘러보기\n코스 플랫폼 프로젝트 구동\n코스 플랫폼의 파일 구조\n\nRStudio 프로젝트 환경 설정하기\nNode.js 패키지 설치하기\n\n어플리케이션 실행\n개발 서버의 구동\n코스 템플리트 앱의 실행\n\n요약\n작업 요약\nI can do it\n\n\n\n\n\n\n\n들어가기\n온라인 코스 실행해볼까요?!\n\n코스 플랫폼 템플리트는 템플리트일 뿐, 하나씩 만들어 넣어 내 코스를 생성해야죠.\n\n만들고 실행해서 동작해보고, 또 다시 만들고 실행하고...\n\n이를 위해서 RStudio에서 생성한 코스를 실행하는 방법을 먼저 배워봅니다.\n\n\n코스 템플리트 둘러보기\n코스 플랫폼을 개발하기 위해서는 콘솔 CLI(Command-line interface)에서의 작업이 필요합니다. 그러나 여기서는 별도의 터미널이 아닌, RStudio 안에서 모두 작업해보겠습니다.\n코스 플랫폼 프로젝트 구동\n이네스는 코스 플랫폼 템플리트로 RStudio 프로젝트를 만들었습니다. 해당 프로젝트를 구동하면, 다음과 같은 화면을 볼 수 있습니다.\n에디터 창에 package.json 파일을 띄워 놓은 화면입니다.\n\n\n\n\n코스 플랫폼의 파일 구조\nRStudio의 Terminal 탭은 운영체제의 터미널 콘솔을 띄워줍니다. 여기서 CLI를 통해서 터미널 명령어를 수행할 수 있습니다. 현재 이 문서는 MacOS에서 작업하고 있기 때문에 zsh 터미널이 구동됩니다. 왜냐하면 2019년도 macOS Catalina나 버전부터 zsh이 기본 쉘이 되었기 때문입니다.\n템플리트의 첫번째 디렉토리를 포함한 파일을 살펴봅니다.\n\ntree -L 1\n\n템플리트의 첫번째 디렉토리를 포함한 파일 구조는 다음과 같습니다.\n\n\n\n\n주요 디렉토리와 파일을 요약하면 다음과 같습니다. (d)는 디렉토리를 의미합니다.\nbinder (d)\nBinder에 설치할 R 이미지와 패키지 설치 관련 정보 파일\n\nchapters (d)\n코스의 챕터별 컨텐츠를 정의하는 마크다운 문서 위치\n\nexercises (d)\n코스의 연습문제를 정의하는 R 스크립트 파일 위치\n\ngatsby-browser.js\nGatsby 앱 브라우징 설정 파일\n\ngatsby-config.js\nGatsby 앱 설정 파일\n\ngatsby-node.js\nGatsby의 Node.js 설정 파일\n\nmeta.json\n이네스 코스를 정의하는 메타정보 설정 파일\n\npackage-lock.json\n이네스 코스 플랫랫폼에서 사용하는 Node.js 패키지의 의존성 정보 파일\n\npackage.json\n이네스 코스 플랫랫폼에서 사용하는 Node.js 패키지 목록 파일\n\nslides (d)\n코스의 슬라이드 컨텐츠를 정의하는 마크다운 문서 위치\n\nsrc (d)\n이네스 코스 플랫랫폼 구성 Gatsby 소스 코드\n\nstatic (d)\n이네스 코스 플랫폼에서 사용하는 정적 이미지 파일 위치\n\ntheme.sass\n이네스 코스 플랫폼의 테마 설정 파일\n\n템플리트의 둘째 디렉토리를 포함한 파일을 살펴봅니다.\n\ntree -L 2\n\n템플리트의 둘째 디렉토리를 포함한 파일 구조는 다음과 같습니다. 이 글이 이네스 플랫폼에 대한 이해 수준의 범위라, 이런 파일들이 있구나 정도로 이해하시기 바랍니다.\n\n\n\nRStudio 프로젝트 환경 설정하기\nNode.js 패키지 설치하기\n이네스가 배포한 템플리트로 만든 RStudio 프로젝트를 처음 구동한 후 Node.js 패키지를 설치합니다.\n운영체제 터미널 CLI 환경에서 설치해야 하지만, RStudio가 터미널을 지원하기 때문에 “Console” 패널의 “Terminal” 탭을 눌러 터미널을 엽니다. 그리고 다음의 명령을 입력합니다. 이미 앞에서 Node.js 패키지 설치 관리자인 npm을 설치했기 때문에 npm 명령어를 입력할 수 있습니다.\n\nnpm install\n\nnpm install 명령어는 package-lock.json 파일에 기술된 Node.js 패키지를 설치합니다. 다음 그림처럼 코스 플랫폼 구축에 필요한 Node.js 패키지들을 설치하기 시작합니다.\n\n\n\n그런데 필자의 환경에서는 Node.js 패키지 설치 시 에러가 발생했습니다. 발생한 에러의 다음 부분(버전 정보)을 주의 깊게 볼 필요가 있습니다. 필자의 Node.js 환경입니다.\n\nnpm ERR! gyp ERR! node -v v17.2.0\nnpm ERR! gyp ERR! node-gyp -v v3.8.0\n\n2021-12-11 기준으로 이네스가 배포한 템플리트는 3년이 경과하였습니다. 그래서 현재 필자의 Node.js 환경과 과거의 패키지가 충돌한 것입니다. 왜냐하면 package-lock.json에는 3년 전에 설치했던 버전의 Node.js 패키지들이 기술되어 있기 때문입니다.\n다음에 package.json 파일과 package-lock.json 파일의 일부분을 비교합니다.\n\n\npackage.json\n\n{\n    \"name\": \"course-starter-r\",\n    \"private\": true,\n    \"description\": \"Starter package to build interactive R courses\",\n    \"version\": \"0.0.1\",\n    \"author\": \"Ines Montani <ines@explosion.ai>\",\n    \"dependencies\": {\n        \"@illinois/react-use-local-storage\": \"^1.1.0\",\n        \"@jupyterlab/outputarea\": \"^0.19.1\",\n        \"@jupyterlab/rendermime\": \"^0.19.1\",\n        \"@phosphor/widgets\": \"^1.6.0\",\n        \"autoprefixer\": \"^9.4.7\",\n        \"classnames\": \"^2.2.6\",\n        \"codemirror\": \"^5.43.0\",\n        \"gatsby\": \"^2.1.4\",\n        \"gatsby-image\": \"^2.0.29\",\n        \"gatsby-plugin-manifest\": \"^2.0.17\",\n        \"gatsby-plugin-offline\": \"^2.0.23\",\n        \"gatsby-plugin-react-helmet\": \"^3.0.6\",\n        \"gatsby-plugin-react-svg\": \"^2.1.1\",\n        \"gatsby-plugin-sass\": \"^2.0.10\",\n\npackage-lock.json\n\n{\n    \"name\": \"course-starter-r\",\n    \"version\": \"0.0.1\",\n    \"lockfileVersion\": 1,\n    \"requires\": true,\n    \"dependencies\": {\n        \"@babel/code-frame\": {\n            \"version\": \"7.0.0\",\n            \"resolved\": \"https://registry.npmjs.org/@babel/code-frame/-/code-frame-7.0.0.tgz\",\n            \"integrity\": \"sha512-OfC2uemaknXr87bdLUkWog7nYuliM9Ij5HUcajsVcMCpQrcLmtxRbVFTIqmcSkSeYRBFBRxs2FiUqFJDLdiebA==\",\n            \"requires\": {\n                \"@babel/highlight\": \"^7.0.0\"\n            }\n        },\n        \"@babel/core\": {\n            \"version\": \"7.4.3\",\n            \"resolved\": \"https://registry.npmjs.org/@babel/core/-/core-7.4.3.tgz\",\n            \"integrity\": \"sha512-oDpASqKFlbspQfzAE7yaeTmdljSH2ADIvBlb0RwbStltTuWa0+7CCI1fYVINNv9saHPa1W7oaKeuNuKj+RQCvA==\",\n            \"requires\": {\n                \"@babel/code-frame\": \"^7.0.0\",\n                \"@babel/generator\": \"^7.4.0\",\n\n\n\nGatsby에서 필요한 패키지를 package.json 파일에 기술합니다. 패키지 이름과 version range를 기술합니다. 패키지 의존성을 위한 선언인 version range는 버전의 범위를 의미합니다. 특정 버전을 의미하는 것은 아닙니다.\npackage.json 파일의 일부분인 다음의 ^2.0.17이 version range입니다. ‘^’로 시작하는 버전 번호 이후의 버전을 설치하라는 의미입니다. 2.0.17 버전을 설치하라는 의미는 아닙니다. 만약 패키지 배포 리파지토리의 ’gatsby-plugin-manifest’ 버전이 2.0.17라면 2.0.17 버전을 설치하지만, 새로운 버전이 패치되어 올라온 시점에서는 2.0.18 혹은 2.1.4와 같은 버전을 설치할 수도 있습니다.1\n\n\"gatsby-plugin-manifest\": \"^2.0.17\",\n\n그런데 실제로 package-lock.json 파일에 담긴 ‘gatsby-plugin-manifest’ 버전은 다음처럼 2.0.29였습니다.\n\n        \"gatsby-plugin-manifest\": {\n            \"version\": \"2.0.29\",\n            \"resolved\": \"https://registry.npmjs.org/gatsby-plugin-manifest/-/gatsby-plugin-manifest-2.0.29.tgz\",\n            \"integrity\": \"sha512-zxUuoKxjjdEyj8Xh/svTIOhaIjONB2txel441yJXoYfHmNndysEn+DcYUMkN4xQwZLIaVKoQZ/fCNcVhUOUGIg==\",\n            \"requires\": {\n                \"@babel/runtime\": \"^7.0.0\",\n                \"sharp\": \"^0.21.3\"\n            }\n        },\n\nnpm install 명령으로 패키지를 설치하면, 설치한 시점에 패키지 배포 리파지토리의 물리적인 패키지를 포함한 의존성 트리 정보를 담아서 package-lock.json 파일을 만듭니다. 그리고 이후에 npm install 명령으로 패키지를 설치하면, package-lock.json 파일에 기술된 패키지 파일을 설치하게 됩니다.\n그러므로 package-lock.json 파일에는 3년 전에 이네스가 코스 플랫폼 개발 시점 패키지의 의존성 트리 정보를 담고 있고, 그 정보대로 패키지를 설치하면서 오류가 발생한 것입니다.\n사실 package-lock.json는 유용한 정보입니다. 개발 환경이 서로 다른 여러 사람들에세 의존성 트리는 개발자가 개발환 환경과 동일하게 패키지를 설치해 주기 때문에 배포한 프로그램이 정상적으로 동작하게 됩니다. 그러나 사례처럼 3년 이 지난 시점에는 이러한 유용한 기능이 오히려 문제가 된 것입니다. 오픈소스 진영에서 3년이라는 세월은 일반 생활에서 10년보다 더 긴 세월이기 때문입니다. 강산이 몇 번 변했을 시간이 흐른 것입니다.\n결단을 내려야 합니다. package-lock.json 파일을 삭제합니다. 그러면 Node.js 패키지 설치 관리자인 npm은 package.json 파일의 version range을 참고하여 패키지를 설치하게 됩니다.\npackage-lock.json 파일을 삭제 후 다음 명령을 재실행 합니다.\n\nnpm install\n\n다음처럼 설치가 완료됩니다.\n\n\n\n무려 2,790개의 패키지가 설치되었습니다. Node.js 패키지는 프로젝트 리렉토리 내의 node_modules 디렉토리에 생성됩니다.\n그런데 security audit에 몇개의 항목이 검출되었습니다.\n보안성 검사에서 도출된 문제의 항목들은,\n\nnpm audit fix\n\n혹은\n\nnpm audit fix --force\n\n명령어로 복구할 수 있다고 하였지만, 해당 명령어를 수행해 본 결과 모든 문제를 해결하지 못했고 반복적으로 몇번 수행하면 한두개씩 줄다가 더이상 줄지 않았습니다. 문제는 코스 플랫폼이 구동 시 에러가 발생하여 어플리케이션이 구동되지 않았습니다.\n일단은, security audit 결과는 무시합니다.\n어플리케이션 실행\n이제 코스 플랫폼의 코스 컨텐츠를 개발할 수 있는 환경이 구성되었습니다.\n개발 서버의 구동\n콘솔에서 다음 명령을 수행하면, 코스 템플리트 앱이 구동됩니다.\n\nnpm run dev  \n\n실행되는 과정에서 다음처럼 에러 메시지 같은 것이 출력됩니다.\n\n\n\n그러나 다음처럼 개발 앱의 build는 성공합니다.\n\n\n\n5개의 경고가 발생하였고 2개의 경고는 수정할 수 있다는 메시지가 있습니다만, 무시합니다.\n숙제가 남은 실행입니다. 경고 메시지를 수정하고 싶은 욕구는 충만하나 Gatsby 개발자가 아니라, 일단은 무시하고 진행합니다.\n코스 템플리트 앱의 실행\n로컬 환경에 개발 서버가 구동되면 8000번 포트로 앱을 실행할 수 있습니다. 웹 브라우저에 다음 URL을 입력하여 템플리트 코스를 실행합니다.\nlocalhost:8000 \n그러면 다음과 같은 템플리트 코스를 만날 수 있습니다.\n\n\n\n요약\n작업 요약\n이네스의 코스 템플리트를 실행할 수 있도록 Node.js 패키지를 설치했습니다.\n개발 서버를 구동해서 코스 템플리트를 실행했습니다.\nI can do it\nGatsby 어플리케이션을 실행할 수 있습니다.\n이네스 템플리트로 사용자 개발 환경을 만들 수 있습니다.\n\nhttps://hyunjun19.github.io/2018/03/23/package-lock-why-need/ 페이지 참고↩︎\n",
    "preview": "posts/2022-03-25-run-course/img/profile_ines.jpeg",
    "last_modified": "2022-03-24T08:05:30+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-24-dev-infra/",
    "title": "코스 개발환경 구축하기",
    "description": "이네스 코스 개발 환경을 구축합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-24",
    "categories": [
      "R Markdown",
      "Reproducible Research",
      "Development"
    ],
    "contents": "\n\nContents\n코스 템플리트 설치하기\nOnline course starter: R 리파지토리 가져오기\n\nRStudio 프로젝트 생성하기\n개발 환경 구축하기\nMS-Windows 운영체제\nmacOs 운영체제\nLinux 운영체제\n\n요약\n작업 요약\nI can do it\n\n\n\n\n\n\n\n\n들어가기\n온라인 코스 50% 한번에 만들기\n\n시작이 반이라는 말이 있죠?!. 이네스로 온라인 R 코스를 만드는 작업을 시작했으니 반을 만든 것이라 보겠습니다.\n\n코스 개발 환경을 만들테니까요.\n\n\n코스 템플리트 설치하기\n이네스는 코스 플랫폼을 만든 이네스는 R 강사를 위해서, “Online course starter: R”을 배포하고 있습니다. 일종의 이네스 코스 템플리트인 셈입니다.\nOnline course starter: R 리파지토리 가져오기\nGithub의 https://github.com/ines/course-starter-r 리파지토리는 이네스의 코스 플랫폼을 R코스로 사용할 수 있는 템플리트의 리파지토리입니다.\n이 URL을 방문하면 다음 그림처럼 “Use this template” 버튼이 있습니다.\n\n\n\n\n이 버튼을 누르면 탬플리트를 가져다가, Github 사용자의 새로운 리파지토리를 생성하는 영역의 웹 페이지로 이동합니다. dlookr 패키지의 사용 방법을 강의하는 코스를 만들려고 합니다. 그래서 그림처럼 리파지토리의 이름을 “course-starter-dlookr”로 만들었습니다.\n\n\n\n\n\nRStudio 프로젝트 생성하기\n온라인 코스는 RStudio 환경에서 개발합니다. CLI 환경에서도 가능하지만, 생산성을 위해서 RStudio에서 작업하는 것이 좋습니다.\n사용자 Github 리파지토리에 “course-starter-dlookr”가 만들어졌습니다. 다음 그림처럼 Git URL을 복사합니다. RStudio에서 프로젝트를 만들기 위함입니다.\n\n\n\n\n\nRStudio에서 프로젝트를 생성합니다. 프로젝트 생성을 선택하면 다음과 같이 프로젝트 생성 위자드가 나타납니다.\n\n\n\n\n\n위자드에서 “Version Control”에서 “Git”을 선택합니다.\n\n\n\n\n위자드에서 “Clone Git Repository”의 “Repository URL”에 앞에서 복사한 Git URL을 입력한 후 프로젝트를 만들 경로를 지정하고 프로젝트를 생성합니다.\n\n\n\n\n\n개발 환경 구축하기\nNode.js, Gatsby 등 JavaScript 라이브러리를 설치합니다. 이 글에서는 간단한 기본 설치 방법을 제시하니, 설치 시 문제가 발생하면 검색을 통해서 자세한 설치 방법과 트러블슈팅을 소개하는 페이지를 참고하시기 바랍니다.\nMS-Windows 운영체제\nNode.js 설치\nhttps://nodejs.org/en/ 홈페이지를 방문하여 설치 파일을 다운로드 한 후 설치합니다.\n이 글을 작성할 시점의 버전은 다음과 같습니다. 원하는 버전을 다운로드하여 설치하시기 바랍니다.\n\n\n\n\n\nGatsby 설치\n터미널에서 Node.js 패키지 관리자인 npm을 이용해서 설치합니다.\n\nnpm install -g gatsby-cli\n\nmacOs 운영체제\nNode.js 설치\n터미널에서 brew로 설치합니다. 그래서 먼저 Homebrew를 설치합니다. 설치되어 있으면 건너뛰세요.\n\n/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n\nNode.js를 설치하고 설치된 버전을 확인합니다. npm도 함께 설치되는데, 이것은 Node.js 패키지를 설치하고 관리하는 유틸리티입니다.\n\nbrew install node\n\nnode -v\nnpm - v\n\nGatsby 설치\nNode.js 패키지 관리자인 npm을 이용해서 설치합니다.\n\nnpm install -g gatsby-cli\n\nmacOs 운영체제에서는 brew로도 설치가 가능합니다.\n\nbrew install gatsby-cli\n\nLinux 운영체제\n우분투 리눅스에 한정해서 설명합니다.\nNode.js 설치\n터미널에서 apt-get로 설치합니다. 주의할 것은 macOs에서 Node.js는 node로 통용되었는데 리눅스에서는 풀 이름인 “nodejs”를 사용한다는 점과 npm도 별도로 설치한다는 점입니다.\n\nsudo apt-get update\nsudo apt-get install nodejs\nsudo apt-get install npm\n\n버전을 확인해 봅니다.\n\n\nnodejs -v\nnpm - v\n\n\n\nGatsby 설치\nNode.js 패키지 관리자인 npm을 이용해서 설치합니다.\n\nnpm install -g gatsby-cli\n\n요약\n작업 요약\n이네스 코스 템플리트를 가져다 Github 리파지토리에 코스 개발용 리파지토리를 만들었습니다.\nGithub 리파지토리와 연동한 RStudio 프로젝트를 만들었습니다.\nNode.js와 Gatsby JavaScript 라이브러리를 설치했습니다.\nI can do it\n이네스 코스를 개발할 수 있는 환경을 구축할 수 있습니다.\n앞으로는 이네스 코스 템플리트를 사용하지 않고, RStudio 프로젝트를 이용해서 새로운 코스 템플리트를 만들 수 있습니다.\n\n\n\n",
    "preview": "posts/2022-03-24-dev-infra/img/Node.js_logo.png",
    "last_modified": "2022-03-23T23:34:12+09:00",
    "input_file": {},
    "preview_width": 1200,
    "preview_height": 734
  },
  {
    "path": "posts/2022-03-23-understand-ines/",
    "title": "이네스 코스 플랫폼 이해",
    "description": "이네스 코스 플랫폼의 구성과 매커니즘을 이해합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-23",
    "categories": [
      "R Markdown",
      "Reproducible Research"
    ],
    "contents": "\n\nContents\n이네스 코스 플랫폼 이해\n이네스 코스 플랫폼의 명명\n이네스 코스 플랫폼 UI/UX\n이네스 코스 플랫폼 적용 코스\n\n이네스 코스 플랫폼 구조\n이네스 코스 플랫폼의 구조\n서버와 클라이언트의 상호 작용\nJuniper\n\nGatsby\nmybinder.org\n코스 컨텐츠 작성\nchapter\nslides\nexercises\n\n결언\n이네스 플랫폼 상세 소개\n\n\n\n\n\n\n\n들어가기\n오픈소스 생태계는 살아 있습니다. 어쩌면 나눔 시장과 같습니다.\n\n필요한 것은 어느 곳에 있기 마련입니다. 때로는 그것을 찾기 어려울 수 있지만요.\n\n이네스가 만든 코스 플랫폼도 그 중 하나가 아닐까요? 어쩌면 그렇게 DataCamp와 유사하게 구현해 놓았을까요.\n\n여러분은 이네스가 펼쳐 놓은 자리에서 코스 플랫폼을 무료로 득템할 수 있는 기회를 얻으셨습니다.\n\n\n이네스 코스 플랫폼 이해\n이네스 코스 플랫폼의 명명\n이네스 몬타니는 인공지능과 자연어 처리 기술을 연구하는 소프트웨어 개발자입니다. 그리고 Explosion의 공동 설립자이자 CEO입니다.\n이네스 코스 플랫폼은 데이터캠프와 결별한 코스 컨텐츠 프로바이더들 중의 이네스 몬타니(Ines Montani)가 개발한 코스 플랫폼입니다. 플랫폼의 고유 명칭이 없기 때문에 편의상 이네스 코스 플랫폼이라 부르겠습니다.\n이네스 코스 플랫폼 UI/UX\n이네스 코스 플랫폼의 UI/UX는 DataCamp와 유사합니다. DataCamp에서 구동되는 코스 플랫폼을 포팅한 것이기 때문에 마치 DataCamp 클론처럼 유사한 기능으로 구성되어 있습니다.\n개인적으로는 learnr의 UI/UX보다 좀더 친숙해서, 상대적으로 끌리는 플랫폼입니다.\n이네스 코스 플랫폼 적용 코스\nR Bootcamp\nhttps://r-bootcamp.netlify.app/\nTed Laderas and Jessica Minnier\n\nSupervised Machine Learning: Case Studies in R\nhttps://supervised-ml-course.netlify.app/\nJulia Silge\n\nGeneralized Additive Models in R\nhttps://noamross.github.io/gams-in-r-course/\nNoam Ross\n\n이네스 코스 플랫폼 구조\n이네스 코스 플랫폼의 구조\n다음 삽화는 이네스가 제시한 플랫폼의 구조1입니다.\n이네스 코스 플랫폼의 구조서버와 클라이언트의 상호 작용\n웹 어플리케이션에서의 UI/UX를 담당하는 프론트 엔드 부분의 클라이언트는 Gastby JavaScript를 사용합니다. 코스를 위한 강의 슬라이드와 연습문제, 퀴즈 등의 처리는 Gastby가 담당합니다. 엄밀히 말하면 슬라이드는 reveal.js가 담당합니다.\n백 엔드의 서버 영역은 Binder를 사용합니다. Binder는 연습문제인 R 스크립트를 실행한 후 결과를 프론트 엔드인 Gastby에 전달해 줍니다.\nJuniper\n이네스는 Jupyter 커널을 사용하여 Binder 컨테이너와의 통신을 처리하기 위해 Juniper라고 하는 Gatsby용 JavaScript 플러그인을 개발했습니다.\nJuniper는 R 스크립트를 Binder의 도커 컨테이너에 보내 스크립트를 실행하고, 실행 결과의 출력과 시각화 이미지를 수신하는 작업을 수행합니다.\nGatsby\n정적 사이트(static site)는 HTML, CSS, JavaScript로만 만들어진 사이트를 의미합니다. R/데이터 분석 등과 같은 기술 문서 기반의 블로그들이 정적 사이트 생성기를 통해서 만들어 지고 있습니다. 대표적인 것이 Jekyll(지킬)과 blogdown 패키지가 채용하고 있는 HUGO(휴고)가 대표적입니다. 그리고 근래에는 Gatsby(개츠비)가 인기를 끌고 있습니다.\n대중성은 Jekyll -> HUGO -> Gatsby의 순서로 무게추가 이동하고 있습니다. 이들 정적 사이트 개발기는 Markdown 파일을 웹 페이지로 변환하여 웹 사이트를 개발합니다. R의 blogdown 패키지는 R Markdown 문서를 정적 웹 페이지로 만들어 주기 때문에 R 사용자가 쉽게 블로그와 같은 웹 사이트를 개발할 수 있습니다. distill 패키지도 마찬가지입니다.\n\n\n\n\n\n\n\n\n\n\n\n“Gatsby는 최근 대세로 자리잡은 React(리액트) 기반으로 웹 사이트를 만드는 프레임워크입니다. 정적 웹 사이트나 블로그를 아주 쉽고 빠르게 개발할 수 있어 때문에 블로그 개발에 자주 활용됩니다. Gatsby는 Markdown 문서를 정적 웹 페이지를 만들어주고, 배포할 때 웹 페이지로 빌드되기 때문에 별도의 웹 어플리케이션 서버(WAS)가 없이도 운영이 가능합니다.”\n\n\n\nGatsby의 스키마는 다음과 같습니다. Gatsby는 GraphQL 이라는 기술을 이용헤서 데이터 소스에서 데이터를 가져다가 정적 사이트를 만들어 줍니다. 마크다운 문서도 데이터 소스 중의 하나입니다.\n\n\n\n\n\n코스 수강자에게 보여지는 코스 컨텐츠와 UI/UX는 Gatsby로 만들어 진 것입니다.\nmybinder.org\nBinder(mybinder.org)는 Github 저장소(Repository) URL만 입력하면 저장되어 있는 소스 코드와 프로젝트 설정으로 Jupyter Notebook을 실행시켜 주는 서비스를 제공합니다. 그러므로 프로젝트의 재현성(Reproducible)을 위한 유용한 도구이자, 무료 도구입니다.\n\n\n\n\n\n\n\n\n\n\n\n“Jupyter Notebook은 코드, 시각화 및 텍스트가 포함된 문서를 만들고 공유하기 위한 웹 애플리케이션입니다. 주로 데이터 과학, 통계 모델링, 기계 학습 등에서 사용되고 있습니다. Interactive한 웹 기반의 IDE로, Python 코드를 넣으면 원격 서버에서 명령이 실행되어 결과가 보여집니다. 주요한 것은 Python 뿐만 아니라, R 등 다수의 오픈소스 컴퓨터 언어를 지원한다는 점입니다.”\n\n\n\nGithub도 Github Pages 기능이 있어, *.github.io와 같은 URL로 정적 웹 페이지를 공유할 수 있어 유용하게 사용하고 있습니다.\nBinder에서 Github 리파지토리를 읽어서 R 실행이 가능한 도커 컨테이너를 만들어 제공하는 것은, 여러 사람들이 동일한 환경에서 Github 리파지토리의 R 프로그램을 재현할 수 있어, 이 기능 역시 매력적입니다. 여러 수강생을 위한 코스 실행 환경에 적합합니다.\n\n\n\n\n\n\n\n\n\n\n\n“mybinder.org가 제공하는 컨테이너는 R을 실행할 수 있는 환경은 물론, RStudio와 Shiny도 지원합니다. 이것은 RStudio Server처럼 웹 환경에서 RStudio를 실행할 수 있고, Shiny 앱도 띄워준다는 것을 의미합니다. 그러므로 shinyapps.io를 대체할 수도 있습니다. 이것은 learnr 패키지로 만든 코스를 shinyapps.io가 아닌 mybinder.org에도 배포가 가능하다는 것을 의미합니다.”\n\n\n\n코스 수강자가 Gatsby로 구현한 정적 웹 페이지에서 작성한 R Script는 Binder의 도커 컨테이너에서 실행됩니다. 이네스가 개발한 Juniper Javascript가 이 두 시스템을 연결시켜주는 기능을 수행합니다.\n코스 컨텐츠 작성\n앞서 Gatsby는 GraphQL를 이용해서 데이터 소스인 Markdown 파일을 가져다 정적 페이지를 작성한다고 언급했습니다. 그러면 이네스 플랫폼에서는 어떻게 학습 컨텐츠로 만들어지는지 살펴봅니다.\nchapter\nchapter 디렉토리의 Markdown 파일은 강의 챕터(chapter)를 만들어 줍니다.\n\n\n챕터 Markdown 소스\n\n---\ntitle: 'Chapter 1: Getting started'\ndescription:\n  'This chapter will teach you about many cool things and introduce you to the\n  most important concepts of the course.'\nprev: null\nnext: /chapter2\ntype: chapter\nid: 1\n---\n\n<exercise id=\"1\" title=\"Introduction\" type=\"slides\">\n\n<slides source=\"chapter1_01_introduction\">\n<\/slides>\n\n<\/exercise>\n\n<exercise id=\"2\" title=\"Getting Started\">\n\nLet''s ask some questions about the slides. Whats the correct answer?\n\n<choice>\n<opt text=\"Answer one\">\n\nThis is not the correct answer.\n\n<\/opt>\n\n<opt text=\"Answer two\" correct=\"true\">\n\nGood job!\n\n<\/opt>\n\n<opt text=\"Answer three\">\n\nThis is not correct either.\n\n<\/opt>\n<\/choice>\n\n<\/exercise>\n\n<exercise id=\"3\" title=\"First steps\">\n\nThis is a code exercise. The content can be formatted in simple Markdown – so\nyou can have **bold text**, `code` or [links](https://spacy.io) or lists, like\nthe one for the instructions below.\n\n- These are instructions and they can have bullet points.\n- The code block below will look for the files `exc_01_03`, `solution_01_03` and\n  `test_01_03` in `/exercises`.\n\n<codeblock id=\"01_03\">\n\nThis is a hint.\n\n<\/codeblock>\n\n<\/exercise>\n\n랜더링된 챕터 컨텐츠\n랜더링된 화면의 일부입니다. Gatsby는 Netlify(넷리파이)와 같은 정적 웹 서비스 호스팅 서버로 배포(Deploy)될 때, 다음과 같은 웹 페이지를 생성합니다.\n챕터 학습목록 화면마크다운 파일에 기술한 퀴즈도 생성해줍니다.\n코스 컨텐츠 중 퀴즈 화면\nYAML 헤더\nMarkdown의 YAML 헤더는 다음과 같습니다.\ntitle: 챕터 타이틀\ndescription: 챕터 개요\nprev: 이전 챕터\nnext: 이후 챕터\ntype: 컨텐츠 타입\n챕터일 경우에는 chapter\n\nid: 컨텐츠 아이디\n주요 Tags\n<exercise> : 챕터 내 학습 내역 구간 정의\n<slides> : 슬라이트 Lecture 정의\n<choice> : 선택형 퀴즈 정의\n<codeblock> : 핸즈온 연습문제 정의\n\nslides\nslides 디렉토리의 Markdown 파일은 강의 슬라이드(slides) 노트를 만들어 줍니다.\n\n\n슬라이드 노트 Markdown 소스\n---\ntype: slides\n---\n\n# Introduction\n\nNotes: Text at the end of a slide prefixed like this will be displayed as\nspeaker notes on the side. Slides can be separated with a divider: ---.\n\n---\n\n# This is a slide\n\n```r\n# Print something\nprint(\"Hello world\", quote = FALSE)\n```\n\n```out\nHello world\n```\n\n- Slides can have code, bullet points, tables and pretty much all other Markdown\n  elements.\n- This is another bullet point.\n\n<img src=\"profile.jpg\" alt=\"This image is in /static\" width=\"25%\">\n\nNotes: Some more notes go here\n\n---\n\n# Let's practice!\n\nNotes: Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nullam tristique\nlibero at est congue, sed vestibulum tortor laoreet. Aenean egestas massa non\ncommodo consequat. Curabitur faucibus, sapien vitae euismod imperdiet, arcu erat\nsemper urna, in accumsan sapien dui ac mi. Pellentesque felis lorem, semper nec\nvelit nec, consectetur placerat enim.\n\n랜더링된 슬라이드 노트 컨텐츠\n랜더링된 슬라이드 노트 화면의 일부입니다. 코스를 숙강하는 수강자들이 데이터분석 이론이나 기술을 학습하는 영역입니다. 스크립트는 강사의 좀 더 구체적인 학습 내용의 설명 등을 기술할 수 있습니다. 프리젠테이션 문서의 발표 스크립트 정도로 이해하시면 됩니다.\n슬라이드 노트 화면슬라이드 노트의 마지막 페이지 예제입니다.\n슬라이드 노트 화면\nYAML 헤더\nMarkdown의 YAML 헤더는 다음과 같습니다.\ntype: “slides”로 기술하는 컨텐츠 타입\n주요 Tags\nMarkdown 태그를 지원하며, 다음의 추가 태그를 제공합니다.\n--- : 슬라이드 노트 페이지 구분자\nNotes: : 학습 내용을 설명하는 스크립트 정의\n```out : 출력을 정의하는 코드 청크\n\nexercises\nexercises 디렉토리의 R 파일은 강의 연습문제(exercises)를 만들어 줍니다.\n연습문제인 exercises 디렉토리에는 하나의 연습문제를 위한 3개의 R 스크립트 파일이 위치합니다.\n챕터(chapter)의 Markdown 문서 중에서 codeblock 태그에 대응하는 파일은 파일 이름의 접두어로 그 기능이 구별됩니다. 그리고 codeblock 태그의 id 속성에 기술한 텍스트가 포함된 R 스크립트 파일을 만들어야 됩니다.\n연습문제 아이디가 ’first_exec’라 가정하면 codeblock 태그는 다음과 같이 정의합니다.\n<codeblock id=\"first_exec\">\n<\/codeblock>\n연습문제 파일\n확장자가 ’exc_’인 파일\nexc_first_exec.R\n\n연습문제 해답 파일\n확장자가 ’solution_’인 파일\nsolution_first_exec.R\n\n연습문제 정답 검증 파일\n확장자가 ’test_’인 파일\ntest_first_exec.R\n\n이네스 플랫폼 템플리트의 챕터 파일에 포함된 연습문제 예제의 일부인 다음 태그는, ’01_03’라는 아이디의 R 연습문제 파일들을 구동합니다.\n<codeblock id=\"01_03\">\n\nThis is a hint.\n\n<\/codeblock>\n\n\n연습문제\nexc_01_03.R\n\nlibrary(ggplot2)\n\nmtcars$gear <- factor(mtcars$gear,levels=c(3,4,5),\n    labels=c(\"3gears\",\"4gears\",\"5gears\"))\nmtcars$am <- factor(mtcars$am,levels=c(0,1),\n    labels=c(\"Automatic\",\"Manual\"))\nmtcars$cyl <- factor(mtcars$cyl,levels=c(4,6,8),\n   labels=c(\"4cyl\",\"6cyl\",\"8cyl\"))\n\n# Print the gear variable of mtcars\nprint(____)\n\n# Assign the length of mtcars to some_var\nsome_var <- ____\n\n연습문제 정답\nsolution_01_03.R\n\n\nlibrary(ggplot2)\n\nmtcars$gear <- factor(mtcars$gear,levels=c(3,4,5),\n    labels=c(\"3gears\",\"4gears\",\"5gears\"))\nmtcars$am <- factor(mtcars$am,levels=c(0,1),\n    labels=c(\"Automatic\",\"Manual\"))\nmtcars$cyl <- factor(mtcars$cyl,levels=c(4,6,8),\n   labels=c(\"4cyl\",\"6cyl\",\"8cyl\"))\n\n# Print the gear variable of mtcars\nprint(mtcars$gear)\n\n# Assign the length of mtcars to some_var\nsome_var <- length(mtcars)\n\n\n\n연습문제 검증\ntest_01_03.R\n\n\ntest <- function() {\n    # Here we can either check objects created in the solution code, or the\n    # string value of the solution, available as .solution. See the testTemplate\n    # in the meta.json for details.\n    if (some_var != length(mtcars)) {\n        stop(\"Are you getting the correct length?\")\n    }\n    if (!grepl(\"print(mtcars$gear)\", .solution, fixed = TRUE)) {\n        stop(\"Are you printing the correct variable?\")\n    }\n\n    # This function is defined in the testTemplate\n    success(\"Well done!\")\n}\n\n\n\n랜더링된 연습문제\n랜더링된 연습문제 화면의 일부입니다. 연습문제는 Binder와 연결되어 실행됩니다.\n다음 연습문제 화면은 여러 버튼을 통해서 연습문제의 수행을 진행합니다.\n연습문제 화면‘Show solution’ 버튼으로 정답을 채운 뒤, ‘Run Code’ 버튼으로 연습문제를 실행하면 R 스크립트를 실행합니다. 처음 연습문제를 실행하면 mybinder.org의 도커 컨테이너를 연결하는 메시지가 출력되면서 R을 실행할 수 있는 환경을 기다린 후 실행합니다. 제법 시간이 소요됩니다.\n연습문제의 실행연습문제를 실행한 결과입니다. 여기서 ‘Submit’ 버튼을 누르면 INPUT 창의 스크립트의 정답 여부를 test_01_03.R 파일의 스크립트가 검증하여, 결과를 출력합니다.\n연습문제의 실행 결과\n결언\n이네스 플랫폼에 대해서 개괄적으로 살펴보았습니다.\n데이터 분석을 위한, R을 위한 여러 학습서나 강의 교재, 온라인 문서가 넘칩니다. 그러나 학습의 효과를 고려한다면, 오프라인 강좌를 제외하면 온라인 대화형 학습의 성취도가 높다고 생각합니다.\n이네스 플랫폼은 온라인 대화형 학습을 지원하는 플랫폼으로, 공개형 코스 생태계를 만들어 나갈 유용한 자원이라 생각합니다. 이 플랫폼을 이용해서 유용한 온라인 코스 컨텐츠가 많이 만들어지는 것을 소망합니다.\n이네스 플랫폼 상세 소개\n이네스 플랫폼에서 코스 컨텐츠를 만들고, 배포하는 방법의 소개인 “이네스 코스 개발하기”를 참고하여, 유용한 온라인 대화형 코스를 만들어 보시기 바랍니다.\n\nhttps://user-images.githubusercontent.com/3315629/60834090-b49d5980-a174-11e9-9d69-966084ba97b9.png↩︎\n",
    "preview": "posts/2022-03-23-understand-ines/img/ines.jpeg",
    "last_modified": "2022-03-23T07:39:42+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-22-understand-learnr/",
    "title": "learnr 코스 플랫폼 이해",
    "description": "learnr 코스 플랫폼의 구성과 매커니즘을 이해합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-22",
    "categories": [
      "R Markdown",
      "Reproducible Research"
    ],
    "contents": "\n\nContents\nlearnr 코스 플랫폼 이해\nlearnr 패키지\nlearnr 코스의 특징과 장점\n\n코스 컨텐츠 작성\nR Markdown 문서 생성\nYAML 헤더\n청크 옵션\n\n결언\n유용한 learnr 코스 소개\nmybinder.org에서의 배포\n\n\n\n\n\n\n\n들어가기\n또 RStudio라구요?!\n\nRStudio의 데이터 과학자, 소프트웨어 개발자들은 R 생태계를 위한 여러 유용한 패키지를 만들었습니다.\n\n교육에도 관심이 많은 RStudio가 R 온라인 학습을 위한 코스 플랫폼인 leanr 패키지를 만든 것은 우연이 아닙니다.\n\n어떤 방법으로 코스를 진행할 수 있는지, 그들의 코스 솔루션을 살펴보자구요.\n\n엇, 동양의 형설지공과는 차원이 차른 서양의 스케일!!! 반딧불이 아닌, 아구 촉수의 빛으로 책을 읽네요.\n\n\nlearnr 코스 플랫폼 이해\nlearnr 코스 플랫폼이라 거창하게 명명했지만, 사실은 learnr 패키지로 만든 대화형 튜토리얼을 만들어서 배포하는 환경을 의미합니다.\nlearnr 패키지\nlearnr는 R Markdown을 사용하여 대화형 튜토리얼(자습서)를 만드는 R 패키지입니다. 내러티브, 이미지, 비디오, 퀴즈 및 연습문제를 조합하여 R과 R 패키지 학습을 위한 자기 주도형 튜토리얼을 만들 수 있습니다.\nR Markdown을 이용해서 대화형 코스를 만들 수 있다는 것은, 온라인 코스를 만드는 과정이 이네스 플랫폼보다 쉽다는 장점입니다. 이네스 플랫폼은 직접 Markdown 문서로 컨텐츠를 만들어야합니다. 이네스 플랫폼은 경우에 따라서 R Markdown을 지원하지 않는 아쉬운 지점이 생기게 되기 때문입니다.\nlearnr 코스의 특징과 장점\nR Markdown 기반의 저작\n앞서 언급한 것처럼 R Markdown을 이용해서 대화형 코스를 만들 수 있다는 것은 장점 중의 하나입니다.\nshiny 앱\nlearnr 패키지로 만든 코스(learnr은 대화형 튜토리얼라 칭하지만 편의상 대화형 코스로 이야기하겠습니다.)는 shiny에서 구동됩니다. 그러므로 여러 수강자가 학습하는 환경을 위해서 배포하려면, shiny 서버에 배포해야 합니다.\n일반적으로 shiny 앱은 shinyapps.io에 배포합니다.\nshinyapps.io 서비스는 다음 가격 정책처럼 5개의 앱 안에서는 무료로 사용할 수 있습니다. 그리고 이 무료 정책은 한달에 25 액티브 시간의 제한이 있습니다. 그러므로 다수의 접속과 학습이 필요한 경우에는 유료 서비스를 사용할 수 있다는 것을 의미합니다.\nshinyapps.io의 가격 정책업무상 shiny 앱을 개발한 적이 있어, https://shiny.rstudio.com/gallery/에서 제공하는 Show cases, Demos, Widgets을 참고한 적이 있습니다. 이들 컨텐츠들은 shinyapps.io에서 구동되는데 로딩 시간이 지루하고 일정 시간 후에 세션이 끊기는 문제로, 얻는 정보와 반대급부로 불편한 환경의 경험이 있어서 shinyapps.io가 썩 끌리지는 않습니다.\n학습 진도의 기억\nlearnr 코스의 장점은 수강자가 학습한 진도를 시스템 내부에서 자동으로 보존하므로, 나중에 다시 학습을 위해서 코스로 돌아오면 중단한 부분부터 다시 학습할 수 있다는 점입니다. 이네스의 플랫폼에서 제공하지 않는 유용한 기능입니다.\n코스 컨텐츠 작성\nlearnr 코스를 만드는 방법을, learnr 패키지가 제공하는 템플리토로 간단하게 소개합니다.\nR Markdown 문서 생성\nRStudio에서 File > New File > R Markdown… 메뉴를 선택하면 다음과 같은 다이얼로그 창이 나타납니다. 여기서 “From Template”를 선택 후, Template에서 learnr의 “Interactive Tutorial”을 선택 후 문서 이름을 정합니다.\n\n\n\n\n\n선택 후 “OK” 버튼을 누르면 다음과 같은 템플리트가 만들어집니다.\nInteractive Tutorial 템플리트YAML 헤더\nlearnr 코스의 YAML 헤더는 다음과 같습니다.\n\n---\ntitle: \"Tutorial\"\noutput: learnr::tutorial\nruntime: shiny_prerendered\n---\n\noutput은 learnr::tutorial이며, runtime은 shiny_prerendered을 지정합니다.\n청크 옵션\nlearnr 코스는 learnr 패키지를 사용하므로, R Markdown 문서의 도입부에서 learnr 패키지를 불러들여야 합니다.\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\n```\nexercise 옵션\nexercise 옵션은 연습문제를 위한 R Script 작성을 지원합니다.\n\n\n옵션 설명\nexercise 옵션 예시\nexercise = TRUE : exercise를 수행할 수 있는 UI/UX를 만들어 줍니다.\nexercise.lines = 5 : 5줄의 스크립트를 입력할 수 있는 영역을 확보하여 UI를 만듭니다.\n템플리트에서의 예제\n## Topic 1\n\n### Exercise \n\n*Here's a simple exercise with an empty code chunk provided for entering the answer.*\n\nWrite the R code required to add two plus two:\n\n```{r two-plus-two, exercise=TRUE}\n\n```\n\n### Exercise with Code\n\n*Here's an exercise with some prepopulated code as well as `exercise.lines = 5` \nto provide a bit more initial room to work.*\n\nNow write a function that adds any two numbers and then call it:\n\n```{r add-function, exercise=TRUE, exercise.lines = 5}\nadd <- function() {\n  \n}\n```\n실행 결과\n\n\n\n\n\nhint 옵션\nhint 옵션은 exercise 옵션과 함께 사용하여 연습문제에 힌트를 제공합니다. ‘Hint’ 버튼을 누르면 힌트 다이얼로그가 출력됩니다. ‘Copy to Clipboard’ 버튼을 누르면 힌트를 클립보드로 복사하여 사용할 수 있습니다.\n\n\n옵션 설명\nexercise 옵션 예시\nexercise.eval = TRUE :\nexercise.eval 옵션의 기본값은 FALSE로, TRUE이면 exercise 청크의 R 스크립트를 실행합니다.\nFALSE이면, 청크의 코드는 실행되지 않고 연습문제 UI에만 출력됩니다.\n\n청크이름-hint : 앞에서 사용한 ‘청크이름’ 뒤에 ’-hint’를 붙여 청크를 만드면, 힌트 청크가 됩니다.\n청크이름 ‘print-limit-hint’는 앞에서 정의한 ’print-limit’ 청크의 힌트 청크를 의미합니다.\n\n템플리트에서의 예제\n## Topic 2\n\n### Exercise with Hint\n\n*Here's an exercise where the chunk is pre-evaluated via the `exercise.eval` \noption (so the user can see the default output we'd like them to customize). \nWe also add a \"hint\" to the correct solution via the chunk immediate below \nlabeled `print-limit-hint`.*\n\nModify the following code to limit the number of rows printed to 5:\n\n```{r print-limit, exercise=TRUE, exercise.eval=TRUE}\nmtcars\n```\n\n```{r print-limit-hint}\nhead(mtcars)\n```\n\n실행 결과\n\n\n\n\nquiz 함수\nquiz는 청크 옵션이 아니라 함수입니다. quiz() 함수는 퀴즈를 만들어 줍니다. 그리고, question() 함수, answer() 함수와 함께 동작합니다.\n\n\n함수 설명\nexercise 옵션 예시\nquiz() :\n퀴즈임을 알려 줍니다.\n\nquestion() :\n퀴즈의 질문을 정의하는 함수입니다.\n\nanswer() :\n퀴즈의 답안을 정의하는 함수입다.\ncorrect = TRUE를 지정하는 답안이 정답을 의미합니다.\ncorrect = TRUE를 하나의 답안에 지정하면, 레디오 박스로\ncorrect = TRUE를 복수로 지정하면, 체크박스로 표현됩니다. (중복 선택)\n\n\n템플리트에서의 예제\n### Quiz\n\n*You can include any number of single or multiple choice questions as a quiz. \nUse the `question` function to define a question and the `quiz` function for \ngrouping multiple questions together.*\n\nSome questions to verify that you understand the purposes of various base and \nrecommended R packages:\n\n```{r quiz}\nquiz(\n  question(\"Which package contains functions for installing other R packages?\",\n    answer(\"base\"),\n    answer(\"tools\"),\n    answer(\"utils\", correct = TRUE),\n    answer(\"codetools\")\n  ),\n  question(\"Which of the R packages listed below are used to create plots?\",\n    answer(\"lattice\", correct = TRUE),\n    answer(\"tools\"),\n    answer(\"stats\"),\n    answer(\"grid\", correct = TRUE)\n  )\n)\n```\n\n실행 결과\n\n\n\n\n결언\nlearnr 플랫폼에 대해서 개괄적으로 살펴보았습니다.\nlearnr은 R Markdown으로 간단하게 코스를 생성하기에 좋은 플랫폼입니다. 이네스 코스 플랫폼을 소개하는 이 웹 페이지에서 참고로 유사 솔루션인 learnr을 다루었지만, 이 글을 읽고 많은 R 사용자가 learnr 코스를 개발하고 공유할 수 있는 기회가 되었으면 합니다.\n유용한 learnr 코스 소개\nRStudio Primers(https://rstudio.cloud/learn/primers)에는 learnr 플랫폼으로 만든 여러 유용한 코스가 많습니다. RStudio Cloud 환경에서 동작하는 코스들입니다. 이 사이트에서 유용한 코스를 학습하시고, learnr 플랫폼의 Look&Feal과 동작을 이해해 보시기 바랍니다.\nmybinder.org에서의 배포\nlearnr 코스 shiny 앱을 shinyapps.io에 배포하지 않고, https://mybinder.org/에 배포할 수도 있습니다. 일장 일단이 있지만, shinyapps.io의 대체재가 될 수 있으니 참고하시기 바랍니다.\n\n\n\n",
    "preview": "posts/2022-03-22-understand-learnr/img/learnr_logo.png",
    "last_modified": "2022-03-21T21:49:13+09:00",
    "input_file": {},
    "preview_width": 240,
    "preview_height": 275
  },
  {
    "path": "posts/2022-03-21-introduce-online-course/",
    "title": "온라인 대화형 코스 플랫폼",
    "description": "온라인 데이터 과학 학습 플랫폼인 데이터캠프(DataCamp)는 R과 Python의 학습 커리큘럼이 풍부합니다. 그러나 2017년 CEO의 성추행 사건을 계기로, 몇몇 코스 컨텐츠 프로바이더들이 DataCamp와 결별하고 이네스 몬타니(Ines Montani)의 코스 플랫폼을 만들어 무료로 배포하고 있습니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-21",
    "categories": [
      "R Markdown",
      "Reproducible Research"
    ],
    "contents": "\n\nContents\n온라인 대화용 코스 플랫폼\n대화형 학습\n코스 플랫폼 종류\n\nDataCamp(데이터캠프)\n코스 구성\n동영상 강의\n핸즈온 학습\n\nlearnr\nR 마크다운 기반의 컨텐츠\n퀴즈\n핸즈온 학습\n\n이네스 몬타니의 코스 플랫폼\n컨텐츠 목록\n강의 컨텐츠\n퀴즈\n핸즈온 학습\n\n컨텐츠 소개\n\n\n\n\n\n\n들어가기\n이 글은 \n'온라인 대화형 코스 플랫폼 소개'\n를 위해서 작성한 홈페이지 내용을 블로그로 가져온 것입니다.\n\n며칠에 걸쳐서 관련 내용을 차례로 블로그에 연재합니다.\n\n\n온라인 대화용 코스 플랫폼\n온라인 대화용 코스 플랫폼(interactive course platform)의 장점은 사용자가 R 환경을 구축하지 않아도 R 학습을 수행할 수 있는 장점이 있습니다. 사용자들의 운영체제의 종류와 버전이 다르고, R 과 패키지들의 버전이 달라도 문제가 되지 않습니다. 이것은 중요한 시사점입니다. R 온라인 코스 플랫폼은 R 초심자가 어렵게 R을 설치하고 패키지를 설치하지 않아도 학습할 수 있는 일관된 환경을 제공합니다.\n대화형 학습\n강사 없이 학습해야 하기 때문에 온라인 코스 플랫폼은 대화형(interactive) 학습을 지향합니다. 자기 주도형으로 강의를 수강하고, 실습을 위해 직접 R 스크립트를 입력하고 결과를 확인합니다. 힌트와 정답 제공은 핸즈온(hands-on) 학습의 성취도를 높이기 위한 필수 도구입니다. 스스로 학습할 수 있는 가이드를 제공합니다.\n코스 플랫폼 종류\nR 코스 플랫폼에는 데이터캠프, learnr, 이네스 몬타니의 코스 플랫폼이 있습니다. 그리고 또다른 플랫폼이 만들어질 수도 있습니다.\nDataCamp(데이터캠프)\nDataCamp(데이터캠프)는 온라인 데이터 과학 교육을 비즈니스로 하는 스타트업입니다. 일부 무료 코스가 있으나 다수의 유로 온라인 학습 컨텐츠를 보유하고 있습니다.\nR 코스 플랫폼은 데이터캠프(DataCamp)가 선두 주자입니다. 개인적으로는 뒤늦게 CEO의 성추행 사건을 접했지만, 데이터캠프가 데이터 과학자와 R/Python 분석가 양성에 기여했음은 인정해야할 것 같습니다.\n코스 구성\n다음은 R의 객체지향형 언어에 대한 코스의 구성 중 일부입니다. 제목 앞의 아이콘을 통해서 동영상 강의와 실습이 포함됨을 알 수 있습니다.\n데이터캠프 컨텐츠 구성 예동영상 강의\n데이터캠프의 강의 자료는 강사의 동영상을 컨텐츠로 제공됩니다. 오프라인 강의가 아니기 때문에 반복 학습, 되돌리기, 건너뛰기가 가능합니다.\n데이터캠프 동영상 강의핸즈온 학습\n핸즈온 학습은 R코드를 작성하고 실행하고, 제출하여 평가를 받습니다. 힌트를 제공해서 정답 스크립트를 작성할 수 있도록 도와줍니다.\n데이터캠프 핸즈온learnr\nshiny 환경에서 구동되는 learnr은 R 마크다운 기반으로 대화형 튜토리얼을 만들수 있는 R 패키지입니다. learnr을 이용하면 쉽게 대화용 코스를 만들 수 있습니다.\nlearnr은 데이터캠프처럼 동영상 컨텐츠를 포함할 수 있습니다. 그리고 퀴즈 및 Exercise를 지원합니다.\nR 마크다운 기반의 컨텐츠\nR 마크다운 문서로 강의 컨텐츠를 작성할 수 있습니다.\nlearnr 강의 컨텐츠퀴즈\nlearnr은 간단한 퀴즈를 낼수 있는 기능을 제공합니다. 학습 내용의 숙지를 간단하게 체크할 수 있는 유용한 기능입니다.\nlearnr 퀴즈핸즈온 학습\n핸즈온 학습은 R코드를 작성하고 실행합니다. 힌트를 제공할수도 있으며, 정답을 제시하기도 합니다.\nlearnr 핸즈온이네스 몬타니의 코스 플랫폼\n데이터캠프와 결별한 코스 컨텐츠 프로바이더들 중의 이네스 몬타니(Ines Montani)가 개발한 코스 플랫폼입니다. (이하 이네스 플랫폼)\n데이터 캠프에 제공하던 코스 컨텐츠를 구현하기 때문에 기능은 데이터캠프와 동일합니다.\n대표적인 코스 컨텐츠에 RBootcamp가 있습니다.\n컨텐츠 목록\n다음은 Rbootcamp 컨텐츠 목록의 일부입니다.\n이네스 플랫폼 코스 컨텐츠 목록 예시강의 컨텐츠\n다음은 Rbootcamp 강의 컨텐츠의 일부입니다.\n이네스 플랫폼 강의 컨텐츠퀴즈\n이네스 플랫폼도 퀴즈를 낼수 있는 기능을 제공합니다. 학습 내용의 숙지를 간단하게 체크할 수 있는 유용한 기능입니다. learnr과 유사합니다.\n이네스 플랫폼 퀴즈핸즈온 학습\n핸즈온 학습은 R코드를 작성하고 실행합니다. 힌트를 제공할수도 있으며, 정답을 제시하기도 합니다.\n이네스 플랫폼 핸즈온컨텐츠 소개\n본 컨텐츠는 R 강사들이 대화형 온라인 R 코스를 개발하는 것을 도와줄 가이드로 다음과 같이 구성되었습니다.\n컨텐츠는 이네스 코스 플랫폼의 소개 컨텐츠입니다. learnr 코스 플랫폼은 R 기반의 코스 플랫폼을 소개하는 목적으로 개괄적인 소개만 다룹니다.\nlearnr 코스 플랫폼 이해\nlearnr 코스 플랫폼에 대한 개괄적인 소개\n\n이네스 코스 플랫폼 이해\n이네스 코스 플랫폼에 대한 개괄적인 소개\n\n이네스 코스 개발하기\n코스 개발환경 구축하기\n코스를 개발하기 위한 사용자 개발 환경 구축 방법 소개\n\n코스 템플리트 실행하기\n코스를 개발하는 과정에서, 컨텐츠 작성 및 실행하는 방법 소개\n\n코스 커스트마이징하기\n코스를 커스트마이징하여, 강사만의 Look & Feal을 만들 수 있는 방법 소개\n\n코스 서비스 배포하기\n개발된 코스 컨텐츠를 배포하여, 수강자가 학습할 수 있는 환경을 구축하는 방법 소개\n\nResources\nR 코스 플랫폼 관련 리소스\n\n\n",
    "preview": "posts/2022-03-21-introduce-online-course/img/rbootcamp.png",
    "last_modified": "2022-03-21T21:40:42+09:00",
    "input_file": {},
    "preview_width": 2226,
    "preview_height": 1300
  },
  {
    "path": "posts/2022-03-19-run-batch/",
    "title": "배치 처리하기",
    "description": "R 스크립트를 배치 작업으로 수행해 봅니다. 이 방법은 정기적으로 데이터를 수집할 때 유용한 기법입니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-19",
    "categories": [
      "Development",
      "Data acquisition",
      "programming"
    ],
    "contents": "\n\nContents\n배치 처리의 이해\n배치 처리의 정의\n배치 처리의 활용\n\n배치 프로그램 작성\n대상 지역 데이터 생성\n배치 프로그램 생성\n\n배치 프로그램 실행\nRStudio 이용한 실행\nShell Script를 이용한 실행\n\n배치작업 스케줄링하기\n스케줄 조회하기\n스케줄 정의하기\ncrontab 스케줄 정의 방법\n\ncron 스케줄 Trouble Shooting\nMac에서의 Trouble Shooting\n작업경로 Trouble Shooting\n한글환경 Trouble Shooting\ntidyverse 한글환경 Trouble Shooting\n\n\n\n\n\n\n\n들어가기\n주기적으로 자주 실행되는 코드가 있습니다.\n한번이 아니라 여러번 실행될 작업\n입니다.\n\n귀찮을 수 있는 성가신 작업을, 안전하고 쉽게 수행할 방법은 없을까요? 여기 그 해답을 알려 줍니다.\n\n\n배치 처리의 이해\n배치 처리의 정의\n다음은 위키백과에 실린 배치 처리의 정의입니다.\n“일괄 처리(batch processing)란 최종 사용자의 개입 없이 또는 (자원이 허가한다면) 실행을 스케줄링할 수 있는 작업(job)의 실행을 의미합니다.1 컴퓨터 프로그램 흐름에 따라 순차적으로 자료를 처리하는 방식입니다.”\n여기서 중요한 의미는 다음과 같습니다.\n프로그램 흐름에 따라 순차적으로 자료를 처리\n사용자의 개입 없이 스케줄링할 수 있는 작업\n배치 처리의 활용\n아파트 실거래 상세 정보를 수집하기 위해서는 여러 번 API를 호출해야 합니다. 이 API는 2006년 1월부터의 데이터를 제공합니다. 그러므로 2006년 1월 이후의 모든 데이터를 수집하기 위해서는 여러 번이 아니라, 여러 날의 작업이 필요합니다. 왜냐하면 API 호출이 하루에 1,000회로 제한되어 있기 때문입니다.\n여기서는 국토교통부의 아파트매매 실거래 상세 자료를 배치 작업으로 수집하는 방법을 소개합니다.\n배치 프로그램 작성\n대상 지역 데이터 생성\nSQLite DBMS에 실거래 상세 자료를 수집할 대상 지역 정보를 저장합니다. koscrap 패키지의 행정구역 코드 정보 데이터인 legal_divisions을 이용합니다.\n\n\n\n배치 프로그램 생성\n02_import_trade_apt.R 파일에 실거래 상세 자료를 수집하는 메인 프로그램을 다음과 같이 만듭니다.\n\n\n\n배치 프로그램 실행\nRStudio 이용한 실행\nRStudio의 “Tools > Jobs > Start Local Job…” 메뉴를 선택합니다.\nJobs 실행 메뉴다음과 같은 메뉴 다이얼로그에서 “R Script”와 “Working Directory”를 선택한 다음, “Start” 버튼을 누르면 해당 스크립트인 02_import_trade_apt.R 파일이 실행됩니다.\n실행 대상의 선택프로그램이 실행되는 과정에서는 우측 상단처럼 프로그레스 바에서 작업의 진행상태를 확인할 수 있습니다.\nJobs 실행 상태프로그램의 실행이 종료되면 다음처럼 프로그레스 바 위치에 작업이 끝났음을 알려주는 정보가 표시됩니다.\nJobs 실행 종류Shell Script를 이용한 실행\nLinux나 Mac의 콘솔에서도 배치 작업을 실행할 수 있습니다.\n먼저 다음과 같은 “excute_trade_apt.sh”이라는 파일의 Shell Script를 작성합니다. 이 스크립트는 R 스크립트가 아닌 쉘 스크립트입니다.\n이 스크립트를 실행하기 앞서서 로그를 쌓을 디렉토리를 만들어야 합니다. “./collect_data”가 작업 경로로 가정한다면, “./collect_data/log” 디렉토리를 미리 생성해 놓습니다.\n\n\n\n그리고 콘솔에서 해당 파일에 실행 권한을 부여합니다.\n\n\n\n콘솔에서 스크립트를 실행해 봅니다. 다음은 필자의 Mac 콘솔에서 실행한 결과입니다. 패키지가 로딩될 때, 메시지가 콘솔에 출력되었습다.\n\n\n\n이 메시지는 작업에 영향을 주지 않습니다만, 보기 싫다면 다음처럼 배치파일을 수정해서 메시지의 출력을 없애줄 수도 있습니다. library() 함수에 warn.conflicts 인수를 사용해서 메시지 출력을 제거합니다.\n\n\n\n작업 실행 중에 생성된 로그를 확인해 봅니다.\n\n\n\n배치작업 스케줄링하기\n배치작업의 스케줄링에서 가장 일반적인 것은 Unix-like 시스템의 cron일 것입이다. 여기서는 cron에 대해서 알아보려 합니다.\n스케줄 조회하기\ncron 스케줄링은 cron table에 정의합니다. cron table에 정의된 스케줄 목록은 Unix-like 시스템의 터미널에서 crontab -l 명령어로 조회합니다. 필자는 MacOS에서 수행합니다. MacOS의 터미널도 Unix-like 시스템의 일종이므로 cron을 사용할 수 있습니다.\n아직 정의된 cron 스케줄링이 없기 때문에 스케줄이 없다는 메시지가 출력되었습니다.\n\n\n\n스케줄 정의하기\n이제 스케줄을 정의해 보겠습니다. crontab -e 명령어로 스케줄을 수정할 수 있습니다. 아직 정의된 스케줄이 없기 때문에 아무 내용도 없는 에디터(vi 에디터) 화면이 나타납니다. 에디터에 스케줄을 정의한 후 저장합니다.\n\n\n\n이번에는 cron table에 정의된 스케줄 목록이 출력됩니다. 방금 정의했던 스케줄입니다. 앞서 만들었던 아파트 실거래 상세정보의 쉘 스크립트를 수행하는 스케줄입니다. 매일 새벽 1:00에 excute_trade_apt.sh을 수행하라는 정의입니다.\n\n\n\ncrontab 스케줄 정의 방법\ncrontab의 작업 스케줄은 화이트스페이스로 분리된 6개의 컬럼으로 정의하합니다. 6개의 컬럼에 대한 의미는 다음과 같습니다.\n\n\n\n분부터 요일까지의 5개 컬럼은 숫자로 정의하며 다음과 같은 범위에서 정의할 수 있습니다.\n분: 0 ~ 59\n시간: 0 ~ 23\n일: 1 ~ 31\n월: 1 ~ 12\n요일: 0 ~ 7\n0과 7은 일요일, 1부터 월요일이며 6은 토요일\n\n아스터리스크(*)는 해당하는 모든 주기를 의미합니다. 분 위치의 아스터리스크(*)는 매분을 의미합니다.\n그러므로 다음 정의는 매월 첫째날, 8시부터 9시 전까지 매분동안 /home/usr1/run.sh을 실행합니다.\n\n\n\n다음 정의는 매일 매시간 0분과 30분에, 즉 30분 주기로 /home/usr1/run.sh을 실행합니다.\n\n\n\n다음 정의는 자정과 정오에 0분부터 30분까지 매분 /home/usr1/run.sh을 실행합니다.\n\n\n\ncron 스케줄 Trouble Shooting\ncron 스케줄의 수행환경은 별도의 가상환경을 만들어 수행하는 것과 흡사합니다. 그래서 익숙하지 않으면 여러 에러가 발생할 수 있습니다. 여기서는 이런 예기치 못한 에러를 해결하는 방법을 다룹니다.\nMac에서의 Trouble Shooting\n기대했던 cron 스케줄이 정상적으로 실행된 것 같지 않습니다. 혹시나 해서 mail 박스를 열어봅니다. 터미널에서 mail 명령어로 가능합니다.\n일반적으로 cron 스케줄 작업에 대한 로그는 메일로 받을 수 있습니다. 다음과 같은 메일이 와 있었습니다.\n\n\n\nOperation not permitted 메시지에 주목해야 합니다. 구글링해보니, MacOS의 보안시스템에 관련된 이슈였습니다. 원인은 cron 프로세스가 MacOS 파일시스템의 전체 디렉토리의 접근 권한이 없기 때문입니다.\n다음과 같은 과정으로 문제를 해결합니다.\n먼저 시스템 환경설정에서 보안 및 개인 정보 보호 메뉴를 선택합니다.\n보안 및 개인 정보 보호 메뉴실행보안 및 개인 정보 보호 메뉴에서 전체 디스크 접근 권한을 선택한 후, 자물쇠를 해제하고 “+” 버튼을 누릅니다.\n전체 디스크 접근 권한 설정화면“⌘” + “⇧” + “G” 버튼을 동시에 누른 후, 나타난 메뉴에 /usr/sbin을 입력 후 이동 버튼을 누릅니다.\n/usr/sbin 폴더 이동/usr/sbin 경로의 파일에서 cron을 선택합니다.\ncron 파일 선택이제 전체 디스크 접근 권한에서 cron이 선택된 것을 확인할 수 있습니다.\ncron 파일 선택작업경로 Trouble Shooting\ncron에 등록할 프로그램에서 파일의 경로는 상대경로가 아닌, 절대경로를 사용해야 합니다.\nexcute_trade_apt.sh에서는 here 패키지를 이용해서 SQLite DBMS의 경로를 상대경로로 지정했습니다. 그래서 cron에서 다음과 같은 에러가 발생했습니다. 상대경로로 정의했던 DBMS 위치라서, DBMS에 접속하지 못했습니다.\n\n\n\n그래서 DBMS에 접속하는 로직을 다음과 같이 수정했습니다. 주석으로 처리한 부분이 기존의 상대경로 정의입니다. 이 부분을 절대경로 위치로 바꿨습니다.\n\n\n\n한글환경 Trouble Shooting\n절대경로로 변경하여, 해당 이슈는 해결되었습니다. 그런데 이번에는 한글과 관련된 에러가 발생했습니다.\n\n\n\ncron에서 한글 이슈가 발생하면, shell script에 다음처럼 export LANG=ko_KR.UTF-8구문을 삽입합니다.\n\n\n\ntidyverse 한글환경 Trouble Shooting\n한글 문제가 해결되었습니다. 그런데 이번에는 한글변수에서 에러가 발생했습니다. 애초 프로그램 언어에서의 사이드 이펙트(부작용, side effect)를 방지하기 위해서의 최선의 방법은 변수의 이름을 영문으로 정의하는 것이 암묵적인 룰인데, 국토교통부의 오픈 API는 변수 이름을 한글로 지정한 것이 안일한 결정이었습니다.\nR 환경이난 Shell 환경에는 분명 정상적으로 수행되던 코드가 cron에서 에러가 발생합니다. tidyverse 패키지군인 dplyr 패키지에서 한글변수를 찾지 못하는 에러가 발생한 것입니다.\n\n\n\n여러 방법을 시도해보았고, 구글링해보았지만 솔루션을 찾지 못했습니다. 그래서 다음처럼 아주 원초적인 방법으로 접근했습니다. 데이터 프레임을 만들고, 변수의 이름을 영문으로 고친 후 dplyr 구문을 사용하는 것으로 코드를 수정하였습니다.\nkoscape 패키지의 trade_apt() 함수 내에서의 get_list() 함수를 다음과 같이 수정하였습니다.\n\n이전 코드\n\n\n  get_list <- function(doc) {\n    doc %>%\n      XML::getNodeSet(\"//item\") %>%\n      XML::xmlToDataFrame() %>%\n      mutate(거래금액 = as.integer(stringr::str_remove(거래금액, \",\"))) %>%\n      mutate(DEAL_DATE =\n               glue::glue(\"{년}-{str_pad(월, width = 2, pad = '0')}-{str_pad(일, width = 2, pad = '0')}\")) %>%\n      mutate(층 = as.integer(층)) %>%\n      mutate(건축년도 = as.integer(건축년도)) %>%\n      select(-년, -월, -일) %>%\n      select(\"LAWD_CD\"       = 지역코드,\n             DEAL_DATE,\n             \"SERIAL\"        = 일련번호,\n             \"DEAL_TYPE\"     = 거래유형,\n             \"BUILD_NM\"      = 아파트,\n             \"FLOOR\"         = 층,\n             \"BUILD_YEAR\"    = 건축년도,\n             \"AREA\"          = 전용면적,\n             \"AMOUNT\"        = 거래금액,\n             \"ROAD_CD\"       = 도로명코드,\n             \"ROAD_NM\"       = 도로명,\n             \"BUILD_MAJOR\"   = 도로명건물본번호코드,\n             \"BUILD_MINOR\"   = 도로명건물부번호코드,\n             \"ROAD_SEQ\"      = 도로명일련번호코드,\n             \"BASEMENT_FLAG\" = 도로명지상지하코드,\n             \"LAND_NO\"       = 지번,\n             \"DONG_NM\"       = 법정동,\n             \"DONG_MAJOR\"    = 법정동본번코드,\n             \"DONG_MINOR\"    = 법정동부번코드,\n             \"EUBMYNDONG_CD\" = 법정동읍면동코드,\n             \"DONG_LAND_NO\"  = 법정동지번코드,\n             \"DEALER_ADDR\"   = 중개사소재지,\n             \"CANCEL_DEAL\"   = 해제여부,\n             \"CANCEL_DATE\"   = 해제사유발생일)\n  }\n\n\n\n수정 코드\n\n\n  get_list <- function(doc) {\n    dframe <- doc %>%\n      XML::getNodeSet(\"//item\") %>%\n      XML::xmlToDataFrame()\n\n    if (NROW(dframe) == 0) {\n      return(data.frame())\n    }\n\n    vname <- c(\"AMOUNT\", \"DEAL_TYPE\", \"BUILD_YEAR\", \"YEAR\", \"ROAD_NM\",\n               \"BUILD_MAJOR\", \"BUILD_MINOR\", \"ROAD_ADMI\", \"ROAD_SEQ\",\n               \"BASEMENT_FLAG\", \"ROAD_CD\", \"DONG_NM\", \"DONG_MAJOR\",\n               \"DONG_MINOR\", \"DONG_ADMI\", \"EUBMYNDONG_CD\", \"DONG_LAND_NO\",\n               \"BUILD_NM\", \"MONTH\", \"DAY\", \"SERIAL\", \"AREA\", \"DEALER_ADDR\",\n               \"LAND_NO\", \"LAWD_CD\", \"FLOOR\", \"CANCEL_DATE\", \"CANCEL_DEAL\")\n    names(dframe) <- vname\n\n    dframe %>%\n      select(\"LAWD_CD\", \"YEAR\", \"MONTH\", \"DAY\", \"SERIAL\", \"DEAL_TYPE\",\n             \"BUILD_NM\", \"FLOOR\", \"BUILD_YEAR\", \"AREA\", \"AMOUNT\", \"ROAD_CD\",\n             \"ROAD_NM\", \"BUILD_MAJOR\", \"BUILD_MINOR\", \"ROAD_SEQ\",\n             \"BASEMENT_FLAG\", \"LAND_NO\", \"DONG_NM\", \"DONG_MAJOR\", \"DONG_MINOR\",\n             \"EUBMYNDONG_CD\", \"DONG_LAND_NO\", \"DEALER_ADDR\", \"CANCEL_DEAL\",\n             \"CANCEL_DATE\") %>%\n      mutate(DEAL_DATE = \n               glue::glue(\"{YEAR}-{str_pad(MONTH, width = 2, pad = '0')}-{str_pad(DAY, width = 2, pad = '0')}\")) %>%\n      mutate(AMOUNT = as.integer(stringr::str_remove(AMOUNT, \",\"))) %>%\n      mutate(FLOOR = as.integer(FLOOR)) %>%\n      mutate(BUILD_YEAR = as.integer(BUILD_YEAR)) %>%\n      select(LAWD_CD, DEAL_DATE, SERIAL:CANCEL_DATE)\n  }\n\n\n\n\n\nhttps://ko.wikipedia.org/wiki/일괄_처리 발췌↩︎\n",
    "preview": "posts/2022-03-19-run-batch/img/batch_job.jpeg",
    "last_modified": "2022-03-19T09:51:25+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-18-custom-image/",
    "title": "docker 이미지 만들기",
    "description": "Shiny 애플리케이션에 필요로 하는 R 패키지를 추가로 설치하여 새로운 docker 이미지를 생성합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-18",
    "categories": [
      "docker",
      "Development",
      "Shiny"
    ],
    "contents": "\n\nContents\nDockerfile 만들기\n명령어의 이해\nBitStat를 운영하기 위한 Dockerfile 파일\n베이스 이미지 가져오기\n개발 라이브러리 설치하기\nR 패키지 설치하기\n포트지정\n사용자 전환\n\ndocker 이미지 생성\n크롬을 위한 설정\ndocker 이미지 생성\n\n컨테이너 생성 및 실행\n기존 컨테이너와 이미지 삭제\n컨테이너 생성 및 실행\n\nBitStat 애플리케이션 실행\n결언\n\n\n\n\n삽화출처: Implementing Embedded Continuous Integration with Jenkins and Docker: Part1\n\n\n들어가기\ndocker의 꽃은 Dockerfile입니다.\n\nDockerfile로 BisStat를 비롯한 몇개의 Shiny 애플리케이션을 실행할 수 있는 환경을 커스트마이징합니다.\n\ndocker 꽃이 활짝 피겠군요.\n\n\n\nDockerfile 만들기\n이제 완전하게 Shiny 애플리케이션이 운용될 docker 컨테이너를 만들고 실행할 차례입니다.\nMakefile이 C/C++로 개발된 소스를 컴파일하고 배포하는 make의 룰을 정의한 파일이라면, Dockerfile은 docker 이미지를 생성하는 방법을 정의한 파일입니다. Dockerfile을 이용하면, 컨테이너 내부에 유틸리티, 개발 라이브러리나 R 패키지를 설치할 수 있습니다. 즉 rocker/shiny-verse 이미지에서 지원하지 못했던, 애플리케이션을 실행시키는데 필요한 패키지를 설치할 수 있습니다.\n명령어의 이해\nDockerfile에 사용하는 대표적인 명령어는 다음과 같습니다.\nFROM\n베이스(base) 이미지를 가져옵니다.\n\nRUN\n설치된 베이스 이미지 파일시스템의 운영체제에서 쉘 스크립트(shell script)를 실행합니다.\n\nCOPY\n호스트에 있는 디렉토리나 파일을 docker 이미지 파일 시스템으로 복사합니다.\n\nEXPOSE\n컨테이너로 들어오는 트래픽을 리스닝할 프로토콜과 포트를 설정합니다.\n\nUSER\n명령어를 수행할 사용자 계정을 지정합니다.\nUSER 명령어 이후부터 적용되며, 이미 생성된 계정이어야 합니다.\n\nBitStat를 운영하기 위한 Dockerfile 파일\n다음과 같은 내용의 Dockerfile 파일을 작성합니다. 의외로 설치하는 R 패키지가 많은 이유는, BitStat뿐만 아니라 개인적으로 만든 여러 개의 Shiny 애플리케이션을 이 환경에서 구동하기 위함입니다.\n\nFROM rocker/shiny:latest\n\n# system libraries of general use\nRUN apt-get update && apt-get install -y \\\n    libxml2-dev \\\n    libcairo2-dev \\\n    libsqlite3-dev \\\n    openjdk-11-jdk \\\n    liblzma-dev \\\n    libbz2-dev \\\n    libssl-dev \\\n    curl\n\nRUN curl -L http://bit.ly/google-chrome-stable -o google-chrome-stable.deb && \\\n    apt-get -y install ./google-chrome-stable.deb && \\\n    rm google-chrome-stable.deb\n\n# install R packages required\nRUN R -e \"install.packages('rJava',              repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('dlookr',             repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('shinyjs',            repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('shinydashboard',     repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('shinydashboardPlus', repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('shinyWidgets',       repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('shinybusy',          repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('shinythemes',        repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('shinycssloaders',    repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('colourpicker',       repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('htmltools',          repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('htmltools',          repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('reactable',          repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('glue',               repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('xlsx',               repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('flextable',          repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('googleVis',          repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('readr',              repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('forcats',            repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('plotly',             repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('DBI',                repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('waffle',             repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('RSQLite',            repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('remotes',            repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('dbplyr',             repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('ggthemes',           repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('treemapify',         repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('sparkline',          repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('formattable',        repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('DT',                 repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"install.packages('tidyverse',          repos = 'http://cran.rstudio.com/')\"\nRUN R -e \"remotes::install_github('dreamRs/shinytreeview')\"\n\nCOPY google-chrome /usr/local/bin/\n\nUSER shiny  \nCOPY Renviron /.Renviron\n\n베이스 이미지 가져오기\n다음 FROM 명령어는 docker hub에서 ‘rocker/shiny’ 이미지의 최신 버전(lasett)을 가져오라는 의미입니디. 이 이미지를 베이스 이미지로 사용할 것입니다. 이미 앞에서 사용했던 이미지입니다.\n\nFROM rocker/shiny:latest\n\n개발 라이브러리 설치하기\n다음 RUN 명령어는 컨테이너 안의 파일시스템에서 Shell 명령어를 수행하라는 의미입니다.\napt-get(Advanced Packaging Tool)은 우분투(Ubuntu)를 포함한 데이안(Debian) 계열의 리눅스 배포본에서의 시스템 패키지 관리 유틸리티입니다. 이 유틸리티로 개발 라이브러리나 시스템 유틸리티를 설치합니다.\n다수의 R 패키지는 시스템 유틸리티나 동적 라이브러리를 인터페이스하여 구현합니다. 예를 들면 RSQLite라는 R 패키지는 sqlite3의 동적 라이브러리를 사용합니다. 그래서 R 환경에서 sqlite를 사용하는 RSQLite 패키지는 개발 라이브러이인 libsqlite3-dev을 설치해야 합니다.\nR 패키지에서 설치하기 까다로운 rJava 패키지는 JVM(Java Virtual Machine)을 인터페이스하기 위해서 JDK를 설치해야 합니다. 이를 위해서 JDK 11 버전인 openjdk-11-jdk를 설치합니다.\n그리고 인터넷을 통해 공유하는 파일을 가져오기 위해서는 curl이 필요할지도 모릅니다.\n이를 위해서 ‘apt-get update && apt-get install -y’ 명령어 구문으로 개발 라이브러리와 시스템 유틸리티를 설치합니다.\n\n# system libraries of general use\nRUN apt-get update && apt-get install -y \\\n    libxml2-dev \\\n    libcairo2-dev \\\n    libsqlite3-dev \\\n    openjdk-11-jdk \\\n    liblzma-dev \\\n    libbz2-dev \\\n    libssl-dev \\\n    curl\n\n\n\n\n\n크롬(Chorme) 설치\n\nBitStat는 pdf 보고서를 생성하기 위해서 pagedown 패키지를 사용합니다. 그런데 이 패키지는 크롬(Chrome)을 이용해서 웹 페이지를 pdf 파일로 변환합니다.\n\n그래서 컨테이너에 리눅스용 크롬 브라우저를 설치해야 합니다.\n\n\n\n다음처럼 curl을 이용해서 크롬을 다운로드한 후 설치하고 삭제합니다. 이를 위해서 앞에서 apt-get로 curl을 설치한 것입니다.\n\nRUN curl -L http://bit.ly/google-chrome-stable -o google-chrome-stable.deb && \\\n    apt-get -y install ./google-chrome-stable.deb && \\\n    rm google-chrome-stable.deb\n\nR 패키지 설치하기\n다음 RUN 명령어는 R 유틸리티로 R 패키지를 설치합니다. -e 옵션은 R이 수행할 R 스크립트를 지정하는 옵션입니다. 따옴표 안의 R 스크립트로 패키지를 설치하게 됩니다.\n\nRUN R -e \"install.packages('rJava', repos = 'http://cran.rstudio.com/')\"\n\n포트지정\n컨테이너가 사용할 포트를 지정하기 위해서 EXPOSE 명령어를 사용하지 않아도 됩니다. 베이스 이미지로 ’rocker/shiny’를 사용했기 때문입니다. 이 베이스 이미지에 이미 EXPOSE 명령어로 3838 포트를 컨테이너가 사용할 수 있도록 설정되어 있기 때문입니다.\n사용자 전환\nroot 권한으로 컨테이너를 실행하면, 무제한의 권한이 부여되므로, 보안 측면에서 취약할 수 있습니다. 그래서 다음처럼 일반 사용자인 shiny 사용자 계정으로 컨테이너를 실행하도록 설정합니다.\n이지 베이스 이미지인 rocker/shiny-verse에서 shiny라는 그룹과 사용자를 만들어 놓았기 때문에 해당 사용자 계정을 만들 필요는 없습니다.\n\nUSER shiny\n\ndocker 이미지 생성\n크롬을 위한 설정\npagedown 패키지가 크롬을 사용한다는 것을 앞에서 언급했습니다. 그런데 pagedown::chrome_print()가 크롬을 호출할 때, 다음처럼 “Error in is_remote_protocol_ok: Cannot find headless Chrome after 20 attempts”라는 에러가 발생합니다.\n\noutput file: diagnosis_paged_temp.knit.md\n\n\nOutput created: diagnosis_paged_temp.html\nWarning: Error in is_remote_protocol_ok: Cannot find headless Chrome after 20 attempts\n  1: runApp\n\nExecution halted\n\n크롬은 ‘샌드박스(sandbox)’라는 보안 개념을 적용합니다. 크롬은 브라우저에서 여러 개의 독립된 탭을 띄우고, 별도의 웹 페이지가 실행됩니다. 별도로 독립된 프로세스가 가동하는 것으로 이들 프로세스는 샌드박스처럼 격리되어서 서로 관여하지 못합니다. 즉, 어느 탭의 웹 페이지에서 악성코드가 침투하거나, 버그 또는 장애로 해당 탭의 페이지가 먹통되어도 다른 탭의 웹 페이지는 정상적으로 동작합니다.\n앞에서의 에러는 크롬의 샌드박스 기능에 기인합니다. 이 에러는 해결하기 위해서는 컨테이너에서 pagedown::chrome_print()가 크롬을 호출할 때, 샌드박스의 기능을 비활성해야 합니다.\n이를 위한 몇 가지 솔루션이 있습니다.\n\npagedown::chrome_print() 함수 호출 수정\nextra_args 인수값에 c(“–no-sandbox”)를 기술하여 호출\ndlookr 패키지를 수정해야 함\n크롬 실행 시 옵션 정의\n–no-sandbox 옵션 추가\n\n여기서는 2번 솔루션을 적용합니다.\n다음과 같은 스크립트를 담은 google-chrome 파일을 ./shiny-docker 경로에 생성합니다.\n\n#!/bin/bash\n\n/usr/bin/google-chrome --no-sandbox $*\n\n그리고 호스트에서 이 파일의 권한을 설정합니다. shiny 계정으로 컨테이너를 실행하기 때문에 755 권한을 부여해야 합니다.\n\nchmod 755 google-chrome\n\n다음으로 다음과 같은 스크립트를 담은 Renviron 파일을 ./shiny-docker 경로에 생성합니다.\n\n\nPATH=\"/:${PATH}\"\n\n\n\n그리고 Dockerfile 파일에서 이들을 다음처럼 컨테이너에 복사합니다.\n\nCOPY google-chrome /usr/local/bin/\nCOPY Renviron /.Renviron\n\ndocker 이미지 생성\n앞에서 만든 Dockerfile는 ./shiny-docker 경로에 저장했습니다. 이 Dockerfile을 빌드해서 docker 이미지를 생성합니다.\nbuild 명령어로 이미지를 생성합니다. -t 옵션은 생성할 이미지의 이름을 지정합니다. 이미지 이름을 ’shiny’로 지정했습니다.\n\ndocker build -t shiny:first ./shiny-docker\n\n컨테이너 생성 및 실행\n기존 컨테이너와 이미지 삭제\n이제 앞에서 만들었던 ‘rocker/shiny-verse’ 이미지와 ‘shiny-server’ 컨테이너는 필요가 없어졌습니다. 그래서 다음처럼 컨테이너와 이미지를 차례대로 삭제합니다.\n\ndocker rm -f shiny-server\ndocker rmi rocker/shiny-verse\n\n컨테이너 생성 및 실행\n이번에는 Docker Desktop를 이용해서 컨테이너를 생성하고 실행합니다. 다음은 Optional Settings 다이얼로그 창에서의 환경설정 내용입니다.\nContainer Name: shiny-server\nPorts:\nLocal Host: 3838\nContainer Port: default\n\nBitStat 애플리케이션 Path\nHost Path: BitStat 애플리케이션이 있는 호스트 경로\nContainer Path: /srv/shiny-server\n\nBitStat 애플리케이션 Path\nHost Path: Shiny 서버 로그를 저장할 호스트 경로\nContainer Path: /var/log/shiny-server\n\nOptional Settings 다이얼로그 창다음 명령도로 동일한 기능을 수행합니다. –name 옵션은 컨테이너의 이름을 지정합니다. -p, -v는 각각 포트와 볼륨을 정의합니다.\n\ndocker run -d -p 3838:3838 --name shiny-server \\\n    -v /Users/choonghyunryu/shiny-server/:/srv/shiny-server/ \\\n    -v /Users/choonghyunryu/Documents/99_logs/shiny/:/var/log/shiny-server/ \\\n    shiny:first\n\n컨테이너를 실행한 후 콘솔로 들어가서 프로세스를 확인하면, 다음처럼 shiny 계정으로 shiny 서버 프로세스가 실행되었음을 확인할 수 있습니다.\n프로세스 현황BitStat 애플리케이션 실행\n이제는 http://localhost:3838/BitStat/ URL로 BitStat 애플리케이션을 실행할 수 있습니다.\n실행된 BitStat 애플리케이션 화면은 다음과 같습니다.\nBiStat 애플리케이션결언\n이제 docker 환경에서 BitStat 애플리케이션을 실행할 수 있습니다. 추가로 개발하는 Shiny 애플리케이션은 앞에서 정의한 로컬 호스트의 /Users/choonghyunryu/shiny-server/ 디렉토리에 배포하면 됩니다.\n어느 정도 목적하는 환경의 Shiny 서버를 구동하려면, 결국은 docker 이미지를 만들어야할 수 밖에 없습니다. 개인적으로 Shiny 애플리케이션의 데모 환경을 만들려는 목적으로 시도한 작업입니다만, docker의 매력을 느끼기에 충분했습니다.\nR과 Shiny를 사용하기에는 MS-Windows 환경이 부족함이 없지 않습니다. 여러 사람들과 협업을 하기 위해서는 동일한 환경에서의 작업이 중요합니다. 이런 대안으로 docker 환경에서 R과 Shiny를 사용하는 것이 유용한 대안이 될 것 같습니다.\n\n\n\n",
    "preview": "posts/2022-03-18-custom-image/img/Dockerfile.png",
    "last_modified": "2022-03-17T21:12:14+09:00",
    "input_file": {},
    "preview_width": 1040,
    "preview_height": 255
  },
  {
    "path": "posts/2022-03-17-custom-docker/",
    "title": "docker 컨테이너 환경 설정하기",
    "description": "docker 기반으로 Shiny 서버를 운영하기 위한 컨테이너 환경설정을 수행합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-17",
    "categories": [
      "docker",
      "Development",
      "Shiny"
    ],
    "contents": "\n\nContents\ndocker 컨테이너 환경 설정하기\nShiny 서버 환경 설정하기\n포트와 볼륨 설정\nDocker Desktop 활용\n콘솔 명령어 사용\n\nBitStat 애플리케이션 실행\n결언\n\n\n\n\n삽화출처: 초보를 위한 도커 안내서 - 도커란 무엇인가?\n\n\n들어가기\n컨테이너 환경설정이라 말하고 실행만 합니다. \n\ndocker 컨테이너를 실행하는 옵션을 GUI 화면에서 어렵게 시도합니다.\n\nCLI에서는 간단한 것을? 그러나 전문가가 아니니까요. Docker Desktop을 애용한답니다.\n\n\n\ndocker 컨테이너 환경 설정하기\n우리는 Shiny 데모 애플리케이션을 실행하기 위해서 Shiny 서버를 설치하지는 않을 것입니다. Shiny 데모 애플리케이션이 구동되는 최소 환경은 “아 Shiny 애플리케이션이 서버가 docker 환경에서 실행되는 구나” 정도를 인지할 뿐, 사용자가 개발한 애플리케이션을 운영하기 위해서는 몇가지 작업을 수행해야 합니다.\n우리는 오픈통계 패키지인 BitStat를 Shiny 서버에서 운영하는 것을 전제로 Shiny 서버의 docker 환경을 설정할 것입니다.\ndocker를 업무에, 혹은 자주 사용하지 않는 관계로 docker에 정통하지는 않습니다. 아주 기본적인 지식만 가지고 있을 뿐이죠. (명령어를 줄줄이 외울 정도로 잦은 주기의 docker 작업을 수행하지 않기 때문에) 그래서 개인적으로 Mac OS에서 Docker Desktop라는 Docker 관리 애플리케이션을 사용하고 있습니다. 이 글은 Docker Desktop을 이용하는 방법을 위주로 작성합니다. 하지만 docker 명령어도 간간히 병기하여 설명하도록 합니다.\nShiny 서버 환경 설정하기\nShiny 서버에서 BitStat 애플리케이션을 운영하기 위해서는 다음과 같은 추가적인 환경설정 작업이 필요합니다.\n애플리케이션을 배포할 경로 설정\nBitStat 애플리케이션이 위치할 경로 설정\n\n애플리케이션 수행시 발생하는 로그 경로 설정\n애플리케이션 운영의 안정성을 위한 Shiny 서버 로그\n\n애플리케이션이 실행에 필요한 R 패키지 설치\nrocker/shiny-verse 이미지에 누락된 R 패키지 설치\n\n포트와 볼륨 설정\nrocker/shiny-verse 이미지에는 BitStat 애플리케이션이 포함되어 있지 않습니다. 그러므로 BitStat 애플리케이션을 docker 컨테이너에 호스트의 볼륨(Volumes)으로 탑재해야 합니다.\n그리고 docker 컨테이너 안에서의 수행하는 Shiny 애플리케이션의 로그를 호스트에서도 확인하기 위해서 로그를 쌓을 볼륨도 설정해야 합니다. 특히 모든 리소스들이 정확하게 설치되어 문제없이 애플리케이션이 실행되기 전까지는 이 로그의 역할이 매우 중요합니다. 트러블슈팅(Troubleshooting)의 실마리를 제공해주는 유일한 환경이기 때문입니다.\nShiny 서버는 3838 포트를 사용합니다. 여기서도 Shiny 서버의 기본 포트인 3838을 사용합니다.\nDocker Desktop 활용\nDocker Desktop의 이미지 목록의 ‘NAME’ 컬럼에 ’rocker/shiny-verse’이 리스트업 되었습니다.\nDocker Desktop의 이미지 목록포트와 볼륨 설정을 위해서 ‘RUN 버튼’을 눌러서 컨테이너 정의 다이얼로그를 엽니다. 그러면 다음과 같은 ‘New Container’ 다이얼로그가 나타납니다. 이 화면에서 바로 ’Run 버튼’누르지 말고 ‘Optional Settings’를 선택합니다.\nDocker Desktop의 이미지 목록이 Optional Settings 다이얼로그 창에서 다음처럼 포트와 볼륨설정을 정의합니다.\ndocker hub 사이트의 rocker/shiny 페이지를 참고하여 옵션을 설정합니다.\nContainer Name: shiny-server\nPorts:\nLocal Host: 3838\nContainer Port: default\n\nBitStat 애플리케이션 Path\nHost Path: BitStat 애플리케이션이 있는 호스트 경로\nContainer Path: /srv/shiny-server\n\nBitStat 애플리케이션 Path\nHost Path: Shiny 서버 로그를 저장할 호스트 경로\nContainer Path: /var/log/shiny-server\n\n설정한 후 ’Run 버튼’을 누르면 Shiny 서버 컨테이너가 실행됩니다.\nOptional Settings 다이얼로그 창이미지 목록 화면에서 ’rocker/shiny-verse’의 ‘INUSE’ 버튼을 누릅니다.\nOptional Settings 다이얼로그 창그러면 다음처럼 ‘rocker/shiny-verse’ 이미지에서 실행된 컨테이너 목록을 확인할 수 있습니다. 목록을 보면 ‘shiny-server’라는 이름의 컨테이너가 3838 포트로 실행중임을 알 수 있습니다.\nOptional Settings 다이얼로그 창콘솔 명령어 사용\nDocker Desktop을 사용하지 않는다면, 다음의 명령어로도 간단하게 ‘rocker/shiny’ 이미지를 실행할 수 있습니다.\n\ndocker run -d -p 3838:3838 --name shiny-server \\\n    -v /Users/choonghyunryu/shiny-server/:/srv/shiny-server/ \\\n    -v /Users/choonghyunryu/Documents/99_logs/shiny/:/var/log/shiny-server/ \\\n    rocker/shiny\n\nBitStat 애플리케이션 실행\n이제는 http://localhost:3838 URL로 Shiny 서버 데모가 실행되지 않습니다. Shiny 서버의 홈 디렉토리 호스트의 경로로 변경했기 때문입니다. 이제는 이 URL은 다음과 같은 화면을 출력합니다.\n로컬 호스트의 /Users/choonghyunryu/shiny-server/ 디렉토리에는 서브 디렉토리로 BitStat 애플리케이션이 위치하기 때문입니다.\nShiny 서버 홈디렉토리 애플리케이션화면에서 BitStat 링크를 클릭하거나 http://localhost:3838/BitStat/ URL로 BitStat 애플리케이션을 실행할 수 있습니다.\n그러나 다음처럼 에러가 발생했습니다.\n오류가 발생한 BiStat 애플리케이션에러가 발생한 원인을 호스트의 로그 볼륨에서 확인할 수 있습니다. 호스트의 /Users/choonghyunryu/Documents/99_logs/shiny 디렉토리에 있는 로그 파일을 열어봅니다.\nBiStat 애플리케이션 로그 파일docker 컨테이너에 shinyjs 패키지가 설치되어 있지 않았기 때문에 에러가 발생한 것입니다. ‘rocker/shiny-verse’ 이미지에는 이 패키지가 포함되어 있지 않기 때문입니다.\n결언\ndocker 컨테이너에 호스트의 애플리케이션 경로와 로그 경로를 연결하고, 포트를 설정했습니다. 그러나 ‘rocker/shiny-verse’ 이미지에 필요한 R 패키지가 없어서 서비스가 정상적으로 구동되지 않았습니다.\n컨테이너 안에 필요한 R 패키지를 설치해야 합니다.\n\n\n\n",
    "preview": "posts/2022-03-17-custom-docker/img/docker-container.png",
    "last_modified": "2022-03-16T21:30:20+09:00",
    "input_file": {},
    "preview_width": 500,
    "preview_height": 389
  },
  {
    "path": "posts/2022-03-16-install-docker-image/",
    "title": "docker 이미지 설치하기",
    "description": "docker 기반으로 Shiny 서버를 운영하기 위한 기초작업을 수행합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-16",
    "categories": [
      "docker",
      "Development"
    ],
    "contents": "\n\nContents\nShiny 서버를 사용하는 이유\nShiny 서버 설치하기\ndocker 이미지 가져오기\nShiny 서버 실행하기\n\n결언\n\n\n\n\n\n\n\n들어가기\nShiny 서버 docker 이미지를 설치하여, 컨테이너를 구동하는 방법을 설명합니다.\n\ndocker가 설치된 환경을 전제로 설명합니다. docker 환경 구축하는 방법은 다루지 않습니다.\n\n\n\nShiny 서버를 사용하는 이유\nRStudio에서 Shiny 애플리케이션을 실행하면 애플리케이션을 종료하기 전까지 RStudio를 사용할 수 없습니다. 그러나 Shiny 서버에서 Shiny 애플리케이션을 구동하면, 언제든지 웹 브라우저에서 애플리케이션을 실행할 수 있고 RStudio는 애플리케이션의 구동과 독립적으로 사용할 수 있습니다.\nShiny 서버를 사용하는 이유에서 눈치챘겠지만, 이 글은 Shiny 애플리케이션을 호스팅할 목적의 작업이라기 보다는 개인 노트북에 Shiny 애플리케이션 데모 환경을 구축하기 위함입니다. 그러나 Shiny 애플리케이션을 호스팅할 목적으로도 활용할 수 있습니다.\nShiny 서버 설치하기\ndocker 이미지 가져오기\nShiny 서버의 운영을 위해서 rocker/shiny-verse 이미지를 가져옵니다. 이 이미지는 Shiny 이미지와 Tidyverse 패키지를 포함하고 있습니다.\nrocker/shiny + rocker/shiny-verse\n다음 명령으로 docker hub 저장소로부터 docker 이미지를 가져옵니다.\n\npull rocker/shiny-verse\n\n이미지 목록에서 가져온 Shiny 이미지를 확인할 수 있습니다.\n\ndocker images \n\nShiny 서버 실행하기\n다음 run 명령으로 docker 컨테이너를 생성하고 실행합니다. 즉, Shiny 서버를 실행합니다.\n\ndocker run --rm -p 3838:3838 rocker/shiny-verse\n\n서버가 실행되면, http://localhost:3838 URL에서 Shiny 데모 애플리케이션을 실행할 수 있습니다.\nShiny 데모 애플리케이션결언\ndocker 컨테이너로 Shiny 데모 애플리케이션을 구동할 수 있습니다. 그러나 아직은 사용자가 개발한 애플리케이션을 구동하기 위한 docker 환경은 아닙니다. 추가적인 환경 설정이 필요해 보입니다.\n\n\n\n",
    "preview": "posts/2022-03-16-install-docker-image/img/shiny-logo.png",
    "last_modified": "2022-03-16T08:08:06+09:00",
    "input_file": {},
    "preview_width": 2206,
    "preview_height": 2557
  },
  {
    "path": "posts/2022-03-15-introduce-pps/",
    "title": "예측력 점수에 대해서",
    "description": "dlookr 0.5.5 버전에 예측력 점수관련 기능을 추가했습니다. 이제는 dlookr로 EDA 과정에서 예측력 점수를 사용할 수 있습니다. 예측력 점수는 비선형 상관관계를 파악할 수 있는 유용한 통계량입니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-15",
    "categories": [
      "dlookr",
      "Statistics"
    ],
    "contents": "\n\nContents\n상관관계에 대하여\n상관관계와 상관계수\n상관계수와 회귀분석\n상관계수\n회귀분석\n시각화\n\ndlookr의 상관관계 분석\n\n예측력 점수를 아시나요?\n예측력 점수 정의\n예측력 점수의 특징\n예측력 점수의 장점\n예측력 점수의 단점\n\n예측력 점수의 계산\n예측 모델\n평가 지표와 예측력 점수\n\n예측력 점수의 활용\ndlookr에서의 예측력 점수\n상관행렬 플롯 그리기\n예측력 점수 계산하기\n예측력 점수 행렬 출력\n예측력 점수 행렬 시각화\n\n마무리\n\n\n\n\n\n\n들어가기\n최근 데이터 분석 필드의 EDA에서 유용하게 사용되고 있는\n예측력 점수\n를 dlookr 패키지에 추가했습니다. 이제는 EDA 과정이 더욱더 풍성해지게 되었습니다.\n\n유용하게 사용할 수 있도록 개념과 사례를 살펴보고자 합니다.\n\n\n상관관계에 대하여\n상관관계와 상관계수\n상관관계는 이변량 데이터 사이의 관계를 의미하는 통계학적인 용어입니다. 통상적으로 탐색적 데이터분석(EDA) 과정에서 변수간의 선형적인 관계를 살펴볼 때 사용합니다. 즉, 상관관계를 파악하는 것이죠. 이것은 절대로 인과관계가 아닙니다.\n상관관계는 상관계수(Correlation Coefficient)를 이용해서 판단합니다. 일반적으로 상관계수라고 부르는 것은 수치변수간에 계산된 피어슨 상관계수(Pearson Correlation Coefficient)로, 두 변수간의 선형적인 관계를 판단하도록 도와줍니다. 아시는 것처럼, 이 상관계수는 -1 ~ 1의 값을 가지며, -1은 완벽한 음의 선형 상관관계, 1은 완벽한 양의 선형 상관관계를 의미합니다. 0은 선형 상관관계가 없음을 의미합니다.\n피어슨의 상관계수 \\(r_{XY}\\)는 다음과 같이 계산됩니다.\n\\(r_{XY} = {공분산_{XY} \\over 표준편차_X \\cdot 표준편차_Y}\\)\n\\(r_{XY} = {\\sum_{i=1}^n(X_i - \\bar{X})(Y_i - \\bar{Y}) \\over \\sqrt{\\sum_{i=1}^n(X_i - \\bar{X})^2} \\cdot \\sqrt{\\sum_{i=1}^n(Y_i - \\bar{Y})^2} }\\)\n다음 이미지는 피어슨의 상관계수(이하 상관계수)별로 두 변수의 관계를 나타내는 이미지입니다.\n\n\n\nFigure 1: 다양한 상관계수별 관계, https://en.wikipedia.org/wiki/Correlation 발췌1\n\n\n\n이 이미지로 몇 가지 사실에 대해서 생각해 봅니다.\n첫째 줄은 상관계수에 대한 상관관계를 잘 설명해줍니다. 상관계수가 양수와 음수, 그리고 0인 경우의 관계를 잘 설명해줍니다.\n특히 상관계수가 -1과 1일 경우에는 X-Y의 유클리드 좌표에서 기울기가 -1과 1로 보여집니다.\n기울기가 1이라는 것은 X의 값이 증가할 때, Y의 값도 똑같이 크기만큼 증가함을 의미합니다.\n기울기가 -1이라는 것은 X의 값이 1 증가할 때, Y의 값은 똑같은 크기로 감소함을 의미합니다.\n그러나 상관계수는 기울기가 아닙니다. 그러나 간혹 기울기로 착각하는 분들이 있습니다.\n\n둘째 줄은 모든 사례가 상관계수가 -1 또는 1인 경우의 사례입니다.\n기울기가 -1이거나 1처럼 보이지 않습니다.\n상관계수의 값이 -1이거나 1인 경우는 모든 데이터들이 좌표에서 직선상에 분포함을 의미합니다.\n\n셋째 줄은 모든 사례의 상관계수가 0인 사례입니다.\n그러나 상관계수가 0이지만 무상관 관계라 보기 어렵습니다.\n비선형관계이지만 유클리드 좌표에 표현된 분포는 두 변수간의 모종의 관계가 있음을 암시합니다.\n\n이상을 종합하면, 상관계수는 선형 관계를 설명하는 통계량임을 알 수 있습니다. 그러므로 상관계수가 두 변수간의 관계를 살펴보는 만능 도구는 아닙니다.\n그 이유는,\n비선형 관계를 설명하지 못합니다.\n수치형 변수가 아닌 범주형 변수의 관계를 설명하지 못합니다.\n상관계수와 회귀분석\n피어슨 상관계수는 단순 선형회귀분석(Simple Linear Regression Analysis)과 깊은 관계가 있습니다.\n상관계수의 제곱값은 다음과 같이 회귀방정식이 직선인 회귀직선(regression straight line)을 의미하는, 단순 선형회귀분석의 결정계수의 값과 같습니다.\n\\(y = \\beta_0 + \\beta_1x\\)\n그러므로 상관계수의 절대값이 1인 것은 모든 데이터들이 회귀직선에 분포하는 것을 의미하며, 회귀식의 오차가 없는 결정계수가 1인 모형이 됩니다.\n상관계수와 단순 선형회귀분석은 비선형 분포를 갖는 데이터를 설명할 수 없으므로, 우리는 데이터 분석 과정에서 상관 계수와 회귀식을 너무 맹신하면 안됩니다. 이러한 오류를 줄이기 위해서는 시각화를 통해서 변수들의 관계를 먼저 이해해야 합니다.\nF. J. Anscombe2의 “Graphs in statistical analysis”에는 가상의 데이터로 이 문제를 다루고 있습니다. R에는 anscombe라는 데이터 프레임으로 이 데이터를 제공하고 있습니다. 이 데이터로 이 이슈를 바라봅니다.\nanscombe 데이터 프레임은 다음과 같습니다.\n\n\nanscombe\n\n\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n\n상관계수\nanscombe의 네 쌍의 변수에 대해서 dlookr 패키지의 correlate()로 상관계수를 구해봅니다.\n\n\nlibrary(dplyr)\n\ntab_corr <- dlookr::correlate(anscombe) %>% \n  mutate(num1 = stringr::str_extract(var1, \"[0-9]\")) %>% \n  mutate(num2 = stringr::str_extract(var2, \"[0-9]\")) %>% \n  filter(num1 == num2) %>% \n  filter(as.integer(var1) < as.integer(var2)) %>% \n  select(-num1, -num2) \n\ntab_corr %>% \n  rename(\"변수 1\" = var1, \n         \"변수 2\" = var2,\n         \"피어슨 상관계수\" = coef_corr) %>% \n  knitr::kable() %>% \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n변수 1\n\n\n변수 2\n\n\n피어슨 상관계수\n\n\nx1\n\n\ny1\n\n\n0.8164205\n\n\nx2\n\n\ny2\n\n\n0.8162365\n\n\nx3\n\n\ny3\n\n\n0.8162867\n\n\nx4\n\n\ny4\n\n\n0.8165214\n\n\n네 쌍의 x와 y 변수의 상관계수는 정확이 일치하지는 않지만, 모두 약 0.816로 동일합니다.\n회귀분석\n이번에는 네 쌍의 데이터에 대해서 단순 선형회귀분석을 수행해봅니다. 그리고 결정계수와 상관계수의 제곱 값을 비교해 봅니다.\n\n\nfit_lm <- tab_corr %>% \n  NROW() %>% \n  seq() %>% \n  purrr::map(\n    function(x) {\n      formula(glue::glue(\"{tab_corr$var2[x]} ~ {tab_corr$var1[x]}\")) %>%\n        lm(data = anscombe) \n    }\n  )\n\ntab_fit <- fit_lm %>% \n  seq() %>% \n  purrr::map_df(\n    function(x) {\n      pearson <- tab_corr$coef_corr[x]\n      pearson.squared <- pearson ^ 2\n        \n      term_list <- broom::tidy(fit_lm[[x]]) %>% \n        select(estimate) %>% \n        unlist()\n    \n      r.squared <- broom::glance(fit_lm[[x]]) %>%\n        select(r.squared) %>% \n        pull()        \n      \n      tibble::tibble(\n        formula = glue::glue(\"{tab_corr$var2[x]} ~ {tab_corr$var1[x]}\"),\n        pearson = pearson,\n        pearson.squared = pearson.squared,        \n        slope = term_list[2],\n        intercept = term_list[1],\n        r.squared = r.squared)\n    }\n  )\n\ntab_fit %>% \n  rename(\"모델 포뮬러\" = formula,\n         \"상관계수\" = pearson,\n         \"상관계수의 제곱\" = pearson.squared,\n         \"회귀식의 계수\" = slope,\n         \"회귀식의 절편\" = intercept,\n         \"결정계수\" = r.squared) %>% \n  knitr::kable() %>% \n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %>% \n  kableExtra::add_header_above(c(\" \", \"상관관계\" = 2, \"회귀분석\" = 3))\n\n\n\n\n\n\n상관관계\n\n\n\n\n회귀분석\n\n\n\n모델 포뮬러\n\n\n상관계수\n\n\n상관계수의 제곱\n\n\n회귀식의 계수\n\n\n회귀식의 절편\n\n\n결정계수\n\n\ny1 ~ x1\n\n\n0.8164205\n\n\n0.6665425\n\n\n0.5000909\n\n\n3.000091\n\n\n0.6665425\n\n\ny2 ~ x2\n\n\n0.8162365\n\n\n0.6662420\n\n\n0.5000000\n\n\n3.000909\n\n\n0.6662420\n\n\ny3 ~ x3\n\n\n0.8162867\n\n\n0.6663240\n\n\n0.4997273\n\n\n3.002454\n\n\n0.6663240\n\n\ny4 ~ x4\n\n\n0.8165214\n\n\n0.6667073\n\n\n0.4999091\n\n\n3.001727\n\n\n0.6667073\n\n\n상관계수의 제곱이 정확히 회귀분석의 결정계수와 일치함을 확인할 수 있습니다. 그러므로 상관계수는 선형관계를 갖는 두 수치변수의 단순 회귀분석의 결과를 어느정도 가늠할 수 있습니다.\n시각화\n플롯을 그려 네 쌍의 데이터에 대한 분포를 파악합니다. 파란색의 직선은 회귀직선입니다.\n\n\npar(mfrow = c(2, 2), mar = 0.1 + c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\nfit_lm %>% \n  seq() %>% \n  purrr::walk(\n    function(x) {\n      formula(glue::glue(\"{tab_corr$var2[x]} ~ {tab_corr$var1[x]}\")) %>%\n      plot(data = anscombe, col = \"red\", pch = 21, bg = \"firebrick\", cex = 1.5,\n           xlim = c(3, 19), ylim = c(3, 13))\n      \n      abline(fit_lm[[x]], col = \"blue\")\n    }\n  )\n\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 2)\n\n\n\n\n네 쌍의 분포를 통해서 첫째 분포는 선형관계에서 상관계수와 단순 선형회귀분석은 어느 정도 의미가 있으며, 셋째 분포는 상관계수와 단순 선형회귀분석는 이상치(outlier)에 영향을 받는, 로버스트한 분석 방법이 아님을 알려 줍니다. 마지막으로 둘째, 넷째 분포는 상관계수와 단순 선형회귀분석은 비선형 관계를 설명하지 못한다는 맹점을 보여줍니다.\n그럼에도 불구하고 데이터 분석 과정에서 변수간의 관계를 파악하기 위해서 상관계수를 널리 사용합니다. 일반적으로 다변량 변수에 대한 데이터 분석을 수행하므로, 상관행렬을 반들어 한 눈에 여러 변수들간의 관계를 파악합니다.\ndlookr의 상관관계 분석\ndlookr의 correlate()는 변수들의 상관계수를 계산해 줍니다. method 인수에 상관계수의 종류를 지정하는데, 기본값인 “pearson”이 피어슨 상관계수를 계산합니다. 만약 다른 상관계수를 계산하려면 다음과 같은 method 인수를 사용합니다.\n수치형 변수의 상관계수\n“pearson”: 피어슨의 상관계수\n“kendall”: 켄달의 순위 상관계수\n“spearman”: 스피어만의 순위 상관계수\n\n범주형 변수의 상관계수\n“cramer”: 크라머의 V 통계량\n“theil”: 티엘의 U 통계량, 엔트로피 계수\n\n\n\nlibrary(dlookr)\n\ntab_corr <- iris %>% \n  correlate()\n\ntab_corr\n\n\n# A tibble: 12 × 3\n   var1         var2         coef_corr\n   <fct>        <fct>            <dbl>\n 1 Sepal.Width  Sepal.Length    -0.118\n 2 Petal.Length Sepal.Length     0.872\n 3 Petal.Width  Sepal.Length     0.818\n 4 Sepal.Length Sepal.Width     -0.118\n 5 Petal.Length Sepal.Width     -0.428\n 6 Petal.Width  Sepal.Width     -0.366\n 7 Sepal.Length Petal.Length     0.872\n 8 Sepal.Width  Petal.Length    -0.428\n 9 Petal.Width  Petal.Length     0.963\n10 Sepal.Length Petal.Width      0.818\n11 Sepal.Width  Petal.Width     -0.366\n12 Petal.Length Petal.Width      0.963\n\nsummary() 함수는 계산된 결과를 상관행렬(Correlation Matrix)로 요약해서 보여줍니다.\n\n\nsummary(tab_corr)\n\n\n* correlation type   : generic \n* variable type      : numeric \n* correlation method : pearson \n\n* Matrix of Correlation\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000\n\n그리고 plot() 함수는 계산된 결과를 상관행렬로 시각화합니다.\n\n\nplot(tab_corr)\n\n\n\n\n예측력 점수를 아시나요?345\n예측력 점수 정의\n예측력 점수(Predictive Power Score, 또는 PPS)는 변수 x(수치형 또는 범주형)를 사용하여 변수 y(수치형 또는 범주형)를 예측하는데 사용할 수 있는 정도를 0(예측력 없음)과 1(가장 높은 예측력)사이에서 알려주는 정규화된 값입니다.\n예측력 점수로 데이터 세트에서 어떤 변수가 다른 변수의 값을 예측하는 데 얼마나 유용한지 파악할 수 있습니다. 일반적으로 1에 가까운 점수(예: 0.8)는 유용한 변수로 간주되며, 이는 해당 변수가 관심있는 변수의 값을 예측할 가능성이 매우 높다는 것을 알려줍니다.\n예측력 점수의 특징\n예측력 점수의 장점\n예측력 점수는 상관계수와 유사성은 존재하지만, 다음과 같은 장점이 있습니다.\n예측력 점수는 x와 y 사이의 비선형 관계를 감지합니다.\n예측력 점수는 대칭 인덱스가 아닙니다.\n즉, PPS(x, y) ≠ PPS(y, x)입니다.\nx가 y를 예측하면 y도 x를 예측한다고 말하지 않습니다 .\n\n예측력 점수는 수치형 및 범주형 변수 모두 지원합니다.\n예측력 점수의 단점\n예측력 점수는 상관계수와 유사성은 존재하지만, 다음과 같은 단점이 있습니다.\n예측력 점수는 상관행렬처럼 대칭이 아니고 복잡한 패턴을 유발할 수 있으므로,\n해석이 상대적으로 어려울 수 있습니다.\n\n예측력 점수는 계산 다양한 계산 알고리즘을 사용할 수 있으므로,\n즉, 다양한 머신러닝 알고리즘, 관련 매개변수, 교차검증 체계 및 모델 평가 메트릭을 사용할 수 있으므로,\n예측 모델의 성능에 영향을 주고, 결국은 예측력 점수에 영향을 주어,\n동일한 결과를 재현할 수 없을 수 있습니다.\n\n예측력 점수는 상관계수에 비해서 더 많은 계산 시간을 소비합니다.\n예측력 점수의 계산\n예측력 점수는 이름에서도 알 수 있듯, 예측을 목적으로 모든 유형의 변수에 적용할 수 있는 비대칭 비선형 지수를 계산합니다. 그러므로 예측력 점수의 계산을 위해서는 변수 y를 예측하는 지도학습 모델 적합해야 합니다. 지도학습 모델의 유형은 다음과 같습니다.\n범주형 변수 y를 예측하기 위해서는 분류 모델(Classification)을 사용합니다.\n수치형 변수 y를 예측하기 위해서는 회귀분석 모델(Regression)을 사용합니다.\n사전에 예측 모델을 적합을 수행하므로 예측력 점수 계산은 많은 연산 시간을 소비합니다. 그리고 알고리즘의 종류에 따라 그 값이 다소 상이할 수 있습니다.\n예측 모델\n일반적으로 예측력 점수는, 이상치 및 부족한 전처리에 로버스트한 의사결정 트리(Decision Trees) 알고리즘을 사용합니다. 모델을 적합하는 목적이 아니므로, 연산의 비용이 높은 성능 좋은 예측모델을 사용지는 않습니다. 요지는 연산의 비용이 높지 않으면서, 예측의 정확도를 어느 정도 담보하는 모델이 유용합니다.\ndlookr은 ppsr6 패키지를 래핑하여, 예측력 점수를 구현합니다. 즉, 내부 처리는 ppsr 패키지가 수행합니다.\nppsr 패키지는 예측력 점수를 계산할 때, 다음과 같은 모델을 사용합니다.\n범주형 변수 예측\nClassification\n이진분류 : glm 모델 사용\n다진분류 : raprt 패키지의 tree 모델 사용\n\n수치형 변수 예측\nRegression\nlm 모델 사용\n\n평가 지표와 예측력 점수\n예측력 점수를 구하기 위한 예측 모델의 모델 평가 메트릭은 다음의 메트릭을 사용합니다.\n범주형 변수 예측\nF1\n\n수치형 변수 예측\nRMSE(Root Mean Square Error) 혹은,\nMAE(Mean Absolute Error)\n\n예측력 점수를 계산해 보겠습니다.\n먼저 평가 지표를 구하기 위해서는 선택한 알고리즘으로 두 개의 모델을 적합합니다.\n\\(n\\)개의 predictor 즉, 변수 \\(x_1, x_2, x_3, \\cdots, x_n\\)으로 \\(y\\)를 예측하는 모델 \\(M\\)을 적합합니다.\n그리고, 예측력 점수를 구하려는 변수 \\(x_i\\)로 나이브(navie) 모델 \\(m_i\\)를 적합합니다.\n예측력 점수 PPS는 다음과 같이 계산합니다.\n범주형 변수를 예측하는 경우에는, 모델 \\(M\\)의 F1을 \\(F1_M\\), 모델 \\(m_i\\)의 F1을 \\(F1_{m_i}\\)이라 할 때, 다음처럼 계산합니다.\n\\(PPS_i = {(F1_M - F1_{m_i}) \\over (1 - F1_{m_i})}\\)\n범주형 변수를 예측하는 경우에는, 모델 \\(M\\)의 MAE를 \\(MAE_M\\), 모델 \\(m_i\\)의 MAE를 \\(MAE_{m_i}\\)이라 할 때, 다음처럼 계산합니다.\n\\(PPS_i = 1 - {MAE \\over mae_i}\\)\n예측력 점수의 활용\n예측력 점수는 다음과 같이 활용됩니다.\n데이터에서 패턴 찾기\n상관행렬의 대안으로 PPS 행렬을 사용하여 데이터의 선형 또는 비선형 패턴을 감지하고 이해할 수 있습니다.\n변수 선택\n예측모델 개발 과정에서의 변수선택(Feature Selection)에 활용합니다. 예측력 점수가 높은 변수를 선택하여, 모델을 개발할 수 있습니다.\n정보 누수(information leakage)7 탐지\n변수들간의 정보누수를 탐지합니다.\n데이터 정규화\n예측력 점수는 정규화된 엔터티이므로 데이터 정규화에도 도움이 됩니다.\ndlookr에서의 예측력 점수\n상관행렬 플롯 그리기\niris 데이터셋의 사완행렬 플롯은 다음과 같이 dlookr 패키지의 correlate()와 plot()으로 쉽게 그릴 수 있습니다. 상관행렬은 수치형 변수에 한해서 만들어지므로, Species 변수는 포함되지 않았습니다.\n\n\nlibrary(titanic)\n\ntitanic <- titanic_train %>% \n  select(\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"Ticket\", \"Fare\", \"Embarked\") %>% \n  rename(\"Class\" = Pclass,\n         \"TicketID\" = Ticket,\n         \"TicketPrice\" = Fare,\n         \"Port\" = Embarked)\n\ntitanic %>% \n  correlate() %>% \n  plot()\n\n\n\n\n\niris %>% \n  correlate() %>% \n  plot()\n\n\n\n상관행렬은 대각선을 기준으로 두 값이 대칭을 이룹니다. 즉, Sepal.Length과 Sepal.Width의 상관계수와 Sepal.Width와 Sepal.Length의 상관계수는 -0.12로 동일합니다.\n예측력 점수 계산하기\n예측력 점수는 pps()로 계산합니다. 그런에 예측력 점수를 계산하기 위해서는 많은 컴퓨팅 연산이 필요합니다. 그만큼 계산하는데 오랜 시간이 걸릴 수 있음을 의미합니다. 그래서 연산 과정에서으 병렬처리를 지원합니다. do_parallel = TRUE로 병렬처리를 활성화합니다.\n\n\ntab_pps <- pps(iris, do_parallel = TRUE)\ntab_pps\n\n\n              x            y                       result_type\n1  Sepal.Length Sepal.Length predictor and target are the same\n2   Sepal.Width Sepal.Length            predictive power score\n3  Petal.Length Sepal.Length            predictive power score\n4   Petal.Width Sepal.Length            predictive power score\n5       Species Sepal.Length            predictive power score\n6  Sepal.Length  Sepal.Width            predictive power score\n7   Sepal.Width  Sepal.Width predictor and target are the same\n8  Petal.Length  Sepal.Width            predictive power score\n9   Petal.Width  Sepal.Width            predictive power score\n10      Species  Sepal.Width            predictive power score\n11 Sepal.Length Petal.Length            predictive power score\n12  Sepal.Width Petal.Length            predictive power score\n13 Petal.Length Petal.Length predictor and target are the same\n14  Petal.Width Petal.Length            predictive power score\n15      Species Petal.Length            predictive power score\n16 Sepal.Length  Petal.Width            predictive power score\n17  Sepal.Width  Petal.Width            predictive power score\n18 Petal.Length  Petal.Width            predictive power score\n19  Petal.Width  Petal.Width predictor and target are the same\n20      Species  Petal.Width            predictive power score\n21 Sepal.Length      Species            predictive power score\n22  Sepal.Width      Species            predictive power score\n23 Petal.Length      Species            predictive power score\n24  Petal.Width      Species            predictive power score\n25      Species      Species predictor and target are the same\n          pps      metric baseline_score model_score cv_folds seed\n1  1.00000000        <NA>             NA          NA       NA   NA\n2  0.04632352         MAE      0.6893222   0.6620058        5    1\n3  0.54913985         MAE      0.6893222   0.3100867        5    1\n4  0.41276679         MAE      0.6893222   0.4040123        5    1\n5  0.40754872         MAE      0.6893222   0.4076661        5    1\n6  0.06790301         MAE      0.3372222   0.3184796        5    1\n7  1.00000000        <NA>             NA          NA       NA   NA\n8  0.23769911         MAE      0.3372222   0.2564258        5    1\n9  0.21746588         MAE      0.3372222   0.2631636        5    1\n10 0.20128762         MAE      0.3372222   0.2677963        5    1\n11 0.61608360         MAE      1.5719667   0.5971445        5    1\n12 0.24263851         MAE      1.5719667   1.1945031        5    1\n13 1.00000000        <NA>             NA          NA       NA   NA\n14 0.79175121         MAE      1.5719667   0.3265152        5    1\n15 0.79049070         MAE      1.5719667   0.3280552        5    1\n16 0.48735314         MAE      0.6623556   0.3377682        5    1\n17 0.20124105         MAE      0.6623556   0.5315834        5    1\n18 0.74378445         MAE      0.6623556   0.1684906        5    1\n19 1.00000000        <NA>             NA          NA       NA   NA\n20 0.75611126         MAE      0.6623556   0.1608119        5    1\n21 0.55918638 F1_weighted      0.3176487   0.7028029        5    1\n22 0.31344008 F1_weighted      0.3176487   0.5377587        5    1\n23 0.91675800 F1_weighted      0.3176487   0.9404972        5    1\n24 0.93985320 F1_weighted      0.3176487   0.9599148        5    1\n25 1.00000000        <NA>             NA          NA       NA   NA\n   algorithm     model_type\n1       <NA>           <NA>\n2       tree     regression\n3       tree     regression\n4       tree     regression\n5       tree     regression\n6       tree     regression\n7       <NA>           <NA>\n8       tree     regression\n9       tree     regression\n10      tree     regression\n11      tree     regression\n12      tree     regression\n13      <NA>           <NA>\n14      tree     regression\n15      tree     regression\n16      tree     regression\n17      tree     regression\n18      tree     regression\n19      <NA>           <NA>\n20      tree     regression\n21      tree classification\n22      tree classification\n23      tree classification\n24      tree classification\n25      <NA>           <NA>\n\ndlookr에 예측력 점수를 계산하는 기능을 추가했습니다만, 엔터프라이즈급의 데이터에 대해서 예측력 점수를 계산하기에는 버겁습니다. 물론 서버급의 장비를 사용하면 수행 시간을 단축할 수 있겠죠. 개인 노트북에서 타이타닉 데이터의 예측력 점수를 계산하는데, 많은 인내를 감내해야 했습니다.\n만약에 목표변수 y를 위한 변수 선택을 수행한다면, 다음처럼 target_by()를 사용해서 목표변수 y를 지정하면 수행시간을 절약할 수 있습니다.\npps()로 만든 결과는 “pps” 클래스 객체를 반환합니다. 이 객체는 R의 제너릭 함수인 summary()와 plot()을 지원합니다.\n\n\ntarget_pps <- iris %>% \n  target_by(Species) %>% \n  pps(do_parallel = TRUE)\n\ntarget_pps\n\n\n             x       y                       result_type       pps\n1 Sepal.Length Species            predictive power score 0.5591864\n2  Sepal.Width Species            predictive power score 0.3134401\n3 Petal.Length Species            predictive power score 0.9167580\n4  Petal.Width Species            predictive power score 0.9398532\n5      Species Species predictor and target are the same 1.0000000\n       metric baseline_score model_score cv_folds seed algorithm\n1 F1_weighted      0.3176487   0.7028029        5    1      tree\n2 F1_weighted      0.3176487   0.5377587        5    1      tree\n3 F1_weighted      0.3176487   0.9404972        5    1      tree\n4 F1_weighted      0.3176487   0.9599148        5    1      tree\n5        <NA>             NA          NA       NA   NA      <NA>\n      model_type\n1 classification\n2 classification\n3 classification\n4 classification\n5           <NA>\n\nclass(tab_pps)\n\n\n[1] \"pps\"        \"data.frame\"\n\nclass(target_pps)\n\n\n[1] \"pps\"        \"data.frame\"\n\nPetal.Length와 Petal.Width의 붓꽃의 품종인 Species를 예측하는 점수가 상당히 높습니다.\n예측력 점수 행렬 출력\n예측력 점수 행렬은 summary() 함수로 구할 수 있습니다. 그러나 target_by()를 사용한 예측력 점수는 요약된 정보만 제공합니다.\n\n\n## 전체 변수를 사용한 사례\nsummary(tab_pps)\n\n\n* PPS type : generic \n* Matrix of Predictive Power Score\n  - Columns : target\n  - Rows    : predictors\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length   1.00000000  0.04632352    0.5491398   0.4127668\nSepal.Width    0.06790301  1.00000000    0.2376991   0.2174659\nPetal.Length   0.61608360  0.24263851    1.0000000   0.7917512\nPetal.Width    0.48735314  0.20124105    0.7437845   1.0000000\nSpecies        0.55918638  0.31344008    0.9167580   0.9398532\n               Species\nSepal.Length 0.4075487\nSepal.Width  0.2012876\nPetal.Length 0.7904907\nPetal.Width  0.7561113\nSpecies      1.0000000\n\n## Target 변수를 지정한 사례\nsummary(target_pps)\n\n\n* PPS type : target_by \n* Target variable : Species \n* Model type : classification \n* Information of Predictive Power Score\n    predictors  target       pps\n1      Species Species 1.0000000\n2  Petal.Width Species 0.9398532\n3 Petal.Length Species 0.9167580\n4 Sepal.Length Species 0.5591864\n5  Sepal.Width Species 0.3134401\n\n예측력 점수 행렬 시각화\nplot() 함수로 예측력 점수를 시각화 합니다.\n변수 선택을 위해서 만든 target_pps 객체는 다음처럼 막대 플롯을 출력합니다.\n\n\nplot(target_pps)\n\n\n\n\nEDA를 위해서 만든 tab_pps 객체는 다음처럼 예측력 점수를 타일 플롯으로 출력합니다.\n\n\nplot(tab_pps)\n\n\n\n\n마무리\n예측력 점수가 핫한 아이템인 것 같습니다. 그러나 어찌 보면 새로운 개념은 아닙니다. 그러나 상관관계와 협업하면 EDA가 좀더 탄탄해질 수 있을 것 같습니다.\n많은 연산 리소스를 요구하는 것이 흠이지만, 여러분의 EDA에 새로운 바람을 넣어보시는 것이 어떨까요?\n\nCorrelation, https://en.wikipedia.org/wiki/Correlation↩︎\nAnscombe, Francis J. (1973). Graphs in statistical analysis. The American Statistician, 27, 17–21. doi: 10.2307/2682899.↩︎\nRIP correlation. Introducing the Predictive Power Score. https://towardsdatascience.com/rip-correlation-introducing-the-predictive-power-score-3d90808b9598↩︎\nUsing The Predictive Power Score in R. https://towardsdatascience.com/using-the-predictive-power-score-in-r-26c43d05dc01↩︎\nWhat is Predictive Power Score (PPS). https://machinelearningknowledge.ai/predictive-power-score-vs-correlation-with-python-implementation/↩︎\nppsr: Predictive Power Score. https://cran.r-project.org/web/packages/ppsr/index.html↩︎\n정보 누수는 훈련 데이터에서 타겟을 예측하는 변수로 포함되지만, 실제 예측에서는 사용할 수 없는 경우 발생합니다. 이는 심지어 검증 데이터 셋에서도 높은 성능으로 학습되지만, 모델이 실제 배포됐을 때에는 제대로 동작하지 않습니다.↩︎\n",
    "preview": "posts/2022-03-15-introduce-pps/img/pearson.png",
    "last_modified": "2022-03-15T07:44:24+09:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 365
  },
  {
    "path": "posts/2022-03-14-reactivity/",
    "title": "반응 출력",
    "description": "반응 출력을 이해합니다. 랜더링 함수의 종류와 기능도 숙지해야 합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-14",
    "categories": [
      "Shiny",
      "Development"
    ],
    "contents": "\n\nContents\n반응형 출력을 아시나요?\n출력 위젯\nUI에 출력 위젯 추가하기\n\n렌더링 함수\n렌더링 함수로 결과 반환하기\n\n\ntutorial\n반응 출력 완성하기 tutorial\ntutorial 페이지\n추가 예제 실행해 보기\n\n\n\n\n\n\n\n들어가기\n의도를 파악하자마자 바로 전달합니다.\n반응출력이라 하지요.\n\n어렵지 않습니다. 유저 인터페이스를 통한 커뮤니케이션에서 화자인 의도에 대해서 그저 어떤 방식으로 반응(랜더링)할지 정하면 됩니다.\n\n\n반응형 출력을 아시나요?\n“반응형 출력(reactive output)은 사용자가 입력 위젯의 값을 변경하면, 이에 반응, 응답하여 출력을 자동으로 만들어주는 것을 의미합니다.” 일반적인 웹 어플리케이션은 사용자가 입력 위젯의 값을 변경한 후, “확인”, “실행” 등의 버튼을 누를 때 출력이 발생하지만, Shiny는 기본적으로 반응형 출력으로 결과를 반환합니다.\n출력 위젯\n출력 위젯의 이름의 접미사는 “Output”로 계산된 결과나 시각화 결과를 사용자에게 보여주는 기능을 수행합니다.\nshiny 패키지의 출력 위젯은 같습니다.\n\n\nlibrary(shiny)\n\nls(pos = \"package:shiny\", pattern = \"Output$\")\n\n\n[1] \"dataTableOutput\"          \"htmlOutput\"              \n[3] \"imageOutput\"              \"plotOutput\"              \n[5] \"snapshotPreprocessOutput\" \"tableOutput\"             \n[7] \"textOutput\"               \"uiOutput\"                \n[9] \"verbatimTextOutput\"      \n\nUI에 출력 위젯 추가하기\n“selected_var”라는 아이디로 출력 위젯 textOutput을 UI에 추가합니다.\n\n\nui <- fluidPage(\n  titlePanel(\"censusVis\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      helpText(\"Create demographic maps with \n               information from the 2010 US Census.\"),\n      \n      selectInput(\"var\", \n                  label = \"Choose a variable to display\",\n                  choices = c(\"Percent White\", \n                              \"Percent Black\",\n                              \"Percent Hispanic\", \n                              \"Percent Asian\"),\n                  selected = \"Percent White\"),\n      \n      sliderInput(\"range\", \n                  label = \"Range of interest:\",\n                  min = 0, max = 100, value = c(0, 100))\n    ),\n    \n    mainPanel(\n      textOutput(\"selected_var\")\n    )\n  )\n)\n\n\n\n렌더링 함수\n렌더링 함수 이름의 접두사는 “render”로 입력 위젯의 값이 변경되면 반응하여 계산 결과를 출력 위젯에 랜더링합니다.\nshiny 패키지의 렌더링 함수는 다음과 같습니다.\n\n\nls(pos = \"package:shiny\", pattern = \"^render\")\n\n\n[1] \"renderCachedPlot\" \"renderDataTable\"  \"renderImage\"     \n[4] \"renderPlot\"       \"renderPrint\"      \"renderTable\"     \n[7] \"renderText\"       \"renderUI\"        \n\n렌더링 함수\n생성 객체\n내용\nrenderDataTable\nDataTable\n데이터 테이블\nrenderImage\n이미지\n이미지 파일 등\nrenderPlot\n플롯\n플롯 결과\nrenderPrint\n텍스트 출력\n모든 출력\nrenderTable\n테이블 구조 객체\ndata frame, matrix 등\nrenderText\n텍스트 출력\n텍스트 출력\nrenderUI\n사용저 정의 UI\n기존 위젯으로 사용자가 정의한 위젯\n렌더링 함수로 결과 반환하기\n다음 예제는 렌더링 함수인 renderText로 아이디가 “selected_var”인 출력 위젯에 “You have selected this”라는 텍스트를 렌더링(출력)합니다. 그러나 이것은 반응 출력이 아닙니다.\n\n\nserver <- function(input, output) {\n  output$selected_var <- renderText({ \n    \"You have selected this\"\n  })\n}\n\n\n\n다음 예제는 렌더링 함수인 renderText로 아이디가 “selected_var”인 출력 위젯에 “You have selected this”라는 텍스트와 아이디가 “var”인 입력 위젯의 값을 붙여서 렌더링(출력)합니다.\n이것은 반응 출력이 입니다. 렌더링 함수에 포함된 입력 위젯인 “var”의 값이 변경될 때마다 renderText 함수가 반응하여 계산된 결과를 출력 위젯인 “selected_var”에 렌더링합니다.\ninput$var는 아이디가 “var”인 입력 위젯을 의미하고, output$selected_var는 아이디가 “selected_var”인 출력 위젯을 의미합니다.\n\n\nserver <- function(input, output) {\n  output$selected_var <- renderText({ \n    paste(\"You have selected\", input$var)\n  })\n}\n\n\n\ntutorial\n반응 출력 완성하기 tutorial\n다음 코드를 입력한 후, app.R이라는 이름의 파일로 저장하고 실행해 보세요. 완성된 반응 출력의 기능을 확인할 수 있습니다. 여러분은 server 파트를 완성하였습니다.\n\n\nlibrary(shiny)\n\nui <- fluidPage(\n  titlePanel(\"censusVis\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      helpText(\"Create demographic maps with \n               information from the 2010 US Census.\"),\n      \n      selectInput(\"var\", \n                  label = \"Choose a variable to display\",\n                  choices = c(\"Percent White\", \n                              \"Percent Black\",\n                              \"Percent Hispanic\", \n                              \"Percent Asian\"),\n                  selected = \"Percent White\"),\n      \n      sliderInput(\"range\", \n                  label = \"Range of interest:\",\n                  min = 0, max = 100, value = c(0, 100))\n    ),\n    \n    mainPanel(\n      textOutput(\"selected_var\"),\n      textOutput(\"min_max\")\n    )\n  )\n)\n\nserver <- function(input, output) {\n  \n  output$selected_var <- renderText({ \n    paste(\"You have selected\", input$var)\n  })\n  \n  output$min_max <- renderText({ \n    paste(\"You have chosen a range that goes from\",\n          input$range[1], \"to\", input$range[2])\n  })\n  \n}\n\nshinyApp(ui, server)\n\n\n\ntutorial 페이지\nShiny 공식 tutorial 페이지를 살펴보고, 반응 출력을 이해하세요.\nhttps://shiny.rstudio.com/tutorial/written-tutorial/lesson4/\n추가 예제 실행해 보기\n예제를 실행시켜 보고, 또다른 반응 출력을 경험해 보세요.\n\n\nshiny::runExample(\"03_reactivity\")\n\n\n\n\n\n\n",
    "preview": "posts/2022-03-14-reactivity/img/fib_no_conductor.png",
    "last_modified": "2022-03-14T07:36:23+09:00",
    "input_file": {},
    "preview_width": 255,
    "preview_height": 180
  },
  {
    "path": "posts/2022-03-13-control-widget/",
    "title": "위젯 추가하기",
    "description": "위젯을 이해합니다. 입력 위젯을 패널에 추가하는 방법을 숙지해야 합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-13",
    "categories": [
      "Shiny",
      "Development"
    ],
    "contents": "\n\nContents\n위젯을 아시나요?\n입력 위젯\n\n위젯을 추가한 예제\ntutorial\n입력 위젯 완성하기 tutorial\nShiny 공식 tutorial 페이지\n추가 예제 실행해 보기\n\n\n\n\n\n\n\n들어가기\n위젯은\n메신저의 눈과 귀\n입니다.\n의도 파악을 위한 듣는 위젯\n과\n내용의 전달을 위한 말하는 위젯\n으로 구성됩니다.\n\n위젯의 기능과 사용방법을 익히는 것은 말을 익히는 것과 유사합니다. 당신은 문법을 익히고, 어휘를 늘려가야 합니다. 관용어구를 하나 더 익혀야할 수도 있습니다.\n\n\n위젯을 아시나요?\n“위젯(widget)은 컴퓨터 프로그래밍에서 위젯(widget) 또는 컨트롤(control)은 컴퓨터 사용자가 상호 작용하는 인터페이스 요소입니다.” - https://ko.wikipedia.org/wiki/GUI_위젯 중에서\n입력 위젯\nShiny에서는 입력 위젯을 컨트롯 위젯(control widgets)이라 부릅니다. 즉, 위젯을 통해서 앱을 컨트롤하기 때문입니다. 굳이 입력 위젯이라고 불렀던 것은 입력된 값을 통해서 기능이 컨트롤되고, 중요한 대부분 컨트롤 위젯 이름의 접미사가 “Input”이기 때문입니다.\nshiny 패키지의 입력 위젯은 버튼을 포함해서 다음과 같은 종류가 있습니다.\n\n\nlibrary(shiny)\n\nsetdiff(union(ls(pos = \"package:shiny\", pattern = \"Input$\"), \n      ls(pos = \"package:shiny\", pattern = \"Button$\")),\n      ls(pos = \"package:shiny\", pattern = \"^update\"))\n\n\n [1] \"checkboxGroupInput\"      \"checkboxInput\"          \n [3] \"dateInput\"               \"dateRangeInput\"         \n [5] \"fileInput\"               \"numericInput\"           \n [7] \"passwordInput\"           \"restoreInput\"           \n [9] \"selectInput\"             \"selectizeInput\"         \n[11] \"sliderInput\"             \"snapshotPreprocessInput\"\n[13] \"textAreaInput\"           \"textInput\"              \n[15] \"varSelectInput\"          \"varSelectizeInput\"      \n[17] \"actionButton\"            \"bookmarkButton\"         \n[19] \"downloadButton\"          \"modalButton\"            \n[21] \"submitButton\"           \n\n기본 입력 위젯은 다음 그림과 같습니다.\n기본 입력 위젯위젯을 추가한 예제\n다음 예제는 레이아웃과 패널에 입력 위젯을 출력한 예제입니다.\n\n\nui <- fluidPage(\n  titlePanel(\"censusVis\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      helpText(\"Create demographic maps with \n               information from the 2010 US Census.\"),\n      \n      selectInput(\"var\", \n                  label = \"Choose a variable to display\",\n                  choices = list(\"Percent White\", \n                                 \"Percent Black\",\n                                 \"Percent Hispanic\", \n                                 \"Percent Asian\"),\n                  selected = \"Percent White\"),\n      \n      sliderInput(\"range\", \n                  label = \"Range of interest:\",\n                  min = 0, max = 100, value = c(0, 100))\n    ),\n    \n    mainPanel()\n  )\n)\n\n\n\n위의 예제의 결과는 다음과 같습니다.\n입력 위젯들그리고 위 그림에서 selectInput은 이름처럼 여러 값 중에서 하나의 값을 선택하는 입력 위젯입니다. 다음처럼 마우스를 올려 놓으면 네 개의 값이 출력되고, 사용자는 하나의 값을 선택하면 됩니다.\nselectInput 위젯tutorial\n입력 위젯 완성하기 tutorial\n다음 코드를 입력한 후, app.R이라는 이름의 파일로 저장하고 실행해 보세요. 완성된 입력 위젯의 기능을 확인할 수 있습니다. 여러분은 UI 파트를 완성하였습니다.\n\n\nlibrary(shiny)\n\n# Define UI ----\nui <- fluidPage(\n  titlePanel(\"censusVis\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      helpText(\"Create demographic maps with \n               information from the 2010 US Census.\"),\n      \n      selectInput(\"var\", \n                  label = \"Choose a variable to display\",\n                  choices = list(\"Percent White\", \n                                 \"Percent Black\",\n                                 \"Percent Hispanic\", \n                                 \"Percent Asian\"),\n                  selected = \"Percent White\"),\n      \n      sliderInput(\"range\", \n                  label = \"Range of interest:\",\n                  min = 0, max = 100, value = c(0, 100))\n    ),\n    \n    mainPanel()\n  )\n)\n\n# Define server logic ----\nserver <- function(input, output) {\n  \n}\n\n# Run the app ----\nshinyApp(ui = ui, server = server)\n\n\n\nShiny 공식 tutorial 페이지\nShiny 공식 tutorial 페이지를 살펴보고, 입력 위젯의 종류와 설정하는 방법을 이해하세요.\nhttps://shiny.rstudio.com/tutorial/written-tutorial/lesson3/\n추가 예제 실행해 보기\n예제를 실행시켜 보고, 다양한 입력 위젯과 출력 위젯을 경험해 보세요.\n\n\nshiny::runExample(\"07_widgets\")\n\n\n\n\n\n\n",
    "preview": "posts/2022-03-13-control-widget/img/Widget_icon.png",
    "last_modified": "2022-03-12T17:32:02+09:00",
    "input_file": {},
    "preview_width": 128,
    "preview_height": 128
  },
  {
    "path": "posts/2022-03-12-userinterface/",
    "title": "사용자 인터페이스 만들기",
    "description": "사용자 인터페이스(UI)를 이해합니다. page, layout, panel을 숙지해야 합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-12",
    "categories": [
      "Shiny",
      "Development"
    ],
    "contents": "\n\nContents\nShiny 사용자 인터페이스\nShiny 앱 Skeleton\n페이지\nshiny 페이지\nshinydashboard 페이지\n\n레이아웃과 패널\n레이아웃\n패널\n레이아웃과 패널의 배치 예제\n\nHTML 태그\nHTML 태그를 만드는 함수\nHTML 태그를 사용 예제\n\n\ntutorial\nUI 뼈대 만들기 tutorial\nShiny 공식 tutorial 페이지\n추가 예제 실행해 보기\n\n\n\n\n\n\n\n들어가기\n사용자 인터페이스는\n사용자와 컴퓨터를 연결해주는 메신저입니다.\n사용자의 의도를 Shiny에게 전달하는\n의도 파악\n과 R의 수행결과를 Shiny를 통해\n내용의 전달\n을 도와줍니다.\n\n사용자 인터페이스를 이해하는 것은 훌륭한 메신저가 되기 위한 여정입니다. 자, 얼른 일어나세요.\n\n\nShiny 사용자 인터페이스\nShiny 앱 Skeleton\n\n\nlibrary(shiny)\n\n# Define UI ----\nui <- fluidPage(\n  \n)\n\n# Define server logic ----\nserver <- function(input, output) {\n  \n}\n\n# Run the app ----\nshinyApp(ui = ui, server = server)\n\n\n\n페이지\nShiny 앱의 UI는 하나의 Page로 구성됩니다. 즉, 페이지(page)에 여러 개의 위젯을 배치하여 앱을 구성합니다. 이 Skeleton에서는 fluidPage를 사용하였군요.\n\n페이지는 화가가 그림을 그릴 캔버스라고 이해하면 쉽습니다. 화가가 캔버스에 여러 가지 과일과 꽃 등을 배치하여 정물화를 그리거나, 나무, 산, 강 등을 적절하게 배치하여 풍경화를 그리는 것처럼 Shiny는 페이지에 위젯을 적절하게 배치하여 앱을 개발하는 것입니다.\n\nshiny 페이지\n다음 이미지는 앞에서 실행해 보았던 예제에서 fluidPage로 정의한 UI입니다.\nfluidPage로 정의한 UI그리고 shiny 패키지는 다음과 같은 몇 개의 페이지 함수를 포함하고 있습니다.\n\n\nlibrary(magrittr)\nlibrary(shiny)\n\nls(pos = \"package:shiny\", pattern = \"Page$\") %>% \n  setdiff(\n    ls(pos = \"package:shiny\", pattern = \"^update\")\n  )\n\n\n[1] \"basicPage\"     \"bootstrapPage\" \"fillPage\"      \"fixedPage\"    \n[5] \"fluidPage\"     \"navbarPage\"   \n\n\n유화를 그리는 화가는 캔버스를 준비하지만, 산수화를 그리는 화가는 화선지를 준비하고, 초등학교 미술시간에는 크레파스와 켄트지를 준비하겠지요. 용도에 따라 몇 개의 페이지를 준비해 놓은 것입니다.\n\nshinydashboard 페이지\n다음은 shiny로 대시보드를 구현할 때 사용하는 shinydashboard 패키지의 dashboardPage로 정의한 UI입니다.\ndashboardPageshinydashboard 패키지는 dashboardPage라는 단 하나의 페이지만 제공합니다.\n\n\nlibrary(shinydashboard)\n\nls(pos = \"package:shinydashboard\", pattern = \"Page$\")\n\n\n[1] \"dashboardPage\"\n\n현재 BitStat는 shinydashboard 패키지를 확장한 shinydashboardPlus 패키지의 dashboardPage로 페이지를 구성하고 있습니다.\n레이아웃과 패널\n페이지에 위젯을 보기 좋게 배치하기 위해서 Shiny는 레이아웃(layout)과 패널(panel)을 제공합니다. 레이아웃은 페이지를 가상으로 구획하는 구도이며, 패널은 위젯을 그룹핑하여 패치할 공간을 의미합니다.\n\n화가는 캔버스를 가상으로 나누는 구도를 잡고 사물을 그리는 것처럼 Shiny는 레이아웃으로 구도를 잡고, 패널을 적당하게 배치한 후 패널 안에 비로소 위젯을 채워나갑니다. 즉, 레이아웃과 패널을 적당히 섞어서 앱의 모양(UI)을 정의합니다.\n\n레이아웃\nshiny 패키지는 몇 개의 레이아웃을 제공하는데 앞의 예제처럼 sidebarLayout이 일반적으로 사용됩니다.\n\n\nls(pos = \"package:shiny\", pattern = \"Layout$\") \n\n\n[1] \"flowLayout\"     \"sidebarLayout\"  \"splitLayout\"   \n[4] \"verticalLayout\"\n\n패널\nshiny 패키지는 몇 개의 패널을 제공하는데 앞의 예제에서는 titlePanel, sidebarPanel, mainPanel이 사용되었습니다.\n\n\nls(pos = \"package:shiny\", pattern = \"Panel$\") %>% \n  setdiff(\n    ls(pos = \"package:shiny\", pattern = \"^update\")\n  )\n\n\n [1] \"absolutePanel\"    \"conditionalPanel\" \"fixedPanel\"      \n [4] \"headerPanel\"      \"inputPanel\"       \"mainPanel\"       \n [7] \"navlistPanel\"     \"sidebarPanel\"     \"tabPanel\"        \n[10] \"tabsetPanel\"      \"titlePanel\"       \"wellPanel\"       \n\n레이아웃과 패널의 배치 예제\n다음 예제는 히스토그램을 그리는 예제에서의 레이아웃과 패널의 배치 방법입니다. 위젯을 포함하지 않은 상태입니다.\n\n\nui <- fluidPage(\n  titlePanel(\"title panel\"),\n\n  sidebarLayout(\n    sidebarPanel(\"sidebar panel\"),\n    mainPanel(\"main panel\")\n  )\n)\n\n\n\n위와 같은 배치는 다음 그림과 같은 사이드 바 구도를 만들어 줍니다.\n사이드바 레이아웃sidebarLayout은 일반적으로 sidebarPanel와 mainPanel을 포함합니다. 이것은 왼쪽에 사이드 바 패널을 만들어서 사용자의 입력을 받는 위젯을 배치하고 오른쪽에는 사용자의 입력에 따른 결과를 출력할 메인 패널을 배치하는 방법으로 동작합니다.\ntitlePanel은 이름처럼 앱의 타이틀을 출력할 패널입니다.\nHTML 태그\n아시다시피 Shiny 앱을 HTML로 동작하는 웹 어플리케이션을 의미합니다. 즉, Shiny가 만들어 주는 것은 웹 어플리케이션을 구성하는 HTML, CSS, Javascript입니다.\nR의 htmltools 패키지는 HTML 태그를 만들어 주는 패키지입니다. shiny 패키지가 htmltools 패키지를 이용해서 HTML 태그를 만들어 줍니다.\nHTML 태그를 만드는 함수\nh1() 함수는 첫번째 레벨의 해더를 생성하는 MTML의 <h1> 태그를 만들어 줍니다. 글쓰기에서 장(Chapter)의 타이틀을 만들 때 사용하는 태그입니다. 헤더(Headers)를 만들어 준다고 이해하면 쉽겠지요. 이니셜을 따서 h, 첫번 째라서 1가 됩니다. 합치면 h1가 되겠지요. 숫자가 클수록 글자의 크기가 작아집니다.\nh1() 함수는 다음처럼 <h1> 태그를 만들어 줍니다.\n\nh1(\"A first level header\")\n[1] <h1>A first level header<\/h1>\n\n 그런데, 이 HTML 태그는 웹 브라우저에서 다음과 같이 출력됩니다.\n\nA first level header\n\nshiny 패키지에서 제공하는 대표적인 HTML 태그 함수는 다음과 같습니다.\n함수\nHTML 태그\n의미\np\n<p>\n파라그래프 텍스트\nh1\n<h1>\n첫번째 헤더\nh2\n<h2>\n두번째 헤더\nh3\n<h3>\n세번째 헤더\nh4\n<h4>\n네번째 헤더\nh5\n<h5>\n다섯번째 헤더\nh6\n<h6>\n여섯번째 헤더\nbr\n<br>\n줄 바꿈\nhr\n<hr>\n수평선 긋기\nimg\n<img>\n이미지 삽입\nstrong\n<strong>\n볼드체\ndiv\n<div>\n동일 스타일의 문자 디비젼\nspan\n<span>\n동일 스타일의 문자 인라인\nHTML 태그를 사용 예제\n다음 예제는 여러 종류의 헤더를 출력하는 UI의 설계입니다. 입력 위젯은 포함하지 않은 상태입니다.\n\n\nui <- fluidPage(\n  titlePanel(\"My Shiny App\"),\n  sidebarLayout(\n    sidebarPanel(),\n    mainPanel(\n      h1(\"First level title\"),\n      h2(\"Second level title\"),\n      h3(\"Third level title\"),\n      h4(\"Fourth level title\"),\n      h5(\"Fifth level title\"),\n      h6(\"Sixth level title\")\n    )\n  )\n)\n\n\n\n위와 같은 배치는 다음 그림과 같은 사이드 바 구도를 만들어 줍니다.\n사이드바 레이아웃tutorial\ntutorial에서는 UI 뼈대를 만들기 위해서 fluidPage, sidebarLayout, mainPanel, tabsetPanel을 사용합니다.\nUI 뼈대 만들기 tutorial\n다음 코드를 입력한 후, app.R이라는 이름의 파일로 저장하고 실행해 보세요. 앞에서 다룬 사이드바 레이아웃 그림과 같은 앱이 출력됩니다.\n\n\nlibrary(shiny)\n\n# Define UI ----\nui <- fluidPage(\n  titlePanel(\"title panel\"),\n  \n  sidebarLayout(position = \"right\",\n                sidebarPanel(\"sidebar panel\"),\n                mainPanel(\"main panel\")\n  )\n)\n\n# Define server logic ----\nserver <- function(input, output) {\n  \n}\n\n# Run the app ----\nshinyApp(ui = ui, server = server)\n\n\n\nShiny 공식 tutorial 페이지\nShiny 공식 tutorial 페이지를 살펴보고, 사용자 인터페이스를 구축하는 방법을 이해하세요.\nhttps://shiny.rstudio.com/tutorial/written-tutorial/lesson2/\n추가 예제 실행해 보기\n예제를 실행시켜 보고, tabsetPanel의 기능을 이해하세요. tabsetPanel은 탭 기능으로 여러 출력 위젯을 선택적으로 한 화면에 표현해 줍니다.\n\n\nshiny::runExample(\"06_tabsets\")\n\n\n\n\n\n\n",
    "preview": "posts/2022-03-12-userinterface/img/User-Interface.jpeg",
    "last_modified": "2022-03-11T07:39:37+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-11-introduce-shiny/",
    "title": "Shiny 아키텍처 이해",
    "description": "Shiny 아키텍처를 이해합니다. UI, server, 입력 위젯(input widget), 출력 위젯(output widget), 렌더링(Rendering) 정도는 숙지해야 합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-11",
    "categories": [
      "Shiny",
      "Development"
    ],
    "contents": "\n\nContents\nShiny 아키텍처\nUI와 Server의 상호작용\nUI와 server의 상호작용 예시\nUI와 server의 상호작용 표준화\n\nShiny 첫 예제\n히스토그램 그리기 예제\n예제 실행해 보기\n\ntutorial\nShiny 공식 tutorial 페이지\n\n\n\n\n\n\n\n들어가기\n본 핸즈온(튜토리얼)은\nShiny를 이해하려는 대상의 OJT를 염두로\n작성되었습니다. 초심자 대상이기 때문에, Shiny의 ABC만   \n     다룹니다. 그 이상의 학습을 원하는 분들은\nResouces\n페이지를 참고하십시요.\n\n학습 내용은\nShiny in seven lessons\n(\nhttps://shiny.rstudio.com/tutorial/\n) 을 참조하여 작성하였습니다.\n\n\nShiny 아키텍처\nUI와 Server의 상호작용\nUI와 Server의 상호작용UI : 사용자 인터페이스(User Interface) 영역으로 입력 위젯(input widget)을 통해 사용자가 의도하는 파라미터(인수)를 server에 전달합니다.\nserver : 입력 위젯의 변화를 감지하여, 미리 정의된 작업을 수행한 후 그 결과를 출력 위젯(output widget)에 렌더링하여(rendering) 사용자에게 전달합니다.\nUI와 server의 상호작용 예시\nUI와 Server의 상호작용 예시sliderInput 위젯은 사용자 입력을 기다리고,\n입력 위젯(input widget)은 사용자의 입력을 대기\n\n입력을 인지한 후, sliderInput 위젯과 연결된 renderPlot() 함수를 호출\n위젯이 변경되면 변경된 위젯이 포함된 렌더링 함수가 자동 호출됩니다.\n\nrenderPlot() 함수는 sliderInput 위젯 값만큼의 막대가 포함된 히스토그램을 그리고,\n렌더링 함수가 실행됩니다.\n\n히스토그램은 출력 위젯(Output widget)인 plotOutput에 렌더링(그려짐)됩니다.\n출력 위젯에 렌더링된 결과가 반영됩니다.\n\nUI와 server의 상호작용 표준화\nUI와 server의 상호작용을 shiny 스크립트 관점에서 표준화해 봅니다.\n위젯들은 개별 위젯을 인식하도록 아이디를 부여해야 합니다.\n입력 위젯은 inputId,\n출력 위젯은 outputId\n\n입력 위젯의 이름은 “기능명 + Input” 포맷으로 정의됩니다.\nsliderInput = slider + Input,\n즉, 슬라이더로 사용자 입력을 받아서 서버로 전달하는 위젯\n\n출력 위젯의 이름은 “기능명 + Output” 포맷으로 정의됩니다.\nplotOutput = plot + Output,\n즉, 시각화된 R 플롯을 출력하는 위젯\n\n렌더링 함수 이름은 “render + 기능명” 포맷으로 정의됩니다.\nrenderPlot = render + Plot,\n즉, 플롯으로 렌더링하는(플롯을 그리는) 함수\n\nShiny 첫 예제\n히스토그램 그리기 예제\nUI와 Server의 상호작용 예시를 구현한 shiny 프로그램은 다음과 같습니다.\n눈으로 한번 쭉 훑어보세요. 이해할 것 같으면서도 어려운 겁니다.\n\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui <- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver <- function(input, output) {\n\n    output$distPlot <- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    <- faithful[, 2]\n        bins <- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white')\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n예제 실행해 보기\n아키텍처를 이해하려는 목적으로 결과를 보고 프로그램도 살펴 보세요.\n\n\nshiny::runExample(\"01_hello\")\n\n\n\ntutorial\nShiny 공식 tutorial 페이지\nShiny 공식 tutorial 페이지를 살펴보고, 앱을 실행시키는 방법을 숙지하세요.\nhttps://shiny.rstudio.com/tutorial/written-tutorial/lesson1/\n\n\n\n",
    "preview": "posts/2022-03-11-introduce-shiny/img/shiny.png",
    "last_modified": "2022-03-10T07:57:22+09:00",
    "input_file": {},
    "preview_width": 2206,
    "preview_height": 2557
  },
  {
    "path": "posts/2022-03-10-use-distill/",
    "title": "distill 패키지의 활용",
    "description": "distill 패키지를 사용해서 Knowledge Base 구축을 위한 홈페이지를 만듭니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-10",
    "categories": [
      "Reproducible Research",
      "Markdown",
      "distill"
    ],
    "contents": "\n\nContents\n들어가기\ndistill\nR 마크다운 문서의 이해\n웹 사이트 만들기\nTemplate 파일의 이해\n\n설정 최적화하기\n웹 페이지 출력 경로의 변경\n테마 변경하기\nfooter 적용하기\nbibliography 적용하기\n이 페이지의 YAML 헤더\n\nR Markdown의 이해\n\n\n\n\n\n\n들어가기\ndistill은 과학 및 기술문서를 작성하기 위한 R Markdown 포맷입니다.\n\nR Markdown은 재현가능한 연구나 개인과 조직의 Knowledge Base 구축을 위해 유용하게 사용됩니다.\n\n이제 여러분은 R Markdownd 기반의 distill으로 여러분의 저술 활동에 날개를 달게 됩니다.\n\n\n들어가기\n재현가능한 연구(Reproducible Research)라는 용어가 생소할 수 있으나 R world에서는 제법 회자되는 용어입니다. 공개한 연구의 결과물이 재현되고 검증될 수 있도록 하는 것을 의미합니다. 이 방법은 결국 연구가 발전하는 방향으로 재창조되는 결실을 가져옵니다. 이를 위해서는 데이터 분석의 방법을 실험 데이터에 연결하여 재현 검증될 수 있는 체계를 구축해야 합니다.\n재현가능한 연구를 이해하기 쉬운 사례로 설명하겠습니다. 데이터 입출력 및 분석 방법의 로직과 분석 결과를 표현하는 R 스크립트를 R 마크다운 문서에 정리합니다. 물론 연구에 대한 자세한 설명과 결과 해석이 문서에 포함되어 있고, 원시 데이터도 별도로 포함합니다. 이 연구 결과를 RStudio 프로젝트나 R 패키지로 배포하면 누구나 동일한 결과를 재현할 수 있게 됩니다.\n공개한 연구 자료에는 데이터, 분석을 위한 R 코드 및 과정의 설명과 결과의 해석 모두 포함되어야 합니다. 그러므로 재현가능한 연구는 오픈소스 정신이 깃들여 있는 셈입니다.\nCRAN Task View: Reproducible Research 페이지를 보면 R에서 재현가능한 연구를 지원하는 에코 시스템이 어마어마하게 많고 다양한 기능을 가지고 있음에 놀랄 것입니다. 다수의 R 패키지가 여러 기능을 통해서 재현가능한 연구를 지원합니다만, R 마크다운 문서가 정적이거나 동적인 HTML 문서의 보고서로 만들어지거나 혹은 LaTeX을 이용해서 PDF 문서로 만들어지는 것이 눈에 잡히는 재현가능한 연구의 도구입니다.\n광의적으로는 rmarkdown, knitr, blogdown, bookdown, distill도 재현가능한 연구를 지원하는 패키지입니다.\ndistill\ndistill 홈페이지인 https://rstudio.github.io/distill/ [@allaire2018distill]를 방문하면, distill의 기능을 익힐 수 있습니다. 헥스로고는 다음과 같이 물방울 이미지를 포함하고 있습니다.\n다음에 distill의 재현가능한 연구에 최적화된 대표적인 기능을 정리해 봅니다.\nRefrences의 표시\n페이지 하단에 참조물을 표현합니다.\nBibtex, YAML format의 bibliography을 지원합니다.\n\nCitation의 표시\n페이지 하단에 Citation을 표현합니다.\n페이지 YAML 헤더를 읽어 Citation을 만들어줍니다.\n\n이 포스트는 기술적인 노하우나 경험을 담고 있지는 않지만, 이해의 측면에서 distill의 재현가능한 연구에 최적화된 대표적인 기능을 표현해 보았습니다.\nR 마크다운 문서의 이해\n웹 사이트 만들기\ndistill 패키지의 create_website() 함수로 웹 사이트의 골격(skeleton) 만듧니다.\n디렉토리 경로는 “./workshop_lecture”\n웹사이트의 타이틀은 “애플리케이션 서버 구축을 위한 R 워크샾”\n다음 스크립트로 웹 사이트 구축을 위한 골격을 만듧니다.\n\n\ndistill::create_website(\n  dir = \"./workshop_lecture\", \n  title = \"애플리케이션 서버 구축을 위한 R 워크샾\"\n)\n\n\n\nTemplate 파일의 이해\ncreate_website() 함수를 실행하면 다음처럼 몇 개의 파일과 디렉토리가 만들어집니다.\n\n\n\n\n대표적인 파일의 내용은 다음과 같습니다. 이제 여러분은 이 파일을 기초로 해서, 기능을 확장하고 내용을 채워나가야 합니다.\n\n_site.yml\nname: \"workshop_lecture\"\ntitle: \"애플리케이션 서버 구축을 위한 R 워크샾\"\ndescription: |\n  애플리케이션 서버 구축을 위한 R 워크샾\noutput_dir: \"_site\"\nnavbar:\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\noutput: distill::distill_article\n\nindex.Rmd\n---\ntitle: \"애플리케이션 서버 구축을 위한 R 워크샾\"\ndescription: |\n  Welcome to the website. I hope you enjoy it!\nsite: distill::distill_website\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\n\n# Learn more about creating websites with Distill at:\n# https://rstudio.github.io/distill/website.html\n\n```\nabount.Rmd\n---\ntitle: \"About this site\"\ndescription: |\n  Some additional details about the website\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = FALSE)\n```\n\n설정 최적화하기\n기본 템플리트를 사용자 환경에 최적화하는 몇 가지 방법을 다룹니다.\n웹 페이지 출력 경로의 변경\n무료로 정적 웹 페이지를 온라인에 배포하는 대표적인 두 가지 방법이 있습니다.\nnetlify\ngithub\n두 서비스는 각각 서로 다른 웹 페이지 출력 경로의 이름을 사용합니다.\nnetlify는 _site를 사용하고, github는 docs를 사용합니다.\ndistill은 기본 netlify에 서비스를 배포하는 것을 염두에 두고 “_site”를 웹 페이지 출력 경로로 사용합니다. 그러므로 github에 배포하기 위해서는 다음처럼 _site.yml 파일에서 output_dir을 수정합니다.\n\nnetlify\nname: \"workshop_lecture\"\ntitle: \"애플리케이션 서버 구축을 위한 R 워크샾\"\ndescription: |\n  애플리케이션 서버 구축을 위한 R 워크샾\noutput_dir: \"_site\"\nnavbar:\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\noutput: distill::distill_article\n\ngithub\nname: \"workshop_lecture\"\ntitle: \"애플리케이션 서버 구축을 위한 R 워크샾\"\ndescription: |\n  애플리케이션 서버 구축을 위한 R 워크샾\noutput_dir: \"docs\"\nnavbar:\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\noutput: distill::distill_article\n\n\n테마 변경하기\n웹 페이지의 테마(theme)를 변경할 수 있습니다. 본 웹 페이지는 테마를 위한 CSS1 파일로 theme.css 를 만들었습니다.\n이 파일을 적용하기 위해서는 R Markdown 파일인 _site.yml를 변경해야 합니다.\nYAML 헤더는 YAML 이해하기를 참고하세요.\nfooter 적용하기\nfooter는 웹 페이지에 공통적으로 적용되는 페이지 하단의 컨텐츠입니다.\nfooter를 footer.html HTML 파일로 만들어서 _site.yml에 설정합니다.\nbibliography 적용하기\nReference를 정의하기 위한 bibliography는 biblio.bib 파일에 정의하고 *.Rmd의 YAML 헤더에 설정합니다.\n이 페이지의 YAML 헤더\n이 페이지의 YAML 헤더는 다음과 같습니다.\n---\ntitle: \"distill 패키지의 활용\"\ndescription: |\n  distill 패키지를 사용해서 Knowledge Base 구축을 위한 홈페이지를 만듭니다. \nsite: distill::distill_website\noutput:\n  distill::distill_article:\n    toc: true\n    toc_depth: 3  \n    theme: theme.css\n    includes:\n      after_body: footer.html    \nauthor:\n  - name: 유충현 \n    url: https://choonghyunryu.github.io/\n    affiliation: 한화생명\nbibliography: biblio.bib    \ndate: 2022-02-01     \n---\nR Markdown의 이해\nR Markdown을 이해하기 위해서 R Markdown의 이해를 학습하기 바랍니다.\n\n캐스케이딩 스타일 시트(Cascading Style Sheets, CSS)는 마크업 언어가 실제 표시되는 방법을 기술하는 스타일 언어(style sheet language)이다. https://ko.wikipedia.org/wiki/CSS↩︎\n",
    "preview": "posts/2022-03-10-use-distill/img/distill_logo.png",
    "last_modified": "2022-03-09T00:22:19+09:00",
    "input_file": {},
    "preview_width": 240,
    "preview_height": 277
  },
  {
    "path": "posts/2022-03-09-understand-markdown/",
    "title": "마크다운 태그 이해하기",
    "description": "마크다운 태그로 다양한 서식의 문서를 문들 수 있습니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-09",
    "categories": [
      "Reproducible Research",
      "Markdown"
    ],
    "contents": "\n\nContents\nR 마크다운 문서의 이해\nR 마크다운 문서의 분해\n튜토리얼의 범위\n\n마크다운의 이해\n마크다운은 무엇인가요?\n마크다운 태크 예제\n마크다운 에디터\n\n글자 형식 태그의 이해\n볼드체와 이탤릭체\n첨자\n취소문자\n\n문장 서식 태그의 이해\n헤더\n인용구\n글머리 기호와 번호 매기기\n수식 편집\n\n테이블 태그와 수평선의 이해\n테이블 그리기\n수평선 그리기\n\n이미지와 링크 삽입 태그의 이해\n이미지 삽입\n링크 삽입\n\n마크다운 튜토리얼\n글자 형식, 문장 서식 태그 설정하기\n테이블 그리고 이미지 넣기\n\n요약\n핸즈온 요약\nI can do it\n\n\n\n\n\n\n\n들어가기\n이제 당신은 에디터가 되고 작가가 됩니다.\n\n마크다운 태그를 이해하면, 마크다운 조판을 통해 멋진 글을 문서로 만들고 책을 출판할 수도 있습니다.\n\n마크다운은 쉽고 빠릅니다. 물론 학습도 빠르겠지요.\n\n\nR 마크다운 문서의 이해\nR 마크다운 문서의 분해\n앞서 만들었던 첫 R 마크다운 문서를 분해하면 다음과 같습니다.\nR 마크다운 문서의 분해YAML 헤더\nR 마크다운의 헤더 영역으로, 출력 문서를 세부적으로 제어합니다.\n\nR 코드 청크\n수행할 R 코드를 포함한 영역입니다.\n\n마크다운 태그를 포함한 텍스트\n문서의 서식과 텍스트를 정의하는 영역입니다.\n\n튜토리얼의 범위\n이 튜토리얼은 R 마크다운 문서의 분해 중에서의 마크다운 태그를 이해할 목적으로 진행합니다. 동일한 마크다운 문서로 어떻게 서식이 적용된 그럴싸한 문서를 만들 수 있는지, 실습을 통해 편집자가 되어보세요.\n마크다운의 이해\n마크다운은 무엇인가요?\n마크다운(Markdown)은 일반 텍스트 기반의 경량 마크업 언어입니다. 일반 텍스트로 서식이 있는 문서를 작성하는 데 사용됩니다.\n마크다운 태크 예제\n이미 우리는 마크다운 태크를 경험했습니다.\n## R Markdown\n앞에서 우리는 샾(#)을 두개 사용해서 “R Markdown”을 텍스트가 아니라 타이틀로 출력했었습니다.\n마크다운 에디터\n마크다운은 R 마크다운에서만 사용하는 R 전유물이 아닙니다. 많은 영역에서 마크다운을 사용하며, 별도로 마크다운 문서를 작성하거나 결과를 표현해주는 에디터들도 많습니다. R 환경에서도 마크다운을 사용하는 여러 패키지들이 있습니다.\n그런데 마크다운은 마크다운을 인식하는 에디터마다 조금씩 문법이 상이합니다. 또한 관련 패키지마다 세부적으로 문법이 조금씩 차이가 나며, 표현되는 문서도 다소 차이가 있을 수 있습니다.\n\n\n\n\n주의\n\n마크다운은 에디터마다, 또한 R 패키지마다 다소 차이가 있습니다. 이것이 마크다운의 단점 중의 하나입니다. 본 튜토리얼이 다른 마크다운 설명서와 다소 차이가 있을 수 있습니다. distill 패키지로 작성한 본 튜토리얼은 YAML 헤더의 output: 태그에 distill::distill_article을 사용했음을 밝혀둡니다. 그래서 html_document와도 다소 차이가 있을 수 있습니다. 그러나 차이가 미미해서 마크다운을 이해하는 교재로서 큰 문제는 없습니다.\n\nR 마크다운에서 차이가 나는 가장 큰 이유는 테마때문입니다. 패키지마다 테마가 차이가 있어서 몇몇 태그의 표현되는 모습이 다를 수 있습니다. 테마는 사용자가 변경할 수 있으나, 본 튜토리얼의 범위를 넘어 언급하지 않겠습니다.\n\n\n글자 형식 태그의 이해\n볼드체와 이탤릭체\n볼드체는 **나 __를 대상이 될 텍스트의 앞과 뒤에 표시합니다.\n에스터리스크(*)나 언더라인(_) 두개\n\n이탤릭체는 *나 _를 대상이 될 텍스트의 앞과 뒤에 표시합니다.\n에스터리스크(*)나 언더라인(_) 하나\n\n\n볼드체와 이탤릭체를 표현할 때, 언더라인 형식은 사용하지 않는 것이 좋습니다. 경우에 따라서 부작용으로 작동되지 않을 수 있습니다. 이 문서를 만든 distill에서는 동작하지 않았습니다.\n\n\n마크다운 예제\n**마크다운(Markdown)**은 일반 *텍스트 기반*의 경량 **마크업 언어**입니다.\n\n__마크다운(Markdown)__은 일반 _텍스트 기반_의 경량 __마크업 언어__입니다.\n출력 결과\n마크다운(Markdown)은 일반 텍스트 기반의 경량 마크업 언어입니다.\n__마크다운(Markdown)__은 일반 _텍스트 기반_의 경량 __마크업 언어__입니다.\n\n\n첨자\n윗첨자는 ^를 대상이 될 텍스트의 앞과 뒤에 표시합니다.\n아래첨자는 ~를 대상이 될 텍스트의 앞과 뒤에 표시합니다.\n\n\n마크다운 예제\nE = mc^2^에 의하면 에너지는 질량에 비례한다.\n\nY~i~ = X~i~ + 3 \n출력 결과\nE = mc2에 의하면 에너지는 질량에 비례한다.\nYi = Xi + 3\n\n\n취소문자\n취소문자는 ~ 두개를 대상이 될 텍스트의 앞과 뒤에 표시합니다.\n\n\n마크다운 예제\n~~하루에 30분 달리기~~\n일주일에 ~~5번~~ 산책하기\n\n출력 결과\n하루에 30분 달리기\n일주일에 5번 산책하기\n\n\n문장 서식 태그의 이해\n헤더\n헤더는 주제목 및 부제목 등 제목을 설정할 때 사용합니다. 한글 문서에서의 장, 절 등을, 영문 문서에는 chapter, subject, sub-subject 등을 구분하기 위한 조판 형식입니다.\n문서를 주제별로 계층적인 구조로 나누는 분류 체계라 할 수 있습니다.\n마크다운에서는 헤더(header)라고 부릅니다. header 1 부터 header 6까지 있으며, 숫자가 작을 수록 글자의 크기가 큰 헤더가 됩니다. 헤더를 조판하기 위해서는 숫자에 해당하는 개수만큼의 ’#’을 사용합니다.\nR 마크다운에서는 통상적으로 제일 큰 타이틀인 chapter(장)에 해당하는 헤더에 header 2를 사용합니다. 즉 ’##’를 사용합니다.\n\n본 튜토리얼에서는 몇몇 개의 헤더에 CSS로 서식을 변경했습니다. 모양과 색상 및 대소문자의 형식이 다를 수 있으니, 글자의 크기만 비교하세요.\n\n\n\n마크다운 예제\n# Header 1\n## Header 2\n### Header 3\n#### Header 4\n##### Header 5\n###### Header 6\n\n출력 결과\nHeader 1\nHeader 2\nHeader 3\nHeader 4\nHeader 5\nHeader 6\n\n\n\n인용구\n인용구는 블록 쿼트 (blockquote)로 부르며, 인용문을 별도로 강조하기 위해서 사용합니다.\n마크다운에서 인용구는 ‘>’ 문자로 정의합니다.\n\n\n마크다운 예제\n**R 마크다운의 의의**는 무엇일까요?\n> \"R 마크다운은 재현 가능한 연구를 지원하는 훌륭한 솔루션입니다.\" \n\n출력 결과\nR 마크다운의 의의는 무엇일까요?\n\n“R 마크다운은 재현 가능한 연구를 지원하는 훌륭한 솔루션입니다.”\n\n\n\n글머리 기호와 번호 매기기\n목록을 표현하기 위한 글머리 문자는 불릿(bullet)이라 부르며, 주목도를 높이기 위해서 텍스트 앞에 삽입하는 기호입니다. 기호 대신에 숫자를 넣는 방법도 있습니다. 이 경우에는 순서의 의미가 부여되기도 합니다.\n글머리 문자는 라인의 맨 앞에 ’*’를 삽입하고 텍스트를 기술합니다. 엔터로 줄바꿈하여, 둘째, 셋째 글머리를 기술합니다.\n기호 대신에 숫자를 넣는 번호 매기기는 ’*’ 대신에 숫자 + “.”로 표현합니다. 숫자는 순서에 맞게 1, 2, 3과 같이 실제 순번을 기술할 수도 있고 임의의 한 숫자로 동일하게 사용할 수도 있습니다. 중요한 것은 번호는 첫째 글머리에 기술한 숫자부터 시작해서 1씩 증가한다는 점입니다. 그래서 통상적으로 임의의 한 숫자로 1을 사용합니다.\n\n\n마크다운 예제\nR 마크다운의 기술적 요소는 다음과 같음:\n\n* knitr\n* markdown\n* pandoc\n\n출력 결과\nR 마크다운의 기술적 요소는 다음과 같음:\nknitr\nmarkdown\npandoc\n\n\n\n마크다운 예제\nR 마크다운 문서를 작성하는 방법은 다음과 같음:\n\n1. 새파일 작성 메뉴를 이용해서 R 마크다운 템플리트를 생성한다.\n1. R 코드 청크에 R 코드를 작성한다.\n1. 코드의 설명과 실행 결과의 설명을 마크다운 태그를 이용해서 작성한다.\n1. YAML 헤더를 수정하여 생성한 조판 서식을 조정한다.\n1. \"knit\"를 눌러 문서를 생성한다. \n\n출력 결과\nR 마크다운 문서를 작성하는 방법은 다음과 같음:\n새파일 작성 메뉴를 이용해서 R 마크다운 템플리트를 생성한다.\nR 코드 청크에 R 코드를 작성한다.\n코드의 설명과 실행 결과의 설명을 마크다운 태그를 이용해서 작성한다.\nYAML 헤더를 수정하여 생성한 조판 서식을 조정한다.\n“knit”를 눌러 문서를 생성한다.\n\n\n불릿이 있다면, 서브 불릿(sub bullet)이 있겠죠. 서브 불릿은 탭이나 스페이스 두어개로 들여쓰기하면 됩니다. 서브 불릿으로는 ’*’ 대신에 ’+’를 사용합니다.\n\n마크다운 예제\nR 마크다운의 기술적 요소는 다음과 같음:\n\n* knitr\n  + 청크를 실행함\n    + R 소스 청크 외에 bash, python 청크도 가능\n* markdown\n  + 마크다운 문서를 조판함\n* pandoc\n  + PDF나 HTML 등 다른 문서로 변환함\n\n출력 결과\nR 마크다운의 기술적 요소는 다음과 같음:\nknitr\n청크를 실행함\nR 소스 청크 외에 bash, python 청크도 가능\n\n\nmarkdown\n마크다운 문서를 조판함\n\npandoc\nPDF나 HTML 등 다른 문서로 변환함\n\n\n\n번호 매기기도 같은 방법입니다.\n\n마크다운 예제\nR 마크다운 문서를 작성하는 방법은 다음과 같음:\n\n1. 새파일 작성 메뉴를 이용해서 R 마크다운 템플리트를 생성한다.\n    1. 에디터에서 그냥 작성해도 무방하다.\n1. R 코드 청크에 R 코드를 작성한다.\n    1. 코드청크 삽입 메뉴 아이콘을 누른다.\n    1. 옵션을 추가하거나 변경합니다.\n1. 코드의 설명과 실행 결과의 설명을 마크다운 태그를 이용해서 작성한다.\n    1. 마크 다운이 아닌 텍스트만 사용해도 무방하다.\n    1. 가급적 친절하게 자세히 기술한다.\n1. YAML 헤더를 수정하여 생성한 파과 조판 서식을 조정한다.\n1. \"knit\"를 눌러 문서를 생성한다. \n\n출력 결과\nR 마크다운 문서를 작성하는 방법은 다음과 같음:\n새파일 작성 메뉴를 이용해서 R 마크다운 템플리트를 생성한다.\n에디터에서 그냥 작성해도 무방하다.\n\nR 코드 청크에 R 코드를 작성한다.\n코드청크 삽입 메뉴 아이콘을 누른다.\n옵션을 추가하거나 변경합니다.\n\n코드의 설명과 실행 결과의 설명을 마크다운 태그를 이용해서 작성한다.\n마크 다운이 아닌 텍스트만 사용해도 무방하다.\n가급적 친절하게 자세히 기술한다.\n\nYAML 헤더를 수정하여 생성한 파과 조판 서식을 조정한다.\n“knit”를 눌러 문서를 생성한다.\n\n\n수식 편집\nLaTeX에서 지원하는 수식의 출력을 지원합니다. ’$’로 수식의 시작과 종료를 알려줘야 합니다.\nLaTeX의 수식 태그를 모를 경우 위키백과:TeX 문법을 참고해서 작성합니다.\n\n\n마크다운 예제\n$E = m \\times c^{2}$에 의하면 에너지는 질량에 비례한다.\n\n$\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}$\n\n출력 결과\n\\(E = m \\times c^{2}\\)에 의하면 에너지는 질량에 비례한다.\n\\(\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\\)\n\n\n테이블 태그와 수평선의 이해\n테이블 그리기\n도표(테이블)을 그리는 것은 다소 성가십니다. 텍스트로 도표의 모양을 그려줘야 하기 때문입니다.\n다음의 형식으로 도표를 그립니다.\nFirst Header  | Second Header\n--------------|-------------\nCell 1-1      | Cell 1-2\nCell 2-1      | Cell 2-2 \n파이프 문자 ’|’는 세로 줄을 의미합니다. 컬럼을 구분하는 구분자로 위 형식에서는 두 컬럼을 분리했습니다.\n\n\n마크다운 예제\nFirst Header  | Second Header\n--------------|-------------\nCell 1-1      | Cell 1-2\nCell 2-1      | Cell 2-2 \n출력 결과\nFirst Header\nSecond Header\nCell 1-1\nCell 1-2\nCell 2-1\nCell 2-2\n\n\n컬럼의 문자열을 정렬할 수 도 있습니다.\n기본값은 좌측 정렬이며, 정렬을 위해서는 헤더를 구분하는 하이픈(-)에 콜론(:)을 사용합니다. 다음의 사례는 첫 컬럼부터 각각 우측 정렬, 좌측 정렬, 가운데 정렬을 의미합니다. 콜론의 위치를 주의깊게 살펴 보시기 바랍니다.\nFirst Header  | Second Header | Third Header\n-------------:|:--------------|:------------:\nCell 1-1      | Cell 1-2      | Cell 1-3  \nCell 2-1      | Cell 2-2      | Cell 2-3  \n\n\n마크다운 예제\nFirst Header  | Second Header | Third Header\n--------------|---------------|--------------\nCell 1-1      | Cell 1-2      | Cell 1-3  \nCell 2-1      | Cell 2-2      | Cell 2-3  \n\n사용자가 정렬 변경하기\n\n우측 정렬     | 좌측 정렬     | 가운데 정렬\n-------------:|:--------------|:------------:\nCell 1-1      | Cell 1-2      | Cell 1-3  \nCell 2-1      | Cell 2-2      | Cell 2-3  \n\n출력 결과\nFirst Header\nSecond Header\nThird Header\nCell 1-1\nCell 1-2\nCell 1-3\nCell 2-1\nCell 2-2\nCell 2-3\n사용자가 정렬 변경하기\n우측 정렬\n좌측 정렬\n가운데 정렬\nCell 1-1\nCell 1-2\nCell 1-3\nCell 2-1\nCell 2-2\nCell 2-3\n\n\n수평선 그리기\n경우에 따라서 수평선을 그려 앞 부분과 뒷 부분을 명확이 구분할 필요가 있습니다.\n아스터리스크(*)이나 하이픈(-) 문자를 3개 이상을 연속으로 기술하면 수평선이 만들어 집니다.\n***\n---\n\n\n마크다운 예제\n앞 문장\n\n***\n\n가운데 문장\n\n------\n\n뒷 문장\n\n출력 결과\n앞 문장\n가운데 문장\n뒷 문장\n\n\n이미지와 링크 삽입 태그의 이해\n이미지 삽입\n\n\n\n\n주의\n\n마크다운의 이미지 삽입 태그는 마크다운 에디터별로 차이가 많습니다. 특히 마크다운 문법 중 이미지 옵션에 관련해서는 R 마크다운에서 적용되지 않는 경우가 많습니다.\n\n\n\n이미지 삽입 태그는 다음 문법을 따릅니다.\n![이미지 캡션](이미지 파일의 경로)\n만약 이미지의 캡션을 사용하지 않을 경우에는\n![](이미지 파일의 경로)\n와 같이 캡션을 넣지 않습니다.\n\n\n마크다운 예제\nimg 디렉토리에 markdwon.png 파일이 위치합니다. 이 파일은 마크다운의 로고입니다. 이 파일을 문서에 포함하기 위해서는 다음 태그를 사용합니다.\n\n![마크다운 로고](img/markdown.png)\n출력 결과\n마크다운 로고마크다운 로고 이미지가 제법 큽니다. 깜짝 놀랐습니다. 이미 파일에 저장된 원 사이즈대로 출력되었습니다. 조금 줄여볼까요?\n\n\n이미지 사이즈 조절\n이미지의 사이즈를 조절하기 위해서는 다음의 옵션을 사용합니다.\n![마크다운 로고](이미지 파일의 경로){: width=\"\" height=\"\"}\n\nwidth와 height에 너비와 높이를 지정합니다.\n너비와 높이는 두 가지 포맷을 지원합니다.\n픽셀을 의미하는 ’px’를 붙이거나 생략해서 필셀 단위의 기술\n“100px” 혹은 “100”\n\n차지할 수 있는 전체 범위를 100%로 감안하여 백분율을 기술\n“50%”\n\n\n마크다운 예제\n너비와 높이를 각각 100 픽셀의 크기로 이미지 사이즈를 조정합니다.\n![마크다운 로고](이미지 파일의 경로){: width=\"100\" height=\"100\"}\n출력 결과\n{: width=“100” height=“100”}\n옵션이 적용되지 않았습니다. 당연히 옵션에 대한 태그가 해석되지 않았기에 화면에 출력됩니다. 그런데, 왜 이미지 사이즈가 작아졌을까요?\n표현할 화면에 이미지와 태그가 한 줄에 출력되어야해서 이미지가 태그 텍스트의 범위만큼 줄어든 것입니다.\n마크다운 문법의 호환성 불일치가 발생하였습니다.\n\n\n\n부작용 없이 사이즈를 조절하기 위해서는 마크다운 태그가 아닌, 다음의 HTML 태그를 사용합니다.\n\n<img src=\"이미지 파일의 경로\" width=\"\" height=\"\">\n\n다시 너비와 높이를 각각 100 픽셀의 크기로 이미지 사이지를 조정합니다. 로고가 길죽한 모양이기 때문에, 이 옵션으로는 이상한 모양의 이미지가 출력될 것입니다.\n\n마크다운 예제\n<img src=\"img/markdown.png\" width=\"100\" height=\"100\">\n출력 결과\n\n\n\n궁극의 방법을 소개합니다. R 소스 청크를 이용하는 방법입니다. 그러나 이 방법은 아쉽게도 캡션을 사용하면 충돌이 발생합니다.\n\n\n마크다운 예제\n너비와 높이 비율을 고정하고 크기를 화면 영역의 50%로 사이즈를 조정합니다. 또한 이미지를 가운데 정렬합니다.\n```{r, echo=FALSE, out.width = \"50%\", fig.align=\"center\"}\nknitr::include_graphics(\"img/markdown.png\")\n```\n출력 결과\n\n\n\n\n링크 삽입\n링크 삽입 태그는 다음 이미지 삽입 태그와 문법이 유사합니다.\n[URL을 링크 걸 텍스트](링크 걸 URL)\n\n\n마크다운 예제\n[R Markdown 홈페이지](https://rmarkdown.rstudio.com/)\n출력 결과\nR Markdown 홈페이지\n\n\n마크다운 튜토리얼\n글자 형식, 문장 서식 태그 설정하기\n1. 다음 마크다운 출력물을 만들 마크다운 문서를 작성해 보세요.\n\n위키백과 마크다운 페이지를 일부 차용했습니다.\n서식은 임의로 편집하였습니다.\n\n\n\n\n힌트 보기\n제목 헤더는 header 2를 적용했습니다.\n강조문자 (볼드체)를 몇 개 사용했습니다.\n인용구를 사용했습니다.\n이미지의 사이즈는 30%이며, 좌측정렬을 수행했습니다.\n모범 답안\n## 마크다운\n\n```{r, echo=FALSE, out.width = \"30%\", fig.align=\"left\"}\nknitr::include_graphics(\"img/markdown.png\")\n```\n\n**마크다운(markdown)**은 일반 텍스트 기반의 경량 마크업 언어다. 일반 텍스트로 서식이 있는 문서를 작성하는 데 사용되며, \n일반 마크업 언어에 비해 문법이 쉽고 간단한 것이 특징이다. **HTML**과 **리치 텍스트(RTF)** 등 서식 문서로 쉽게 변환되기 \n때문에 응용 소프트웨어와 함께 배포되는 README 파일이나 온라인 게시물 등에 많이 사용된다.\n\n> 마크다운은 일반 텍스트 기반의 경량 마크업 언어다. - 위키피디아\n\n## 역사\n존 그루버는 2004년에 문법 면에서 에런 스워츠와 중대한 협업을 통해 마크다운 언어를 만들었으며, \n사람들이 읽기 쉽고 쓰기 쉬운 플레인 텍스트 포맷을 사용하여 쓸 수 있으면서 구조적으로 유효한 \nXHTML(또는 HTML)로 선택적 변환이 가능하게 하는 것이 목표이다.\n\n\n테이블 그리고 이미지 넣기\n2. 다음 테이블을 만들어 보세요\n\n이미지는 다음 링크에서 다운로드 하세요.\nknitr.png\nrmarkdown.png\nmarkdown.png\n\n테이블 안의 이미지는 사이즈를 조절하지 않습니다.\n\n\nknitr\nrmarkdown\nmarkdown\n\n\n\nR 소스 청크를 해석\nYAML 헤더를 해석하고 문서 생성\n텍스트에 서식을 부여, 문서 조판\nR 패키지\n청크 실행\n\nR 패키지\nPandoc 연동\n\nR 패키지 아님\n문서 조판\n\n\n\n힌트 보기\n이미지는 다음 태그로 출력합니다.\n![](이미지 파일의 경로)\n표 안에 머리글 기호를 넣습니다.\n모범 답안\n+---------------------+--------------------------------+---------------------------------+\n| knitr               | rmarkdown                      | markdown                        |\n+=====================+================================+=================================+\n| ![](img/knitr.png)  | ![](img/rmarkdown.png)         | ![](img/markdown.png)           |\n+---------------------+--------------------------------+---------------------------------+\n| R 소스 청크를 해석      | YAML 헤더를 해석하고 문서 생성       | 텍스트에 서식을 부여, 문서 조판        |\n+---------------------+--------------------------------+---------------------------------+\n| * R 패키지            | * R 패키지                      | * R 패키지 아님                    |\n| * 청크 실행           | * Pandoc 연동                   | * 문서 조판                       |\n+---------------------+--------------------------------+---------------------------------+\n\n\n요약\n핸즈온 요약\n마크다운 글자 형식 태그와 문장 서식 태그를 이해했습니다.\n마크다운으로 테이블을 그렸습니다.\n마크다운 문서에 이미지를 삽입하고 링크를 걸었습니다.\nI can do it\n마크다운 문서를 작성할 수 있어, R 마크다운 문서에 여러 서식을 지정할 수 있습니다.\n\n\n\n",
    "preview": "posts/2022-03-09-understand-markdown/img/markdown.png",
    "last_modified": "2022-03-08T00:13:36+09:00",
    "input_file": {},
    "preview_width": 300,
    "preview_height": 185
  },
  {
    "path": "posts/2022-03-08-rmarkdown-yaml/",
    "title": "YAML 이해하기",
    "description": "YMAL 헤더의 변경으로 문서의 형식과 모양이 바꾸는 것을 이해합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-08",
    "categories": [
      "Reproducible Research",
      "R Markdown"
    ],
    "contents": "\n\nContents\nR 마크다운 문서의 이해\nR 마크다운 문서의 분해\n튜토리얼의 범위\n\nYAML 헤더의 이해\nYAML 헤더는 무엇인가요?\nYAML 헤더 예제\n\n문서파일 형식 변경\n핸즈온 파일 준비하기\nMS 워드파일 생성하기\nPDF 파일 생성하기\n\n문서 서식 변경\n목차 넣기\n목차에 순번 넣기\n\n요약\n핸즈온 요약\nI can do it\n\n\n\n\n\n\n\n들어가기\n문서 파일의 형식을 바꾸거나 전체 페이지의 테마를 바꾸고 싶다구요?!\n\nYAML로 문서 형식을 바꾸고 테마를 변경해 보세요. 그리고 덤으로 R 코드 실행을 위한 파라미터 값 설정으로 같은 포맷 다른 결과의 문서를 만들 수 있습니다.\n\n\nR 마크다운 문서의 이해\nR 마크다운 문서의 분해\n앞서 만들었던 첫 R 마크다운 문서를 분해하면 다음과 같습니다.\nR 마크다운 문서의 분해YAML 헤더\nR 마크다운의 헤더 영역으로, 출력 문서를 세부적으로 제어합니다.\n\nR 코드 청크\n수행할 R 코드를 포함한 영역입니다.\n\n마크다운 태그를 포함한 텍스트\n문서의 서식과 텍스트를 정의하는 영역입니다.\n\n튜토리얼의 범위\n이 튜토리얼은 R 마크다운 문서의 분해 중에서의 YAML 헤더를 이해할 목적으로 진행합니다. 동일한 R 마크다운 문서로 어떻게 이질적인 형식의 파일과 모양을 빚어낼 수 있는지를 경험해 보세요.\nYAML 헤더의 이해\nYAML 헤더는 무엇인가요?\nYAML는 ’또 다른 마크업 언어(Yet Another Markup Language)’를 의미합니다. 복잡하고 어렵게 생각하지 마시고 생성할 문서 전체의 형식을 정의하는 헤더라고 이해하면 됩니다.\n헤더라는 의미는 R 마크다운 문서의 맨 앞에 위치한다는 것을 의미합니다. 그리고 YAML라는 것을 알려주기 위해서 청크는, “---”으로 시작해서 “---”로 끝납니다.\nYAML 헤더 예제\n이미 우리는 YAML 헤더를 경험했습니다.\n---\ntitle: \"첫 R 마크다운\"\nauthor: \"홍길동\"\ndate: \"10/31/2021\"\noutput: html_document\n---\nYAML 헤더에서 옵션 이름과 옵션 값은 콜론(:)으로 구분합니다. 한 줄에 한 쌍의 옵션 이름과 옵션 값이 올 수 있습니다.\ntitle: “첫 R 마크다운”은 문서 제목(타이틀)이 “첫 R 마크다운”임을 의미합니다. 그리고 output: html_document은 생성할 문서 파일이 HTML, 즉 웹 문서임을 rmarkdown 패키지에게 알려줍니다. 그러면 rmarkdown 패키지가 pandoc을 호출하서 웹 문서를 생성하게 됩니다.\n어떤 옵션은 서브 옵션을 포함하기도 합니다. 대표적인 것이 output 옵션입니다. 서브 옵션은 옵션 다음 줄에 기술해야 하며, 들어쓰기를 하여 자식 옵션임을 알려 주는 것이 헤더를 이해하는데 용이한, 암묵적인 룰입니다.\n다음과 같이 기술하는 것을 권장합니다.\noutput:\n  html_document:\n    toc: true\n문서파일 형식 변경\n핸즈온 파일 준비하기\n“첫 R 마크다운 문서 만들기” 핸즈온에서 생성한 “first_rmarkdown.Rmd” 파일을 이용하여 핸즈온을 진행하려 합니다.\n만약 파일이 없다면,\n핸즈온 튜토리얼을 수행하기 위해서 아래 “YAML 이해하기 튜토리얼 파일” 링크의 R 마크다운 파일을 다운로드합니다.\n링크에 마우스를 올려, 오른쪽 버튼으로 “다른이름으로 파일저장”을 실행합니다.\n\n다운로드한 파일을 RStudio로 읽어들입니다.\nYAML 이해하기 튜토리얼 파일\nMS 워드파일 생성하기\n1. MS 워드파일 생성하는 YAML 편집하기\n\nfirst_rmarkdown.Rmd은 HTML 문서를 생성합니다.\nYAML을 수정해서 MS 워드 파일을 생성하세요.\n’first_rmarkdown.Rmd’의 YAML을 변경 후 실행해 보세요.\n\n\n\n힌트 보기\nMS 워드파일을 생성하는 몇 가지의 방법이 있습니다.\n새 파일 만들기 아이콘 클릭 > New R Markdown 다이얼로그에서\n“Default Output Format:”의 Word 레디오 버튼 클릭하여 템플리트 생성/편집 후\nknit 아이콘을 클릭하여 워드파일 생성\n\nknit 아이콘을 마우스 오론쪽 클릭 후\n“Knit to Word”를 선택하여 워드파일 생성\n\nYAML 헤더를 수정하여 워드파일 생성\n“output:”의 값을 수정 \n\n3번 방법을 사용하세요.\n모범 답안\n---\ntitle: \"첫 R 마크다운\"\nauthor: \"홍길동\"\ndate: \"10/31/2021\"\noutput: word_document\n---\n\n\n생성한 워드파일은 다음과 같은 2페이지를 포함합니다.\n\n\n\n\nPDF 파일 생성하기\nR 마크다운으로 PDF 파일을 생성할 때, knitr은 LaTeX을 만드는 것은 이미 앞에서 다루었습니다. 그런데 PDF 파일을 생성할 때, 한가지 이슈가 발생합니다. 한글이 포함된 R 마크다운 문서의 경우는 다음과 같은 오류가 발생합니다.\n! Package inputenc Error: Unicode character 홍 (U+D64D)\n(inputenc)                not set up for use with LaTeX.\n\n이것은 LaTeX이 한글을 처리하지 못해서 발생하는 LaTeX 에러입니다.\n한글을 처리하기 위해서는 LaTeX 패키지인 kotex을 사용해야 합니다. 그리고 YAML 헤더에서, LaTeX이 kotex을 사용하도록 설정해 주어야 합니다.\n2. PDF 파일 생성하는 YAML 편집하기\n\nfirst_rmarkdown.Rmd은 HTML 문서를 생성합니다.\nYAML 헤더를 수정해서 PDF 파일을 생성하세요.\n’first_rmarkdown.Rmd’의 YAML을 변경 후 실행해 보세요.\n힌트를 참고해서 kotex을 사용하도록 설정하세요.\n\n\n\n힌트 보기\n“output:” 옵션으로 “pdf_document:”을 사용하세요.\n“pdf_document:” 옵션의 서브옵션 “includes:”에 LaTeX 헤더 파일을 설정하세요.\nLaTeX 헤더 파일은 “in_header:” 옵션으로 설정합니다.\n옵션 값에 “use_korean.tex” 파일을 기술하세요.\n\nYAML 헤더를 수정하여 워드파일 생성\n“output:”의 값을 수정 \n\n“use_korean.tex” 파일은 다음의 내용을 담고 있습니다. 직접 파일을 만들거나, 아래 경로에서 다운로드 받을 수 있습니다.\nuse_korean.tex 파일\n\\usepackage[hangul]{kotex}\n모범 답안\n---\ntitle: \"첫 R 마크다운\"\nauthor: \"홍길동\"\ndate: \"10/31/2021\"\noutput:\n  pdf_document:\n    includes:\n      in_header: use_korean.tex\n---\n\n\n생성한 PDF 파일은 다음과 같은 2페이지를 포함합니다.\n\n\n\n\n문서 서식 변경\n목차 넣기\n하나의 파일에 여러 페이지 분량의 컨텐츠를 담을 수 있는 MS 워드나 PDF는 문서의 앞 부분에 목차(toc, Table of Contents)를 넣을 수 있습니다. 목차는 전체 문서의 구성을 이해하는 데 용이할 뿐아니라 링크를 통해서 해당 빠르게 이동할 수 있는 장점이 있습니다.\n3. 문서에 목차를 포함하는 YAML 편집하기\n먼저 아래 경로에서 “second_rmarkdown.Rmd” 파일을 다운로드 받으십시요.\nsecond_rmarkdown.Rmd 파일\n\nsecond_rmarkdown.Rmd을 다운로드합니다.\n목차를 넣도록 옵션을 추가합니다.\ntoc 옵션을 사용합니다.\n\n목차는 4단계까지의 타이틀을 목차에 포함하십시요.\ntoc_depth 옵션을 사용합니다.\n\n목차에 넣을 타이틀 단계를 변경해가면서 타이틀의 모양을 비교해보십시요.\n\n\n힌트 보기\ntoc 옵션의 값은 논리값을 사용하는데, 대소문자를 구별하지 않습니다.\ntrue, True, TRUE, flase, False, FALSE로 표현이 가능합니다.\n\n본 튜토리얼의 모든 R 마크다운 문서는 가장 큰 타이들을 두번째 타이틀인 ##를 사용합니다.\n첫번째 타이틀인 #의 타이틀 제목이 필요 이상으로 커서 두번째 타이틀인 ##를 기준으로 사용합니다.\n그러므로 ##의 depth는 2입니다.\n\n모범 답안\n---\ntitle: \"두번째 R 마크다운\"\nauthor: \"홍길동\"\ndate: \"10/31/2021\"\noutput:\n  word_document:\n    toc: true\n    toc_depth: 4\n---\n\n\n생성한 워드 파일은 다음과 같이 목차를 포함합니다.\n\n\n\n\n목차에 순번 넣기\n목차에 순번을 넣을 수 있습니다.\n4. 문서에 순번이 있는 목차를 포함하는 YAML 편집하기\n\nsecond_rmarkdown.Rmd를 사용합니다.\n목차를 넣도록 옵션을 추가합니다.\ntoc 옵션을 사용합니다.\n\n목차는 3단계까지의 타이틀을 목차에 포함하십시요.\ntoc_depth 옵션을 사용합니다.\n\n타이틀과 목차에 순번을 넣으십시요.\nnumber_sections 옵션을 사용합니다.\n\n\n\n힌트 보기\nnumber_sections 옵션의 값은 논리값도 대소문자를 구별하지 않습니다.\n모범 답안\n---\ntitle: \"두번째 R 마크다운\"\nauthor: \"홍길동\"\ndate: \"10/31/2021\"\noutput:\n  word_document:\n    toc: true\n    toc_depth: 3\n    number_sections: true\n---\n\n\n생성한 워드 파일은 다음과 같이 목차를 포함합니다.\n\n\n\n\n요약\n핸즈온 요약\nR 마크다운 문서로 PDF 파일의 문서를 작성했습니다.\nR 마크다운 문서로 워드 파일의 문서를 작성했습니다.\nR 마크다운 문서에 목차를 삽입하고, 그 모양을 바꾸어 보았습니다.\nI can do it\nYAML 헤더를 사용하며, 생성하는 파일의 포맷을 바꾸고 목차 모양을 원하는대로 조정할 수 있습니다.\n\n\n\n",
    "preview": "posts/2022-03-08-rmarkdown-yaml/img/rmarkdown.png",
    "last_modified": "2022-03-07T00:18:44+09:00",
    "input_file": {},
    "preview_width": 240,
    "preview_height": 277
  },
  {
    "path": "posts/2022-03-07-chunk-knitr/",
    "title": "청크 옵션 이해하기",
    "description": "knitr R 코드 청크의 옵션을 활용하는 방법을 익힙니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-07",
    "categories": [
      "Reproducible Research",
      "R Markdown",
      "knitr"
    ],
    "contents": "\n\nContents\nR 마크다운 문서의 이해\nR 마크다운 문서의 분해\n튜토리얼의 범위\n\nknitr의 R 코드 청크\n청크의 구조 이해하기\nknitr의 R 코드 청크 옵션\n\n핸즈온 준비하기\n핸즈온 파일 다운로드하기\n\n핸즈온\n1. R 코드 출력 제어하기\n2. 플롯 출력하기\n\n요약\n핸즈온 요약\nI can do it\n\n\n\n\n\n\n\n들어가기\nRStudio의 R 마크다운은 기본적으로 knitr 코드 청크를 사용합니다.\n\n청크 옵션을 아는만큼 문서의 품질이 향상됩니다. 대표적인 청크 옵션을 사용하는 방법을 익혀서 여러분의 문서에 날개를 달아주기 바랍니다.\n\n\nR 마크다운 문서의 이해\nR 마크다운 문서의 분해\n앞서 만들었던 첫 R 마크다운 문서를 분해하면 다음과 같습니다.\nR 마크다운 문서의 분해YAML 헤더\nR 마크다운의 헤더 영역으로, 출력 문서를 세부적으로 제어합니다.\n\nR 코드 청크\n수행할 R 코드를 포함한 영역입니다.\n\n마크다운 태그를 포함한 텍스트\n문서의 서식과 텍스트를 정의하는 영역입니다.\n\n튜토리얼의 범위\n이 튜토리얼은 R 마크다운 문서의 분해 중에서의 knitr의 R 코드 청크 옵션을 이해할 목적으로 진행합니다. 대표적인 청크 옵션을 이해하고, 다루지 않는 다른 옵션의 사용을 시도할 수 있는 자신감 함양을 유도합니다.\nknitr의 R 코드 청크\n청크의 구조 이해하기\n우리는 지금 R 코드 청크를 다루고 있습니다. 그런데 knitr은 bash sehll(리눅스 쉘 스크립트), python(파이썬) 코드 청크 외에 몇개의 코드 청크를 지원합니다.\n예를 들어 R 마크다운에서 리눅스 쉘 스크립트 청크를 사용하려면 다음과 같이 기술합니다. 이것은 리눅스에서 날짜를 조회하는 명령어인 date를 수행한 후 그 결과를 마크다운 문서에 삽입합니다.\n\nbash 코드청크\n```{bash}\n# 날짜와 일시 출력\ndate\n\n# 'YYYY-MM-DD' 포맷의 날짜 출력\ndate +'%Y-%m-%d'\n```\n수행결과\n\n# 날짜와 일시 출력\ndate\n\n# 'YYYY-MM-DD' 포맷의 날짜 출력\ndate +'%Y-%m-%d'\n## Sun Mar  6 14:40:22 KST 2022\n## 2022-03-06\n\n\n브레이스({) 뒤에 오는 ’bash’는 bash 쉘이 코드 청크를 실행하라는 의미입니다. 그러면, R에게 실행을 요청하기 위한 청크는 다음과 같이 사용합니다. 당연히 브레이스 뒤에 ’r’이 따라옵니다.\n\n\n```{r 청크이름, 옵션이름=옵션값, 옵션이름=옵션값, ...}\nR 코드 삽입 영역\n```\n\n\n청크 이름은 청크를 식별하는 식별자입니다.\n청크 이름을 기술하지 않아도 청크는 정상적으로 수행됩니다.\nR 마크다운 컴파일의 진행경과\n오류가 발생하는 청크의 인식 등을 위해 기술하는 것이 좋습니다.\n\n옵션이름 = 옵션값 형식으로 옵션을 기술합니다.\n옵션을 기술하지 않아도 됩니다.\n이 경우에는 옵션의 기본 설정값이 적용됩니다.\n\nknitr의 R 코드 청크 옵션\nknitr 홈페이지의 https://yihui.org/knitr/options/ 페이지에는 청크 옵션에 대해서 잘 설명되어 있습니다.\n다음은 대표적인 청크 옵션의 목록입니다. 이 옵션들은 반드시 숙지하시기 바랍니다.\n코드와 결과 출력 관련 청크 옵션\n튜토리얼에서 익혀야 할 출력 관련 대표적인 청크 옵션입니다.\nR 코드와 결과 출력 관련 청크 옵션 목록\n옵션\n기본값\n기능\neval\nTRUE\n청크를 실행하고, 그 결과를 삽입하는 여부 설정\necho\nTRUE\n실행한 명령어도 함께 출력하는지의 여부 설정\nwarning\nTRUE\n경고 메시지의 출력 여부 설정\nerror\nFALSE\n에러 메시지의 출력 여부 설정\nmessage\nTRUE\n경고, 에러 외의 메시지의 출력 여부 설정\ntidy\nFALSE\nR 코드를 깔끔하게 정돈해서 출력할지의 여부 설정\ncomment\n“##”\n실행 결과 출력 각 라인의 앞에 넣을 prefix\n시각화 관련 청크 옵션\n튜토리얼에서 익혀야 할 시각화 관련 대표적인 청크 옵션입니다.\nR 코드와 결과 시각화 관련 청크 옵션 목록\n옵션\n기본값\n기능\nfig.width\n7\n출력을 위해 생성할 이미지 파일의 너비, 단위: 인치.\nfig.height\n7\n출력을 위해 생성할 이미지 파일의 높이, 단위: 인치.\nfig.align\n“default”\n플롯의 정렬 방법. \"left\", \"right\", \"center\"에서 선택\nfig.path\n‘figure/’\n시각화 이미지 파일을 저장할 디렉토리 경로\nfig.cap\nNULL\n플롯의 캡션 문자 정의\nout.width\n\n시각화가 화면에 출력되는 너비, 예) “75%”, “300px”\n핸즈온 준비하기\n핸즈온 파일 다운로드하기\n핸즈온 튜토리얼을 수행하기 위해서 아래 “청크 옵션 이해하기 튜토리얼 파일” 링크의 R 마크다운 파일을 다운로드합니다.\n링크에 마우스를 올려, 오른쪽 버튼으로 “다른이름으로 파일저장”을 실행합니다.\n\n다운로드한 파일을 RStudio로 읽어들입니다.\n청크 옵션 이해하기 튜토리얼 파일\n핸즈온\n1. R 코드 출력 제어하기\n1.1. R 코드와 함께 결과 출력하기\n\n다음 지문을 수행할, 분석 경험을 위해 코드와 결과를 공유할 청크를 만들려고 합니다.\niris 데이터에서 중복인 데이터가 1건 있습니다. 추출하여 출력해 보세요.\nR 코드와 결과를 함께 출력해 보세요.\n청크 이름은 ’dup_iris’로 정의합니다.\n\n’understand_chunk.Rmd’의 1-1. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\n여러 방법이 있는데, 다음 코드를 사용해 보세요. \niris[duplicated(iris), ]\n모범 답안\n```{r dup_iris}\niris[duplicated(iris), ]\n```\n실행 결과\n\n\niris[duplicated(iris), ]\n\n\n##     Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\n## 143          5.8         2.7          5.1         1.9 virginica\n\n\n\n1.2. 결과만 출력하기\n\n다음 지문을 수행할, 분석 결과만 공유할 청크를 만들려고 합니다.\nsummary() 함수로 iris 데이터의 각 변수들을 요약해 보세요.\n결과만 출력해 보세요.\n청크 이름은 ’dup_iris_result’로 정의합니다.\n\n’understand_chunk.Rmd’의 1-2. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\n다음 코드를 사용해 보세요. \nsummary(iris)\n모범 답안\n```{r dup_iris_result, echo=FALSE}\nsummary(iris)\n```\n실행 결과\n\n##   Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n##  Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n##  1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n##  Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n##  Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n##  3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n##  Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50  \n##                 \n##                 \n## \n\n\n\n1.3. 소스 코드만 출력하기\n\n소스만 설명하려 합니다. 즉, 소스를 실행하지 않고, 출력만 해야 합니다.\nsummary() 함수로 iris 데이터의 각 변수들을 요약하는 소스를 출력하세요.\n청크 이름은 ’iris_not_run’으로 정의합니다.\n\n’understand_chunk.Rmd’의 1-3. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\n다음 코드를 사용해 보세요. \nsummary(iris)\n모범 답안\n```{r iris_not_run, echo=TRUE, eval=FALSE}\nsummary(iris)\n```\n실행 결과\n\n\nsummary(iris)\n\n\n\n\n\n1.4. 경고 메시지 출력하기\n\n다음을 수행해 보세요.\n-2부터 2까지의 정수 5개의 로그 값을 계산해 보세요.\n청크 이름은 ’log_integer’로 정의합니다.\n\n’understand_chunk.Rmd’의 1-4. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n어떤 결과가 출력되었나요?\n음수일 경우에 발생하는 경고 메시가 출력됨을 확인하세요.\n\n\n\n힌트 보기\n다음 코드를 사용해 보세요. \nlog(-3:3)\n모범 답안\n```{r log_integer}\nlog(-3:3)\n```\n실행 결과\n\n\nlog(-3:3)\n\n\n## Warning in log(-3:3): NaNs produced\n## [1]       NaN       NaN       NaN      -Inf 0.0000000 0.6931472\n## [7] 1.0986123\n\n\n\n1.5. 경고 메시지 출력하지 않기\n\n다음을 수행할 때 경고 메시지가 출력되는 것을 이미 알고 있습니다.\n-2부터 2까지의 정수 5개의 로그 값을 계산해 보세요.\n경고 메시지를 출력하고 싶지 않습니다.\n청크 이름은 ’no_warning’로 정의합니다.\n\n’understand_chunk.Rmd’의 1-5. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\n다음 코드를 사용해 보세요. \nlog(-3:3)\n모범 답안\n```{r no_warning, warning=FALSE}\nlog(-3:3)\n```\n실행 결과\n\n\nlog(-3:3)\n\n\n## [1]       NaN       NaN       NaN      -Inf 0.0000000 0.6931472\n## [7] 1.0986123\n\n\n\n1.6. 커맨트 변경하기\n\n앞의 튜토리얼 결과를 보면, 출련된 각 라인에 “##”가 앞에 출력되었습니다.\nlm(Sepal.Width ~ Sepal.Length, data = iris)를 실행하세요.\n“##”가 보기 싫습니다. 차라리 출력되지 않았으면 좋겠습니다.\n청크 이름은 ’change_comment’로 정의합니다.\n\n’understand_chunk.Rmd’의 1-6. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\ncomment 옵션을 사용합니다.\n모범 답안\n```{r change_comment, comment=\"\"}\nlm(Sepal.Width ~ Sepal.Length, data = iris)\n```\n실행 결과\n\n\nlm(Sepal.Width ~ Sepal.Length, data = iris)\n\n\n\nCall:\nlm(formula = Sepal.Width ~ Sepal.Length, data = iris)\n\nCoefficients:\n (Intercept)  Sepal.Length  \n     3.41895      -0.06188  \n\n\n\n2. 플롯 출력하기\n2.1. R 코드와 함께 플롯 출력하기\n\n다음 지문을 수행할, 분석 경험을 위해 코드와 결과를 공유할 청크를 만들려고 합니다.\niris 데이터에서 Sepal.Width ~ Sepal.Length 관계를 산점도로 시각화 하세요.\nSpecies별로 도형의 모양과 색상을 달리 그리세요.\nloess 산점도 위에 추세선도 출력하세요..\n\nR 코드와 결과를 함께 출력해 보세요.\n청크 이름은 ’scatter’로 정의합니다.\n\n’understand_chunk.Rmd’의 2-1. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\n여러 방법이 있는데, 다음 코드를 사용해 보세요.\n\nlibrary(ggplot2)\n\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, \n                        color = Species, shape = Species)) +\n  geom_point() +\n  geom_smooth()\n모범 답안\n```{r scatter}\nlibrary(ggplot2)\n\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, \n                        color = Species, shape = Species)) +\n  geom_point() +\n  geom_smooth()\n```\n실행 결과\n\n\nlibrary(ggplot2)\n\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, \n                        color = Species, shape = Species)) +\n  geom_point() +\n  geom_smooth()\n\n\n## `geom_smooth()` using method = 'loess' and formula 'y ~ x'\n\n\n\n\n2.2. 플롯만 출력하기\n\n2.1. 결과에서 R 소스와 메시지의 출력 없이 플롯만 출력하려 합니다.\n청크 이름은 ’plot_only’로 정의합니다.\n\n’understand_chunk.Rmd’의 2-2. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\n여러 방법이 있는데, 다음 코드를 사용해 보세요.\n\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, \n                        color = Species, shape = Species)) +\n  geom_point() +\n  geom_smooth()\n모범 답안\n```{r plot_only, echo=FALSE, message=FALSE}\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, \n                        color = Species, shape = Species)) +\n  geom_point() +\n  geom_smooth()\n```\n실행 결과\n\n\n\n\n\n2.3. 플롯 정렬하기\n\n2.2. 결과에서 플롯을 화면의 가운데 정렬로 출력하려 합니다.\n청크 이름은 ’plot_center’로 정의합니다.\n\n’understand_chunk.Rmd’의 2-3. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\nfig.align 인수를 사용합니다.\n모범 답안\n```{r plot_center, echo=FALSE, message=FALSE, fig.align=\"center\"}\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, \n                        color = Species, shape = Species)) +\n  geom_point() +\n  geom_smooth()\n```\n실행 결과\n\n\n\n\n\n2.4. 플롯 화면출력 크기 조절하기\n\n2.3. 결과에서 플롯을 화면의 영역의 1/2 사이즈로 출력하려 합니다.\n청크 이름은 ’plot_half_width’로 정의합니다.\n\n’understand_chunk.Rmd’의 2-4. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\nout.width 인수를 사용합니다.\n모범 답안\n```{r plot_half_width, echo=FALSE, message=FALSE, fig.align=\"center\", out.width=\"50%\"}\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, \n                        color = Species, shape = Species)) +\n  geom_point() +\n  geom_smooth()\n```\n실행 결과\n\n\n\n\n\n2.5. 플롯 출력에 캡션 넣기\n\n2.3. 결과에 캡션을 출력하려 합니다.\n“iris 품종별 산점도”라는 캡션을 추가합니다.\n청크 이름은 ’plot_caption’으로 정의합니다.\n\n’understand_chunk.Rmd’의 2-5. 빈 영역을 채우고 실행해 봅니다.\n모범 답안을 보지 않고 만들어 보세요.\n\n\n\n힌트 보기\nfig.cap 인수를 사용합니다.\n모범 답안\n```{r plot_caption, echo=FALSE, message=FALSE, fig.align=\"center\", fig.cap=\"iris 품종별 산점도\"}\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, \n                        color = Species, shape = Species)) +\n  geom_point() +\n  geom_smooth()\n```\n실행 결과\n\n\n\n(#fig:plot_caption)iris 품종별 산점도\n\n\n\n\n\n요약\n핸즈온 요약\nknitr 청크의 종류별로 다양한 출력 포맷의 문서를 작성했습니다.\nknitr 시각화 컨트롤 청크를 이용해서 플롯을 다양하게 출력했습니다.\nI can do it\nknitr의 대표적인 청크 옵션을 숙지해서, 화면에 분석 결과와 플롯 출력의 모양을 원하는대로 조정할 수 있습니다.\n\n\n\n",
    "preview": "posts/2022-03-07-chunk-knitr/img/knitr.png",
    "last_modified": "2022-03-06T14:40:28+09:00",
    "input_file": {},
    "preview_width": 200,
    "preview_height": 232
  },
  {
    "path": "posts/2022-03-06-rstudio/",
    "title": "첫 R 마크다운 문서 만들기",
    "description": "RStudio에서 R 마크다운 문서를 활용하는 방법을 익힙니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-06",
    "categories": [
      "Reproducible Research",
      "R Markdown"
    ],
    "contents": "\n\nContents\nR 마크다운 문서 만들기\nR 마크다운 문서 만들기\nR 마크다운 문서 저장하기\n\nHTML 생성하기\n보고서 파일 생성하기\n보고서 파일 살펴보기\n\n요약\n핸즈온 요약\nI can do it\n\n\n\n\n\n\n\n들어가기\nRStudio를 제대로 사용하는 것은 R 마크다운을 사용할 줄 안다는 것입니다.\n\nRStudio는 재현가능한 연구를 위한 최적의 솔루션이기 때문입니다. 이제 여러분은 RStudio를 제대로 사용할 수 있게 됩니다.\n\n운영체제에 따라 화면 모습에 다소 차이가 있을 수 있습니다. 염두에 두시기 바랍니다.\n\n\nR 마크다운 문서 만들기\nR 마크다운 문서를 생성하는 방법을 학습합니다. 지시에 따라 R 마크다운 파일을 생성합니다.\nR 마크다운 문서 만들기\nRStudio에서 R 마크다운 문서를 만들어 보겠습니다. RStudio의 좌측 상단의 새로운 파일 생성 아이콘을 누른 후, 메뉴에서 “R Markdown…”을 선택합니다.\n새 파일 만들기 아이콘New R Markdown 다이얼로그에서 다음을 입력하고 ‘OK’ 버튼을 누르세요.\n제목에 ’첫 R 마크다운’을 입력합니다.\n저자에 당신의 이름을 입력합니다. 예시에는 ’홍길동’을 입력했습니다.\n출력 포맷에서 HTML을 선택합니다. 기본값입니다.\n‘OK’ 버튼을 눌러, R 마크다운 문서를 생성하세요.\nNew R Markdown 다이얼로그다이얼로그의 왼쪽 헤더에는 R 마크다운의 종류를 선택할 수 있는 메뉴가 있습니다. 메뉴를 선택하면, 오른쪽에 해당 메뉴에 따라 R 마크다운 파일을 생성하는 옵션들이 나타납니다.\nDocument\n정적 문서(static documents)를 생성하기 위한 메뉴입니다.\nHTML, PDF, 워드 파일을 생성할 수 있습니다.\n\nPresentation\n발표 슬라이드를 생성하기 위한 메뉴입니다.\nHTML, PDF, 파워포인트 파일의 슬라이드를 생성할 수 있습니다.\n\nShiny\nShiny 기반의 동적 문서(dynamic documents)를 생성하기 위한 메뉴입니다.\nShiny 웹앱이나 Shiny 슬라이드를 생성합니다.\n\nForm Template\n특정 목적을 위해서 미리 준비한 R 마크다운 문서 템플리트를 사용하는 메뉴입이다.\n관련 템플리트를 지원하는 R 패키지가 설치되어야 합니다.\n\n\n\n\n\n솔루션\n\nNew R Markdown 다이얼로그를 여는 방법은 두가지입니다. 튜토리얼에서 제시하는 메뉴 아이콘 선택 방법과 \"File > New File > Markdown File\" 메뉴를 이용하는 방법입니다. 메뉴 아이콘 이용 방법이 간결하고 쉽습니다.\n\n\n메뉴를 이용하는 방법생성된 R 마크다운 템플리트 문서를 살펴보세요.\n제목과 당신의 이름은 어느 위치에 표현되어 있습니까?\n’output:’은 어떻게 기술되어 있습니까?\n어떤 내용의 문서를 생성할 것이지를 이야기해 보세요.\n두 개의 R 코드 청크를 해석해 보세요.\n‘pressure’ 청크는 어떤 플롯을 그리는 것일까요?\n\nR 마크다운 문서의 이름이 ’Untitled1’인 것을 확인하세요.\n아직은 파일로 저장되지 않은 상태입니다.\n\n생성된 R 마크다운 템플리트 문서R 마크다운 문서 저장하기\n생성된 R 마크다운 파일의 이름이 정해지지 않았음을 발견했습니다.\n좌측 상단의 디스켓 모양의 아이콘을 눌러 파일을 저장하세요.\n파일 이름에 ’first_rmarkdown’을 입력하세요.\n‘Save’ 버튼을 눌러 파일을 저장하세요.\n\n생성된 R 마크다운 템플리트 문서HTML 생성하기\nRStduio는 knitr을 이용해서 R 마크다운 문서를 마크다운 문서로 변환합니다. 그래서 메뉴에 ‘knit’라고 표현하는 것은 지정한 포맷의 보고서 파일을 생성하라는 의미입니다.\n마크다운 문서로 최종 보고서 파일을 생성하는 방법을 학습합니다. 지시에 따라 보고서 파일을 생성합니다.\n보고서 파일 생성하기\n좌측 상단의 뜨개실 이미지 아이콘을 눌러 보고서 파일을 생성하세요.\n뜨개실 이미지 아이콘 옆에 ’Knit’라는 문자열과 역삼각형 이미지를 확인하세요.\n’Knit’라는 문자열과 역삼각형 이미지를 클릭하세요.\n주의) 역삼각형 이미지가 아닌 ‘Knit’ 문자열을 클릭하면 메뉴의 팝업없이 바로 실행됩니다.\n\n팝업된 메뉴에서 ’Knit to HTML’을 눌러 보고서 파일을 생성합니다.\nknit 메뉴 아이콘팝업 메뉴에는 최종 파일의 포맷을 선택하는 기능, 파라미터를 추가하는 메뉴 등이 포함되어 있습니다.\nKnit to HTML\nHTML 문서를 생성합니다.\n\nKnit to PDF\nPDF 문서를 생성합니다.\n\nKnit to Word\n워드 문서를 생성합니다.\n\nKnit with Parameters…\nR 마크다운 문서에 입력 파라미터를 설정했을 경우, 파라미터의 값을 정의합니다.\n\nKnit Directory\n생성한 최종 보고서를 저장할 경로를 설정합니다.\nDocument Directory\nR 마크다운 파일이 있는 디렉토리에 저장합니다.\n기본값입니다.\n\nProject Directory\nR 프로젝트 안에서의 R 마크다운 파일일 경우, 프로젝트 루트에 저장합니다.\n\nCurrent Working Directory\n현재 R 작업 경로에 저장합니다.\n\n\n보고서 파일 살펴보기\n브라우저에 출력된 HTML 문서를 확인하세요.\n제목과 저장 이름이 출력된 위치와 크기를 확인하세요.\nR 코드 청그 위치에 어떤 내용이 표시되어 있습니까?\nHTML 본문의 https://rmarkdown.rstudio.com을 눌러보세요.\n\n브라우저에 출력된 HTML 문서요약\n핸즈온 요약\nR 마크다운 문서를 생성했습니다.\nR 마크다운 문서로 HTML 문서를 생성했습니다.\nI can do it\nRStudio의 메뉴를 이용해서 R 마크다운 문서를 작성하고, R 마크다운 문서의 R 코드 청크를 실행하여 HTML 문서를 생성할 수 있습니다.\n\n\n\n",
    "preview": "posts/2022-03-06-rstudio/img/rstudio.png",
    "last_modified": "2022-03-06T14:42:04+09:00",
    "input_file": {},
    "preview_width": 1280,
    "preview_height": 449
  },
  {
    "path": "posts/2022-03-05-submit-package/",
    "title": "Submit R package to CRAN",
    "description": "개발한 R 패키지를 CRAN에 등록하는 방법을 살펴봅니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-05",
    "categories": [
      "programming",
      "package"
    ],
    "contents": "\n\nContents\n개요\nR-devel 환경 만들기\nDownloading R-devel\nBuilding R-devel\nInstall R-devel\n\nCRAN에 패키지 등록하기\nbuild package\nsubmit package\n\n\n\n\n\n\n\n들어가기\nCRAN!!!\n\n처음에는 어렵지만, 도전해볼만 합니다.\n\n자신만의 패키지가 모두의 패키지로 탈바꿈하니까요. 이 기회에 한번 도전해보세요.\n\n\n개요\nCRAN에 패키지를 submit하는 방법을 간단하게 다룹니다. 또한 CRAN에 패키지를 submit하기 위해서는 R-devel 환경에서도 무결성을 check해야 합니다. 생략할 수도 있지만, 특정 기능이 R development 버전에서 오류가 발생할 수도 있기 때문이다. 반드시 수행해야 합니다.\n본 note에서는 Mac OS X 버전의 R 3.5.2 환경에서 개발 버전을 설치하는 방법과 CRAN에 패키지를 submit하는 방법을 개괄적으로 다룹니다.\n\n\nsessionInfo()\n\n\nR version 4.1.2 (2021-11-01)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.1/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods  \n[7] base     \n\nother attached packages:\n[1] htmltools_0.5.2 shiny_1.7.1    \n\nloaded via a namespace (and not attached):\n [1] Rcpp_1.0.8          rstudioapi_0.13     knitr_1.37         \n [4] magrittr_2.0.2      downlit_0.4.0       xtable_1.8-4       \n [7] R6_2.5.1            rlang_1.0.1         fastmap_1.1.0      \n[10] fansi_1.0.2         highr_0.9           stringr_1.4.0      \n[13] tools_4.1.2         xfun_0.29           cli_3.2.0          \n[16] jquerylib_0.1.4     ellipsis_0.3.2      yaml_2.3.5         \n[19] digest_0.6.29       lifecycle_1.0.1     later_1.3.0        \n[22] vctrs_0.3.8         sass_0.4.0          promises_1.2.0.1   \n[25] distill_1.3         memoise_2.0.1       cachem_1.0.6       \n[28] evaluate_0.15       mime_0.12           rmarkdown_2.11     \n[31] xaringanExtra_0.5.5 stringi_1.7.6       compiler_4.1.2     \n[34] bslib_0.3.1         jsonlite_1.8.0      httpuv_1.6.5       \n\nBuilding R-devel on Mac OS X를 참고하였습다.\nR-devel 환경 만들기\n아마 대부분의 R 사용자는 버전 번호는 다르지만, stable 버전의 R을 사용하고 있을 것입니다. 필자도 R을 CRAN에 제출하기 위해서 R development 버전을 설치하였습니다.\nDownloading R-devel\n다음처럼 https://stat.ethz.ch/R/daily URL에서 최신 R development 버전을 다운로드 합니다.:\n\ncurl -O https://stat.ethz.ch/R/daily/R-devel_2019-03-15.tar.gz\n\nBuilding R-devel\n소스 파일의 압축을 풉니다.:\n\ntar xvzf R-devel_2019-03-15.tar.gz\ncd R-devel\n\nconifg.site 파일의 FC 변수를 다음과 같이 정의합니다.\nF77=\"gfortran -arch x86_64\"\nFC=$F77\n소스 파일을 컴파일 합니다. CRAN에 패키지를 제출할 때 무결성 체크를 위한 목적으로만 사용하기 때문에 Gui 환경의 binary를 생성하지 않을 것입니다. configure 명령을 –without-x 옵션으로 수행하면 됩니다.\n\n./configure --without-x\nmake\nmake check\nmake pdf\nmake info\n\nInstall R-devel\nMac OS X 환경에서 prefix 변수의 값을 지정하지 않으면, 현재 설치된 stable 버전을 엎어칩니다. 그러므로 반드시 설치할 경로를 지정해야 합니다. 필자는 /usr/local/lib/R-devel 경로에 개발 버전을 설치하였습니다.\n\nmake prefix=/usr/local/lib/R-devel install\n\n이상으로 개발 버전을 설치하였다. 그리고 개발 버전은 다음과 같이 실행합니다.:\n\n/usr/local/lib/R-devel/bin/R\n\n다음처럼 심볼릭 링크를 생성하면 쉽게 사용할 수 있습니다.:\n\n\nln -s /usr/local/lib/R-devel/bin/R /usr/local/bin/R-devel\n\n\n\n이제는 콘솔에서 R-devel만 입력하면 R 개발 버전이 실행될 것입니다. 그러므로 다음과 같이 개발 버전 환경에서 패키지를 체크할 수 있습니다.\n\nR-devel CMD check –as-cran 패키지이름.tar.gz\n\nCRAN에 패키지 등록하기\nCRAN에 패키지를 등록하기 위해서는 몇 가지의 제약을 준수해야 합니다. 어쩌면 매우 까다로운 작업일 수도 있습니다. 아마 처음 패키지를 등록하는 패키지 개발자는 당황할 수도 있습니다.\nbuild package\n다음과 같이 패키지를 build합니다. 여기서는 dlookr 0.3.9 버전을 기준으로 설명합니다.\n예제에서 dlookr은 패키지 디렉토리를 의미합니다. R CMD build 명령로 패키지를 생성합니다.\n\nR CMD build dlookr\n\n상기 명령이 수행되면 DESCRIPTION 파일에서 패키지의 버전 번호를 가져와 컴파일된 패키지 파일을 생성합니다. 그러므로 dlookr_0.3.9.tar.gz이라는 파일이 생성됩니다.\n패키지의 무결성을 진단하기 위해서 다음의 명령을 차례로 입력합니다.:\n\nR CMD check --as-cran dlookr_0.3.9.tar.gz\nR-devel CMD check --as-cran dlookr_0.3.9.tar.gz\n\n오랜 시간동안 명령어가 수행됩니다. Error, Warning, Note 등이 발생된다면 원인을 찾아 수정해야 합니다. 그리고 그것들이 발생하지 않아야 CRAN에 제출할 수 있습니다.\nsubmit package\nhttps://cran.r-project.org/submit.html URL에서 패키지를 제출합니다. 이 URL은 그림처럼 개발자 이름, 이메일 주소 등을 입력하고 패키지 파일을 업로드할 수 있는 컴포넌트로 구성됩니다.\n\n\n\n(#fig:done_gtk)R 패키지 제출 화면\n\n\n\n정보를 입력하고, 파일을 업로드하면 면 단계 화면을 거치고 기술한 이메일 주소로 Submit 확인 메일이 발송됩니다. 발송된 메일의 URL에서 최종 제출 작업을 수행해야 합니다. 메일의 내용은 그림처럼 URL 링크를 포함하고 있습니다.\n\n\n\nFigure 1: R 패키지 제출 확인 메일\n\n\n\n패키지가 제출되면 몇 분 간격으로 접수되었다는 메일(그림)과 CRAN 사이트에 제출된 버전이 올라간다는 메일(그림)이 발송됩니다. 물론 제출된 패키지가 check에서 이상이 없었을 경우입니다.\n\n\n\nFigure 2: R 패키지 접수 확인 메일\n\n\n\n대부분의 정보는 DESCRIPTION 파일에 기재된 내용의 요약입니다.\n\n\n\n(#fig:goto_cran)R 패키지 등록 확인 메일\n\n\n\n접수 확인 메일 발송 후 정확히 30분 이후에 CRAN으로 패키지가 등록된다는 메일이 발송됨을 알 수 있습니다.\n\n\n\n",
    "preview": "posts/2022-03-05-submit-package/img/CRAN.png",
    "last_modified": "2022-03-05T00:08:16+09:00",
    "input_file": {},
    "preview_width": 225,
    "preview_height": 225
  },
  {
    "path": "posts/2022-03-04-develop-package/",
    "title": "R 패키지 개발하기",
    "description": "몇 가지 Open API를 이용한 데이터 수집 기능을 구현한 R 패키지를 만들어 봅니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-04",
    "categories": [
      "programming",
      "package"
    ],
    "contents": "\n\nContents\nR 패키지 시나리오\nR 패키지의 구성\n패키지 구성 파일\n파일이름 명명규칙\n\nR 패키지 골격 만들기\nR 패키지 골격 살펴보기\nR 패키지 개발하기\nDESCRIPTION 파일 수정하기\n소스파일 만들기\n\n패키지 관련 파일의 자동 생성\nroxygen2\nroxygen2 태그\nroxygen2 태그 기반 자동 생성\n\nR 데이터 생성\nR 데이터 파일 생성\nR 데이터 도움말 파일 생성\n\nvignette 작성하기\nvignette 골격 생성하기\nvignette 작성하기\nvignette 빌드하기\n\n개발자의 koscrap 패키지 설치하기\nR 패키지 설치하기\n\nkoscrap 패키지 사용하기\n사용자의 koscrap 패키지 설치하기\ndevtools 패키지 설치\nkoscrap 패키지 설치\n\n\n\n\n\n\n\n들어가기\n패키지, 그 범접할 수 없는 아우라!!!\n\n그러나 사실 어렵지 않게 만들 수 있습니다.\n\n시도를 해 보지 않아서 어렵게 느낄뿐 한번 만들고 보면, 이내 두번째 패키지를 만들고 있을 겁니다.\n\n\nR 패키지 시나리오\n앞에서 네이버 오픈 API를 이용해서 뉴스를 검색하는 함수와 공공데이터포털의 오픈 API를 이용해서 아파트 실거래 상세 정보를 조회하는 함수를 만들어 보았니다.\n이제 이들 함수를 포함한 R 패키지를 만들어 보려 합니다. 다음의 내용을 담을 계획입니다.\n함수\n네이버 오픈 API를 이용한, 뉴스 검색 함수\n공공데이터포털의 오픈 API를 이용한, 아파트 실거래 상세 정보 조회 함수\n\n데이터\n법정동 코드 데이터\n\n소품(Vignettes)1\n두 함수를 사용하는 방법을 소개한 비네트\n\nR 패키지의 구성\n패키지 구성 파일\nDESCRIPTION\n패키지의 기본적인 정보를 담고 있는 메타 파일로 패키지의 버전, 저자, 종속성 등이 담겨 있습니다. CRAN의 R 패키지 페이지에서는 패키지의 여러 정보를 제공하는데, 대부분이 DESCRIPTION의 정보로 표현합니다.\nNAMESPACE\n개발하는 패키지가 참조하는 다른 패키지의 객체(함수나 메소드 등), 개발하는 패키지가 제공하는 객체를 네임스페이스에 등록하기 위한 정보를 담습니다.\n소스\nR 디렉토리에 프로그램 소스 파일을 담는다. *.R 형식으로 이름을 정의합니다.\n도움말\nman 디렉토리에 데이터나 함수, 메소드에 대한 도움말 파일을 담는다. *.Rd 형식으로 이름을 정의합니다.\n소품(vignettes)\nvignettes 디렉토리에 도움말 보다 좀 더 친절한 패키지 사용 방법을 소개하는 소품 파일을 담습니다. R 마크다운 편집 파일로 *.Rmd 형식으로 이름을 정의합니다.\n데이터\ndata 디렉토리에 R 데이터 파일을 담습니다. R 데이터 파일로 *.rda 형식으로 이름을 정의합니다.\n기타 제공 파일\ninst 디렉토리에 패키지를 구성하는 표준 파일이 아니지만 패키지에 제공할 파일을 넣을 수 있습니다.\n파일이름 명명규칙\nDESCRIPTION, NAMESPACE 파일 이름은 변경이 불가능합니다. 소스파일과 소품 파일은 이름으로도 그 기능을 유추할 수 있도록 간결하면서 명확한 이름을 만들어야 합니다. 도움말 파일의 이름은 도움말을 제공하는 함수 혹은 메소드, 데이터 이름으로 정합니다.\n파일 이름은 절대로 non-ASCII 문자를 포함하면 안됩니다.\nR 패키지 골격 만들기\nRStudio의 기능을 이용하면 R 패키지의 골격을 쉽게 만들 수 있습니다.\n패키지의 이름을 koscrap이라고 미리 정합니다. “Korean Scraping”을 연상하여 이름을 정했습니다.\nFile > New Project… 메뉴를 선택하면, 다음과 같은 다이얼로그 창이 뜹니다.\n\n\n\nFigure 1: New Project Wizard 화면\n\n\n\n새로운 디렉토리에 패키지를 만들 것이므로 첫번째, New Directory를 선택합니다.\n\n\n\nFigure 2: R Package 선택 화면\n\n\n\nR 패키지 프로젝트를 만들 것이므로 두번째, R Package를 선택합니다.\n\n\n\nFigure 3: R Package 정의 화면\n\n\n\n패키지 이름에 “koscrap”을 기입하고, 패키지를 설치할 디렉토리도 선정합니다. 그리고 “Open in new session”의 체크박스를 선택합니다. 이 체크 박스는 패키지가 만들어지면 RStudio의 새로운 세션에서 해당 패키지 프로젝트를 열어 줍니다.\n“Create Project” 버튼을 누르면 RStudio의 새로운 세션이 다음처럼 열립니다.\n\n\n\nFigure 4: koscrap 패키지 프로젝트 세션 화면\n\n\n\nR 패키지 골격 살펴보기\n만들어진 R 패키지의 골격은 다음과 같습니다\n\n\n\nFigure 5: koscrap 패키지 디렉토리 구조\n\n\n\nhello.R이라는 R 소스 파일과 hello.Rd라는 도움말 파일이 만들어져 있습니다. 이들 파일의 내용을 살펴보겠습니다.\n\nDESCRIPTION\nPackage: koscrap\nType: Package\nTitle: What the Package Does (Title Case)\nVersion: 0.1.0\nAuthor: Who wrote it\nMaintainer: The package maintainer <yourself@somewhere.net>\nDescription: More about what it does (maybe more than one line)\n    Use four spaces when indenting paragraphs within the Description.\nLicense: What license is it under?\nEncoding: UTF-8\nLazyData: true\nNAMESPACE\n\n\nexportPattern(\"^[[:alpha:]]+\")\n\n\n\nhello.R\n\n\n# Hello, world!\n#\n# This is an example function named 'hello' \n# which prints 'Hello, world!'.\n#\n# You can learn more about package authoring with RStudio at:\n#\n#   http://r-pkgs.had.co.nz/\n#\n# Some useful keyboard shortcuts for package authoring:\n#\n#   Install Package:           'Cmd + Shift + B'\n#   Check Package:             'Cmd + Shift + E'\n#   Test Package:              'Cmd + Shift + T'\n\nhello <- function() {\n  print(\"Hello, world!\")\n}\n\n\n\nhello.Rd\n\n\\name{hello}\n\\alias{hello}\n\\title{Hello, World!}\n\\usage{\nhello()\n}\n\\description{\nPrints 'Hello, world!'.\n}\n\\examples{\nhello()\n}\n\n\nR 패키지 개발하기\nDESCRIPTION 파일 수정하기\n\nPackage: koscrap\nType: Package \nTitle: Scrap from Public Data Portal & Naver \nVersion: 0.1.1.9000\nDate: 2022-01-31\nAuthors@R: c(\n  person(\"Choonghyun\", \"Ryu\",, \"choonghyun.ryu@gmail.com\", role = c(\"aut\", \"cre\"))\n  )\nDescription: Collect data using Open API from public data portal and scrap NAVER news article information.\nImports: \n    dplyr,\n    httr,\n    XML,\n    stringr,\n    purrr,\n    glue\nAuthor: Choonghyun Ryu [aut, cre]\nMaintainer: Choonghyun Ryu <choonghyun.ryu@gmail.com>\nLicense: GPL-2 | file LICENSE\nEncoding: UTF-8\nLazyData: true\nRoxygen: list(markdown = TRUE)\nRoxygenNote: 7.1.1\n\n대표적인 태그의 의미는 다음과 같습니다.\nPackage\n패키지 이름 정의\n\nTitle, Description\n패키지 타이틀과 소개 정의\n\nVersion\n패키지 버전 정의\n0.1.1.9000\n0.1.1: 버전\n9000: 개발버전\n\n\nImports\n패키지의 종속성 정의\n패키지의 기능을 구현하기 위해서 반드시 필요한 패키지로 해당 패키지 설치시 자동으로 설치됩니다.\n\nAuthor, Maintainer\n패키지의 저자 및 유지보수 관리자\n\nLazyData\n데이터를 사용하기 전까지 메모리에 로드되지 않을지의 여부\ntrue일 경우에 사용전에는 메모리에 로드되지 않음\n\n소스파일 만들기\n다음처럼 두개의 소스 파일을 R 디렉토리에 복사해 넣습니다.\n\nsearch_naver.R\n\n\n#' 네이버 뉴스 검색\n#'\n#' @description 네이버 뉴스 검색 결과를 출력해주는 REST API를 호출하여, 뉴스 정보를 검색합니다.\n#'\n#' @details 네이버에서 발급받은 Client ID, Client Secret는 개인이 발급받은 키를 사용하며,\n#' 유출되어서는 안됩니다.\n#'\n#' @param query character. 검색을 원하는 문자열\n#' @param chunk_no integer. 검색 시작 위치로 최대 1000까지 가능\n#' @param chunk integer. 검색 결과 출력 건수 지정 (1~100)\n#' @param do_done logical. 한번의 호출로 모든 조회 결과를 가져오지 못할 경우,\n#' 추가로 호출해서 모든 결과를 가져올지의 여부\n#' @param sort character. 정렬 옵션: sim (유사도순), date (날짜순)\n#' @param max_record integer. 최대 조회할 건수. 실제로 검색한 건수는 max_record와 정확히 일치하지 않을 수 있습니다.\n#' chunk의 개수로 데이터를 수집하기 때문에 일반적인 경우에는 max_record보다 같거나  큰 chunk의 배수만큼 데이터를 가져옵니다.\n#' do_done가 FALSE일 경우에는 적용되지 않습니다.\n#' @param client_id character. 애플리케이션 등록 시 발급받은 Client ID\n#' @param client_secret character. 애플리케이션 등록 시 발급받은 Client Secret\n#'\n#' @return data.frame\n#' 변수 목록은 다음과 같음.:\n#' \\itemize{\n#' \\item title : character. 기사의 타이틀\n#' \\item originallink : character. 검색 결과 문서의 제공 언론사 하이퍼텍스트 link\n#' \\item link : character. 검색 결과 문서의 제공 네이버 하이퍼텍스트 link\n#' \\item description : character. 검색 결과 문서의 내용을 요약한 패시지 정보.\n#' 문서 전체의 내용은 link를 따라가면 읽을 수 있음. 패시지에서 검색어와 일치하는 부분은 태그로 감싸져 있음\n#' \\item publish_date : POSIXct. 검색 결과 문서가 네이버에 제공된 시간\n#' \\item title_text : character. 타이틀에서 HTML 태크를 제거한 텍스트\n#' \\item description_text : character. 요약한 패시지 정보에서 HTML 태크를 제거한 텍스트\n#' }\n#'\n#' @examples\n#' \\donttest{\n#' # Your authorized API keys\n#' client_id <- \"XXXXXXXXXXXXXXXXXXXXXXX\"\n#' client_secret <- \"XXXXXXXXX\"\n#'\n#' search_list <- search_naver(\n#'   \"불평등\", client_id = client_id, client_secret = client_secret\n#' )\n#'\n#' search_list <- search_naver(\n#'   \"불평등\", client_id = client_id, client_secret = client_secret,\n#'   do_done = TRUE, max_record = 350\n#' )\n#'\n#' }\n#'\n#' @import dplyr\n#' @importFrom XML xmlParse getNodeSet xmlValue xmlToDataFrame\n#' @importFrom httr GET add_headers\n#' @importFrom purrr map_df\n#' @importFrom glue glue\n#' @export\n#'\nsearch_naver <- function(query = NULL, chunk = 100, chunk_no = 1,\n                         sort = c(\"date\", \"sim\"), do_done = FALSE,\n                         max_record = 1000L, client_id = NULL,\n                         client_secret = NULL, verbose = TRUE) {\n  if (is.null(query)) {\n    stop(\"검색 키워드인 query를 입력하지 않았습니다.\")\n  }\n\n  if (chunk < 1 & chunk > 100) {\n    stop(\"chunk 요청 변수값이 허용 범위(1~100)인지 확인해 보세요.\")\n  }\n\n  if (chunk_no < 1 & chunk_no > 100) {\n    stop(\"chunk_no 요청 변수값이 허용 범위(1~1000)인지 확인해 보세요.\")\n  }\n\n  sort <- match.arg(sort)\n\n  get_list <- function(doc) {\n    doc %>%\n      XML::getNodeSet(\"//item\") %>%\n      XML::xmlToDataFrame() %>%\n      rename(\"publish_date\" = pubDate) %>%\n      mutate(publish_date = as.POSIXct(publish_date,\n                                       format = \"%a, %d %b %Y %H:%M:%S %z\")) %>%\n      mutate(title_text = stringr::str_remove_all(\n        title, \"&\\\\w+;|<[[:punct:]]*b>\")) %>%\n      mutate(title_text = stringr::str_remove_all(\n        title_text, \"[[:punct:]]*\")) %>%\n      mutate(description_text = stringr::str_remove_all(\n        description,\n        \"&\\\\w+;|<[[:punct:]]*b>|[“”]\"))\n  }\n\n  searchUrl <- \"https://openapi.naver.com/v1/search/news.xml\"\n\n  query <- query %>%\n    enc2utf8() %>%\n    URLencode()\n\n  url <- glue::glue(\"{searchUrl}?query={query}&display={chunk}&start={chunk_no}&sort={sort}\")\n\n  doc <- url %>%\n    httr::GET(\n      httr::add_headers(\n        \"X-Naver-Client-Id\"     = client_id,\n        \"X-Naver-Client-Secret\" = client_secret\n      )\n    ) %>%\n    toString() %>%\n    XML::xmlParse()\n\n  total_count <- doc %>%\n    XML::getNodeSet(\"//total\") %>%\n    XML::xmlValue() %>%\n    as.integer()\n\n  if (verbose) {\n    glue::glue(\"* 검색된 총 기사 건수는 {total_count}건입니다.\\n\\n\") %>%\n      cat()\n\n    glue::glue(\"  - ({chunk}/{min(total_count, max_record)})건 호출을 진행합니다.\\n\\n\") %>%\n      cat()\n  }\n\n  search_list <- doc %>%\n    get_list()\n\n  records <- NROW(search_list)\n\n  if (!do_done | records >= total_count | records >= max_record) {\n    return(search_list)\n  } else {\n    total_count <- min(total_count, max_record)\n\n    cnt <- total_count %/% chunk\n    if (total_count %% chunk == 0) {\n      cnt <- cnt - 1\n    }\n\n    idx <- (seq(cnt) + 1)\n\n    add_list <- idx[idx <= 1000] %>%\n      purrr::map_df({\n        function(x) {\n          if (verbose) {\n            glue::glue(\"  - ({chunk * x}/{total_count})건 호출을 진행합니다.\\n\\n\") %>%\n              cat()\n          }\n\n          glue::glue(\n            \"{searchUrl}?query={query}&display={chunk}&start={x}&sort={sort}\"\n          ) %>%\n            httr::GET(\n              httr::add_headers(\n                \"X-Naver-Client-Id\"     = client_id,\n                \"X-Naver-Client-Secret\" = client_secret\n              )\n            ) %>%\n            toString() %>%\n            XML::xmlParse() %>%\n            get_list()\n        }\n      })\n\n    search_list %>%\n      bind_rows(\n        add_list\n      ) %>%\n      return()\n  }\n}\n\n\n\ntrade_apt.R\n\n\n#' 아파트 실거래 데이터 가져오기\n#'\n#' @description 공공데이터포털에서 REST open API로 아파트 실거래 데이터를 수집합니다.\n#'\n#' @details 공공데이터포털에서 발급받은 API 인증키는 개인이 발급받은 키를 사용하며,\n#' 유출되어서는 안됩니다.\n#'\n#' @param auth_key character. 공공데이터포털에서 발급받은 API 인증키\n#' @param LAWD_CD character. 지역코드. 각 지역별 코드 행정표준코드관리시스템\n#' (www.code.go.kr)의 법정동코드 10자리 중 앞 5자리\n#' @param DEAL_YMD character. 실거래 자료의 계약년월(6자리)\n#' @param chunk_no integer. 페이지번호\n#' @param chunk integer. 한 페이지 결과 수\n#' @param do_done logical. 한번의 호출로 모든 조회 결과를 가져오지 못할 경우,\n#' 추가로 호출해서 모든 결과를 가져올지의 여부\n#'\n#' @return data.frame\n#' 변수 목록은 다음과 같음.:\n#' \\itemize{\n#' \\item LAWD_CD : character. 지역코드\n#' \\item DEAL_DATE : character. 거래일자\n#' \\item SERIAL : character. 일련번호\n#' \\item BUILD_NM : character. 아파트 이름\n#' \\item FLOOR : integer. 층\n#' \\item BUILD_YEAR : integer. 건축년도\n#' \\item AREA : numeric. 전용면적\n#' \\item AMOUNT : integer. 거래금액\n#' \\item ROAD_CD : character. 도로명코드\n#' \\item ROAD_NM : character. 도로명\n#' \\item BUILD_MAJOR : character. 도로명건물본번호코드\n#' \\item BUILD_MINOR : character. 도로명건물부번호코드\n#' \\item ROAD_SEQ : character. 도로명일련번호코드\n#' \\item BASEMENT_FLAG : character. 도로명지상지하코드\n#' \\item LAND_NO : character. 지번\n#' \\item DONG_NM : character. 법정동\n#' \\item DONG_MAJOR : character. 법정동본번코드\n#' \\item DONG_MINOR : character. 법정동부번코드\n#' \\item EUBMYNDONG_CD : character. 법정동읍면동코드\n#' \\item DONG_LAND_NO : character. 법정동지번코드\n#' \\item DEALER_ADDR : character. 중개사소재지\n#' \\item CANCEL_DEAL : character. 해제여부\n#' \\item CANCEL_DATE : character. 해제사유발생일\n#' }\n#'\n#' @examples\n#' \\donttest{\n#' # Your authorized API keys\n#' auth_key <- \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n#'\n#' result <- trade_apt(auth_key, LAWD_CD = \"11680\", DEAL_YMD = \"202104\")\n#' result <- trade_apt(auth_key, LAWD_CD = \"11680\", DEAL_YMD = \"202104\", do_done = TRUE)\n#'\n#' }\n#'\n#' @import dplyr\n#' @importFrom XML xmlParse getNodeSet xmlValue xmlToDataFrame\n#' @importFrom stringr str_pad\n#' @importFrom purrr map_df\n#' @importFrom glue glue\n#' @export\ntrade_apt <- function(auth_key, LAWD_CD = \"11110\", DEAL_YMD = \"202112\",\n                      chunk_no = 1, chunk = 400, do_done = FALSE) {\n  library(dplyr)\n\n  get_list <- function(doc) {\n    doc %>%\n      XML::getNodeSet(\"//item\") %>%\n      XML::xmlToDataFrame() %>%\n      mutate(거래금액 = stringr::str_remove(거래금액, \",\") %>%\n                   as.integer()) %>%\n      mutate(DEAL_DATE = glue::glue(\"{년}-{str_pad(월, width = 2, pad = '0')}-{\n                                  str_pad(일, width = 2, pad = '0')}\")) %>%\n      mutate(층 = as.integer(층)) %>%\n      mutate(건축년도 = as.integer(건축년도)) %>%\n      select(-년, -월, -일) %>%\n      select(\"LAWD_CD\"       = 지역코드,\n             DEAL_DATE,\n             \"SERIAL\"        = 일련번호,\n             \"DEAL_TYPE\"     = 거래유형,\n             \"BUILD_NM\"      = 아파트,\n             \"FLOOR\"         = 층,\n             \"BUILD_YEAR\"    = 건축년도,\n             \"AREA\"          = 전용면적,\n             \"AMOUNT\"        = 거래금액,\n             \"ROAD_CD\"       = 도로명코드,\n             \"ROAD_NM\"       = 도로명,\n             \"BUILD_MAJOR\"   = 도로명건물본번호코드,\n             \"BUILD_MINOR\"   = 도로명건물부번호코드,\n             \"ROAD_SEQ\"      = 도로명일련번호코드,\n             \"BASEMENT_FLAG\" = 도로명지상지하코드,\n             \"LAND_NO\"       = 지번,\n             \"DONG_NM\"       = 법정동,\n             \"DONG_MAJOR\"    = 법정동본번코드,\n             \"DONG_MINOR\"    = 법정동부번코드,\n             \"EUBMYNDONG_CD\" = 법정동읍면동코드,\n             \"DONG_LAND_NO\"  = 법정동지번코드,\n             \"DEALER_ADDR\"   = 중개사소재지,\n             \"CANCEL_DEAL\"   = 해제여부,\n             \"CANCEL_DATE\"   = 해제사유발생일)\n  }\n\n  api <- \"http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev\"\n  url <- glue::glue(\n    \"{api}?ServiceKey={auth_key}&pageNo={chunk_no}&numOfRows={chunk}&LAWD_CD={LAWD_CD}&DEAL_YMD={DEAL_YMD}\"\n  )\n\n  doc <- XML::xmlParse(url)\n\n  resultCode <- doc %>%\n    XML::getNodeSet(\"//resultCode\") %>%\n    XML::xmlValue()\n\n  if (resultCode != \"00\") {\n    result_msg <- doc %>%\n      XML::getNodeSet(\"//resultMsg\") %>%\n      XML::xmlValue()\n\n    stop(result_msg)\n  }\n\n  total_count <- doc %>%\n    XML::getNodeSet(\"//totalCount\") %>%\n    XML::xmlValue() %>%\n    as.integer()\n\n  deal_list <- doc %>%\n    get_list()\n\n  records <- NROW(deal_list)\n\n  if (!do_done | records >= total_count) {\n    return(deal_list)\n  } else {\n    cnt <- total_count %/% chunk\n    if (total_count %% chunk == 0) {\n      cnt <- cnt - 1\n    }\n\n    add_list <- (seq(cnt) + 1) %>%\n      purrr::map_df({\n        function(x) {\n          url <- glue::glue(\n            \"{api}?ServiceKey={auth_key}&pageNo={x}&numOfRows={chunk}&LAWD_CD={LAWD_CD}&DEAL_YMD={DEAL_YMD}\"\n          )\n\n          XML::xmlParse(url) %>%\n            get_list()\n        }\n      })\n\n    deal_list %>%\n      bind_rows(\n        add_list\n      ) %>%\n      return()\n  }\n}\n\n\n\n\n패키지 관련 파일의 자동 생성\nroxygen2\nroxygen2 패키지는 패키지를 개발할 때 help page를 기술하는 파일인 Rd 파일과 NAMESPACE 파일을 자동으로 작성해줍니다.\n일반적으로 Rd 파일은 패키지 홈 디렉토리의 man 디렉토리에 .Rd, .rd 포맷의 이름으로 만듧니다. 그런데 roxygen2 패키지는 패키지를 정의하는 R 소스 파일에 roxygen 형식으로 스크립트를 작성하면, 자동으로 Rd 파일을 생성해줍니다.\nNAMESPACE 파일에는 패키지에서 참조하는 다른 패키지나 패키지의 함수를 지정하는 import 정보 및 패키지의 함수에 접근할 수 있는 export 정보를 기술합니다.\n다음은 roxygen 형식으로 스크립트를 작성한 R 소스 파일의 예제입다.\n\n\n#' 두 월도 사이의 개월수 구하기\n#' @description 두개의 년월 사이에 몇 개의 개월이 포함되는 지를 구함\n#' @param start_year integer. 시작하는 년도를 나타내는 수치값\n#' @param start_month integer. 시작하는 월을 나타내는 수치값\n#' @param end_year integer. 종료하는 년도를 나타내는 수치값\n#' @param end_month integer. 종료하는 월을 나타내는 수치값\n#' @return 개월수를 나타내는 수치 벡터\n#' @author 유충현\n#' Maintainer: 유충현 <choonghyun.ryu@gmail.com>\n#' @seealso \\code{\\link{get_next_month}}, \\code{\\link{is_indate}}\n#' @examples\n#' get_month_length(2015, 3, 2017, 3)\n#' @export\nget_month_length <- function(start_year, start_month, end_year, end_month) {\n  ifelse(start_month > end_month, 12 + end_month - start_month +\n    (end_year - start_year - 1) * 12 + 1, end_month - start_month +\n    (end_year - start_year) * 12 + 1)\n}\n\n\n\nroxygen 스크립트의 시작은 #'로 시작합니다. 즉, R 소스 파일에서 #'로 시작하는 라인은 roxygen2 패키지가 해석하는 라인입니다.\nroxygen2 태그\nroxygen2의 대표적인 태그는 다음과 같습니다.:\n@title : Rd 파일의 \\title 태그로 기술될 태그\n함수의 타이틀을 기술\n@title을 기술하지 않으면, 첫 줄의 내용이 \\title 태그로 기술됨\n\n@description : Rd 파일의 태그로 기술될 태그\n함수의 상세 설명을 기술\n\n@param : Rd 파일의 태그 내의\n태그로 기술될 태그\n함수의 인수 이름과 그 내용을 기술\n\n@return : Rd 파일의 태그로 기술될 태그\n함수가 반환하는 값에 대한 설명을 기술\n\n@author : Rd 파일의 \\author 태그로 기술될 태그\n함수 개발자 정보를 기술\n\n@seealso : Rd 파일의 태그로 기술될 태그\n함께 알아두면 유용한 관련 함수를 기술\n\n@examples : Rd 파일의 태그로 기술될 태그\n함수의 사용 예제를 기술\n\n@export : NAMESPACE 파일의 export() 함수로 기술될 태그\nNAMESPACE 파일에서의 export 정보를 기술할지의 여부를 지정\n해당 태그를 기술할 경우만 export 정보를 기술한다.\n\nroxygen2 태그 기반 자동 생성\nroxygen2 패키지의 roxygenise() 함수는 R 소스 파일의 roxygen2 태그를 해석하여 도움말 파일인 Rd 파일과 NAMESPACE 파일을 생성합니다. 또한 DESCRIPTION 파일도 업데이트합니다.\n다음 스크립트를 실행하면 R/misc.R 파일을 읽어서 Rd 파일과 NAMESPACE 파일을 생성하고, DESCRIPTION 파일을 업데이트 합니다.\n\n\nroxygen2::roxygenise()\n\n\n\nRd 파일의 생성\nroxygenise()의 실행으로, 앞에서 만든 “search_naver.R”과 “trade_apt.R” 파일의 roxygen2 태그는 도움말 파일을 만들어졌습니다. 이 두 개의 도움말 중에서 “search_naver.Rd”는 다음과 같습니다.\n\n% Generated by roxygen2: do not edit by hand\n% Please edit documentation in R/search_naver.R\n\\name{search_naver}\n\\alias{search_naver}\n\\title{네이버 뉴스 검색}\n\\usage{\nsearch_naver(\n  query = NULL,\n  chunk = 100,\n  chunk_no = 1,\n  sort = c(\"date\", \"sim\"),\n  do_done = FALSE,\n  max_record = 1000L,\n  client_id = NULL,\n  client_secret = NULL,\n  verbose = TRUE\n)\n}\n\\arguments{\n\\item{query}{character. 검색을 원하는 문자열}\n\n\\item{chunk}{integer. 검색 결과 출력 건수 지정 (1~100)}\n\n\\item{chunk_no}{integer. 검색 시작 위치로 최대 1000까지 가능}\n\n\\item{sort}{character. 정렬 옵션: sim (유사도순), date (날짜순)}\n\n\\item{do_done}{logical. 한번의 호출로 모든 조회 결과를 가져오지 못할 경우,\n추가로 호출해서 모든 결과를 가져올지의 여부}\n\n\\item{max_record}{integer. 최대 조회할 건수. 실제로 검색한 건수는 max_record와 정확히 일치하지 않을 수 있습니다.\nchunk의 개수로 데이터를 수집하기 때문에 일반적인 경우에는 max_record보다 같거나  큰 chunk의 배수만큼 데이터를 가져옵니다.\ndo_done가 FALSE일 경우에는 적용되지 않습니다.}\n\n\\item{client_id}{character. 애플리케이션 등록 시 발급받은 Client ID}\n\n\\item{client_secret}{character. 애플리케이션 등록 시 발급받은 Client Secret}\n}\n\\value{\ndata.frame\n변수 목록은 다음과 같음.:\n\\itemize{\n\\item title : character. 기사의 타이틀\n\\item originallink : character. 검색 결과 문서의 제공 언론사 하이퍼텍스트 link\n\\item link : character. 검색 결과 문서의 제공 네이버 하이퍼텍스트 link\n\\item description : character. 검색 결과 문서의 내용을 요약한 패시지 정보.\n문서 전체의 내용은 link를 따라가면 읽을 수 있음. 패시지에서 검색어와 일치하는 부분은 태그로 감싸져 있음\n\\item publish_date : POSIXct. 검색 결과 문서가 네이버에 제공된 시간\n\\item title_text : character. 타이틀에서 HTML 태크를 제거한 텍스트\n\\item description_text : character. 요약한 패시지 정보에서 HTML 태크를 제거한 텍스트\n}\n}\n\\description{\n네이버 뉴스 검색 결과를 출력해주는 REST API를 호출하여, 뉴스 정보를 검색합니다.\n}\n\\details{\n네이버에서 발급받은 Client ID, Client Secret는 개인이 발급받은 키를 사용하며,\n유출되어서는 안됩니다.\n}\n\\examples{\n\\donttest{\n# Your authorized API keys\nclient_id <- \"XXXXXXXXXXXXXXXXXXXXXXX\"\nclient_secret <- \"XXXXXXXXX\"\n\nsearch_list <- search_naver(\n  \"불평등\", client_id = client_id, client_secret = client_secret\n)\n\nsearch_list <- search_naver(\n  \"불평등\", client_id = client_id, client_secret = client_secret,\n  do_done = TRUE, max_record = 350\n)\n\n}\n\n}\n\n그리고 패키지가 Build되면 이 도움말은 다음과 같은 도움말을 제공하게 됩니다.\n\n\n\nFigure 6: search_naver 함수의 도움말 화면\n\n\n\nNAMESPACE 파일의 생성\nNAMESPACE 파일도 변경되었습니다.\n@export 태그를 사용한 두 함수는 export()로, 그리고 참조하는 다른 패키지와 함수들은 import(), importFrom()로 기술되었습니다.\n\n\n# Generated by roxygen2: do not edit by hand\n\nexport(search_naver)\nexport(trade_apt)\nimport(dplyr)\nimportFrom(XML,getNodeSet)\nimportFrom(XML,xmlParse)\nimportFrom(XML,xmlToDataFrame)\nimportFrom(XML,xmlValue)\nimportFrom(glue,glue)\nimportFrom(httr,GET)\nimportFrom(httr,add_headers)\nimportFrom(purrr,map_df)\nimportFrom(stringr,str_pad)\n\n\n\nR 데이터 생성\n아파트 실거래 상세 조회를 위해서 법정동 코드가 필요합니다. 그래서 이 코드 정보를 패키지에 포함하려 합니다.\nR 데이터 파일 생성\nR 패키지에서 외부 파일을 넣는 디렉토리인 “inst”에 “meta”라는 디렉토리를 만든 후 법정동 코드 파일인 “법정동코드 전체자료.txt”을 복사해 넣은 뒤에 다음의 코드를 실행합니다.\n\n\nlibrary(dplyr)\nfname <- here::here(\"inst\", \"meta\", \"법정동코드 전체자료.txt\")\nlegal_divisions <- fname %>%\n  read.table(sep = \"\\t\", header = TRUE, fileEncoding = \"cp949\",\n             col.names = c(\"DIVISION_ID\", \"DIVISION_NM\", \"MAINTAIN\")) %>%\n  mutate(DIVISION_ID = format(DIVISION_ID, scientific = FALSE, trim = TRUE)) %>%\n  mutate(MAINTAIN = case_when(\n    MAINTAIN == \"존재\" ~ \"Y\",\n    MAINTAIN == \"폐지\" ~ \"N\")\n  ) %>%\n  mutate(MEGA_CD = substr(DIVISION_ID, 1, 2),\n         MEGA_NM = stringr::str_extract(DIVISION_NM, \"^[\\\\w]+\")) %>%\n  mutate(CTY_CD = substr(DIVISION_ID, 1, 5),\n         CTY_NM = stringr::str_extract(DIVISION_NM, \" [\\\\w]+\") %>%\n           stringr::str_remove(\"\\\\s\")) %>%\n  mutate(ADMI_CD = substr(DIVISION_ID, 1, 8),\n         ADMI_NM = stringr::str_remove(DIVISION_NM, \"^[\\\\w]+ [\\\\w]+ \")) %>%\n  filter(!stringr::str_detect(DIVISION_ID, \"000000$\"))\n\nsave(legal_divisions, file = \"data/legal_divisions.rda\")\n\n\ndb_name <- here::here(\"inst\", \"meta\", \"GISDB.sqlite\")\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), db_name)\nDBI::dbWriteTable(con, \"TB_LEGAL_DIVISIONS\", legal_divisions, overwrite = TRUE)\nDBI::dbDisconnect(con)\n\n\n\n이 코드가 실행되면, “./inst/meta” 경로에 “법정동코드 전체자료.txt”, “GISDB.sqlite”가 위치하고 “./data” 경로에 “legal_divisions.rda” 파일이 위치하게 됩니다.\nR 데이터 도움말 파일 생성\nlegal_divisons.R 파일에 도움말을 위한 roxygen2 태그를 기술하고, 앞에서 데이터를 만든 코드를 주석으로 보관합니다. 데이터의 도움말을 생성하는 roxygen2 태그는 함수의 도움말을 생성하는 것과 다소 차이가 있습니다.\n\n\n#' 행정구역 코드 정보\n#'\n#' @description\n#' 행정표준관리시스템의 법정동코드 전체자료로부터 추출한 광역시도,시군구, 읍면동 레벨의 코드 정보.\n#'\n#' @details\n#' 공공데이터포털에서 Open API로 행정구역 관련 데이터를 수집할 때, 입력 파라미터로 조직 코드를 사용할 경우가 많습니다.\n#' 이때 이 정보를 이용해서 원하는 지역의 정보를 수집할 수 있습니다.\n#'\n#' @format 45953 관측치와 9 변수를 갖는 data.frame. 다음과 같은 변수를 포함합니다.:\n#' \\describe{\n#'   \\item{DIVISION_ID}{charcter. 법정동 코드.}\n#'   \\item{DIVISION_NM}{charcter. 법정동 이름.}\n#'   \\item{MAINTAIN}{logical. 유지여부.}\n#'   \\item{MEGA_CD}{charcter. 광역시도 코드.}\n#'   \\item{MEGA_NM}{charcter. 광역시도 이름.}\n#'   \\item{CTY_CD}{charcter. 시군구 코드.}\n#'   \\item{CTY_NM}{charcter. 시군구 이름.}\n#'   \\item{ADMI_CD}{charcter. 읍면동 코드.}\n#'   \\item{ADMI_NM}{charcter. 읍면동 이름.}\n#' }\n#' @docType data\n#' @keywords datasets\n#' @name legal_divisions\n#' @usage data(legal_divisions)\n#' @source {\n#' \"행정표준관리시스템\" <https://www.code.go.kr/stdcodesrch/codeAllDownloadL.do>\n#' }\nNULL\n\n# library(dplyr)\n# fname <- here::here(\"inst\", \"meta\", \"법정동코드 전체자료.txt\")\n# legal_divisions <- fname %>%\n#   read.table(sep = \"\\t\", header = TRUE, fileEncoding = \"cp949\",\n#              col.names = c(\"DIVISION_ID\", \"DIVISION_NM\", \"MAINTAIN\")) %>%\n#   mutate(DIVISION_ID = format(DIVISION_ID, scientific = FALSE, trim = TRUE)) %>%\n#   mutate(MAINTAIN = case_when(\n#     MAINTAIN == \"존재\" ~ \"Y\",\n#     MAINTAIN == \"폐지\" ~ \"N\")\n#   ) %>%\n#   mutate(MEGA_CD = substr(DIVISION_ID, 1, 2),\n#          MEGA_NM = stringr::str_extract(DIVISION_NM, \"^[\\\\w]+\")) %>%\n#   mutate(CTY_CD = substr(DIVISION_ID, 1, 5),\n#          CTY_NM = stringr::str_extract(DIVISION_NM, \" [\\\\w]+\") %>%\n#            stringr::str_remove(\"\\\\s\")) %>%\n#   mutate(ADMI_CD = substr(DIVISION_ID, 1, 8),\n#          ADMI_NM = stringr::str_remove(DIVISION_NM, \"^[\\\\w]+ [\\\\w]+ \")) %>%\n#   filter(!stringr::str_detect(DIVISION_ID, \"000000$\"))\n#\n# save(legal_divisions, file = \"data/legal_divisions.rda\")\n#\n#\n# db_name <- here::here(\"inst\", \"meta\", \"GISDB.sqlite\")\n#\n# con <- DBI::dbConnect(RSQLite::SQLite(), db_name)\n# DBI::dbWriteTable(con, \"TB_LEGAL_DIVISIONS\", legal_divisions, overwrite = TRUE)\n# DBI::dbDisconnect(con)\n\n\n\nvignette 작성하기\nvignette 골격 생성하기\nusethis::use_vignette(“intro”, title = “introduce koscrap”) 명령어를 통해서 vignette 골격을 생성합니다. 이 명령어는 vignette의 타이틀이 “introduce koscrap”인 “intro.Rmd”라는 마크다운 문서 템플리트를 생성합니다. 또한 출력되는 메시지처럼 여러 작업을 수행합니다.\n\n> usethis::use_vignette(\"intro\", title = \"introduce koscrap\")\n✓ Setting active project to '/Users/choonghyunryu/Documents/01_Personal/00_bitr/01_packages/koscrap'\n✓ Adding 'knitr' to Suggests field in DESCRIPTION\n✓ Setting VignetteBuilder field in DESCRIPTION to 'knitr'\n✓ Adding 'inst/doc' to '.gitignore'\n✓ Creating 'vignettes/'\n✓ Adding '*.html', '*.R' to 'vignettes/.gitignore'\n✓ Adding 'rmarkdown' to Suggests field in DESCRIPTION\n✓ Writing 'vignettes/intro.Rmd'\n• Modify 'vignettes/intro.Rmd'\n> \n\nvignette 템플리트인 “intro.Rmd”은 다음과 같습니다.\n---\ntitle: \"introduce koscrap\"\noutput: rmarkdown::html_vignette\nvignette: >\n  %\\VignetteIndexEntry{introduce koscrap}\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteEncoding{UTF-8}\n---\n\n```{r, include = FALSE}\nknitr::opts_chunk$set(\n  collapse = TRUE,\n  comment = \"#>\"\n)\n```\n\n```{r setup}\nlibrary(koscrap)\n```\n\nvignette 작성하기\n템플리트를 토대로 해서 다음과 같은 vignette을 작성하였습니다. vignette에 명령어와 결과를 포함하여 작성한 이유는, API를 호출하기 위한 클라이언트 아이디, 보안키 등을 노출하지 않기 위함입니다. 만약 실행 코드만으로 vignette을 작성하려면, 클라이언트 아이디, 보안키 등의 노출되기 때문입니다.\n---\ntitle: \"introduce koscrap\"\noutput: rmarkdown::html_vignette\nvignette: >\n  %\\VignetteIndexEntry{introduce koscrap}\n  %\\VignetteEngine{knitr::rmarkdown}\n  %\\VignetteEncoding{UTF-8}\n---\n\n\n\n## koscrap에 대하여\nkoscrap(Korean Scraping)은 공공데이터포털의 아파트실거래 상세 내역과 NAVER의 뉴스검색 데이터를 수집하는 패키지입니다. REST 오픈 API를 이용해서 정해진 데이터를 수집할 수 있습니다.\n\n## koscrap 기능\n현재 구현된 koscrap의 기능을 다음과 같습니다.\n\n* 공공데이터포털의 아파트실거래\n  - trade_apt()\n* NAVER의 뉴스검색 \n  - search_naver()\n\n## 공공데이터포털의 아파트실거래 예제\n서울특별시 노원구의 2021년도 12월의 아파트 거래 내역을 수집합니다.\n\n먼저 공공데이터포털에서 발급받은 서비스키를 기술합니다. 본 예제는 서비스키는 개인의 정보이므로 여기서는 임의의 문자로 형식만 표기합니다.\n\n```{r, eval=FALSE}\nlibrary(\"koscrap\")\nlibrary(\"dplyr\")\n\nauth_key <- \"xxxxxxxxxxxxxxxxxxx\"\n```\n\n패키지에서 제공하는 법정동 코드 데이터인 `legal_divisions`로부터 노원구의 시군구 코드를 가져옵니다. 그리고 `trade_apt()`로 아파트거래정보를 수집합니다.\n\n```{r, eval=FALSE}\nLAWD_CD <- legal_divisions %>% \n  filter(CTY_NM %in% \"노원구\") %>% \n  select(CTY_CD) %>% \n  filter(row_number() == 1) %>% \n  pull()\n\nnowon <- trade_apt(auth_key, \n          LAWD_CD = LAWD_CD, \n          DEAL_YMD = \"202112\")\n\nglimpse(nowon)\n#> Rows: 59\n#> Columns: 24\n#> $ LAWD_CD       <chr> \"11350\", \"11350\", \"11350\", \"11350\", \"11350\", \"11350\", \"1…\n#> $ DEAL_DATE     <glue> \"2021-12-05\", \"2021-12-06\", \"2021-12-07\", \"2021-12-10\",…\n#> $ SERIAL        <chr> \"11350-149\", \"11350-133\", \"11350-150\", \"11350-151\", \"113…\n#> $ DEAL_TYPE     <chr> \"중개거래\", \"중개거래\", \"중개거래\", \"직거래\", \"중개거래\"…\n#> $ BUILD_NM      <chr> \"주공2\", \"한진한화그랑빌\", \"청백3\", \"청백4\", \"주공2\", \"…\n#> $ FLOOR         <int> 4, 24, 13, 14, 12, 2, 15, 9, 9, 12, 14, 19, 15, 8, 16, 1…\n#> $ BUILD_YEAR    <int> 1992, 2002, 1998, 1998, 1992, 2000, 1992, 1987, 1994, 19…\n#> $ AREA          <chr> \"58.65\", \"114.97\", \"39.84\", \"84.54\", \"44.52\", \"84.98\", \"…\n#> $ AMOUNT        <int> 67500, 125000, 42500, 75000, 53800, 84500, 48000, 73600,…\n#> $ ROAD_CD       <chr> \"4130377\", \"3109006\", \"4130332\", \"4130332\", \"4130377\", \"…\n#> $ ROAD_NM       <chr> \"초안산로1길\", \"마들로\", \"월계로45가길\", \"월계로45가길\",…\n#> $ BUILD_MAJOR   <chr> \"00018\", \"00031\", \"00089\", \"00094\", \"00018\", \"00049\", \"0…\n#> $ BUILD_MINOR   <chr> \"00000\", \"00000\", \"00000\", \"00000\", \"00000\", \"00000\", \"0…\n#> $ ROAD_SEQ      <chr> \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"01\", \"0…\n#> $ BASEMENT_FLAG <chr> \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"0\", \"…\n#> $ LAND_NO       <chr> \"556\", \"18\", \"780\", \"781\", \"556\", \"929\", \"556\", \"12\", \"7…\n#> $ DONG_NM       <chr> \" 월계동\", \" 월계동\", \" 월계동\", \" 월계동\", \" 월계동\", \"…\n#> $ DONG_MAJOR    <chr> \"0556\", \"0018\", \"0780\", \"0781\", \"0556\", \"0929\", \"0556\", …\n#> $ DONG_MINOR    <chr> \"0000\", \"0000\", \"0000\", \"0000\", \"0000\", \"0000\", \"0000\", …\n#> $ EUBMYNDONG_CD <chr> \"10200\", \"10200\", \"10200\", \"10200\", \"10200\", \"10200\", \"1…\n#> $ DONG_LAND_NO  <chr> \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"…\n#> $ DEALER_ADDR   <chr> \"서울 강북구, 서울 성북구\", \"서울 노원구\", \"서울 노원구\"…\n#> $ CANCEL_DEAL   <chr> \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \"…\n#> $ CANCEL_DATE   <chr> \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \", \"…\n```\n\n노원구에서 12월에 가장 고가로 매매된 아파트의 가격은 13억3천5백만원입니다. 그리고 매매건수는 상계동>중계동>공릉동 순입니다.\n\n```{r, eval=FALSE}\nnowon %>% \n  summarise(max_price = max(AMOUNT))\n#>   max_price\n#> 1    133500\n\nnowon %>% \n  group_by(DONG_NM) %>% \n  tally() %>% \n  arrange(desc(n))\n#> # A tibble: 5 x 2\n#>   DONG_NM       n\n#>   <chr>     <int>\n#> 1 \" 상계동\"    27\n#> 2 \" 중계동\"    11\n#> 3 \" 공릉동\"    10\n#> 4 \" 월계동\"     8\n#> 5 \" 하계동\"     3\n```\n2006년 10월의 성동구의 매매 현황을 수집합니다. chunk의 값을 800으로 지정해서 765건을 수집했습니다. chunk의 기본값은 100으로 100건을 수집합니다.\n\n```{r, eval=FALSE}\nLAWD_CD <- legal_divisions %>% \n  filter(CTY_NM %in% \"성동구\") %>% \n  select(CTY_CD) %>% \n  filter(row_number() == 1) %>% \n  pull()\n\nsungdong <- trade_apt(auth_key, \n                      LAWD_CD = \"11200\", \n                      DEAL_YMD = \"200610\",\n                      chunk = 800)\n\nNROW(sungdong)\n#> [1] 765\n```\n  \n## 네이버 뉴스 검색 예제\n\"불평등\"이라는 키워드로 뉴스현황을 검색합니다.\n\n사용자의 클라이언트 아이디와 보안키를 입력합니다. 사전에 네이버로부터 발급받아야 합니다.\n\n```{r, eval=FALSE}\n# Your authorized API keys\nclient_id <- \"XXXXXXXXXXXXXXXXXXXXXXX\"\nclient_secret <- \"XXXXXXXXX\"\n```\n\n총 285,030건의 뉴스가 검색되었고 100건을 수집했습니다.\n\n```{r, eval=FALSE}\ninequality <- search_naver(\n  \"불평등\", client_id = client_id, client_secret = client_secret\n)\n#> * 검색된 총 기사 건수는 285036건입니다.\n#> - (100/285036)건 호출을 진행합니다.\n```\n\n1000건을 수집해 보겠습니다.\n\n```{r, eval=FALSE}\ninequality_1000 <- search_naver(\n  \"불평등\", client_id = client_id, client_secret = client_secret,\n  do_done = TRUE, max_record = 1000\n)\n#> * 검색된 총 기사 건수는 285036건입니다.\n#> - (100/1000)건 호출을 진행합니다.\n#> - (200/1000)건 호출을 진행합니다.\n#> - (300/1000)건 호출을 진행합니다.\n#> - (400/1000)건 호출을 진행합니다.\n#> - (500/1000)건 호출을 진행합니다.\n#> - (600/1000)건 호출을 진행합니다.\n#> - (700/1000)건 호출을 진행합니다.\n#> - (800/1000)건 호출을 진행합니다.\n#> - (900/1000)건 호출을 진행합니다.\n#> - (1000/1000)건 호출을 진행합니다.\n```\n\n```{r, eval=FALSE}\nhead(inequality_1000)\n#>                                                                               title\n#> 1                                          이재명 후보, 문화예술 분야 6대 공약 발표\n#> 2                        조영수 이사장 “다른 사람의 관점에서 나를 성찰하게 하는 책”\n#> 3                [D:이슈] 장르물에 녹여낸 한국 사회…넷플릭스 오리지널 장점이자 한계\n#> 4              &quot;기후위기 대응 64개국 중 59위…우물 안 K-대통령 돼선 안 돼&quot;\n#> 5                       [지자체의 SR] 의정부시, 스마트시티 조성 탄력…국비 20억 확보\n#> 6 이재명 &quot;미래산업 핵심은 재생에너지&quot; 윤석열 &quot;데이터·AI가 핵심&quot;\n#>                                                                     originallink\n#> 1 http://www.cine21.com/news/view/?mag_id=99572&utm_source=naver&utm_medium=news\n#> 2                                         http://ch.yes24.com/Article/View/46942\n#> 3                          https://www.dailian.co.kr/news/view/1079823/?sc=Naver\n#> 4                                       https://www.nocutnews.co.kr/news/5700762\n#> 5                       http://www.srtimes.kr/news/articleView.html?idxno=107666\n#> 6                        https://www.eroun.net/news/articleView.html?idxno=27524\n#>                                                                                      link\n#> 1 https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=106&oid=140&aid=0000046515\n#> 2                                                  http://ch.yes24.com/Article/View/46942\n#> 3 https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=103&oid=119&aid=0002572721\n#> 4 https://news.naver.com/main/read.naver?mode=LSD&mid=sec&sid1=102&oid=079&aid=0003604477\n#> 5                                http://www.srtimes.kr/news/articleView.html?idxno=107666\n#> 6                                 https://www.eroun.net/news/articleView.html?idxno=27524\n#>                                                                                                                                                                                                                                                                         description\n#> 1                                                               방식의 영화 제작 지원을 확대하고, 애니메이션 투자펀드를 만들겠다는 것이다. 이 후보는 대기업의 독과점, 계약 관계의 <b>불평등<\/b> 등을 시정해 문화예술 분야에 공정한 생태계를 조성할 수 있도록 노력하겠다고 강조했다.\n#> 2                                                               그리고 글로벌 각국이 갖는 <b>불평등<\/b>에 대한 의문을 해소해주는 책이죠. 『수레바퀴 밑에서』 헤르만 헤세 저 수레바퀴 밑에서 헤르만 헤세 저 | 강태정 역 일신서적출판사 주인공 한스의 방황과 고뇌가 고등학생 시절... \n#> 3                                                              나아가 극한 경쟁에 내몰린 현대인과 개인의 힘으로는 극복하기 힘든 <b>불평등<\/b> 문제 등 작품에 녹아있는 메시지도 ‘오징어 게임’을 한층 새로운 데스 게임 드라마로 보이게 했다. 그러나 국내 시청자들은 ‘오징어 게임’... \n#> 4                                                 그다음에 <b>불평등<\/b> 여러 가지 쭉 나오는데 이 기후위기는 10위 안에도 못 들어갔습니다. ◇이윤상&gt; 기후위기가... 물론 부동산, 일자리, <b>불평등<\/b> 모두 중요하지만 기후위기는 모든 인류를 다 끝내는 거 아닙니까 그래서 이런... \n#> 5                                                            사업 대상지 내 중랑천 약 4Km 구간에 다목적 AI 스마트 폴 및 스마트 게이트 등을 구축해 사물인터넷과 스마트 정보 체계를 연동하고 홍수 등 재난 위험 상황에 대한 즉각 대응체계 마련 및 정보 서비스 <b>불평등<\/b> 환경을... \n#> 6 尹 &quot;핵폐기물 처리기술 고도화될 것&quot; 李 &quot;원전 정략대상 아냐&quot; 심상정 &quot;<b>불평등<\/b> 해소·녹색전환&quot; 안철수... 沈 &quot;<b>불평등<\/b> 해소·녹색전환&quot; 安 &quot;과학기술 주권국가&quot; 강조 토론 중에는 각 후보의 일자리·성장 방안에 대한 철학도... \n#>          publish_date\n#> 1 2022-02-04 15:17:00\n#> 2 2022-02-04 14:56:00\n#> 3 2022-02-04 14:49:00\n#> 4 2022-02-04 14:44:00\n#> 5 2022-02-04 14:42:00\n#> 6 2022-02-04 14:14:00\n#>                                                       title_text\n#> 1                        이재명 후보 문화예술 분야 6대 공약 발표\n#> 2       조영수 이사장 다른 사람의 관점에서 나를 성찰하게 하는 책\n#> 3 D이슈 장르물에 녹여낸 한국 사회넷플릭스 오리지널 장점이자 한계\n#> 4         기후위기 대응 64개국 중 59위우물 안 K대통령 돼선 안 돼\n#> 5        지자체의 SR 의정부시 스마트시티 조성 탄력국비 20억 확보\n#> 6       이재명 미래산업 핵심은 재생에너지 윤석열 데이터AI가 핵심\n#>                                                                                                                                                                                                  description_text\n#> 1    방식의 영화 제작 지원을 확대하고, 애니메이션 투자펀드를 만들겠다는 것이다. 이 후보는 대기업의 독과점, 계약 관계의 불평등 등을 시정해 문화예술 분야에 공정한 생태계를 조성할 수 있도록 노력하겠다고 강조했다.\n#> 2    그리고 글로벌 각국이 갖는 불평등에 대한 의문을 해소해주는 책이죠. 『수레바퀴 밑에서』 헤르만 헤세 저 수레바퀴 밑에서 헤르만 헤세 저 | 강태정 역 일신서적출판사 주인공 한스의 방황과 고뇌가 고등학생 시절... \n#> 3   나아가 극한 경쟁에 내몰린 현대인과 개인의 힘으로는 극복하기 힘든 불평등 문제 등 작품에 녹아있는 메시지도 ‘오징어 게임’을 한층 새로운 데스 게임 드라마로 보이게 했다. 그러나 국내 시청자들은 ‘오징어 게임’... \n#> 4 그다음에 불평등 여러 가지 쭉 나오는데 이 기후위기는 10위 안에도 못 들어갔습니다. ◇이윤상 기후위기가... 물론 부동산, 일자리, 불평등 모두 중요하지만 기후위기는 모든 인류를 다 끝내는 거 아닙니까 그래서 이런... \n#> 5 사업 대상지 내 중랑천 약 4Km 구간에 다목적 AI 스마트 폴 및 스마트 게이트 등을 구축해 사물인터넷과 스마트 정보 체계를 연동하고 홍수 등 재난 위험 상황에 대한 즉각 대응체계 마련 및 정보 서비스 불평등 환경을... \n#> 6         尹 핵폐기물 처리기술 고도화될 것 李 원전 정략대상 아냐 심상정 불평등 해소·녹색전환 안철수... 沈 불평등 해소·녹색전환 安 과학기술 주권국가 강조 토론 중에는 각 후보의 일자리·성장 방안에 대한 철학도...\n```\nvignette 빌드하기\ndevtools::build_vignettes() 명령어를 통해서 vignette을 빌드합니다.\n\n> devtools::build_vignettes()\nBuilding koscrap vignettes\n--- re-building ‘intro.Rmd’ using rmarkdown\n--- finished re-building ‘intro.Rmd’\n\n✓ Creating 'doc/'\n✓ Adding '^doc$' to '.Rbuildignore'\n✓ Adding 'doc' to '.gitignore'\nMoving intro.html, intro.R to doc/\nCopying intro.Rmd to doc/\n✓ Creating 'Meta/'\n✓ Adding '^Meta$' to '.Rbuildignore'\n✓ Adding 'Meta' to '.gitignore'\nBuilding vignette index\n> \n\n“Meta” 디렉토리는 vignette에 대한 메타정보를 포함하고 있습니다.\n빌드 후 생성된 “doc” 리렉토리는 “inst” 디렉토리 하위 디렉토리로 이동시켜야 합니다.\n개발자의 koscrap 패키지 설치하기\n이제 R패키가 얼추 만들어졌습니다. 마지막으로 패키지를 빌드하는 일만 남았습니다.\nR 패키지 설치하기\n패키지를 설치하기 위해서 RStudio의 “build 탭 메뉴”의 “Install and Restart”를 사용합니다.\n\n\n\nFigure 7: R 패키지 설치\n\n\n\nR 패키지 설치하는 과정이 집행된 후, 정상적으로 패키지가 만들어지면 자동적으로 콘솔 화면에서 library() 함수로 패키지를 로딩합니다.\nkoscrap 패키지 사용하기\nkoscrap 패키지의 도움말을 선택하면, 다음과 같은 화면을 만나게 됩니다.\n\n\n\nFigure 8: koscrap 도움말 홈페이지\n\n\n\n다음 명령어를 입력하면 vignette 문서를 볼 수 있습니다.\n\n\nvignette(\"intro\", package = \"koscrap\")\n\n\n\n\n\n\nFigure 9: koscrap vignette 화면\n\n\n\n공공데이터포털의 아파트 실거래 상세조회를 수행해 봅니다.\n먼저 사용자의 서비스키를 입력합니다. 여기서는 서비스키의 보안 유지를 위해서 임의의 문자로 채워 넣었습니다.\n\n\nlibrary(\"koscrap\")\nauth_key <- \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n\n\n2020년 6월 지역코드가 “11290”인 곳의 거래 정보를 가져옵니다. chunk가 1000이므로 한번 호출에 1000건 이내의 정보를 가져옵니다. 물론 해당 지역의 거래건수 내에서 가져오게 됩니다. do_done이 TRUE인 의미는 만약 해당 지역의 해당원 거래량이 chunk에서 지정한 건수보다 클 경우에, 함수 내부적으로 추가 호출하여 전 건을 가져오라는 의미입니다.\n\nresult <- trade_apt(auth_key, LAWD_CD = \"11290\", DEAL_YMD = \"202006\", chunk = 1000, do_done = TRUE)\n\n> head(result)\n  LAWD_CD  DEAL_DATE   SERIAL DEAL_TYPE BUILD_NM FLOOR BUILD_YEAR   AREA AMOUNT ROAD_CD      ROAD_NM\n1   11290 2020-06-01  11290-4               성북     3       1971  37.36  17000 4121350   성북로31길\n2   11290 2020-06-03  11290-4               성북     1       1971  37.36  14500 4121350   성북로31길\n3   11290 2020-06-11  11290-4               성북     2       1971  37.36  15000 4121350   성북로31길\n4   11290 2020-06-15 11290-11           성북좋은     5       2002 113.54  80000 4121353  성북로6가길\n5   11290 2020-06-18  11290-4               성북     1       1971  37.36  16000 4121350   성북로31길\n6   11290 2020-06-22  11290-8               주암     3       1972  48.73  23000 4121349 성북로31가길\n  BUILD_MAJOR BUILD_MINOR ROAD_SEQ BASEMENT_FLAG LAND_NO DONG_NM DONG_MAJOR DONG_MINOR EUBMYNDONG_CD\n1       00040       00012       01             0   348-3  성북동       0348       0003         10100\n2       00040       00012       01             0   348-3  성북동       0348       0003         10100\n3       00040       00012       01             0   348-3  성북동       0348       0003         10100\n4       00023       00000       01             0  179-96  성북동       0179       0096         10100\n5       00040       00012       01             0   348-3  성북동       0348       0003         10100\n6       00005       00000       01             0     286  성북동       0286       0000         10100\n  DONG_LAND_NO DEALER_ADDR CANCEL_DEAL CANCEL_DATE\n1            1                                    \n2            1                                    \n3            1                                    \n4            1                                    \n5            1                                    \n6            1                                     \n\n이상으로 koscrap 패키지의 간단한 사용을 통해서 패키지가 이상없이 만들어졌음을 파악했습니다.\n사용자의 koscrap 패키지 설치하기\nkoscrap 패키지는 github의 public repository에 올려 놓았습니다. github은 CRAN과 더불어 많은 R 패키지를 배포하는 채널이기도 합니다. CRAN은 까다로운 조건 때문에 R 패키지를 submit하기 쉽지 않습니다. 어떤 경우에는 제약을 해결하지 못하는 관계로 github에만 패키지를 등록하는 경우도 있습니다.\n여기서는 github에 올려 놓은 패키지를 설치하는 방법을 다룹니다.\ndevtools 패키지 설치\n먼저 devtools 패키지를 설치합니다.\n\n\ninstall.packages(\"devtools\")\n\n\n\nkoscrap 패키지 설치\ndevtools 패키지의 install_github() 함수로 koscrap 패키지를 설치합니다.\nkoscrap 패키지의 리파지토리 URL이 https://github.com/choonghyunryu/koscrap이므로 뒷 자리의 정보 사용자/리파지토리이름을 기술하여 설치합니다.\n\n\ndevtools::install_github(\"choonghyunryu/koscrap\")\n\n\n\n\n소품(Vignettes)은 비네트라고 발음합니다. 패키지의 기능을 소개하는 짧은 문서를 의미합니다. 함수의 도움말보다는 규모가 큽니다.↩︎\n",
    "preview": "posts/2022-03-04-develop-package/img/rpackage.jpeg",
    "last_modified": "2022-03-04T00:04:48+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-03-coding-style/",
    "title": "R 코딩 스타일 가이드",
    "description": "패키지 개발에 앞서서, 중요하지만 간과하는 R 코딩 스타일을 살펴봅니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-03",
    "categories": [
      "programming"
    ],
    "contents": "\n\nContents\nR 코딩 스타일\n중요한 R 코딩 스타일\n기타 중요한 규약\n객체 이름\n중괄호 내에서는 항상 들여 쓰기\n코드의 라인 길이는 80자 이내\n코드 들여쓰기(indentation)은 공백 2자로, 탭을 사용하지 않는다.\n할당 연산자의 선택\n주석의 생활화\n\n코딩 스타일 지원 패키지\nformatR\nlintr\n\n\n\n\n\n\n\n들어가기\n혼자면 충분합니다.\n\n둘이면 어쩌면 필요합니다.\n\n셋 이상이면 필수입니다. 뭐냐구요? 협업을 위해서 필요한 R 코딩 스타일 가이드입니다.\n\n\nR 코딩 스타일\n혼자서 패키지를 개발할 경우에는 문제가 덜하지만, 여럿이 협업해서 패키지를 개발할 경우에는 R 코딩 스타일이 이슈가 될 수 있습니다. 사람마다 서로 다른 코딩 스타일을 사용할 경우에는 코드의 일관성이 결여되고 협업에 어려움이 따르게 됩니다. 이 경우에는 서로 합일한 R 코딩 스타일 규칙을 따르는 것이 필요합니다. 굳이 협업이 아니더라도 잘 정의된 코딩 스타일을 고수할 필요 있습니다.\n\nGood coding style is like using correct punctuation. You can manage without it, but it sure makes things easier to read. – Hadley Wickham\n\nR 코딩 스타일은, 아주 오랜 전에 나온 Google의 R 스타일도 있지만, The tidyverse style guide를 준수하는 것을 권장합니다.\n중요한 R 코딩 스타일\nThe tidyverse style guide에 기술된 대표적인 코딩 스타일 컨벤션\n구분\n규약\nGood\nBad\n객체 이름\n소문자로 정의\nday_one, day_1\nDayOne, dayone\n객체 이름\n예약어나, 기정의 이름 불가\n\nT, mean\n공백\n컴마 뒤에 공백문자\nx[, 1]\nx[,1], x[ ,1]\n\n함수 호출 괄호 앞뒤에 공백문자 불가\nmean(x, na.rm = TRUE)\nmean (x, na.rm = TRUE)\n\n\n\nmean( x, na.rm = TRUE )\n\n괄호 뒤에 공백문자\nfunction(x) {}\nfunction (x) {}\n\n\n\nfunction(x){}\n\n연산자 앞 뒤에 공백문자\nheight <- (feet * 12) + inches\nheight<-feet*12+inches\n\n\nmean(x, na.rm = TRUE)\nmean(x, na.rm=TRUE)\n\n연산자 앞 뒤에 공백문자 불가\nsqrt(x^2 + y^2)\nsqrt(x ^ 2 + y ^ 2)\n\n\ndf$z\ndf $ z\n\n\nx <- 1:10\nx <- 1 : 10\n기타 중요한 규약\n객체 이름\n객체의 이름은 영문과 숫자, 그리고 문자 \"_\"으로 정의합니다. 그리고, 간결하지만, 의미를 충분히 전달해야 합니다.\n변수는 명사를 사용하고,\n예) item, score\n\n함수는 동사를 사용하고\n예) edit_table, print\n\n중괄호 내에서는 항상 들여 쓰기\n\n\n# Good\nif (y < 0 && debug) {\n  message(\"y is negative\")\n}\n\n# Bad\nif (y < 0 && debug) {\nmessage(\"Y is negative\")\n}\n\n\n\n코드의 라인 길이는 80자 이내\n코드의 라인 길이는 80자 이내로 관리합니다. Rstudio의 Global Options… > Code > Display 메뉴에서 Show margin을 체크하면, 에디터에 기준선이 80자를 표현해서 80자 이내로 관리하는데 유용합니다.\n\n\n\n코드 들여쓰기(indentation)은 공백 2자로, 탭을 사용하지 않는다.\n코드의 들여쓰기는 공백 2로 설정합니다. Rstudio의 Global Options… > Code > Edit 메뉴에서 Insert spaces for tab을 체크한 후, 탭을 공백 2개로 설정하면 유용합니다. 공백의 개수는 2개, 4개 등 호불호가 있기는 합니다만 개인적으로 2개를 추천합니다.\n\n\n\n할당 연산자의 선택\n이름에 객체를 할당할 때에는 <- 연산자를 사용\n예) average <- mean(x, na.rm = TRUE)\n\n함수에서 인수에 인수값을 할당할 때에는 =를 사용\n예) edit_table, print\n\n주석의 생활화\n주요 로직이나, 공유해야할 사항, 남겨야할 정보 등을 반드시 주석으로 남겨야 합니다.\n코딩 스타일 지원 패키지\n이 장에서는 tidyverse R 코딩 스타일 가이드에 어느 정도 부합하는 몇 가지 R 패키지를 소개합니다. 그리고 이들 패키지로 사용자가 작성한 소스가 R 코딩 스타일에 부합하는지의 여부를 점검하거나, 패키지에서 추천하는 코딩 스타일로 소스를 변경해주는 방법을 알아봅니다.\n\n이들 패키지가 100% 완전한 코딩 스타일을 준수하는 코드로 바꿔주거나 제언해주지 않음을 염두에 두어야 합니다.\n\nformatR\nformatR 패키지는 R 코드의 포맷을 자동으로 바꿔주는 기능을 수행하는 패키지입니다. 이 패키지를 사용하여 작성한 R 코드를 스타일 가이드에 의거하여 적절하게 바꿔주는 방법을 살펴봅니다.\n예제는 패키지의 R 소스 코드를 담고 있는 “R” 디렉토리 내의 misc.R 파일에 getMonthLenth() 함수 정의 코드가 들어 있음을 가정합니다.\nR/misc.R 파일의 getMonthLenth() 함수를 정의하는 코드는 다음과 같습니다.\n\n\ngetMonthLenth <- function(startYear, startMonth, endYear, endMonth) {\n    ifelse(startMonth > endMonth,\n         12 + endMonth - startMonth + (endYear - startYear - 1) * 12 + 1,\n         endMonth - startMonth + (endYear - startYear) * 12 + 1)\n}\n\n\n\nformatR 패키지의 tidy_file() 함수는 지정한 파일에 대해서 R 코드의 포맷을 자동으로 바꿔주고, tidy_dir() 함수는 지정한 디렉토리에 포함된 R 파일에 대해서 코드 포맷을 보정해 줍니다.\n다음은 패키지의 R 소스 코드에 대해서 포맷을 보정해 주는 명령어입니다.\n\n> formatR::tidy_dir(\"R\", indent = 2, width.cutoff = 60)\ntidying R/misc.R\n\n상기 코드가 실행되면 R/misc.R 파일의 getMonthLenth() 함수를 정의하는 코드는 다음과 같이 보정됩니다. indent 인수의 값이 2인것은 들여쓰기를 스페이스 2개로 지정한 것이고, width.cutoff 인수는 할 줄에 표현하는 코드의 길이를 60 character로 지정한다는 의미입니다.\n\n\ngetMonthLenth <- function(startyear, startmonth, endyear, endmonth) {\n  ifelse(startmonth > endmonth, 12 + endmonth - startmonth + \n    (endyear - startyear - 1) * 12 + 1, endmonth - startmonth + \n    (endyear - startyear) * 12 + 1)\n}\n\n\n\nlintr\nlintr 패키지는 R 코딩 스타일 부합 여부를 점검해줍니다. R 코드를 변경하지 않고 점검만 해주는 점이 formatR 패키지와의 차이점입니다. 그러나 lintr 패키지의 점검 내용은 포맷보다는 코딩 스타일에 가깝습니다. 예를 들면 변수의 이름은 소문자로 사용하는 것, 할당 연산자를 = 대신에 <-를 사용하는 것 등의 암묵적인 R 코딩 스타일의 준수 여부를 체크합니다.\nlint_file() 함수는 지정하는 R 파일에 대해서 R 코딩 스타일의 준수 여부를 체크하고, lint_package() 함수는 패키지 내의 R 파일에 대해서 R 코딩 스타일의 준수 여부를 체크합니다.\n다음은 패키지의 R 소스 코드에 대해서 R 코딩 스타일의 준수 여부를 체크합니다. RStudio 상에서의 결과는 아래의 그림처럼 Markers 탭에 표현되며, 체크에서 미준수 항목으로 출력된 개별 라인을 클릭하면, 소스 파일의 해당 위치로 커서가 옮겨집니다.\n\n> lintr::lint_package()\n\n\n\n\n체크에서 미준수 항목으로 출력된 개별 라인을 수정한 후 lint_package() 함수를 재 실행하면서 코드를 수정하면 R 코딩 스타일에 부합하는 코드를 작성할 수 있습니다.\n미준수 항목으로 출력된 개별 라인을 수정하여, getMonthLenth() 함수를 다음과 같이 재작성하였습니다.\n\n\nget_month_length <- function(startyear, startmonth, endyear, endmonth) {\n  ifelse(startmonth > endmonth, 12 + endmonth - startmonth +\n    (endyear - startyear - 1) * 12 + 1, endmonth - startmonth +\n    (endyear - startyear) * 12 + 1)\n}\n\n\n\n\n\n\n",
    "preview": "posts/2022-03-03-coding-style/img/coding.jpeg",
    "last_modified": "2022-03-06T09:16:54+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-02-apt-api/",
    "title": "공공데이터포털 오픈 API를 이용한 데이터 수집",
    "description": "공공데이터포털 오픈 API를 이용한 데이터 수집 로직을 구현해 봅니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-02",
    "categories": [
      "Open API",
      "XML",
      "Data acquisition"
    ],
    "contents": "\n\nContents\n공공데이터포털\n아파트매매 실거래 상세 자료 수집\nAPI 기본 정보\n요청 변수 (request parameter)\n출력 결과\n준비사항\n\n아파트매매 실거래 상세 자료 조회 프로그램 개발\n요청 URL 생성\n법정동 코드 준비하기\nAPI 호출\nXML 파싱\n다건 처리 로직\n함수의 완성\n함수의 호출\n\n\n\n\n\n\n\n들어가기\n여러분은 NAVER의 오픈 API를 다루어 보았습니다.\n\n그것은 또 다른 오픈 API를 사용할 수 있다는 것을 의미합니다.\n\n이제 여세를 몰아 여러분의 내공이 공공데이터의 수집을 갈망하게 됩니다.\n\n\n공공데이터포털\n공공데이터포털은 정부/지자체의 공공 데이터 제공을 목적으로 행정안전부에서 운영하는 사이트입니다.\n2022-02-03일 기준으로 68,323건의 데이터를 공개하고 있으며, 오픈 API로는 8,745건을 공개하고 있습니다. 모든 데이터가 질적으로 우수하다고 단언하기는 어렵지만 몇몇 데이터는 인기가 많습니다.\n데이터를 사용하기 위해서는 회원가입이 필수이며, 오픈 API를 사용하기 위해서는 활용신청 후 승인을 받아야 합니다.\n아파트매매 실거래 상세 자료 수집\n우리는 이제 국토교통부의 아파트매매 실거래 상세 자료를 수집하는 방법을 살펴봅니다. 별도의 아파트매매 실거래 자료 API가 있으므로 주의해야 합니다.\nAPI 기본 정보\n국토교통부_아파트매매 실거래 상세 자료 상세 페이지에서 여러 정보를 확인할 수 있습니다.\n출력 포맷이 XML인 API이며, API 서비스 주소는 다음과 같습니다.\n서비스 URL\nhttp://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev\n\n요청 변수 (request parameter)\n요청 변수는 다음과 같습니다. 역시 인증을 위한 서비스키가 필요합니다.\n영문명의 일관성이 떨어집니다. 대문자로만 구성된 영문명과 대소문자로 구성된 영문명이 혼재합니다. 단어의 결합 방법도 다릅니다. 언더라인(_) 사용한 것과 캡문자로 구분하는 방법이 섞여있습니다.\n항목명(국문)\n항목명(영문)\n항목크기\n항목구분\n샘플데이터\n항목설명\n서비스키\nServiceKey\n20\n필수\n-\n공공데이터포털에서 받은 인증키\n페이지 번호\npageNo\n4\n옵션\n1\n페이지번호\n한 페이지 결과 수\nnumOfRows\n4\n옵션\n10\n한 페이지 결과 수\n지역코드\nLAWD_CD\n5\n필수\n11110\n지역코드\n계약월\nDEAL_YMD\n6\n필수\n201512\n계약월\n출력 결과\n출력 결과 정보는 너무 실망스럽습니다. 영문명은 일부 헤더 데이터만 제공하고, 실 데이터는 영문명없이 한글명으로 이루어져 있습니다. 이것은 한글사랑, 애국의 문제가 아닙니다.\nAPI 프로그래밍을 위해서 변수명(영문명)은 영문으로 정의되어야 합니다. 변수명은 아스키(ASCII)1 문자 안에서의 알파벳, 숫자와 르포그램언어에서 지원하는 몇개의 문자로 구성해야 합니다.\n더욱 심각한 문제는 이 정보가 현행화되어 있지 않습니다. 아마도 항목이 늘어난 것 같은데, 일부 데이터 항목이 이 상세 페이지의 출력결과에 누락되어 있습니다.\n항목명(국문)\n항목명(영문)\n항목크기\n항목구분\n샘플데이터\n항목설명\n결과코드\nresultCode\n2\n필수\n00\n결과코드\n결과메시지\nresultMsg\n50\n필수\nOK\n결과메시지\n한 페이지 결과 수\nnumOfRows\n4\n필수\n10\n한 페이지 결과 수\n페이지 번호\npageNo\n4\n필수\n1\n페이지번호\n전체 결과 수\ntotalCount\n4\n필수\n3\n전체 결과 수\n거래금액\n거래금액\n40\n필수\n82,500\n거래금액\n건축년도\n건축년도\n4\n필수\n2008\n건축년도\n년\n년\n4\n필수\n2015\n년\n도로명\n도로명\n40\n필수\n사직로8길\n도로명\n도로명건물본번호코드\n도로명건물본번호코드\n5\n필수\n00004\n도로명건물본번호코드\n도로명건물부번호코드\n도로명건물부번호코드\n5\n필수\n00000\n도로명건물부번호코드\n도로명시군구코드\n도로명시군구코드\n5\n필수\n11110\n도로명시군구코드\n도로명일련번호코드\n도로명일련번호코드\n2\n필수\n03\n도로명일련번호코드\n도로명지상지하코드\n도로명지상지하코드\n1\n필수\n0\n도로명지상지하코드\n도로명코드\n도로명코드\n7\n필수\n4100135\n도로명코드\n법정동\n법정동\n40\n필수\n사직동\n법정동\n법정동본번코드\n법정동본번코드\n4\n필수\n0009\n법정동본번코드\n법정동부번코드\n법정동부번코드\n4\n필수\n0000\n법정동부번코드\n법정동시군구코드\n법정동시군구코드\n5\n필수\n11110\n법정동시군구코드\n법정동읍면동코드\n법정동읍면동코드\n5\n필수\n11500\n법정동읍면동코드\n법정동지번코드\n법정동지번코드\n1\n필수\n1\n법정동지번코드\n아파트\n아파트\n40\n필수\n광화문풍림스페이스본(9-0)\n아파트\n월\n월\n2\n필수\n12\n월\n일\n일\n6\n필수\n1~10\n일\n일련번호\n일련번호\n14\n필수\n11110-2203\n일련번호\n전용면적\n전용면적\n20\n필수\n94.51\n전용면적\n지번\n지번\n10\n필수\n9\n지번\n지역코드\n지역코드\n5\n필수\n11110\n지역코드\n층\n층\n4\n필수\n11\n층\n오픈API 상세 화면에서 참고문서인 아파트 매매 상세자료 조회 기술문서.hwp 파일을 다운로드하면 유용하게 활용할 수 있습니다.\n\n\n\nFigure 1: 오픈API 상세 화면\n\n\n\n이 파일에는 홈페이지에서 누락된 출력 결과가 기술되어 있습니다. 다음은 파일에서의 출력결과 정보의 일부입니다.\n\n\n\nFigure 2: 출력결과 정보의 일부\n\n\n\n파일의 정보에는 영문명이 있으나, 이것은 프로그래밍을 위한 변수 이름으로 사용할 수 없겠습니다. 결국은 API 프로그램 개발자가 변수 이름을 정의해야할 것 같습니다. 아쉬운 점이 많습니다.\n준비사항\n국토교통부_아파트매매 실거래 상세 자료 상세 오픈 API를 사용하기 위해서는 활용신청을 통해서 미리 승인을 받아야 합니다.\n승인이 되면 마이페이지에서 개발계정 상세보기에서 다음과 같은 정보를 확인할 수 있습니다. 일반 인증키 역시 외부로 노출되지 않도록 주의해야 합니다. 이 서비스는 일일 트래픽이 1000회로 제한되어 있습니다.\n\n\n\nFigure 3: 개발계정 상세보기 화면\n\n\n\n아파트매매 실거래 상세 자료 조회 프로그램 개발\n요청 URL 생성\nXML 출력 포맷을 사용하기 때문에 다음 요청 URL을 사용합니다.\nhttp://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev\nGET 방식의 호출 URL이기 때문에 요청변수 영역을 ?로 구분하고, 요청 변수들은 &로 구분합니다. 요청변수는 다음과 같습니다.\nServiceKey : 공공데이터포털에서 받은 인증키\npageNo : 페이지번호\nnumOfRows : 한 페이지 결과 수\nLAWD_CD : 지역코드\nDEAL_YMD : 계약월\n\n\n  api <- \"http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev\"\n  url <- glue::glue(\n    \"{api}?ServiceKey={auth_key}&pageNo={chunk_no}&numOfRows={chunk}&LAWD_CD={LAWD_CD}&DEAL_YMD={DEAL_YMD}\"\n  )\n\n\n\n여기서 한가지 문제가 발생합니다. 서비스를 위해서는 지역코드가 필요합니다. 그나마 아파트 매매 상세자료 조회 기술문서.hwp 파일의 요청 메시지 명세 섹션에 “각 지역별 코드 행정표준코드관리시스템(www.code.go.kr)의 법정동코드 10자리 중 앞 5자리”라고 설명되어 있습니다.\n법정동 코드 준비하기\n만약에 서울특별시 노원구의 아파트매매 실거래 상세 자료를 조회하기 위해서는 노원구의 지역코드를 알아야 합니다.\n행정표준코드관리시스템에서 아주 어렵게 법정동 코드를 조회하는 법정동코드목록조회 화면을 찾았습니다.\n\n\n\nFigure 4: 법정동코드목록조회 화면\n\n\n\n노원구의 법정동코드가 “1135000000”임을 알았습니다. 그리고 API를 이용하기 위해서는 이 코드의 앞 다섯자리인 “11350”을 사용해야 합니다. 그런데 매번 특정 지역의 정보를 확인하기 위해서 행정표준코드관리시스템에 접속해야할까요? 그래서 “법정동 코드 전체자료”를 다운로드했습니다.\n해당 파일의 압축을 풀고, 데이터 프레임 객체를 만든 다음 R 데이터 파일과 SQLite DBMS의 테이블에 저장해 두었습니다.\n\n\nlibrary(dplyr)\nfname <- here::here(\"inst\", \"meta\", \"법정동코드 전체자료.txt\")\nlegal_divisions <- fname %>%\n  read.table(sep = \"\\t\", header = TRUE, fileEncoding = \"cp949\",\n             col.names = c(\"DIVISION_ID\", \"DIVISION_NM\", \"MAINTAIN\")) %>%\n  mutate(DIVISION_ID = format(DIVISION_ID, scientific = FALSE, trim = TRUE)) %>%\n  mutate(MAINTAIN = case_when(\n    MAINTAIN == \"존재\" ~ \"Y\",\n    MAINTAIN == \"폐지\" ~ \"N\")\n  ) %>%\n  mutate(MEGA_CD = substr(DIVISION_ID, 1, 2),\n         MEGA_NM = stringr::str_extract(DIVISION_NM, \"^[\\\\w]+\")) %>%\n  mutate(CTY_CD = substr(DIVISION_ID, 1, 5),\n         CTY_NM = stringr::str_extract(DIVISION_NM, \" [\\\\w]+\") %>%\n           stringr::str_remove(\"\\\\s\")) %>%\n  mutate(ADMI_CD = substr(DIVISION_ID, 1, 8),\n         ADMI_NM = stringr::str_remove(DIVISION_NM, \"^[\\\\w]+ [\\\\w]+ \")) %>%\n  filter(!stringr::str_detect(DIVISION_ID, \"000000$\"))\n\nsave(legal_divisions, file = \"data/legal_divisions.rda\")\n\n\ndb_name <- here::here(\"inst\", \"meta\", \"GISDB.sqlite\")\n\ncon <- DBI::dbConnect(RSQLite::SQLite(), db_name)\nDBI::dbWriteTable(con, \"TB_LEGAL_DIVISIONS\", legal_divisions, overwrite = TRUE)\nDBI::dbDisconnect(con)\n\n\n\nAPI 호출\n만들어 놓은 url을 XML 패키지의 xmlParse()로 API를 호출합니다. 만약에 호출 결과가 정상이 아닐 경우에는 에러를 발생시킵니다. 이때, 결과에 대한 메시지를 보여줍니다.\n\n\n  doc <- XML::xmlParse(url)\n\n  resultCode <- doc %>%\n    XML::getNodeSet(\"//resultCode\") %>%\n    XML::xmlValue()\n\n  if (resultCode != \"00\") {\n    result_msg <- doc %>%\n      XML::getNodeSet(\"//resultMsg\") %>%\n      XML::xmlValue()\n\n    stop(result_msg)\n  }\n\n\n\nXML 파싱\n조회된 매매정보의 건수를 가져옵니다. 지역코드에 따라 매매정보의 건수에 대한 편차가 클 것입니다. 도심의 대규모 아파트단지를 포함한 지역은 매매건수가 많을 것이고, 아파트 수가 적은 지방의 지역은 매매건수가 적을 것입니다.\n\n\n  total_count <- doc %>%\n    XML::getNodeSet(\"//totalCount\") %>%\n    XML::xmlValue() %>%\n    as.integer()\n\n\n\nXML 포멧에서는 item 태그로 개별 검색 결과를 반환합니다. 역시 getNodeSet()로 item 노드를 가져다 조작합니다. xmlToDataFrame()가 이들 개별 결과들을 데이터 프레임 객체로 변환합니다.\nget_list() 함수를 정의했습니다. 이 함수의 로직은 한글명을 영문 변수명으로 변경하는 로직과 필요한 항목만 가져오는 로직이 있습니다.\n\n\n  get_list <- function(doc) {\n    doc %>%\n      XML::getNodeSet(\"//item\") %>%\n      XML::xmlToDataFrame() %>%\n      mutate(거래금액 = stringr::str_remove(거래금액, \",\") %>% \n                   as.integer()) %>%\n      mutate(DEAL_DATE = glue::glue(\n        \"{년}-{str_pad(월, width = 2, pad = '0')}-{str_pad(일, width = 2, pad = '0')}\")\n      ) %>%\n      mutate(층 = as.integer(층)) %>%\n      mutate(건축년도 = as.integer(건축년도)) %>%\n      select(-년, -월, -일) %>%\n      select(\"LAWD_CD\"       = 지역코드,\n             DEAL_DATE,\n             \"SERIAL\"        = 일련번호,\n             \"DEAL_TYPE\"     = 거래유형,\n             \"BUILD_NM\"      = 아파트,\n             \"FLOOR\"         = 층,\n             \"BUILD_YEAR\"    = 건축년도,\n             \"AREA\"          = 전용면적,\n             \"AMOUNT\"        = 거래금액,\n             \"ROAD_CD\"       = 도로명코드,\n             \"ROAD_NM\"       = 도로명,\n             \"BUILD_MAJOR\"   = 도로명건물본번호코드,\n             \"BUILD_MINOR\"   = 도로명건물부번호코드,\n             \"ROAD_SEQ\"      = 도로명일련번호코드,\n             \"BASEMENT_FLAG\" = 도로명지상지하코드,\n             \"LAND_NO\"       = 지번,\n             \"DONG_NM\"       = 법정동,\n             \"DONG_MAJOR\"    = 법정동본번코드,\n             \"DONG_MINOR\"    = 법정동부번코드,\n             \"EUBMYNDONG_CD\" = 법정동읍면동코드,\n             \"DONG_LAND_NO\"  = 법정동지번코드,\n             \"DEALER_ADDR\"   = 중개사소재지,\n             \"CANCEL_DEAL\"   = 해제여부,\n             \"CANCEL_DATE\"   = 해제사유발생일)\n  }\n\n  deal_list <- doc %>%\n    get_list()\n\n\n\n다건 처리 로직\n다음은 chunk 사이즈보다 큰 다건의 검색 결과 처리를 위한 로직입니다. NAVER 뉴스 검색 로직과 유사합니다.\n\n\n  records <- NROW(deal_list)\n\n  if (!do_done | records >= total_count) {\n    return(deal_list)\n  } else {\n    cnt <- total_count %/% chunk\n    if (total_count %% chunk == 0) {\n      cnt <- cnt - 1\n    }\n\n    add_list <- (seq(cnt) + 1) %>%\n      purrr::map_df({\n        function(x) {\n          url <- glue::glue(\n            \"{api}?ServiceKey={auth_key}&pageNo={x}&numOfRows={chunk}&LAWD_CD={LAWD_CD}&DEAL_YMD={DEAL_YMD}\"\n          )\n\n          XML::xmlParse(url) %>%\n            get_list()\n        }\n      })\n\n    deal_list %>%\n      bind_rows(\n        add_list\n      ) %>%\n      return()\n  }\n\n\n\n함수의 완성\n이상의 로직을 통합해서 아파트매매 실거래 상세 자료를 조회하는 함수를 다음과 같이 정의하였습니다.\n\n\ntrade_apt <- function(auth_key, LAWD_CD = \"11110\", DEAL_YMD = \"202112\",\n                      chunk_no = 1, chunk = 100, do_done = FALSE) {\n  library(dplyr)\n\n  get_list <- function(doc) {\n    doc %>%\n      XML::getNodeSet(\"//item\") %>%\n      XML::xmlToDataFrame() %>%\n      mutate(거래금액 = stringr::str_remove(거래금액, \",\") %>%\n                   as.integer()) %>%\n      mutate(DEAL_DATE = glue::glue(\n        \"{년}-{str_pad(월, width = 2, pad = '0')}-{str_pad(일, width = 2, pad = '0')}\")\n      ) %>%\n      mutate(층 = as.integer(층)) %>%\n      mutate(건축년도 = as.integer(건축년도)) %>%\n      select(-년, -월, -일) %>%\n      select(\"LAWD_CD\"       = 지역코드,\n             DEAL_DATE,\n             \"SERIAL\"        = 일련번호,\n             \"DEAL_TYPE\"     = 거래유형,\n             \"BUILD_NM\"      = 아파트,\n             \"FLOOR\"         = 층,\n             \"BUILD_YEAR\"    = 건축년도,\n             \"AREA\"          = 전용면적,\n             \"AMOUNT\"        = 거래금액,\n             \"ROAD_CD\"       = 도로명코드,\n             \"ROAD_NM\"       = 도로명,\n             \"BUILD_MAJOR\"   = 도로명건물본번호코드,\n             \"BUILD_MINOR\"   = 도로명건물부번호코드,\n             \"ROAD_SEQ\"      = 도로명일련번호코드,\n             \"BASEMENT_FLAG\" = 도로명지상지하코드,\n             \"LAND_NO\"       = 지번,\n             \"DONG_NM\"       = 법정동,\n             \"DONG_MAJOR\"    = 법정동본번코드,\n             \"DONG_MINOR\"    = 법정동부번코드,\n             \"EUBMYNDONG_CD\" = 법정동읍면동코드,\n             \"DONG_LAND_NO\"  = 법정동지번코드,\n             \"DEALER_ADDR\"   = 중개사소재지,\n             \"CANCEL_DEAL\"   = 해제여부,\n             \"CANCEL_DATE\"   = 해제사유발생일)\n  }\n\n  api <- \"http://openapi.molit.go.kr/OpenAPI_ToolInstallPackage/service/rest/RTMSOBJSvc/getRTMSDataSvcAptTradeDev\"\n  url <- glue::glue(\n    \"{api}?ServiceKey={auth_key}&pageNo={chunk_no}&numOfRows={chunk}&LAWD_CD={LAWD_CD}&DEAL_YMD={DEAL_YMD}\"\n  )\n\n  doc <- XML::xmlParse(url)\n\n  resultCode <- doc %>%\n    XML::getNodeSet(\"//resultCode\") %>%\n    XML::xmlValue()\n\n  if (resultCode != \"00\") {\n    result_msg <- doc %>%\n      XML::getNodeSet(\"//resultMsg\") %>%\n      XML::xmlValue()\n\n    stop(result_msg)\n  }\n\n  total_count <- doc %>%\n    XML::getNodeSet(\"//totalCount\") %>%\n    XML::xmlValue() %>%\n    as.integer()\n\n  deal_list <- doc %>%\n    get_list()\n\n  records <- NROW(deal_list)\n\n  if (!do_done | records >= total_count) {\n    return(deal_list)\n  } else {\n    cnt <- total_count %/% chunk\n    if (total_count %% chunk == 0) {\n      cnt <- cnt - 1\n    }\n\n    add_list <- (seq(cnt) + 1) %>%\n      purrr::map_df({\n        function(x) {\n          url <- glue::glue(\n            \"{api}?ServiceKey={auth_key}&pageNo={x}&numOfRows={chunk}&LAWD_CD={LAWD_CD}&DEAL_YMD={DEAL_YMD}\"\n          )\n\n          XML::xmlParse(url) %>%\n            get_list()\n        }\n      })\n\n    deal_list %>%\n      bind_rows(\n        add_list\n      ) %>%\n      return()\n  }\n}\n\n\n\n함수의 호출\n다음은 2021년 4월 서울 용산구의 아파트매매 실거래 상세 자료를 조회하는 예제입니다. 실행하면 100건의 결과를 가져옵니다.\n\n\n# Your authorized API keys\nauth_key <- \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n\nresult <- trade_apt(auth_key, LAWD_CD = \"11170\", DEAL_YMD = \"200603\")\n\n\n\n다음은 총 312건의 결과를 가져옵니다. 즉, chunk가 300이므로 함수 내부에서 2번의 API 호출이 이루어집니다.\n\n\nresult <- trade_apt(auth_key, LAWD_CD = \"11170\", DEAL_YMD = \"200603\", chunk = 300,  do_done = TRUE)\n\n\n\n\n미국정보교환표준부호(영어: American Standard Code for Information Interchange), 또는 줄여서 ASCII( /ˈæski/, 아스키)는 영문 알파벳을 사용하는 대표적인 문자 인코딩이다. 아스키는 컴퓨터와 통신 장비를 비롯한 문자를 사용하는 많은 장치에서 사용되며, 대부분의 문자 인코딩이 아스키에 기초를 두고 있다. 출처: https://ko.wikipedia.org/wiki/ASCII↩︎\n",
    "preview": "posts/2022-03-02-apt-api/img/public.jpeg",
    "last_modified": "2022-03-01T09:56:24+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-01-open-api/",
    "title": "네이버 오픈 API를 이용한 데이터 수집",
    "description": "네이버 오픈 API를 이용한 데이터 수집 로직을 구현해 봅니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-03-01",
    "categories": [
      "Open API",
      "XML",
      "Data acquisition"
    ],
    "contents": "\n\nContents\n오픈 API\n대상 API\n네이버 뉴스 검색\nAPI 기본 정보\n요청 변수 (request parameter)\n출력 결과\n준비사항\n\n네이버 뉴스 검색 프로그램 개발\n요청 URL 생성\nAPI 호출\nXML 파싱\n다건 처리 로직\n함수의 완성\n함수의 호출\n\n\n\n\n\n\n\n들어가기\n공공 데이터 및 기업의 인터넷 서비스의 다수가 오픈 API를 통해서 관련 데이터를 제공합니다.\n\n이제는 데이터 수집을 위해서 오픈 API를 이용할 수 있는 기술을 습득해야 합니다.\n\n이제 여러분은 NAVER의 오픈 API를 다룰 수 있게 됩니다.\n\n\n오픈 API\n오픈 API(Open Application Programming Interface, Open API, 공개 API) 또는 공개 API는 개발자라면 누구나 사용할 수 있도록 공개된 API를 말하며, 개발자에게 사유 응용 소프트웨어나 웹 서비스의 프로그래밍 적인 권한을 제공합니다.1\n\n\n\n\n수년 전만해도 공동 데이터의 수집과 NAVER 웹 페이지의 데이터를 수집하기 위해서는 웹 페이지를 핸들링하는 기술을 통해서 데이터를 Scraping 해야 했습니다. 그러나 이제는 해당 기관과 업체에서 오픈 API를 제공하기 때문에, 합법적인 방법으로 원하는 데이터를 수집할 수 있는 세상이 되었습니다.\n대상 API\n네이버 뉴스 검색\n네이버\n\n아파트 실거래 데이터 가져오기\n공공 데이터 포털\n\n네이버 뉴스 검색\n네이버 뉴스 검색 결과를 출력해주는 REST API를 이용해서 뉴스 데이터를 수집합니다. Documents > 서비스 API > 검색 > 뉴스에 해당 API에 대한 스팩이 설명되어 있습니다.\nAPI 기본 정보\n다음과 같은 두 가지의 API가 있습니다. 여기서는 출력 포맷이 XML인 API를 이용합니다.\n메서드\n인증\n요청 URL\n출력 포맷\nGET\n-\nhttps://openapi.naver.com/v1/search/news.xml\nXML\nGET\n-\nhttps://openapi.naver.com/v1/search/news.json\nJSON\n요청 변수 (request parameter)\n요청 변수\n타입\n필수여부\n기본값\n설명\nquery\nstring\nY\n-\n검색을 원하는 문자열로서 UTF-8로 인코딩\ndisplay\ninteger\nN\n10(기본값), 100(최대)\n검색 결과 출력 건수 지정\nstart\ninteger\nN\n1(기본값), 1000(최대)\n검색 시작 위치로 최대 1000까지 가능\nsort\nstring\nN\nsim, date(기본값)\n정렬 옵션: sim (유사도순), date (날짜순)\n출력 결과\n필드\n타입\n설명\nrss\n-\n디버그를 쉽게 하고 RSS 리더기만으로 이용할 수 있게 하기 위해 만든 RSS 포맷의 컨테이너이며 그 외의 특별한 의미는 없다.\nchannel\n-\n검색 결과를 포함하는 컨테이너이다. 이 안에 있는 title, link, description 등의 항목은 참고용으로 무시해도 무방하다.\nlastBuildDate\ndatetime\n검색 결과를 생성한 시간이다.\ntotal\ninteger\n검색 결과 문서의 총 개수를 의미한다.\nstart\ninteger\n검색 결과 문서 중, 문서의 시작점을 의미한다.\ndisplay\ninteger\n검색된 검색 결과의 개수이다.\nitem/items\n-\nXML 포멧에서는 item 태그로, JSON 포멧에서는 items 속성으로 표현된다. 개별 검색 결과이며 title, originallink, link, description, pubDate를 포함한다.\ntitle\nstring\n개별 검색 결과이며, title, originallink, link, description, pubDate 를 포함한다.\noriginallink\nstring\n검색 결과 문서의 제공 언론사 하이퍼텍스트 link를 나타낸다.\nlink\nstring\n검색 결과 문서의 제공 네이버 하이퍼텍스트 link를 나타낸다.\ndescription\nstring\n검색 결과 문서의 내용을 요약한 패시지 정보이다. 문서 전체의 내용은 link를 따라가면 읽을 수 있다. 패시지에서 검색어와 일치하는 부분은 태그로 감싸져 있다.\npubDate\ndatetime\n검색 결과 문서가 네이버에 제공된 시간이다.\n준비사항\n애플리케이션 등록: 네이버 오픈 API로 개발하시려면 먼저 ‘Application-애플리케이션 등록’ 메뉴에서 애플리케이션을 등록하셔야 합니다.\n\n\n\nFigure 1: 애플리케이션 등록 (API 이용신청) 화면\n\n\n\n클라이언트 ID와 secret 확인: ‘내 애플리케이션’에서 등록한 애플리케이션을 선택하면 Client ID와 Client Secret 값을 확인할 수 있습니다.\n\n\n\nFigure 2: 애플리케이션 정보 화면\n\n\n\n이 화면은 중요한 정보를 보여줍니다. 하루에 검색 API 호출이 25000회로 제한되어 있습니다.\n\n\n\n\n주의\n\n클라이언트 아이디(Client ID)와  클라이언트 보안키(Client Secret)는 개인 사용자에게 발급된 정보이므로 반드시 보안에 주의해야 합니다. 타인에게 공개 및 공유하면 안된 중요한 정보이므로 사용에 주의해야 합니다.\n\n\n네이버 뉴스 검색 프로그램 개발\n요청 URL 생성\nXML 출력 포맷을 사용하기 때문에 다음 요청 URL을 사용합니다.\nhttps://openapi.naver.com/v1/search/news.xml\n검색 질의어인 query는 UTF-8로 인코딩해야 하기 때문에 enc2utf8()와 URLencode()을 사용합니다. GET 방식의 호출 URL이기 때문에 요청변수 영역을 ?로 구분하고, 요청 변수들은 &로 구분합니다.\n\n\n  searchUrl <- \"https://openapi.naver.com/v1/search/news.xml\"\n\n  query <- query %>%\n    enc2utf8() %>%\n    URLencode()\n\n  url <- glue::glue(\"{searchUrl}?query={query}&display={chunk}&start={chunk_no}&sort={sort}\")\n\n\n\nAPI 호출\nGET으로 호출할 때 HTTP Header에 애플리케이션 등록 시 발급받은 Client ID와 Client Secret 값을 같이 전송해야 합니다. 그러므로 httr 패키지를 사용합니다.\n\n\n  doc <- url %>%\n    httr::GET(\n      httr::add_headers(\n        \"X-Naver-Client-Id\"     = client_id,\n        \"X-Naver-Client-Secret\" = client_secret\n      )\n    ) %>%\n    toString() %>%\n    XML::xmlParse()\n\n\n\nXML 파싱\n키워드로 검색된 뉴스의 건수를 가져옵니다. 아마도 많은 경우는 대체로 많은 건수의 뉴스가 검색될 것입니다. 검색 결과 문서의 총 개수를 의미하는 total 노드를 가져다가 정수로 변환합니다.\n\n\n  total_count <- doc %>%\n    XML::getNodeSet(\"//total\") %>%\n    XML::xmlValue() %>%\n    as.integer()\n\n\n\nXML 포멧에서는 item 태그로 개별 검색 결과를 반환합니다. 역시 getNodeSet()로 item 노드를 가져다 조작합니다. xmlToDataFrame()가 이들 개별 결과들을 데이터 프레임 객체로 변환합니다.\n그리고 다음과 같은 데이터 변환을 수행합니다.\npubDate:\n날짜-시간을 표현하는 POSIXct 객체로 변환\n\ntitle:\nHTML 태그 들을 제거한 텍스트 생성하여,\ntitle_text 변수 파생\n\ndescription:\nHTML 태그 들을 제거한 텍스트 생성하여,\ndescription_text 변수 파생\n\n\n\n    doc %>%\n      XML::getNodeSet(\"//item\") %>%\n      XML::xmlToDataFrame() %>%\n      rename(\"publish_date\" = pubDate) %>%\n      mutate(publish_date = as.POSIXct(publish_date,\n                                       format = \"%a, %d %b %Y %H:%M:%S %z\")) %>%\n      mutate(title_text = stringr::str_remove_all(\n        title, \"&\\\\w+;|<[[:punct:]]*b>\")) %>%\n      mutate(title_text = stringr::str_remove_all(\n        title_text, \"[[:punct:]]*\")) %>%\n      mutate(description_text = stringr::str_remove_all(\n        description,\n        \"&\\\\w+;|<[[:punct:]]*b>|[“”]\"))\n\n\n\n다건 처리 로직\n요청 변수의 display는 API 호출에서 가져올 결과의 건수입니다. 한번 호출에 최대 100건까지 가져올 수 있습니다. 그러므로 검색 결과가 100건이 넘는 경우에는 여러번 호출을 통해서 해당하는 모든 건을 가져올 수 있습니다. 이 경우 검색 시작 위치인 start로 분할해서 가져올 페이지 번호를 지정할 수 있습니다. 만약 100건을 가져왔다면 다음 호출의 start는 101이어야 합니다.\nstart의 최대값은 1000으로 제한되어 있습니다. 그러므로 API로 가져올 수 있는 뉴스의 개별 결과는 100,000건입니다.\nMax(start) * Max(display) = 1000 * 100 = 100,000\n\n\n\n\n주의\n\nR은 눈사람을 만드는 것처럼 데이터를 키워나가면 안됩니다.\n\nrbind() 함수를 이용해서 API를 순차적으로 호출하면서 데이터 프레임에 결과를 붙여나가면 안됩니다. R 데이터 프레임에 관측치를 붙여나가면서 데이터를 크게 불리는 작업은 치명적인 성능 감소를 가져옵니다.\n\n\n다음은 chunk 사이즈보다 큰 다건의 검색 결과 처리를 위한 로직입니다.\npurrr 패키지의 map_df()의 프로그래밍 함수를 이용해서, 여러 번 API를 호출합니다. 이 로직은 주의에서 언급한 방법을 회피하는 로직입니다.\n\n\n  search_list <- doc %>%\n    get_list()\n\n  records <- NROW(search_list)\n\n  if (!do_done | records >= total_count | records >= max_record) {\n    return(search_list)\n  } else {\n    total_count <- min(total_count, max_record)\n\n    cnt <- total_count %/% chunk\n    if (total_count %% chunk == 0) {\n      cnt <- cnt - 1\n    }\n\n    idx <- (seq(cnt) + 1)\n\n    add_list <- idx[idx <= 1000] %>%\n      purrr::map_df({\n        function(x) {\n          if (verbose) {\n            glue::glue(\"  - ({chunk * x}/{total_count})건 호출을 진행합니다.\\n\\n\") %>%\n              cat()\n          }\n\n          glue::glue(\n            \"{searchUrl}?query={query}&display={chunk}&start={x}&sort={sort}\"\n          ) %>%\n            httr::GET(\n              httr::add_headers(\n                \"X-Naver-Client-Id\"     = client_id,\n                \"X-Naver-Client-Secret\" = client_secret\n              )\n            ) %>%\n            toString() %>%\n            XML::xmlParse() %>%\n            get_list()\n        }\n      })\n\n    search_list %>%\n      bind_rows(\n        add_list\n      ) %>%\n      return()\n  }\n\n\n\n함수의 완성\n이상의 로직을 통합해서 네이버 뉴스를 검색하는 함수를 다음과 같이 정의하였습니다.\n\n\nsearch_naver <- function(query = NULL, chunk = 100, chunk_no = 1,\n                         sort = c(\"date\", \"sim\"), do_done = FALSE,\n                         max_record = 1000L, client_id = NULL,\n                         client_secret = NULL, verbose = TRUE) {\n  if (is.null(query)) {\n    stop(\"검색 키워드인 query를 입력하지 않았습니다.\")\n  }\n\n  if (chunk < 1 & chunk > 100) {\n    stop(\"chunk 요청 변수값이 허용 범위(1~100)인지 확인해 보세요.\")\n  }\n\n  if (chunk_no < 1 & chunk_no > 100) {\n    stop(\"chunk_no 요청 변수값이 허용 범위(1~1000)인지 확인해 보세요.\")\n  }\n\n  sort <- match.arg(sort)\n\n  get_list <- function(doc) {\n    doc %>%\n      XML::getNodeSet(\"//item\") %>%\n      XML::xmlToDataFrame() %>%\n      rename(\"publish_date\" = pubDate) %>%\n      mutate(publish_date = as.POSIXct(publish_date,\n                                       format = \"%a, %d %b %Y %H:%M:%S %z\")) %>%\n      mutate(title_text = stringr::str_remove_all(\n        title, \"&\\\\w+;|<[[:punct:]]*b>\")) %>%\n      mutate(title_text = stringr::str_remove_all(\n        title_text, \"[[:punct:]]*\")) %>%\n      mutate(description_text = stringr::str_remove_all(\n        description,\n        \"&\\\\w+;|<[[:punct:]]*b>|[“”]\"))\n  }\n\n  searchUrl <- \"https://openapi.naver.com/v1/search/news.xml\"\n\n  query <- query %>%\n    enc2utf8() %>%\n    URLencode()\n\n  url <- glue::glue(\"{searchUrl}?query={query}&display={chunk}&start={chunk_no}&sort={sort}\")\n\n  doc <- url %>%\n    httr::GET(\n      httr::add_headers(\n        \"X-Naver-Client-Id\"     = client_id,\n        \"X-Naver-Client-Secret\" = client_secret\n      )\n    ) %>%\n    toString() %>%\n    XML::xmlParse()\n\n  total_count <- doc %>%\n    XML::getNodeSet(\"//total\") %>%\n    XML::xmlValue() %>%\n    as.integer()\n\n  if (verbose) {\n    glue::glue(\"* 검색된 총 기사 건수는 {total_count}건입니다.\\n\\n\") %>%\n      cat()\n\n    glue::glue(\"  - ({chunk}/{min(total_count, max_record)})건 호출을 진행합니다.\\n\\n\") %>%\n      cat()\n  }\n\n  search_list <- doc %>%\n    get_list()\n\n  records <- NROW(search_list)\n\n  if (!do_done | records >= total_count | records >= max_record) {\n    return(search_list)\n  } else {\n    total_count <- min(total_count, max_record)\n\n    cnt <- total_count %/% chunk\n    if (total_count %% chunk == 0) {\n      cnt <- cnt - 1\n    }\n\n    idx <- (seq(cnt) + 1)\n\n    add_list <- idx[idx <= 1000] %>%\n      purrr::map_df({\n        function(x) {\n          if (verbose) {\n            glue::glue(\"  - ({chunk * x}/{total_count})건 호출을 진행합니다.\\n\\n\") %>%\n              cat()\n          }\n\n          glue::glue(\n            \"{searchUrl}?query={query}&display={chunk}&start={x}&sort={sort}\"\n          ) %>%\n            httr::GET(\n              httr::add_headers(\n                \"X-Naver-Client-Id\"     = client_id,\n                \"X-Naver-Client-Secret\" = client_secret\n              )\n            ) %>%\n            toString() %>%\n            XML::xmlParse() %>%\n            get_list()\n        }\n      })\n\n    search_list %>%\n      bind_rows(\n        add_list\n      ) %>%\n      return()\n  }\n}\n\n\n\n함수의 호출\n다음은 불평등이라는 단어가 포함된 네이버 뉴스를 검색하는 예제입니다. 실행하면 100건의 결과를 가져옵니다.\n\n\n# Your authorized API keys\nclient_id <- \"XXXXXXXXXXXXXXXXXXXXXXX\"\nclient_secret <- \"XXXXXXXXX\"\n\nsearch_list <- search_naver(\n  \"불평등\", client_id = client_id, client_secret = client_secret\n)\n\n\n\n다음은 불평등이라는 단어가 포함된 네이버 뉴스를 검색하는 예제입니다. 총 3000건의 결과를 가져옵니다. 그러므로 함수 내부에서 30번의 API 호출이 이루어집니다.\n\n\nsearch_list <- search_naver(\n  \"불평등\", client_id = client_id, client_secret = client_secret,\n  do_done = TRUE, max_record = 3000\n)\n\n\n\n\n출처: https://ko.wikipedia.org/wiki/오픈_API↩︎\n",
    "preview": "posts/2022-03-01-open-api/img/naver.jpeg",
    "last_modified": "2022-03-01T09:32:18+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-28-forcats/",
    "title": "Wrangle factor with forcats",
    "description": "forcats 패키지로 범주형 데이터를 조작합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-02-28",
    "categories": [
      "Tidyverse",
      "forcats",
      "manipulate data"
    ],
    "contents": "\n\nContents\n왜 factor인가?\n레벨 변경 함수\nfct_recode\nfct_collapse, fct_other\n\n레벨 순서 변경 함수\nfct_relevel\nfct_rev\nfct_infreq\nfct_reorder\n\n기타 함수\nfct_drop\nfct_expand\nfct_c\n\n\n\n\n\n\n\n들어가기\n범주형 데이터라 부르고 factor라 읽는다.\n\nforcats 패키지는 R에서 조작하는 것이 까다롭다는 factor의 조작에 날개를 달아줍니다.\n\n\n\n왜 factor인가?\n성별을 나타내는 gender는 두 가지로 다음처럼 표현이 가능합니다. 어느 가족의 성별 데이터를 가정합니다.\n\n\nfamily <- c(\"father\", \"mother\", \"child\", \"child\")\ngender <- c(\"male\", \"female\", \"male\", \"male\")\nage <- c(52, 49, 22, 18)\n\ngender\n\n\n[1] \"male\"   \"female\" \"male\"   \"male\"  \n\n그런데, 어떤 R 사용자는 다음과 같이 데이터를 코딩합니다. 범주형 데이터를 문자인 character가 아닌 factor로 코딩하는 겁니다.\n\n\nfamily <- factor(c(\"father\", \"mother\", \"child\", \"child\"))\ngender <- factor(c(\"male\", \"female\", \"male\", \"male\"))\nage <- c(52, 49, 22, 18)\n\ngender\n\n\n[1] male   female male   male  \nLevels: female male\n\nR을 사용하는 초보자들은 factor를 다루는 것을 불편해 합니다. 그리고 질문을 합니다.\n\n왜 굳이 factor를 만듭니까? character로 사용해도 충분하지 않나요? 나는 이제껏 character로 사용해도 문제가 없었습니다.\n\n\nR은 factor에 대해서 많은 유용한 기능을 제공하고 있습니다. 예를 들면, 가족의 성별의 구성을 시각화할 경우에 아래처럼 그저 plot() 함수만 호출하면 됩니다. summary() 함수는 알아서 돗수분포표를 계산해줍니다. character는 지원하지 않는 기능입니다.\n\n\nsummary(gender)\n\n\nfemale   male \n     1      3 \n\nplot(gender)\n\n\n\nplot(family)\n\n\n\n\n레벨 변경 함수\nposition factor에서의 “father”, “mother”, “child”를 레벨(levels)라 합니다. 쉽게 이야기하면 범주의 종류인 셈입니다. 여기서는 범주의 종류인 레벨을 변경할 때 사용하는 함수를 다룹니다.\n다음과 같은 함수를 익히게 됩니다.\n함수 이름\n기능\nfct_recode\n매뉴얼로 levels을 변경함\nfct_collapse\n매뉴얼로 levels을 정의된 그룹으로 축소함\nfct_other\nlevels을 “other”로 교체\nfct_recode\n가족을 나타내는 family의 레벨이 “father”, “mother”, “son”, “daughter”입니다. 이 영문 레벨을 한글 레벨로 고치고 싶습니다.\nfct_recode() 함수는 레벨을 쉽게 재정의할 수 있습니다.\n\n\nlibrary(forcats)\n\nfamily <- factor(c(\"father\", \"mother\", \"son\", \"daughter\"))\nfamily\n\n\n[1] father   mother   son      daughter\nLevels: daughter father mother son\n\nfct_recode(family, 아빠 = \"father\", 엄마 = \"mother\", 자녀 = \"son\", 자녀 = \"daughter\")\n\n\n[1] 아빠 엄마 자녀 자녀\nLevels: 자녀 아빠 엄마\n\nfct_collapse, fct_other\nfct_collapse(), fct_other() 함수도 fct_recode() 함수와 유사한 기능을 가지고 있습니다. fct_recode() 함수로도 가능한 기능이지만, 몇 개의 레벨을 합쳐서 하나의 레벨을 만들어줍니다.\n\nHands-on 1\n데이터 분석 과정에서 family의 레벨인 “son”, “daughter”을 “child”로 합치고 싶습니다.\nfct_recode() 함수를 이용하세요.\nfct_collapse() 함수를 이용하세요.\nfct_other() 함수를 이용하세요.\n\n\n\n힌트\nfct_recode() 함수에서 “child”로 변경하는 작업이 두 번 이루어집니다.\nfct_other() 함수의 other_level 인수를 사용해서 레벨을 “child”로 조정합니다.\n\npurrr\n다음은 변수를 minmax로 표준화하는 purrr 패키지 구문입니다.\n\n\nfamily <- factor(c(\"father\", \"mother\", \"son\", \"daughter\"))\n\n# 1.1\nfct_recode(family, child = \"son\", child = \"daughter\")\n\n\n[1] father mother child  child \nLevels: child father mother\n\n# 1.2\nfct_collapse(family, child = c(\"son\", \"daughter\"))\n\n\n[1] father mother child  child \nLevels: child father mother\n\n# 1.3\nfct_other(family, keep = c(\"father\", \"mother\"), other_level = \"child\")\n\n\n[1] father mother child  child \nLevels: father mother child\n\n\n레벨 순서 변경 함수\n앞의 예제에서 fct_recode(), fct_collapse() 함수와 fct_other() 함수로 변경한 레벨의 순서가 다름을 발견할 수 있습니다. 명목척도가 코딩된 범주형 데이터인 factor는 큰 문제가 없으나, 서열척도가 코딩된 범주형 데이터인 ordered factor의 경우는 문제가 발생할 수 있습니다.\n다음과 같은 함수가 레벨의 순서를 바꿔줍니다.\n함수 이름\n기능\nfct_relevel\n매뉴얼로 levels의 순서를 변경함\nfct_infreq\n돗수의 크기로 levels의 순서를 변경함\nfct_rev\nlevels의 순서를 역순으로 변경함\nfct_reorder\n다른 변수의 정렬 순서로 levels의 순서 변경함\nfct_relevel\n앞의 예제에서 fct_recode()의 결과의 레벨 순서 “child”, “father”, “mother”은 알파벳 순서로 만들어진 것입니다. 이것을 fct_relevel() 함수를 이용해서 “father”, “mother”, “child”로 변경해 보겠습니다.\n\n\nfamily2 <- fct_recode(family, child = \"son\", child = \"daughter\")\nfamily2\n\n\n[1] father mother child  child \nLevels: child father mother\n\nfct_relevel(family2, \"father\", \"mother\", \"child\")\n\n\n[1] father mother child  child \nLevels: father mother child\n\nfct_rev\nfct_rev() 함수는 레벨의 순서를 역방향으로 바꿔주기 때문에, “child”, “father”, “mother” 순서가 “mother”, “father”, “child” 순서로 변경됩니다.\n\n\nfamily2 <- fct_recode(family, child = \"son\", child = \"daughter\")\nfamily2\n\n\n[1] father mother child  child \nLevels: child father mother\n\nfct_rev(family2)\n\n\n[1] father mother child  child \nLevels: mother father child\n\nfct_infreq\n앞에서 plot(gender)로 그림 돗수분포 시각화는 돗수의 무관하게 출력되었습니다. 그런데, 이제는 돗수가 제일 큰 레벨이 왼쪽에 출력되도록 레벨의 순서를 변경하려 합니다.\n\n\ngender\n\n\n[1] male   female male   male  \nLevels: female male\n\ngender2 <- fct_infreq(gender)\ngender2\n\n\n[1] male   female male   male  \nLevels: male female\n\nplot(gender2)\n\n\n\n\nfct_reorder\nforcats 패키지에는 사회조사 샘플 데이터를 gss_cat라는 이름의 데이터 프레임으로 제공하고 있습니다.\n응답자의 결혼상태별로 일 평균 TV 시청시간을 막대 그래프로 그렸습니다.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ngss_cat %>% \n  group_by(marital) %>% \n  summarise(tvhours = mean(tvhours, na.rm = TRUE)) %>% \n  ggplot(aes(x = marital, y = tvhours)) +\n  geom_col() + \n  coord_flip()\n\n\n\n\n\nHands-on 2\n데이터 해석의 용이성을 위해서 TV 시청시간의 크기별로 결혼상태의 레벨 순서를 조정해서 시각화하세요.\nfct_reorder() 함수를 이용하세요.\nmutate() 함수를 이용하세요.\n\n\n\n힌트\nmutate() 함수를 사용한다는 것은, 데이터 집계 후 레벨의 순서를 바꾼다는 의미입니다.\npurrr\n다음은 변수를 minmax로 표준화하는 purrr 패키지 구문입니다.\n\n\ngss_cat %>% \n  group_by(marital) %>% \n  summarise(tvhours = mean(tvhours, na.rm = TRUE)) %>% \n  mutate(marital = fct_reorder(marital, tvhours)) %>%\n  ggplot(aes(x = marital, y = tvhours)) +\n  geom_col() + \n  coord_flip()\n\n\n\n\n\n\n\n\n\n솔루션\n\nfct_reorder() 함수를 사용하여 시각화에서의 범주 순서를 바꾸는 경우는 자주 발생하는 사례이니, 꼭 기억하시기 바랍니다.\n\n\n\n기타 함수\nlevels를 추가/삭제하거나 여러 개의 factor를 병합하는 함수도 제공합니다.\n다음과 같은 함수가 레벨의 순서를 바꿔줍니다.\n함수 이름\n기능\nfct_drop\n사용하지 않는 levels를 제거함\nfct_expand\n새로운 levels를 추가함\nfct_c\n서로 다른 factor의 levels를 병합함\nfct_drop\nfct_drop() 함수는 불필요한 레벨을 삭제해 줍니다..\n\n\nf <- factor(c(\"a\", \"b\"), levels = c(\"a\", \"b\", \"c\"))\nf\n\n\n[1] a b\nLevels: a b c\n\nfct_drop(f)\n\n\n[1] a b\nLevels: a b\n\nfct_expand\nfct_expand() 함수는 데이터로는 포함되어 있지 않는 새로운 레벨을 추가해 줍니다. 이 기능을 데이터를 모델링하기 위해서 테스트 셋과 트레이닝 셋을 나눌 때, 어느 한 쪽의 데이터 셋에서 특정 레벨이 누락되는 것을 막아줍니다.\n\n\ngender <- factor(c(\"male\", \"male\", \"male\", \"male\"))\ngender\n\n\n[1] male male male male\nLevels: male\n\nfct_expand(gender, \"female\")\n\n\n[1] male male male male\nLevels: male female\n\nfct_c\nfct_c() 함수는 factor를 결합하고, 레벨을 합쳐줍니다.\n\n\nfa <- factor(c(\"a\", \"b\"))\nfb <- factor(\"x\")\n\nfct_c(fa, fb)\n\n\n[1] a b x\nLevels: a b x\n\n\n\n\n",
    "preview": "posts/2022-02-28-forcats/img/forcats.png",
    "last_modified": "2022-03-01T09:33:45+09:00",
    "input_file": {},
    "preview_width": 696,
    "preview_height": 807
  },
  {
    "path": "posts/2022-02-27-stringr/",
    "title": "Operate string with stringr",
    "description": "stringr 패키지로 문자를 조작하는 방법을 숙지합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-02-27",
    "categories": [
      "Tidyverse",
      "stringr",
      "manipulate data"
    ],
    "contents": "\n\nContents\n미리보기\n대표함수 목록\n대표함수 기능\n\n대표함수 예제\n문자열 포맷 바꾸기\n문자열 정리하기\n화이트 스페이스 제거\n공백 채우기\n줄바꿈 처리\n지정 길이로 변경\n\n문자열 길이 관련 처리\n글자수 계산\n\n문자열 정렬과 순서\n문자열 정렬과 순서\n\n문자열 결합\n문자열 결합\n파라미터 이용 문자열 결합\n\n문자열 자르기\n문자열 패턴 분리\n\n문자열 패턴 매치\n문자열 패턴 매치 여부\n문자열 패턴 매치 위치\n문자열 패턴 매치 추출\n문자열 패턴 매치 제거\n문자열 패턴 매치 대체\n\n\n\n\n\n\n\n\n들어가기\n나랏말쌈이 듕국과 달라도, 문자열 데이터를 파싱하고 조작하는 방법은 동일합니다.\n\n정규표현식이라는 세계 공통 기술의 기초를 알고 있는 당신은, \n\n이제, stringr 패키지를 이용해 보세요. 이름에도 문자열이 있으니 전용 패키지 같은 느낌이 듭니다.\n\n\n\n미리보기\n대표함수 목록\nstringr 패키지의 대표적인 함수는 “str_”로 시작하게 작명되었습니다. 다음의 함수들이 대표적인 함수들입니다. 함수 이름을 통해서 개별 함수들의 기능을 유출할 수 있습니다. 도움말을 보면 완벽하게 이해할 수 있겠죠?\n\n\nlibrary(stringr)\n\nls(pos = \"package:stringr\", pattern = \"^str_\") %>% \n  str_subset(\"<\", negate = TRUE)\n\n\n [1] \"str_c\"           \"str_conv\"        \"str_count\"      \n [4] \"str_detect\"      \"str_dup\"         \"str_ends\"       \n [7] \"str_extract\"     \"str_extract_all\" \"str_flatten\"    \n[10] \"str_glue\"        \"str_glue_data\"   \"str_interp\"     \n[13] \"str_length\"      \"str_locate\"      \"str_locate_all\" \n[16] \"str_match\"       \"str_match_all\"   \"str_order\"      \n[19] \"str_pad\"         \"str_remove\"      \"str_remove_all\" \n[22] \"str_replace\"     \"str_replace_all\" \"str_replace_na\" \n[25] \"str_sort\"        \"str_split\"       \"str_split_fixed\"\n[28] \"str_squish\"      \"str_starts\"      \"str_sub\"        \n[31] \"str_subset\"      \"str_to_lower\"    \"str_to_sentence\"\n[34] \"str_to_title\"    \"str_to_upper\"    \"str_trim\"       \n[37] \"str_trunc\"       \"str_view\"        \"str_view_all\"   \n[40] \"str_which\"       \"str_wrap\"       \n\n\n대표함수 기능\n함수이름\n반환 데이터 유형\n함수 기능\n유사 기능 함수\nstr_c\ncharacter\n문자열을 묶음\npaste\nstr_conv\ncharacter\n문자열의 인코딩 변경\nEncoding\nstr_count\ninteger\n(포함) 글자수 계산\nnchar\nstr_detect\nlogical\n패턴 포함 여부\ngrep\nstr_ends\nlogical\n패턴으로 끝나는지 여부\n\nstr_extract\ncharacter\n패턴 매치 추출\n\nstr_extract_all\ncharacter\n모든 패턴 매치 추출\n\nstr_flatten\ncharacter\n문자열을 묶음\npaste\nstr_glue\ncharacter\n문자열 표현식 결과와 묶음\n\nstr_glue_data\ncharacter\n데이터와 문자열 표현식 결과를 묶음\n\nstr_interp\ncharacter\n파라미터를 받아 적절한 문장 생성\nsprintf\nstr_length\ninteger\n글자수 계산\nnchar\nstr_locate\ninteger matrix\n패턴매치 (시작, 끝) 위치 반환\n\nstr_locate_all\ninteger matrix\n모든 패턴매치 (시작, 끝) 위치 반환\n\nstr_match\ncharacter matrix\n패턴 매치 추출\n\nstr_match_all\ncharacter matrix\n모든 패턴 매치 추출\n\nstr_order\ninteger\n문자열 벡터의 사전 순서 반환\norder\nstr_pad\ncharacter\n문자열 공백 채움\n\nstr_remove\ncharacter\n패턴 매치 제거\nsub\nstr_remove_all\ncharacter\n패턴 매치 전체 제거\ngsub\nstr_replace_all\ncharacter\n패턴 매치 대체\nsub\nstr_replace_na\ncharacter\n패턴 매치 전체 대체\ngsub\nstr_sort\ncharacter\n문자열 벡터의 사전 순서 정렬\nsort\nstr_split\nlist\n특정문자로 문자열 나눔\nstrsplit\nstr_split_fixed\nlist\n특정문자로 갯수만큼 문자열 나눔\nstrsplit\nstr_squish\ncharacter\n불필요 화이트스페이스 제거\n\nstr_starts\nlogical\n패턴으로 시작하는지 여부\n\nstr_sub\ncharacter\n문자열 부분제거 및 대체\nsubstring\nstr_subset\ncharacter\n패턴포함 문자열 반환\ngrep\nstr_to_lower\ncharacter\n소문자로 변경\ntolower\nstr_to_sentence\ncharacter\n영어문장 포맷으로 변경\n\nstr_to_title\ncharacter\n타이틀 포맷으로 변경 (캡문자 변경)\n\nstr_to_upper\ncharacter\n대문자로 변경\ntoupper\nstr_trim\ncharacter\n문자열 앞뒤 공백 제거\n\nstr_trunc\ncharacter\n문자열을 길이에 부합하게 줄여줌\n\nstr_view\nHTML rendering\n패턴매치 문자열을 웹에서 조회함\n\nstr_view_all\nHTML rendering\n모든 패턴매치 문자열을 웹에서 조회\n\nstr_which\ninteger\n문자벡터의 패턴매치 위치 반환\n\nstr_wrap\ncharacter\n문자열 줄바꿈 처리\n\n대표함수 예제\n문자열 포맷 바꾸기\n한글에서는 필요하지 않는 기능이나, 영문에서는 유용한 기능입니다.\n\n\ndog <- \"The quick brown dog\"\nstr_to_upper(dog)\n\n\n[1] \"THE QUICK BROWN DOG\"\n\nstr_to_lower(dog)\n\n\n[1] \"the quick brown dog\"\n\nstr_to_title(dog)\n\n\n[1] \"The Quick Brown Dog\"\n\nstr_to_sentence(\"the quick brown dog\")\n\n\n[1] \"The quick brown dog\"\n\nmountain <- \"동해물과 백두산이 마르고 닳도록\"\nstr_to_upper(mountain)\n\n\n[1] \"동해물과 백두산이 마르고 닳도록\"\n\nstr_to_lower(mountain)\n\n\n[1] \"동해물과 백두산이 마르고 닳도록\"\n\nstr_to_title(mountain)\n\n\n[1] \"동해물과 백두산이 마르고 닳도록\"\n\n문자열 정리하기\n화이트 스페이스 제거\n문자열 앞 뒤에 있는 불필요한 공백 등을 제거할 때 유용합니다.\n\n\nmountain <- \"  동해물과 백두산이 마르고 닳도록\"\nmountain2 <- \"\\n\\n동해물과 백두산이 마르고 닳도록\\n\\n\"\n\nstr_trim(mountain)\n\n\n[1] \"동해물과 백두산이 마르고 닳도록\"\n\nstr_trim(mountain2)\n\n\n[1] \"동해물과 백두산이 마르고 닳도록\"\n\nstr_squish(mountain)\n\n\n[1] \"동해물과 백두산이 마르고 닳도록\"\n\nstr_squish(mountain2)\n\n\n[1] \"동해물과 백두산이 마르고 닳도록\"\n\n공백 채우기\n문장의 길이를 맞추기 위해서 문자열 앞 뒤에 공백을 채웁니다.\n\n\nstr_pad(\"뚜벅뚜벅 걷다가 뒤돌아\", 25, \"left\")\n\n\n[1] \"   뚜벅뚜벅 걷다가 뒤돌아\"\n\nstr_pad(\"뚜벅뚜벅 걷다가 뒤돌아\", 25, \"right\")\n\n\n[1] \"뚜벅뚜벅 걷다가 뒤돌아   \"\n\nstr_pad(\"뚜벅뚜벅 걷다가 뒤돌아\", 25, \"both\")\n\n\n[1] \" 뚜벅뚜벅 걷다가 뒤돌아  \"\n\n줄바꿈 처리\n문장의 길이를 맞추기 위해서 정해진 자리수에서 개행합니다.\n\n\nstr_wrap (\"뚜벅뚜벅 걷다가 뒤돌아\", 8)\n\n\n[1] \"뚜벅뚜벅\\n걷다가\\n뒤돌아\"\n\n지정 길이로 변경\n문자열의 길이를 맞추기 위해서 길이가 넘쳐나는 문자열을 줄여줍니다.\n\n\nstr_trunc (\"뚜벅뚜벅 걷다가 뒤돌아\", 10)\n\n\n[1] \"뚜벅뚜벅 걷다...\"\n\n문자열 길이 관련 처리\n글자수 계산\n문자열의 길이를 계산합니다.\n\n\nstar <- \"  별 하나에 추억과 별 하나에 사랑과\"\nstar2 <- \"\\n\\n별 하나에 추억과\\n\\n별 하나에 사랑과\"\n\nstr_length(star)\n\n\n[1] 21\n\nstr_length(star2)\n\n\n[1] 22\n\nstr_count(star)\n\n\n[1] 21\n\nstr_count(star2)\n\n\n[1] 22\n\nstr_count(star, \"별\")\n\n\n[1] 2\n\nstr_count(star2, \"\\n\")\n\n\n[1] 4\n\n문자열 정렬과 순서\n문자열 정렬과 순서\n문자열을 정렬하거나, 사전적 순서를 계산합니다.\n\n\ncity <- c(\"서울\", \"춘천\", \"군산\", \"제주\", \"경주\")\n\nstr_order(city)\n\n\n[1] 5 3 1 4 2\n\nstr_sort(city)\n\n\n[1] \"경주\" \"군산\" \"서울\" \"제주\" \"춘천\"\n\n문자열 결합\n문자열 결합\n여러 문자열을 하나로 결합합니다.\n\n\ncity <- c(\"서울\", \"춘천\", \"군산\", \"제주\", \"경주\")\n\nstr_c(\"도시: \", city)\n\n\n[1] \"도시: 서울\" \"도시: 춘천\" \"도시: 군산\" \"도시: 제주\" \"도시: 경주\"\n\nstr_flatten(city, \"-\")\n\n\n[1] \"서울-춘천-군산-제주-경주\"\n\n파라미터 이용 문자열 결합\n문자열을 변수들의 값을 동적으로 받아서 꾸며줍니다.\n\n\nPhi <- \"파이\"\n\nstr_glue(\"원주율을 {Phi}라고 하며, 대략 {pi}이다.\")\n\n\n원주율을 파이라고 하며, 대략 3.14159265358979이다.\n\nstr_interp(\"원주율을 {Phi}라고 하며, 대략 $[.2f]{pi}이다.\")\n\n\n[1] \"원주율을 {Phi}라고 하며, 대략 3.14이다.\"\n\n문자열 자르기\n문자열 패턴 분리\n문자열의 특정 패턴을 구분자로하여 나눠줍니다.\n\n\nstar3 <- \" 별 하나에 추억과 별 하나에 사랑과 별 하나에 쓸쓸함과\"\n\n\nstr_split(star3, \"과[[:space:]]*\")\n\n\n[[1]]\n[1] \" 별 하나에 추억\"  \"별 하나에 사랑\"   \"별 하나에 쓸쓸함\"\n[4] \"\"                \n\nstr_split(star3, \"과[[:space:]]*\") %>% \n  unlist() %>% \n  str_subset(\"별\")\n\n\n[1] \" 별 하나에 추억\"  \"별 하나에 사랑\"   \"별 하나에 쓸쓸함\"\n\n# 2개로 나눔\nstr_split_fixed(star3, \"과[[:space:]]*\", n = 2)\n\n\n     [,1]              [,2]                                 \n[1,] \" 별 하나에 추억\" \"별 하나에 사랑과 별 하나에 쓸쓸함과\"\n\n문자열 패턴 매치\n문자열 패턴 매치 여부\n매턴 매치되는 문자열을 논리값으로 알려줍니다.\n\n\nfruit <- c(\"apple\", \"banana\", \"pear\", \"pinapple\")\n\nstr_detect(fruit, \"a\")\n\n\n[1] TRUE TRUE TRUE TRUE\n\nstr_detect(fruit, \"a$\")\n\n\n[1] FALSE  TRUE FALSE FALSE\n\nstr_starts(fruit, \"p\")\n\n\n[1] FALSE FALSE  TRUE  TRUE\n\nstr_starts(fruit, \"p\", negate = TRUE)\n\n\n[1]  TRUE  TRUE FALSE FALSE\n\nstr_ends(fruit, \"e\")\n\n\n[1]  TRUE FALSE FALSE  TRUE\n\nstr_ends(fruit, \"e\", negate = TRUE)\n\n\n[1] FALSE  TRUE  TRUE FALSE\n\n문자열 패턴 매치 위치\n매턴 매치되는 문자열이 몇번째 위치인지 정수로로 알려줍니다.\n\n\nfruit <- c(\"apple\", \"banana\", \"pear\", \"pinapple\")\n\nstr_locate(fruit, \"$\")\n\n\n     start end\n[1,]     6   5\n[2,]     7   6\n[3,]     5   4\n[4,]     9   8\n\nstr_locate(fruit, \"a\")\n\n\n     start end\n[1,]     1   1\n[2,]     2   2\n[3,]     3   3\n[4,]     4   4\n\nstr_locate(fruit, c(\"a\", \"b\", \"p\", \"p\"))\n\n\n     start end\n[1,]     1   1\n[2,]     1   1\n[3,]     1   1\n[4,]     1   1\n\nstr_locate_all(fruit, \"a\")\n\n\n[[1]]\n     start end\n[1,]     1   1\n\n[[2]]\n     start end\n[1,]     2   2\n[2,]     4   4\n[3,]     6   6\n\n[[3]]\n     start end\n[1,]     3   3\n\n[[4]]\n     start end\n[1,]     4   4\n\nstr_locate_all(fruit, c(\"a\", \"b\", \"p\", \"p\"))\n\n\n[[1]]\n     start end\n[1,]     1   1\n\n[[2]]\n     start end\n[1,]     1   1\n\n[[3]]\n     start end\n[1,]     1   1\n\n[[4]]\n     start end\n[1,]     1   1\n[2,]     5   5\n[3,]     6   6\n\n문자열 패턴 매치 추출\n문자열에서 패턴 매치된 값들을 보여줍니다.\n\n\nfruit <- c(\"apple\", \"banana\", \"pear\", \"pinapple\")\n\nstr_subset(fruit, \"^a\")\n\n\n[1] \"apple\"\n\nstr_subset(fruit, \"a$\")\n\n\n[1] \"banana\"\n\nstr_subset(fruit, \"b\")\n\n\n[1] \"banana\"\n\nstr_subset(fruit, \"[aeiou]\")\n\n\n[1] \"apple\"    \"banana\"   \"pear\"     \"pinapple\"\n\nstr_extract(fruit, \"na\")\n\n\n[1] NA   \"na\" NA   \"na\"\n\nstr_extract(fruit, \"^p\")\n\n\n[1] NA  NA  \"p\" \"p\"\n\nstr_extract_all(fruit, \"na\")\n\n\n[[1]]\ncharacter(0)\n\n[[2]]\n[1] \"na\" \"na\"\n\n[[3]]\ncharacter(0)\n\n[[4]]\n[1] \"na\"\n\nstr_extract_all(fruit, \"^p\")\n\n\n[[1]]\ncharacter(0)\n\n[[2]]\ncharacter(0)\n\n[[3]]\n[1] \"p\"\n\n[[4]]\n[1] \"p\"\n\nstr_match(fruit, \"a\")\n\n\n     [,1]\n[1,] \"a\" \n[2,] \"a\" \n[3,] \"a\" \n[4,] \"a\" \n\nstr_match_all(fruit, \"a\")\n\n\n[[1]]\n     [,1]\n[1,] \"a\" \n\n[[2]]\n     [,1]\n[1,] \"a\" \n[2,] \"a\" \n[3,] \"a\" \n\n[[3]]\n     [,1]\n[1,] \"a\" \n\n[[4]]\n     [,1]\n[1,] \"a\" \n\n문자열 패턴 매치 제거\n문자열에서 패턴 매치된 값들을 제거합니다.\n\n\nfruits <- c(\"one apple\", \"two pears\", \"three bananas\")\n\nstr_remove(fruits, \"[aeiou]\")\n\n\n[1] \"ne apple\"     \"tw pears\"     \"thre bananas\"\n\nstr_remove_all(fruits, \"[aeiou]\")\n\n\n[1] \"n ppl\"    \"tw prs\"   \"thr bnns\"\n\n문자열 패턴 매치 대체\n문자열에서 패턴 매치된 값들을 다른 값으로 대체합니다.\n\n\nfruits <- c(\"one apple\", \"two pears\", \"three bananas\")\n\nstr_replace(fruits, \"[aeiou]\", \"-\")\n\n\n[1] \"-ne apple\"     \"tw- pears\"     \"thr-e bananas\"\n\nstr_replace_all(fruits, \"[aeiou]\", \"-\")\n\n\n[1] \"-n- -ppl-\"     \"tw- p--rs\"     \"thr-- b-n-n-s\"\n\nstr_replace_all(fruits, \"[aeiou]\", toupper)\n\n\n[1] \"OnE ApplE\"     \"twO pEArs\"     \"thrEE bAnAnAs\"\n\nstr_replace_all(fruits, \"b\", NA_character_)\n\n\n[1] \"one apple\" \"two pears\" NA         \n\n\n\n\n",
    "preview": "posts/2022-02-27-stringr/img/stringr.png",
    "last_modified": "2022-03-01T09:34:35+09:00",
    "input_file": {},
    "preview_width": 736,
    "preview_height": 853
  },
  {
    "path": "posts/2022-02-26-purrr/",
    "title": "Functional programming with purrr",
    "description": "purrr 패키지로 함수형 프로그램하는 방법을 숙지합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-02-26",
    "categories": [
      "Tidyverse",
      "purrr",
      "Functional programming"
    ],
    "contents": "\n\nContents\n미리보기\nfor loop, apply 계열 함수, purrr 비교\n\nmap 계열 함수\nmap 계열 함수 예제\nmap 이런게 가능한가요?\n\n\nmap2 계열 함수\nmap2 계열 함수 예제\n\n\n\n\n\n\n\n들어가기\n반복문?  이제 for문을 잊어주세요.\n\nlapply, sapply? 당신도 비켜주실래요?\n\npurrr 패키지가 반복문을 깔끔하게 함수형 프로그램으로 만들어 줍니다.\n\n\n\n미리보기\nfor loop, apply 계열 함수, purrr 비교\nfor loop, apply 계열 함수, purrr를 비교 다음과 같은 데이터 프레임과, min-max 표준화를 수행하는 함수를 만들었습니다.\n\n\ndistribution <- tibble::tibble(\n  uniform = runif(10),\n  normal = rnorm(10),\n  student_t = rt(10, df = 3)\n)\n\ndistribution\n\n\n# A tibble: 10 × 3\n   uniform  normal student_t\n     <dbl>   <dbl>     <dbl>\n 1  0.483   0.389     -0.586\n 2  0.966  -0.367     -0.234\n 3  0.833  -0.572     -0.534\n 4  0.817   0.409     -0.784\n 5  0.831  -0.451      0.756\n 6  0.0795 -0.276     -0.471\n 7  0.218   2.77       0.104\n 8  0.718  -0.0110     0.988\n 9  0.825  -0.761     -4.56 \n10  0.321   0.749     -0.257\n\nminmax <- function(x) {\n  (x - min(x)) / diff(range(x))\n}\n\n\n\n\n이제 distribution의 변수인 일량난수, 정규난수, t난수를 min-max 표준화하려 합니다.\n\n\nfor loop\n다음은 변수를 minmax로 표준화하는 for loop 구문입니다.\n\n\ndistribution_minmax <- distribution\n\nfor (nm in names(distribution)) {\n  distribution_minmax[[nm]] <- minmax(distribution[[nm]])\n}\n\ndistribution_minmax\n\n\n# A tibble: 10 × 3\n   uniform normal student_t\n     <dbl>  <dbl>     <dbl>\n 1   0.455 0.325      0.716\n 2   1     0.112      0.780\n 3   0.850 0.0536     0.726\n 4   0.832 0.331      0.681\n 5   0.847 0.0879     0.958\n 6   0     0.137      0.737\n 7   0.156 1          0.841\n 8   0.720 0.212      1    \n 9   0.840 0          0    \n10   0.273 0.427      0.776\n\napply 계열 함수\n다음은 변수를 minmax로 표준화하는 sapply 구문입니다.\n\n\ndistribution_minmax <- sapply(distribution, minmax)\n\ntibble::as_tibble(distribution_minmax)\n\n\n# A tibble: 10 × 3\n   uniform normal student_t\n     <dbl>  <dbl>     <dbl>\n 1   0.455 0.325      0.716\n 2   1     0.112      0.780\n 3   0.850 0.0536     0.726\n 4   0.832 0.331      0.681\n 5   0.847 0.0879     0.958\n 6   0     0.137      0.737\n 7   0.156 1          0.841\n 8   0.720 0.212      1    \n 9   0.840 0          0    \n10   0.273 0.427      0.776\n\npurrr\n다음은 변수를 minmax로 표준화하는 purrr 패키지 구문입니다.\n\n\npurrr::map_df(distribution, minmax)\n\n\n# A tibble: 10 × 3\n   uniform normal student_t\n     <dbl>  <dbl>     <dbl>\n 1   0.455 0.325      0.716\n 2   1     0.112      0.780\n 3   0.850 0.0536     0.726\n 4   0.832 0.331      0.681\n 5   0.847 0.0879     0.958\n 6   0     0.137      0.737\n 7   0.156 1          0.841\n 8   0.720 0.212      1    \n 9   0.840 0          0    \n10   0.273 0.427      0.776\n\n\nmap 계열 함수\npurrr 패키지에서 대표적인 map 계열 함수는 map() 함수와 다음의 함수가 있습니다.\nmap 계열 함수는 함수형 프로그램에서 인수를 하나만 사용하고 계산된 결과는 “map_” 뒤에 표현되는 데이터 타입으로 반환하는 함수다.\n\n\nlibrary(purrr)\n\nls(pos = \"package:purrr\", pattern = \"^map2_\") %>% \n  stringr::str_remove(\"2\")\n\n\n[1] \"map_chr\" \"map_dbl\" \"map_df\"  \"map_dfc\" \"map_dfr\" \"map_int\"\n[7] \"map_lgl\" \"map_raw\"\n\n\n함수이름\n반환 데이터 유형\n반환 데이터 유형\n비고\nmap\nlist\n리스트\n\nmap_chr\ncharacter\n문자형\n\nmap_dbl\ndouble\n실수형\n\nmap_df\ndata.frame\n데이터프레임\n\nmap_dfc\ndata.frame\n데이터프레임\n원소를 열로 붙여서 데이터프레임 생성\nmap_dfr\ndata.frame\n데이터프레임\n원소를 행으로 붙여서 데이터프레임 생성\nmap_int\ninteger\n정수형\n\nmap_lgl\nlogical\n논리형\n\nmap_raw\nraw\n문자형\n\nmap 계열 함수 예제\nHands-on 1\n앞에서 만든 distribution의 다음과 같은 통계량을 구하세요.\n산술평균과 표준편차를 계산하세요.\n\n다음의 함수를 사용해서 최소값과 최대값을 구하여 결과를 비교하세요.\nmap()\nmap_df()\n\n\n\n힌트\nmean(), sd() 함수를 사용합니다.\n최소값과 최대값은 range() 함수로 쉽게 구할 수 있습니다.\n\npurrr 솔루션\n기존에 정의된 함수를 사용합니다.\n\n\n# 1.\ndistribution %>% \n  map_dbl(mean)\n\n\n   uniform     normal  student_t \n 0.6091583  0.1882322 -0.5579321 \n\ndistribution %>% \n  map_dbl(sd)\n\n\n  uniform    normal student_t \n0.3092581 1.0294314 1.5221225 \n\n# 2.\ndistribution %>% \n  map(range)\n\n\n$uniform\n[1] 0.0795344 0.9661807\n\n$normal\n[1] -0.7614494  2.7733921\n\n$student_t\n[1] -4.562987  0.987915\n\ndistribution %>% \n  map_df(range)\n\n\n# A tibble: 2 × 3\n  uniform normal student_t\n    <dbl>  <dbl>     <dbl>\n1  0.0795 -0.761    -4.56 \n2  0.966   2.77      0.988\n\n\n\nHands-on 2\nmonth.name는 12개 월의 영문명을 나타내는 내장 상수입니다.\nmonth.name을 조회해 보세요.\n12개 월에 대해서, “문자의 개수 * 순서”(예, 8월은 8, 12월은 12)를 계산하세요.\n\n\n\n힌트\n문자의 개수를 세는 함수는 nchar() 입니다.\n문자열 매치를 수행하여 매치되는 위치는 반환하는 match() 함수를 사용해 보세요.\npurrr 패키지의 map_int() 함수가 결과를 정수로 반환합니다.\n\npurrr 솔루션\n함수형 프로그램은 기존의 함수를 사용하거나, 사용자가 함수를 정의해야 합니다.\n\n\nmonth.name\n\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"      \n [6] \"June\"      \"July\"      \"August\"    \"September\" \"October\"  \n[11] \"November\"  \"December\" \n\nmonth.name %>% \n  purrr::map_int(\n    function(x) {\n      nchar(x) * match(x, month.name)\n    }\n  )\n\n\n [1]  7 16 15 20 15 24 28 48 81 70 88 96\n\n\nmap 이런게 가능한가요?\nhttps://purrr.tidyverse.org/ 페이지의 purrr 소개에는 선형모형을 적합한 후, R2를 계산하는 예제가 있습니다.\n다음을 간단하게 purrr 패키지로 처리합니다.\nmtcars 데이터에서 차량의 무게와 연비의 관계를 살펴보기 위한 mpg ~ wt 모형을,\n차량의 실린더 개수의 조합별로 수행합니다.\n수행한 모형에서 R2만 발췌합니다.\n현 수행 결과를 다음 단계의 입력 값으로 사용하는 파이프 기능(%>% 연산자)이 활약하는 좋은 사례입니다.\n\n\nlibrary(purrr)\n\nmtcars %>%\n  split(.$cyl) %>% # from base R\n  map(~ lm(mpg ~ wt, data = .)) %>%\n  map(summary) %>%\n  map_dbl(\"r.squared\")\n\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\nmap2 계열 함수\npurrr 패키지에서 대표적인 map2 계열 함수는 map2() 함수와 다음의 함수가 있습니다.\n\n\nlibrary(purrr)\n\nls(pos = \"package:purrr\", pattern = \"^map2_\")\n\n\n[1] \"map2_chr\" \"map2_dbl\" \"map2_df\"  \"map2_dfc\" \"map2_dfr\" \"map2_int\"\n[7] \"map2_lgl\" \"map2_raw\"\n\n\n함수이름\n반환 데이터 유형\n반환 데이터 유형\n비고\nmap2\nlist\n리스트\n\nmap2_chr\ncharacter\n문자형\n\nmap2_dbl\ndouble\n실수형\n\nmap2_df\ndata.frame\n데이터프레임\n\nmap2_dfc\ndata.frame\n데이터프레임\n원소를 열로 붙여서 데이터프레임 생성\nmap2_dfr\ndata.frame\n데이터프레임\n원소를 행으로 붙여서 데이터프레임 생성\nmap2_int\ninteger\n정수형\n\nmap2_lgl\nlogical\n논리형\n\nmap2_raw\nraw\n문자형\n\nmap2 계열 함수 예제\nHands-on 3\n정규 난수 5개 쌍을 다음 파라미터 기준으로 생성하고 싶습니다.\n평균 : (5, 10, -3)\n표준편차 : (1, 5, 10)\n\n즉, X ~ N(5, 1), X ~ N(10, 5), X ~ N(-3, 10)을 따르는 분포에서의 난수를 각각 5개씩 추출합니다.\n\n\n힌트\nrnorm() 함수로 정규 난수를 생성합니다.\npurrr 솔루션\n다음 다이어그램을 참고하세요.\n정규난수 산출 개념도\n\nmu <- c(5, 10, -3)\nsigma <- c(1, 5, 10)\n\nmap2(mu, sigma, rnorm, n = 5)\n\n\n[[1]]\n[1] 5.819224 4.572258 4.431857 5.008439 5.037252\n\n[[2]]\n[1]  6.927533  9.880156  4.715674  9.070893 13.899599\n\n[[3]]\n[1] -10.623390  -6.438565  -6.869899 -20.728252  -4.245847\n\n\n\n\n\n",
    "preview": "posts/2022-02-26-purrr/img/purrr.png",
    "last_modified": "2022-03-06T09:18:12+09:00",
    "input_file": {},
    "preview_width": 736,
    "preview_height": 853
  },
  {
    "path": "posts/2022-02-25-tidyr/",
    "title": "Wrangle data with tidyr",
    "description": "tidyr 패키지로 데이터를 정형화하는 방법을 숙지합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-02-25",
    "categories": [
      "Tidyverse",
      "tidyr",
      "manipulate data"
    ],
    "contents": "\n\nContents\n준비하기\n사용할 데이터\n데이터 살펴보기\nwho\npopulation\n\n데이터 준비하기\n\n피봇팅 워밍업\npivot_wider\npivot_longer\n\n피봇팅 응용\ndplyr 패키지와의 콜라보\n\n\n\n\n\n\n\n들어가기\n피봇팅은 엑셀만의 전유물이 아닙니다.\n\n이제 tidyr로 피봇팅과 유사한 데이터 조작이 가능합니다. 자주쓰지는 않지만, 감초처럼 등장하는 이 작업에 더이상 reshape2 패키지를 사용하지 마세요.\n\n\n\n준비하기\n사용할 데이터\ntidyr 패키지는 세계보건기구(WHO, World Health Organization)에서 발표한 결핵 신규 환자 현황 데이터인 who와 관련된 국가별 인구통계 데이터인 population을 제공합니다.\nwho\n결핵 신규발생 현황 통계 데이터\n변수\ncountry : 국가\niso2, iso3 : ISO 국가 코드 2자리와 3자리\nyear : 연도\nnew_sp_m014 - new_rel_f65 : 그룹별 신규 결핵 사례 수\n다음의 세 가지의 코드가 결합되었음\n진단방법(method of diagnosis)\nrel : relapse(재발)\nsn : negative pulmonary smear(폐 도말검사1 음성)\nsp : positive pulmonary smear(폐 도말검사 양성)\nep : extrapulmonary(폐외부)\n\n성별\nf : female(여성)\nm : male(남성)\n\n연령 그룹\n014 : 0-14세\n1524 : 15-24세\n2534 : 25-34세\n3544 : 35-44세\n4554 : 45-54세\n5564 : 55-64세\n65 : 65세+\n\n\n\n\npopulation\n인구통계 데이터\n변수\ncountry : 국가\nyear : 연도\npopulation : 인구수\n\n\n데이터 살펴보기\nwho\n\n\nlibrary(dplyr)\nlibrary(tidyr)\n\nwho\n\n\n# A tibble: 7,240 x 60\n   country     iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534\n   <chr>       <chr> <chr> <int>       <int>        <int>        <int>\n 1 Afghanistan AF    AFG    1980          NA           NA           NA\n 2 Afghanistan AF    AFG    1981          NA           NA           NA\n 3 Afghanistan AF    AFG    1982          NA           NA           NA\n 4 Afghanistan AF    AFG    1983          NA           NA           NA\n 5 Afghanistan AF    AFG    1984          NA           NA           NA\n 6 Afghanistan AF    AFG    1985          NA           NA           NA\n 7 Afghanistan AF    AFG    1986          NA           NA           NA\n 8 Afghanistan AF    AFG    1987          NA           NA           NA\n 9 Afghanistan AF    AFG    1988          NA           NA           NA\n10 Afghanistan AF    AFG    1989          NA           NA           NA\n# … with 7,230 more rows, and 53 more variables: new_sp_m3544 <int>,\n#   new_sp_m4554 <int>, new_sp_m5564 <int>, new_sp_m65 <int>,\n#   new_sp_f014 <int>, new_sp_f1524 <int>, new_sp_f2534 <int>,\n#   new_sp_f3544 <int>, new_sp_f4554 <int>, new_sp_f5564 <int>,\n#   new_sp_f65 <int>, new_sn_m014 <int>, new_sn_m1524 <int>,\n#   new_sn_m2534 <int>, new_sn_m3544 <int>, new_sn_m4554 <int>,\n#   new_sn_m5564 <int>, new_sn_m65 <int>, new_sn_f014 <int>,\n#   new_sn_f1524 <int>, new_sn_f2534 <int>, new_sn_f3544 <int>,\n#   new_sn_f4554 <int>, new_sn_f5564 <int>, new_sn_f65 <int>,\n#   new_ep_m014 <int>, new_ep_m1524 <int>, new_ep_m2534 <int>,\n#   new_ep_m3544 <int>, new_ep_m4554 <int>, new_ep_m5564 <int>,\n#   new_ep_m65 <int>, new_ep_f014 <int>, new_ep_f1524 <int>,\n#   new_ep_f2534 <int>, new_ep_f3544 <int>, new_ep_f4554 <int>,\n#   new_ep_f5564 <int>, new_ep_f65 <int>, newrel_m014 <int>,\n#   newrel_m1524 <int>, newrel_m2534 <int>, newrel_m3544 <int>,\n#   newrel_m4554 <int>, newrel_m5564 <int>, newrel_m65 <int>,\n#   newrel_f014 <int>, newrel_f1524 <int>, newrel_f2534 <int>,\n#   newrel_f3544 <int>, newrel_f4554 <int>, newrel_f5564 <int>,\n#   newrel_f65 <int>\n\n# 한국의 인구통계\nwho %>% \n  filter(country %in% \"Republic of Korea\")\n\n\n# A tibble: 34 x 60\n   country     iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534\n   <chr>       <chr> <chr> <int>       <int>        <int>        <int>\n 1 Republic o… KR    KOR    1980          NA           NA           NA\n 2 Republic o… KR    KOR    1981          NA           NA           NA\n 3 Republic o… KR    KOR    1982          NA           NA           NA\n 4 Republic o… KR    KOR    1983          NA           NA           NA\n 5 Republic o… KR    KOR    1984          NA           NA           NA\n 6 Republic o… KR    KOR    1985          NA           NA           NA\n 7 Republic o… KR    KOR    1986          NA           NA           NA\n 8 Republic o… KR    KOR    1987          NA           NA           NA\n 9 Republic o… KR    KOR    1988          NA           NA           NA\n10 Republic o… KR    KOR    1989          NA           NA           NA\n# … with 24 more rows, and 53 more variables: new_sp_m3544 <int>,\n#   new_sp_m4554 <int>, new_sp_m5564 <int>, new_sp_m65 <int>,\n#   new_sp_f014 <int>, new_sp_f1524 <int>, new_sp_f2534 <int>,\n#   new_sp_f3544 <int>, new_sp_f4554 <int>, new_sp_f5564 <int>,\n#   new_sp_f65 <int>, new_sn_m014 <int>, new_sn_m1524 <int>,\n#   new_sn_m2534 <int>, new_sn_m3544 <int>, new_sn_m4554 <int>,\n#   new_sn_m5564 <int>, new_sn_m65 <int>, new_sn_f014 <int>,\n#   new_sn_f1524 <int>, new_sn_f2534 <int>, new_sn_f3544 <int>,\n#   new_sn_f4554 <int>, new_sn_f5564 <int>, new_sn_f65 <int>,\n#   new_ep_m014 <int>, new_ep_m1524 <int>, new_ep_m2534 <int>,\n#   new_ep_m3544 <int>, new_ep_m4554 <int>, new_ep_m5564 <int>,\n#   new_ep_m65 <int>, new_ep_f014 <int>, new_ep_f1524 <int>,\n#   new_ep_f2534 <int>, new_ep_f3544 <int>, new_ep_f4554 <int>,\n#   new_ep_f5564 <int>, new_ep_f65 <int>, newrel_m014 <int>,\n#   newrel_m1524 <int>, newrel_m2534 <int>, newrel_m3544 <int>,\n#   newrel_m4554 <int>, newrel_m5564 <int>, newrel_m65 <int>,\n#   newrel_f014 <int>, newrel_f1524 <int>, newrel_f2534 <int>,\n#   newrel_f3544 <int>, newrel_f4554 <int>, newrel_f5564 <int>,\n#   newrel_f65 <int>\n\npopulation\n\n\npopulation\n\n\n# A tibble: 4,060 x 3\n   country      year population\n   <chr>       <int>      <int>\n 1 Afghanistan  1995   17586073\n 2 Afghanistan  1996   18415307\n 3 Afghanistan  1997   19021226\n 4 Afghanistan  1998   19496836\n 5 Afghanistan  1999   19987071\n 6 Afghanistan  2000   20595360\n 7 Afghanistan  2001   21347782\n 8 Afghanistan  2002   22202806\n 9 Afghanistan  2003   23116142\n10 Afghanistan  2004   24018682\n# … with 4,050 more rows\n\n# 한국의 인구통계\npopulation %>% \n  filter(country %in% \"Republic of Korea\")\n\n\n# A tibble: 19 x 3\n   country            year population\n   <chr>             <int>      <int>\n 1 Republic of Korea  1995   44652994\n 2 Republic of Korea  1996   44940974\n 3 Republic of Korea  1997   45220543\n 4 Republic of Korea  1998   45489131\n 5 Republic of Korea  1999   45742103\n 6 Republic of Korea  2000   45977210\n 7 Republic of Korea  2001   46192932\n 8 Republic of Korea  2002   46393993\n 9 Republic of Korea  2003   46591762\n10 Republic of Korea  2004   46801310\n11 Republic of Korea  2005   47033082\n12 Republic of Korea  2006   47291491\n13 Republic of Korea  2007   47572585\n14 Republic of Korea  2008   47867970\n15 Republic of Korea  2009   48164969\n16 Republic of Korea  2010   48453931\n17 Republic of Korea  2011   48732640\n18 Republic of Korea  2012   49002683\n19 Republic of Korea  2013   49262698\n\n데이터 준비하기\nwho 데이터 프레임의 변수 이름이 포맷에 벗어난 것이 있습니다. 이것을 바로 잡고 시작합니다.\n앞에서 변수에 대한 설명에서 new_sp_m014 - new_rel_f65의 첫번째 코드는 진단방법(method of diagnosis)입니다. 그런데 진단방법의 rel의 경우에는 다른 변수 이름과는 달리 “new”라는 접두어와 언다라인(_)으로 분리되지 않습니다. 그래서 일관성을 유지하기 위해서 언다라인을 붙입니다.\n\n\nnames(who)\n\n\n [1] \"country\"      \"iso2\"         \"iso3\"         \"year\"        \n [5] \"new_sp_m014\"  \"new_sp_m1524\" \"new_sp_m2534\" \"new_sp_m3544\"\n [9] \"new_sp_m4554\" \"new_sp_m5564\" \"new_sp_m65\"   \"new_sp_f014\" \n[13] \"new_sp_f1524\" \"new_sp_f2534\" \"new_sp_f3544\" \"new_sp_f4554\"\n[17] \"new_sp_f5564\" \"new_sp_f65\"   \"new_sn_m014\"  \"new_sn_m1524\"\n[21] \"new_sn_m2534\" \"new_sn_m3544\" \"new_sn_m4554\" \"new_sn_m5564\"\n[25] \"new_sn_m65\"   \"new_sn_f014\"  \"new_sn_f1524\" \"new_sn_f2534\"\n[29] \"new_sn_f3544\" \"new_sn_f4554\" \"new_sn_f5564\" \"new_sn_f65\"  \n[33] \"new_ep_m014\"  \"new_ep_m1524\" \"new_ep_m2534\" \"new_ep_m3544\"\n[37] \"new_ep_m4554\" \"new_ep_m5564\" \"new_ep_m65\"   \"new_ep_f014\" \n[41] \"new_ep_f1524\" \"new_ep_f2534\" \"new_ep_f3544\" \"new_ep_f4554\"\n[45] \"new_ep_f5564\" \"new_ep_f65\"   \"newrel_m014\"  \"newrel_m1524\"\n[49] \"newrel_m2534\" \"newrel_m3544\" \"newrel_m4554\" \"newrel_m5564\"\n[53] \"newrel_m65\"   \"newrel_f014\"  \"newrel_f1524\" \"newrel_f2534\"\n[57] \"newrel_f3544\" \"newrel_f4554\" \"newrel_f5564\" \"newrel_f65\"  \n\nnames(who) <- names(who) %>% \n  stringr::str_replace(\"newrel\", \"new_rel\")\n\nnames(who)\n\n\n [1] \"country\"       \"iso2\"          \"iso3\"          \"year\"         \n [5] \"new_sp_m014\"   \"new_sp_m1524\"  \"new_sp_m2534\"  \"new_sp_m3544\" \n [9] \"new_sp_m4554\"  \"new_sp_m5564\"  \"new_sp_m65\"    \"new_sp_f014\"  \n[13] \"new_sp_f1524\"  \"new_sp_f2534\"  \"new_sp_f3544\"  \"new_sp_f4554\" \n[17] \"new_sp_f5564\"  \"new_sp_f65\"    \"new_sn_m014\"   \"new_sn_m1524\" \n[21] \"new_sn_m2534\"  \"new_sn_m3544\"  \"new_sn_m4554\"  \"new_sn_m5564\" \n[25] \"new_sn_m65\"    \"new_sn_f014\"   \"new_sn_f1524\"  \"new_sn_f2534\" \n[29] \"new_sn_f3544\"  \"new_sn_f4554\"  \"new_sn_f5564\"  \"new_sn_f65\"   \n[33] \"new_ep_m014\"   \"new_ep_m1524\"  \"new_ep_m2534\"  \"new_ep_m3544\" \n[37] \"new_ep_m4554\"  \"new_ep_m5564\"  \"new_ep_m65\"    \"new_ep_f014\"  \n[41] \"new_ep_f1524\"  \"new_ep_f2534\"  \"new_ep_f3544\"  \"new_ep_f4554\" \n[45] \"new_ep_f5564\"  \"new_ep_f65\"    \"new_rel_m014\"  \"new_rel_m1524\"\n[49] \"new_rel_m2534\" \"new_rel_m3544\" \"new_rel_m4554\" \"new_rel_m5564\"\n[53] \"new_rel_m65\"   \"new_rel_f014\"  \"new_rel_f1524\" \"new_rel_f2534\"\n[57] \"new_rel_f3544\" \"new_rel_f4554\" \"new_rel_f5564\" \"new_rel_f65\"  \n\n피봇팅 워밍업\npivot_wider\nHands-on 1\npivot_wider() 함수는 Long 포맷 테이블을 Wide 포맷 테이블로 변환해주는 함수입니다.\n2010년부터 2013년도의 한국의 인구수 조회해 보세요.\n조회한 내용의 변수 population는 인수입니다.\n2010, 2011, 2012, 2013이라는 변수를 만들고, 해당 년도의 인구수를 넣어 보세요.\n\n수치로만 이루어진 변수 이름을 사용할 때는 문자 “`”를 앞 뒤에 붙여 주어야 합니다.\n성가신 작업일 수 있으므로 2010, 2011, 2012, 2013이라는 변수이름 앞에 “y”를 붙여주세요.\n\n\n\n힌트\ndplyr 패키지에서 학습한 방법을 사용하세요.\npivot_wider() 함수를 사용합니다.\ndplyr 패키지의 rename() 함수를 사용합니다.\ndplyr 솔루션\nrename_if() 함수는 조건을 만족하는 변수들의 변수 이름을 변경하는 dplyr 패키지의 함수입니다.\n\n\n# 1.\npopulation %>% \n  filter(country %in% \"Republic of Korea\") %>% \n  filter(year >= 2010)\n\n\n# A tibble: 4 x 3\n  country            year population\n  <chr>             <int>      <int>\n1 Republic of Korea  2010   48453931\n2 Republic of Korea  2011   48732640\n3 Republic of Korea  2012   49002683\n4 Republic of Korea  2013   49262698\n\n# 2.\npopulation_wide <- population %>% \n  filter(country %in% \"Republic of Korea\") %>% \n    filter(year >= 2010) %>% \n  pivot_wider(names_from = \"year\",\n              values_from = \"population\")\n\npopulation_wide\n\n\n# A tibble: 1 x 5\n  country             `2010`   `2011`   `2012`   `2013`\n  <chr>                <int>    <int>    <int>    <int>\n1 Republic of Korea 48453931 48732640 49002683 49262698\n\n# 3.\npopulation_wide2 <- population_wide %>% \n  rename_if(is.integer, function(x) paste0(\"y\", x))\n\npopulation_wide2\n\n\n# A tibble: 1 x 5\n  country              y2010    y2011    y2012    y2013\n  <chr>                <int>    <int>    <int>    <int>\n1 Republic of Korea 48453931 48732640 49002683 49262698\n\n\npivot_longer\npivot_longer() 함수는 Wide 포맷 테이블을 Long 포맷 테이블로 변환해주는 함수입니다.\nHands-on 2\n앞의 예제에서 만들었던 Wide 포맷 테이블인 population_wide2을 Long 포맷 테이블로 변환하세요.\ny2010, y2011, y2012, y2013에서 “y”를 떼어내세요.\n\n\n힌트\npivot_longer() 함수를 사용합니다.\ndplyr 패키지의 rename() 함수를 사용합니다.\ndplyr 솔루션\npopulation_long() 함수는 pivot_wider() 함수의 역함수입니다.\n\n\n# 1.\npopulation_long <- population_wide2 %>% \n  pivot_longer(y2010:y2013, \n               names_to = \"year\",\n               values_to = \"population\")\n\npopulation_long\n\n\n# A tibble: 4 x 3\n  country           year  population\n  <chr>             <chr>      <int>\n1 Republic of Korea y2010   48453931\n2 Republic of Korea y2011   48732640\n3 Republic of Korea y2012   49002683\n4 Republic of Korea y2013   49262698\n\n# 2.\npopulation_long2 <- population_long %>% \n  mutate(year = stringr::str_replace(year, \"y\", \"\") %>% \n           as.integer())\n\npopulation_long2\n\n\n# A tibble: 4 x 3\n  country            year population\n  <chr>             <int>      <int>\n1 Republic of Korea  2010   48453931\n2 Republic of Korea  2011   48732640\n3 Republic of Korea  2012   49002683\n4 Republic of Korea  2013   49262698\n\n\n피봇팅 응용\ndplyr 패키지와의 콜라보\ntidyr 패키지는 단독으로 사용되기보다는 dplyr 패키지와 혼용되는 경우가 많습니다.\nHands-on 3\n2010년도 한국의 결핵환자 중 폐외부에 결핵이 발생한 환자의 수를 성별 연령대별로 표현하세요.\nwho에서 한국의 2013년도 통계를 추출한 후, Long 포맷 테이블로 변환하세요.\nLong 포맷 테이블 변수 이름은 “group_code”, “cases”로 지정하세요.\n\n모든 환자수는 몇명인가요?\n\nLong 포맷 테이블을 Long 포맷 테이블로 피봇팅하세요.\n행의 차원은 연령그룹, 열의 차원은 성별로 표현하세요.\n성별은 “male”, “female”로 표현하세요.\n연령그룹은 “age”_숫자로 표현하세요.\n\n집계하여 몇 개의 변수를 추가하세요.\n연령대별 성별 합계를 구하여, total이라는 변수에 넣으세요.\n연령대별로 전체 돗수의 백분율을 percent라는 변수에 넣으세요.\n\n\n\n\n힌트\n폐외부 결핵의 진단방법에 대한 그룹 코드는 “ep”를 포함합니다.\ndplyr 패키지의 select() 함수로 폐외부에 결핵이 발생한 환자 정보만 가져옵니다.\npivot_longer() 함수를 사용합니다.\n집계는 summarise() 함수를 사용합니다.\n\ndplyr 패키지의 mutate() 함수로 변수 gender와 age_group를 파생합니다.\nstringr 패키지로 변수 이름을 조작할 필요가 있습니다.\npivot_wider() 함수를 사용합니다.\ndplyr 패키지의 mutate() 함수로 변수 total과 percent를 파생합니다.\n\ndplyr 솔루션\nstringr::str_detect(group_code, \"_m“)은”_m\"이 포함될 경우에 TRUE를 반환합니다.\n정규표현식 “[^[:digit:]]”은 숫자를 제외한 것과 패턴매칭됩니다.\n\n\n# 1.1\nep_long <- who %>% \n  filter(country %in% \"Republic of Korea\") %>% \n  filter(year %in% \"2010\") %>% \n  select(new_ep_m014:new_ep_f65) %>% \n  pivot_longer(new_ep_m014:new_ep_f65, \n               names_to = \"group_code\",\n               values_to = \"cases\")\n\nep_long\n\n\n# A tibble: 14 x 2\n   group_code   cases\n   <chr>        <int>\n 1 new_ep_m014     62\n 2 new_ep_m1524   511\n 3 new_ep_m2534   646\n 4 new_ep_m3544   690\n 5 new_ep_m4554   687\n 6 new_ep_m5564   571\n 7 new_ep_m65    1088\n 8 new_ep_f014     56\n 9 new_ep_f1524   439\n10 new_ep_f2534   685\n11 new_ep_f3544   644\n12 new_ep_f4554   780\n13 new_ep_f5564   584\n14 new_ep_f65    1352\n\n# 1.2\nep_long %>% \n  summarise(total_cases = sum(cases, na.rm = TRUE))\n\n\n# A tibble: 1 x 1\n  total_cases\n        <int>\n1        8795\n\n# 2.1\nep_wide <- ep_long %>% \n  mutate(gender = case_when (\n    stringr::str_detect(group_code, \"_m\") ~ \"male\",\n    !stringr::str_detect(group_code, \"_m\") ~ \"female\")\n  ) %>% \n  mutate(age_group = stringr::str_remove_all(group_code, \"[^[:digit:]]\")) %>% \n  mutate(age_group = paste(\"age\", age_group, sep = \"_\")) %>% \n  select(-group_code) %>% \n  pivot_wider(names_from = \"gender\",\n              values_from = \"cases\")\n\n# 2.2\nep_wide %>% \n  mutate(total = male + female) %>% \n  mutate(percent = round(total / sum(total) * 100, 2))\n\n\n# A tibble: 7 x 5\n  age_group  male female total percent\n  <chr>     <int>  <int> <int>   <dbl>\n1 age_014      62     56   118    1.34\n2 age_1524    511    439   950   10.8 \n3 age_2534    646    685  1331   15.1 \n4 age_3544    690    644  1334   15.2 \n5 age_4554    687    780  1467   16.7 \n6 age_5564    571    584  1155   13.1 \n7 age_65     1088   1352  2440   27.7 \n\n\n\n현미경을 이용한 세포관찰법의 하나로 광학현미경 프레파라트를 짧은 시간에 간단하게 만들어 세포 전체를 관찰할 수 있는 방법입니다. (https://www.scienceall.com/도말표본검사 참고)↩︎\n",
    "preview": "posts/2022-02-25-tidyr/img/tidyr.png",
    "last_modified": "2022-03-01T09:36:05+09:00",
    "input_file": {},
    "preview_width": 736,
    "preview_height": 850
  },
  {
    "path": "posts/2022-02-24-dplyr/",
    "title": "Wrangle data with dplyr",
    "description": "dplyr 패키지로 데이터를 조작하는 방법을 숙지합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-02-24",
    "categories": [
      "Tidyverse",
      "dplyr",
      "manipulate data"
    ],
    "contents": "\n\nContents\n미리보기\nSQL과 dplyr 비교\n\n준비하기\n데이터 준비하기\n\n워밍업\n데이터 살펴보기\ndplyr 기초 다지기\n필터링과 집계\n파생변수 만들기\n순위 계산하기\n\n\n응용하기\n집계 데이터의 시각화\nggplot2 패키지와의 연동\n\n데이터 결합\n조인하기\n\n윈도우 함수 사용하기\n전후 시간대의 상황 이해하기\n\n\n\n\n\n\n\n\n들어가기\nSQL에 익숙한 당신, dplyr를 쉽게 배울 수 있습니다.\n\n파이프를 이용한 dplyr의 사용은 SQL의 순차적인 Syntax를 그대로 이식시켜 주며, 집합론적인 사고의 로직도 쉽게 적용할 수 있기 때문입니다. \n\n\n\n미리보기\nSQL과 dplyr 비교\n가상의 데이터를 집계하는 다음의 Script를 보면 dplyr의 문법이 SQL 문번과 상당히 유사함을 발견할 수 있습니다.\n\nSQL\n다음은 데이터를 집계하는 가상의 SQL 구문입니다.\n\nSELECT col1,\n       CASE col2\n         WHEN '1' THEN 'TRUE'\n         WHEN '0' THEN 'FALSE'\n         ELSE 'FALSE'\n       END AS col2,\n       SUM(col3) AS col3\n  FROM T_TABS\n WHERE col3 > 34\n   AND col4 in ('a', 'b', 'c')\n GROUP BY col1, \n       CASE col2\n         WHEN '1' THEN 'TRUE'\n         WHEN '0' THEN 'FALSE'\n         ELSE 'FALSE' \n ORDER BY col3 DESC\n\ndplyr\n데이터를 집계하는 가상의 SQL 구문을 dplyr로 변환하면 다음과 같습니다.\n\n\nT_TABS %>% \n  filter(col3 > 3) %>% \n  filter(col4 %in% c('a', 'b', 'c')) %>% \n  mutate(col1 = case_when(\n    col1 == '1' ~ 'TRUE',\n    col1 == '2' ~ 'FALSE',\n    TRUE ~ 'FALSE')\n  ) %>% \n  group_by(col1, col2) %>% \n  summarize(col3 = sum(col3)) %>% \n  arrange(desc(col3))\n\n\n\n\n데이터를 조인하는 방법도 유사합니다.\n\nSQL\n다음은 두 개의 데이터를 조인하는 가상의 SQL 구문입니다.\n\nSELECT a.empno\n     , a.ename\n     , a.job\n     , a.mgr\n     , a.deptno\n     , b.dname\n  FROM emp AS a\n LEFT JOIN dept AS b\n    ON a.deptno = b.deptno\n\ndplyr\n두 개의 데이터를 조인하는 가상의 SQL 구문을 dplyr로 변환하면 다음과 같습니다.\n\n\nemp %>% \n  select(empno, ename, job, deptno) %>% \n  left_join(\n    dept %>% \n      select(deptno, dname),\n    by = c(\"deptno\")\n  )\n\n\n\n\n\n\n\n\n솔루션\n\n데이터 조작은 속도도 중요하지만, 빠른 시간 내에 정확한 스크립트를 작성하거나, 공유를 위한 해석의 용이성도 중요합니다. 이것을 만족하는 것이 dplyr입니다.\n\n\n준비하기\n데이터 준비하기\nnycflights13 패키지는 2013년도의 NYC 공항 출발 항공편과 관련된 데이터를 담은, 유용한 데이터 패키지입니다.\n데이터의 종류\nflights\n메인 데이터로 NYC 공항 출발 항공편 데이터\nOn-time data for all flights that departed NYC (i.e. JFK, LGA or EWR) in 2013.\n\nairlines\n항공사에 대한 메타 데이터\nLook up airline names from their carrier codes.\n\nairports\n공항에 대한 메타 데이터\nUseful metadata about airports.\n\nplanes\n항공기에 대한 메타 데이터\nPlane metadata for all plane tailnumbers.\n\nweather\n시간대별 기상 데이터\nHourly meterological data for LGA, JFK and EWR.\n\n\n워밍업\n데이터 살펴보기\nflights 데이터 프레임에는 2013년에 뉴욕에서 출발한 모든 336,776개의 항공편이 포함되어 있습니다. 이 데이터는 미국 교통 통계국에서 가져왔습니다.\n\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nflights\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\ndplyr 기초 다지기\n필터링과 집계\nHands-on 1\n년말(12월 31일)과 년초(1월 1일)의 비행 현황을 살펴보려 합니다. 해당 데이터를 추출해서 flights_1231_0101 데이터 프레임에 저장해보세요.\n추출한 데이터를 날짜별로 몇 건인지 확인해 보세요.\n\n\n힌트\n변수 month와 day가 월과 일을 나타냅니다.\nfilter() 함수를 사용합니다.\n논리합 연산자 |와 논리곱 연산자 &를 사용하면 되겠지요.\n\ngroup_by()와 summarise() 함수를 사용합니다.\nn() 함수는 개수를 세는 함수입니다.\n\n\nR의 논리연산자dplyr 솔루션\n건수만 살펴볼 경우에는 tally() 함수가 유용합니다.\n\n\nflights_1231_0101 <- flights %>% \n  filter((month == 12 & day == 31) |\n         (month == 1 & day == 1))  \n\nflights_1231_0101 %>% \n  group_by(month, day) %>% \n  summarise(cnt = n())\n\n\n# A tibble: 2 x 3\n# Groups:   month [2]\n  month   day   cnt\n  <int> <int> <int>\n1     1     1   842\n2    12    31   776\n\nflights_1231_0101 %>% \n  group_by(month, day) %>% \n  tally() \n\n\n# A tibble: 2 x 3\n# Groups:   month [2]\n  month   day     n\n  <int> <int> <int>\n1     1     1   842\n2    12    31   776\n\n\n파생변수 만들기\nHands-on 2\n년말(12월 31일) 년초(1월 1일)와 그렇지 않은 날짜의 출발 지연시간의 평균을 비교해 보세요.\n\n\n힌트\nmutate() 함수를 사용합니다.\nifelse() 함수를 사용할 수 있습니다.\n역시 논리합 연산자 |와 논리곱 연산자 &를 사용해야 겠지요.\ngroup_by()와 summarise() 함수를 사용합니다.\ndep_delay가 출발 지연시간을 나타내는 변수입니다.\n\ndplyr 솔루션\nifelse() 함수 대신에 dplyr 패키지의 case_when() 함수를 사용해도 됩니다. 이 함수는 조건의 개수가 여러 개일 때 유용합니다.\n\n\nflights %>% \n  mutate(class_date = ifelse((month == 12 & day == 31) |\n         (month == 1 & day == 1), \"연말년초\", \"연말년초 아님\")) %>%   \n  group_by(class_date) %>% \n  summarise(dep_delay_avg = mean(dep_delay, na.rm = TRUE))\n\n\n# A tibble: 2 x 2\n  class_date    dep_delay_avg\n  <chr>                 <dbl>\n1 연말년초               9.38\n2 연말년초 아님         12.7 \n\nflights %>% \n  mutate(class_date = case_when(\n    (month == 12 & day == 31) | (month == 1 & day == 1) ~ \"연말년초\", \n    TRUE ~ \"연말년초 아님\")) %>%   \n  group_by(class_date) %>% \n  summarise(dep_delay_avg = mean(dep_delay, na.rm = TRUE))\n\n\n# A tibble: 2 x 2\n  class_date    dep_delay_avg\n  <chr>                 <dbl>\n1 연말년초               9.38\n2 연말년초 아님         12.7 \n\n\n순위 계산하기\nHands-on 3\n출발 지연시간의 평균이 큰 5개의 날짜를 추출하세요.\n\n\n힌트\ngroup_by() 함수와 summarise() 함수로 집계합니다.\narrange() 함수와 desc() 함수로 정렬합니다.\nrow_number() 함수로 관측치의 순번을 구한 후, filter() 함수로 원하는 대상을 가져옵니다.\n\ndplyr 솔루션\n상위 n개의 관측치를 가져오기 위해서 filter() 함수와 row_number() 함수를 사용할 수 있으나, top_n() 함수를 사용하면 쉽게 계산할 수 있습니다.\n\n\nflights %>% \n  group_by(month, day) %>% \n  summarise(dep_delay_avg = mean(dep_delay, na.rm = TRUE),\n            .groups = \"drop\") %>% \n  arrange(desc(dep_delay_avg)) %>% \n  filter(row_number() <= 5)\n\n\n# A tibble: 5 x 3\n  month   day dep_delay_avg\n  <int> <int>         <dbl>\n1     3     8          83.5\n2     7     1          56.2\n3     9     2          53.0\n4     7    10          52.9\n5    12     5          52.3\n\nflights %>% \n  group_by(month, day) %>% \n  summarise(dep_delay_avg = mean(dep_delay, na.rm = TRUE),\n            .groups = \"drop\") %>% \n  arrange(desc(dep_delay_avg)) %>% \n  top_n(5)\n\n\n# A tibble: 5 x 3\n  month   day dep_delay_avg\n  <int> <int>         <dbl>\n1     3     8          83.5\n2     7     1          56.2\n3     9     2          53.0\n4     7    10          52.9\n5    12     5          52.3\n\n\n응용하기\n집계 데이터의 시각화\nggplot2 패키지와의 연동\nHands-on 4\n비행거리와 도착 지연 시간과의 관계를 살펴보려고 합니다.\n목적지인 dest별로 운항횟수를 계산하세요.\n목적지인 dest별로 거리(distance)의 평균과 도착 지연시간(arr_delay)의 평균을 구하세요.\n년간 운행횟수가 20회보다 큰 노선을 대상으로 집계하세요.\n\n1번 결과로 거리별 평균 도착 지연시간의 관계를 시각화하세요.\n산점도를 그리되, 운행횟수로 점의 크기를 나타내세요.\nx-축은 거리를, y-축은 도착지연시간을 나타내세요.\nSmooth 곡선을 추가해 보세요.\n\n\n\n힌트\n먼저 group_by()와 summarise() 함수를 사용 후, filter() 함수를 사용합니다.\n\ngeom_point() 함수로 산점도를 그리고,\ngeom_smooth() 함수로 Smooth 곡선을 그립니다.\n\ndplyr 솔루션\ndplyr 결과를 바로 ggplot2 시각화 함수인 ggplot() 함수로 보낼 수 있습니다. dplyr 패키지의 파이프인 “%>%”와 ggplot2의 파이프인 “+”를 혼동해서는 안됩니다.\n\n\nflights %>% \n  group_by(dest) %>%\n  summarise(\n    cnt = n(),\n    dist = mean(distance, na.rm = TRUE),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) %>%   \n  filter(cnt > 20) %>% \n  ggplot(mapping = aes(x = dist, y = delay)) +\n  geom_point(aes(size = cnt), alpha = 1/3) +\n  geom_smooth(se = FALSE)\n\n\n\n\n\n데이터 결합\n조인하기\nHands-on 5\n항공기별로 데이터를 집계합니다.\nflights 데이터와 planes 데이터를 조인합니다.\n도착 지연시간이 큰 상위 10개의 항공기 제조사, 항공기 모델을 추출하세요.\n어떤 엔진을 사용하고 있나요?\n\n\n\n힌트\nleft_join() 함수를 사용합니다. 그리고 조인 변수는 “tailnum”입니다.\n순서로 일부 데이터를 가져오기 때문에 summarise() 함수에는 .groups = “drop”를 적용합니다.\ntop_n() 함수를 사용합니다.\n\ndplyr 솔루션\nsummarise() 함수에는 .groups = “drop”를 적용하지 않으면, 상위 n개가 아닌 전체 건이 추출됩니다. .groups = “drop”는 집계된 데이터에 그룹정보를 삭제하는 옵션입니다.\n\n\nflights %>% \n  left_join(\n    planes,\n    by = \"tailnum\"\n  ) %>% \n  group_by(manufacturer, model, engine) %>% \n  summarise(delay = mean(arr_delay, na.rm = TRUE),\n            .groups = \"drop\") %>% \n  arrange(desc(delay)) %>% \n  top_n(n = 10)            \n\n\n# A tibble: 10 x 4\n   manufacturer         model    engine      delay\n   <chr>                <chr>    <chr>       <dbl>\n 1 AIRBUS INDUSTRIE     A330-223 Turbo-jet   219  \n 2 BOEING               747-451  Turbo-jet   120  \n 3 BOEING               757-351  Turbo-jet    72.5\n 4 GULFSTREAM AEROSPACE G-IV     Turbo-fan    41.2\n 5 BOEING               MD-90-30 Turbo-fan    40.9\n 6 BOEING               777-224  Turbo-fan    40.8\n 7 AIRBUS               A319-115 Turbo-fan    33.5\n 8 AGUSTA SPA           A109E    Turbo-shaft  30.6\n 9 AIRBUS INDUSTRIE     A340-313 Turbo-jet    29.8\n10 BOEING               737-76N  Turbo-fan    28.4\n\n\n윈도우 함수 사용하기\n전후 시간대의 상황 이해하기\n항공기가 아닌 공항의 이슈에 기인한 항공기의 출발 지연은 이후 다른 항공 스케줄에 영향을 줄 수 있습니다.\nHands-on 6\n항공기의 출발 지연 비율이 가장 높은 날자를 추출하세요.\n편성 노선 대비 출발 지연 노선의 비율 상위 10개 일자를 추출하세요.\n상위 1개 일자의 전체 노선에 대해서 출발 지연시간인 dep_delay를 Bar Cart로 시각화 해 보세요.\n\n윈도우 함수를 이용해서 바로 이전 편성보다 출발 지연시간이 큰 경우의 수가 많은 날자를 추출하세요.\nlag() 함수는 이전 계열의 데이터를 참조할 수 있는 윈도우 함수입니다.\n상위 1개 일자의 전체 노선에 대해서 이전 시간 대비 지연 증감시간을 Bar Cart로 시각화 해 보세요.\n\n\n\n힌트\n출발 지연시간인 dep_delay이 0보다 큰 것이 몇 건인지 집계합니다.\n순서로 일부 데이터를 가져오기 때문에 summarise() 함수에는 .groups = “drop”를 적용합니다.\ntop_n() 함수를 사용합니다.\ntime_hour 변수는 노선 스케줄의 일자와 시간까지 기록된 변수입니다. 이것을 조작해 보세요.\nggplot2의 geom_bar() 함수와 geom_smooth() 함수를 사용합니다.\n\n현재의 값에서 lag() 함수로 이전의 값을 가져와서 빼줍니다. dep_delay - lag(dep_delay)\n출발 지연의 증감을 계산하기 전에 데이터를 시간의 순으로 정렬해야 합니다.\n증감량이 0보다 크고, 현 시점의 출발지연 시간도 0보다 큰 건의 개수를 세야 합니다.\n\ndplyr 솔루션\n솔루션 1\nhms 패키지도 Tidiverse 패키지군에 포함된 패키지입니다. 이 패키지는 시간 데이터를 조작하는 패키지입니다. as.hms() 함수로 문자열을 시간:분:초 포맷의 hms 데이터로 변환합니다.\n\n\n# 1.1\ntop_delay <- flights %>%\n  group_by(month, day) %>% \n  summarise(cnt = n(),\n            cnt_delay = sum(dep_delay > 0, na.rm = TRUE),\n            rate_delay = cnt_delay / cnt, .groups = \"drop\") %>% \n  arrange(desc(rate_delay)) %>% \n  top_n(10)\n\ntop_delay\n\n\n# A tibble: 10 x 5\n   month   day   cnt cnt_delay rate_delay\n   <int> <int> <int>     <int>      <dbl>\n 1    12    23   985       674      0.684\n 2     7     1   966       652      0.675\n 3     3     8   979       653      0.667\n 4     6    25   993       649      0.654\n 5    12    22   895       583      0.651\n 6     7    23   997       645      0.647\n 7    12    17   949       608      0.641\n 8     5    24   978       621      0.635\n 9    12     9   962       606      0.630\n10     2    27   945       584      0.618\n\n# 1.2\nflights %>%\n  arrange(time_hour, sched_dep_time) %>% \n  filter(month == top_delay$month[1] & day == top_delay$day[1]) %>% \n  mutate(sched_time = sprintf(\"%04d\", sched_dep_time)) %>% \n  mutate(time_hour = \n    hms::as.hms(paste(substr(sched_time, 1, 2), \n          substr(sched_time, 3, 4), \"00\", sep = \":\"))) %>% \n  select(time_hour, dep_delay) %>% \n  ggplot(aes(x = time_hour, y = dep_delay)) +\n  geom_bar(stat = \"identity\") + \n  geom_smooth()\n\n\n\n\n솔루션 2\nsched_dep_time 변수는 출발 스케줄을 분 단위까지 포함하고 있기 때문에 윈도우 함수를 사용하기 전에 이 변수 기준으로 오름차순으로 정렬해야 합니다.\n\n\n# 2.1\ntop_increase <- flights %>%\n  group_by(month, day) %>% \n  arrange(sched_dep_time) %>% \n  mutate(delta_delay = dep_delay - lag(dep_delay)) %>% \n  summarise(cnt = n(),\n            increase_delay = sum(delta_delay > 0 & dep_delay > 0, na.rm = TRUE),\n            rate_increase = increase_delay / cnt, .groups = \"drop\") %>% \n  arrange(desc(rate_increase)) %>% \n  top_n(10)\n\ntop_increase\n\n\n# A tibble: 10 x 5\n   month   day   cnt increase_delay rate_increase\n   <int> <int> <int>          <int>         <dbl>\n 1    12    23   985            513         0.521\n 2     6    25   993            505         0.509\n 3     7     1   966            490         0.507\n 4    12    22   895            453         0.506\n 5     5    24   978            492         0.503\n 6    12    18   956            479         0.501\n 7    12    21   811            404         0.498\n 8     7    23   997            490         0.491\n 9    12     9   962            468         0.486\n10     2    27   945            459         0.486\n\nflights %>%\n  filter(month == top_increase$month[1] & day == top_increase$day[1]) %>% \n  mutate(sched_time = sprintf(\"%04d\", sched_dep_time)) %>% \n  mutate(time_hour = \n    hms::as.hms(paste(substr(sched_time, 1, 2), \n          substr(sched_time, 3, 4), \"00\", sep = \":\"))) %>% \n  arrange(time_hour) %>% \n  mutate(delta_delay = dep_delay - lag(dep_delay)) %>%   \n  mutate(delta = ifelse(delta_delay > 0, \"Increase\", \"Decrease\") %>% \n           as.factor()) %>% \n  ggplot(aes(x = time_hour, y = delta_delay, fill = delta)) +\n  geom_bar(stat = \"identity\") \n\n\n\n\n\n\n\n\n",
    "preview": "posts/2022-02-24-dplyr/img/dplyr.png",
    "last_modified": "2022-03-01T09:36:53+09:00",
    "input_file": {},
    "preview_width": 736,
    "preview_height": 850
  },
  {
    "path": "posts/2022-02-23-readr/",
    "title": "Import data with readr",
    "description": "readr 패키지로 데이터를 R로 가져오는 방법을 숙지합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-02-23",
    "categories": [
      "Tidyverse",
      "readr",
      "import data"
    ],
    "contents": "\n\nContents\n준비하기\n데이터 파일 준비하기\n\n실습하기\nCSV 파일 읽기\n대용량 TSV 파일 읽기\n\n\n\n\n\n\n\n들어가기\n대용량 데이터는 이제부터 readr을 사용하자. 왜? 속도가 빠르기 때문에 필요없는 대기 시간을 줄여주기 때문입니다.\n\nreadr은 파일의 인코딩을 유추할 수 있는 기능이 있어 특히 한글 파일을 import할 때 유용합니다.\n\n\n\n준비하기\n데이터 파일 준비하기\n다음과 같은 데이터 파일을 다운로드 한다. 브라우저의 다른 이름으로 링크 저장 기능을 이용해서 파일을 다운로드 합니다.\n아파트매매_실거래가_200001.csv\n한글이 포함된 데이터 파일\nCSV 파일(컴마를 구분자로 한 텍스트 파일)\n한글이 “cp949”로 인코딩되어 있음\n\nFoodFacts.tsv\n대용량 데이터 파일\nTSV 파일(탭문자를 구분자로 한 텍스트 파일)\n\n“data” 라는 디렉토리를 생성하고, 다운로드한 데이터 파일을 복사해 넣습는다.\n실습하기\nCSV 파일 읽기\n\n기존 방법\nread.csv() 함수는 CSV 파일을 읽을 수 있는 함수입니다.\n\n\napt <- read.csv(file = \"data/아파트매매_실거래가_200001.csv\", \n                header = TRUE, \n                fileEncoding = \"cp949\",\n                stringsAsFactors = FALSE)\n\nstr(apt)\n\n\n'data.frame':   58805 obs. of  13 variables:\n $ 시군구        : chr  \"강원도 강릉시 견소동\" \"강원도 강릉시 견소동\" \"강원도 강릉시 견소동\" \"강원도 강릉시 견소동\" ...\n $ 번지          : chr  \"202\" \"202\" \"202\" \"202\" ...\n $ 본번          : chr  \"0202\" \"0202\" \"0202\" \"0202\" ...\n $ 부번          : chr  \"0000\" \"0000\" \"0000\" \"0000\" ...\n $ 단지명        : chr  \"송정한신\" \"송정한신\" \"송정한신\" \"송정한신\" ...\n $ 전용면적...   : num  43.4 59.8 59.8 84.9 116.2 ...\n $ 계약년월      : int  202001 202001 202001 202001 202001 202001 202001 202001 202001 202001 ...\n $ 계약일        : int  3 15 18 18 21 23 6 20 27 4 ...\n $ 거래금액.만원.: chr  \"12,000\" \"10,000\" \"10,500\" \"13,500\" ...\n $ 층            : int  12 3 12 11 5 8 10 15 8 14 ...\n $ 건축년도      : int  1997 1997 1997 1997 1997 1997 2005 2009 2009 1999 ...\n $ 도로명        : chr  \"경강로2539번길 8\" \"경강로2539번길 8\" \"경강로2539번길 8\" \"경강로2539번길 8\" ...\n $ 해제사유발생일: logi  NA NA NA NA NA NA ...\n\ndim(apt)\n\n\n[1] 58805    13\n\nreadr\nread_csv() 함수는 readr 패키지에서 CSV 파일을 읽을 수 있는 함수입니다.\n그런데 read_csv() 함수로 영문 파일이 아닌 멀티 바이트 문자 파일을 읽을 때 에러가 발생 할 수 있습니다. 이 경우는 guess_encoding() 함수로 파일의 인코딩을 유추할 수 있습니다. 그러나 read.csv()에서의 사용하는 인코딩 이름과 체계가 다를 수 있습니다. “cp949”가 “EUC-KR”로 유추됩니다. read_csv() 함수에 “EUC-KR” 인코딩 이름을 적용해도 결과는 동일합니다.\n\n\nlibrary(readr)\nlibrary(dplyr)\n\napt2 <- read_csv(file = \"data/아파트매매_실거래가_200001.csv\",\n                 locale = locale(encoding = \"cp949\"))\n\nstr(apt2)\n\n\nspec_tbl_df [58,805 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ 시군구        : chr [1:58805] \"강원도 강릉시 견소동\" \"강원도 강릉시 견소동\" \"강원도 강릉시 견소동\" \"강원도 강릉시 견소동\" ...\n $ 번지          : chr [1:58805] \"202\" \"202\" \"202\" \"202\" ...\n $ 본번          : chr [1:58805] \"0202\" \"0202\" \"0202\" \"0202\" ...\n $ 부번          : chr [1:58805] \"0000\" \"0000\" \"0000\" \"0000\" ...\n $ 단지명        : chr [1:58805] \"송정한신\" \"송정한신\" \"송정한신\" \"송정한신\" ...\n $ 전용면적(㎡)  : num [1:58805] 43.4 59.8 59.8 84.9 116.2 ...\n $ 계약년월      : num [1:58805] 202001 202001 202001 202001 202001 ...\n $ 계약일        : num [1:58805] 3 15 18 18 21 23 6 20 27 4 ...\n $ 거래금액(만원): num [1:58805] 12000 10000 10500 13500 19000 ...\n $ 층            : num [1:58805] 12 3 12 11 5 8 10 15 8 14 ...\n $ 건축년도      : num [1:58805] 1997 1997 1997 1997 1997 ...\n $ 도로명        : chr [1:58805] \"경강로2539번길 8\" \"경강로2539번길 8\" \"경강로2539번길 8\" \"경강로2539번길 8\" ...\n $ 해제사유발생일: logi [1:58805] NA NA NA NA NA NA ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   시군구 = col_character(),\n  ..   번지 = col_character(),\n  ..   본번 = col_character(),\n  ..   부번 = col_character(),\n  ..   단지명 = col_character(),\n  ..   `전용면적(㎡)` = col_double(),\n  ..   계약년월 = col_double(),\n  ..   계약일 = col_double(),\n  ..   `거래금액(만원)` = col_number(),\n  ..   층 = col_double(),\n  ..   건축년도 = col_double(),\n  ..   도로명 = col_character(),\n  ..   해제사유발생일 = col_logical()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\ndim(apt2)\n\n\n[1] 58805    13\n\n# 파일의 10줄을 읽어서 인코딩을 유추합니다.\nread_lines(\"data/아파트매매_실거래가_200001.csv\", n_max = 10) %>% \n  guess_encoding()\n\n\n# A tibble: 3 x 2\n  encoding confidence\n  <chr>         <dbl>\n1 EUC-KR         1   \n2 GB18030        0.58\n3 Big5           0.34\n\n\nread.csv() 함수는 data.frame 객체를 반환하지만, read 패키지의 함수들은 tibble 객체를 반환합니다.\n대용량 TSV 파일 읽기\nFoodFacts.tsv 파일은 약 92MB의 용량으로 대용량이라 보기 어렵지만, 실습 차원에서 대용량으로 간주하여 진행합니다.\n\n\nfile.size(\"data/FoodFacts.tsv\") / 1024^2\n\n\n[1] 91.65345\n\n\n기존 방법\nread.csv() 함수는 TSV 파일을 읽을 수 있는 함수입니다.\n\n\nelapse_old <- system.time(\n  foodfact <- read.csv(file = \"data/FoodFacts.tsv\",\n                       sep = \"\\t\",\n                       header = TRUE, \n                       stringsAsFactors = FALSE)\n)\n\nelapse_old\n\n\n   user  system elapsed \n  2.568   0.086   2.663 \n\ndim(foodfact)\n\n\n[1] 23179   163\n\nstr(foodfact)\n\n\n'data.frame':   23179 obs. of  163 variables:\n $ code                                      : num  3087 4530 4559 16087 16094 ...\n $ url                                       : chr  \"http://world-en.openfoodfacts.org/product/0000000003087/farine-de-ble-noir-ferme-t-y-r-nao\" \"http://world-en.openfoodfacts.org/product/0000000004530/banana-chips-sweetened-whole\" \"http://world-en.openfoodfacts.org/product/0000000004559/peanuts-torn-glasser\" \"http://world-en.openfoodfacts.org/product/0000000016087/organic-salted-nut-mix-grizzlies\" ...\n $ creator                                   : chr  \"openfoodfacts-contributors\" \"usda-ndb-import\" \"usda-ndb-import\" \"usda-ndb-import\" ...\n $ created_t                                 : int  1474103866 1489069957 1489069957 1489055731 1489055653 1489055651 1489055730 1489055711 1489055651 1489055654 ...\n $ created_datetime                          : chr  \"2016-09-17T09:17:46Z\" \"2017-03-09T14:32:37Z\" \"2017-03-09T14:32:37Z\" \"2017-03-09T10:35:31Z\" ...\n $ last_modified_t                           : int  1474103893 1489069957 1489069957 1489055731 1489055653 1489055651 1489055730 1489055712 1489055651 1489055654 ...\n $ last_modified_datetime                    : chr  \"2016-09-17T09:18:13Z\" \"2017-03-09T14:32:37Z\" \"2017-03-09T14:32:37Z\" \"2017-03-09T10:35:31Z\" ...\n $ product_name                              : chr  \"Farine de blé noir\" \"Banana Chips Sweetened (Whole)\" \"Peanuts\" \"Organic Salted Nut Mix\" ...\n $ generic_name                              : chr  \"\" \"\" \"\" \"\" ...\n $ quantity                                  : chr  \"1kg\" \"\" \"\" \"\" ...\n $ packaging                                 : chr  \"\" \"\" \"\" \"\" ...\n $ packaging_tags                            : chr  \"\" \"\" \"\" \"\" ...\n $ brands                                    : chr  \"Ferme t'y R'nao\" \"\" \"Torn & Glasser\" \"Grizzlies\" ...\n $ brands_tags                               : chr  \"ferme-t-y-r-nao\" \"\" \"torn-glasser\" \"grizzlies\" ...\n $ categories                                : chr  \"\" \"\" \"\" \"\" ...\n $ categories_tags                           : chr  \"\" \"\" \"\" \"\" ...\n $ categories_en                             : chr  \"\" \"\" \"\" \"\" ...\n $ origins                                   : chr  \"\" \"\" \"\" \"\" ...\n $ origins_tags                              : chr  \"\" \"\" \"\" \"\" ...\n $ manufacturing_places                      : chr  \"\" \"\" \"\" \"\" ...\n $ manufacturing_places_tags                 : chr  \"\" \"\" \"\" \"\" ...\n $ labels                                    : chr  \"\" \"\" \"\" \"\" ...\n $ labels_tags                               : chr  \"\" \"\" \"\" \"\" ...\n $ labels_en                                 : chr  \"\" \"\" \"\" \"\" ...\n $ emb_codes                                 : chr  \"\" \"\" \"\" \"\" ...\n $ emb_codes_tags                            : chr  \"\" \"\" \"\" \"\" ...\n $ first_packaging_code_geo                  : chr  \"\" \"\" \"\" \"\" ...\n $ cities                                    : logi  NA NA NA NA NA NA ...\n $ cities_tags                               : chr  \"\" \"\" \"\" \"\" ...\n $ purchase_places                           : chr  \"\" \"\" \"\" \"\" ...\n $ stores                                    : chr  \"\" \"\" \"\" \"\" ...\n $ countries                                 : chr  \"en:FR\" \"US\" \"US\" \"US\" ...\n $ countries_tags                            : chr  \"en:france\" \"en:united-states\" \"en:united-states\" \"en:united-states\" ...\n $ countries_en                              : chr  \"France\" \"United States\" \"United States\" \"United States\" ...\n $ ingredients_text                          : chr  \"\" \"Bananas, vegetable oil (coconut oil, corn oil and/or palm oil) sugar, natural banana flavor.\" \"Peanuts, wheat flour, sugar, rice flour, tapioca starch, salt, leavening (ammonium bicarbonate, baking soda), s\"| __truncated__ \"Organic hazelnuts, organic cashews, organic walnuts almonds, organic sunflower oil, sea salt.\" ...\n $ allergens                                 : chr  \"\" \"\" \"\" \"\" ...\n $ allergens_en                              : logi  NA NA NA NA NA NA ...\n $ traces                                    : chr  \"\" \"\" \"\" \"\" ...\n $ traces_tags                               : chr  \"\" \"\" \"\" \"\" ...\n $ traces_en                                 : chr  \"\" \"\" \"\" \"\" ...\n $ serving_size                              : chr  \"\" \"28 g (1 ONZ)\" \"28 g (0.25 cup)\" \"28 g (0.25 cup)\" ...\n $ no_nutriments                             : logi  NA NA NA NA NA NA ...\n $ additives_n                               : int  NA 0 0 0 0 0 0 1 0 0 ...\n $ additives                                 : chr  \"\" \" [ bananas -> en:bananas  ]  [ vegetable-oil -> en:vegetable-oil  ]  [ oil -> en:oil  ]  [ coconut-oil -> en:co\"| __truncated__ \" [ peanuts -> en:peanuts  ]  [ wheat-flour -> en:wheat-flour  ]  [ flour -> en:flour  ]  [ sugar -> en:sugar  ]\"| __truncated__ \" [ organic-hazelnuts -> en:organic-hazelnuts  ]  [ hazelnuts -> en:hazelnuts  ]  [ organic-cashews -> en:organi\"| __truncated__ ...\n $ additives_tags                            : chr  \"\" \"\" \"\" \"\" ...\n $ additives_en                              : chr  \"\" \"\" \"\" \"\" ...\n $ ingredients_from_palm_oil_n               : chr  \"\" \"0\" \"0\" \"0\" ...\n $ ingredients_from_palm_oil                 : logi  NA NA NA NA NA NA ...\n $ ingredients_from_palm_oil_tags            : chr  \"\" \"\" \"\" \"\" ...\n $ ingredients_that_may_be_from_palm_oil_n   : chr  \"\" \"0\" \"0\" \"0\" ...\n $ ingredients_that_may_be_from_palm_oil     : chr  \"\" \"\" \"\" \"\" ...\n $ ingredients_that_may_be_from_palm_oil_tags: chr  \"\" \"\" \"\" \"\" ...\n $ nutrition_grade_uk                        : int  NA NA NA NA NA NA NA NA NA NA ...\n $ nutrition_grade_fr                        : chr  \"\" \"d\" \"b\" \"d\" ...\n $ pnns_groups_1                             : chr  \"\" \"\" \"\" \"\" ...\n $ pnns_groups_2                             : chr  \"\" \"\" \"\" \"\" ...\n $ states                                    : chr  \"en:to-be-completed, en:nutrition-facts-to-be-completed, en:ingredients-to-be-completed, en:expiration-date-to-b\"| __truncated__ \"en:to-be-completed, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed,\"| __truncated__ \"en:to-be-completed, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed,\"| __truncated__ \"en:to-be-completed, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed,\"| __truncated__ ...\n $ states_tags                               : chr  \"en:to-be-completed,en:nutrition-facts-to-be-completed,en:ingredients-to-be-completed,en:expiration-date-to-be-c\"| __truncated__ \"en:to-be-completed,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:\"| __truncated__ \"en:to-be-completed,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:\"| __truncated__ \"en:to-be-completed,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:\"| __truncated__ ...\n $ states_en                                 : chr  \"To be completed,Nutrition facts to be completed,Ingredients to be completed,Expiration date to be completed,Cha\"| __truncated__ \"To be completed,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Packaging-code-\"| __truncated__ \"To be completed,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Packaging-code-\"| __truncated__ \"To be completed,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Packaging-code-\"| __truncated__ ...\n $ main_category                             : chr  \"\" \"\" \"\" \"\" ...\n $ main_category_en                          : chr  \"\" \"\" \"\" \"\" ...\n $ image_url                                 : chr  \"\" \"\" \"\" \"\" ...\n $ image_small_url                           : chr  \"\" \"\" \"\" \"\" ...\n $ energy_100g                               : chr  \"\" \"2243\" \"1941\" \"2540\" ...\n $ energy.from.fat_100g                      : chr  \"\" \"\" \"\" \"\" ...\n $ fat_100g                                  : chr  \"\" \"28.57\" \"17.86\" \"57.14\" ...\n $ saturated.fat_100g                        : chr  \"\" \"28.57\" \"0\" \"5.36\" ...\n $ X.butyric.acid_100g                       : chr  \"\" \"\" \"\" \"\" ...\n $ X.caproic.acid_100g                       : num  NA NA NA NA NA NA NA NA NA NA ...\n $ X.caprylic.acid_100g                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ X.capric.acid_100g                        : logi  NA NA NA NA NA NA ...\n $ X.lauric.acid_100g                        : num  NA NA NA NA NA NA NA NA NA NA ...\n $ X.myristic.acid_100g                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ X.palmitic.acid_100g                      : chr  \"\" \"\" \"\" \"\" ...\n $ X.stearic.acid_100g                       : logi  NA NA NA NA NA NA ...\n $ X.arachidic.acid_100g                     : int  NA NA NA NA NA NA NA NA NA NA ...\n $ X.behenic.acid_100g                       : chr  \"\" \"\" \"\" \"\" ...\n $ X.lignoceric.acid_100g                    : chr  \"\" \"\" \"\" \"\" ...\n $ X.cerotic.acid_100g                       : chr  \"\" \"\" \"\" \"\" ...\n $ X.montanic.acid_100g                      : num  NA NA NA NA NA NA NA NA NA NA ...\n $ X.melissic.acid_100g                      : logi  NA NA NA NA NA NA ...\n $ monounsaturated.fat_100g                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ polyunsaturated.fat_100g                  : num  NA NA NA NA NA NA NA NA NA NA ...\n $ omega.3.fat_100g                          : num  NA NA NA NA NA NA NA NA NA NA ...\n $ X.alpha.linolenic.acid_100g               : num  NA NA NA NA NA NA NA NA NA NA ...\n $ X.eicosapentaenoic.acid_100g              : logi  NA NA NA NA NA NA ...\n $ X.docosahexaenoic.acid_100g               : chr  \"\" \"\" \"\" \"\" ...\n $ omega.6.fat_100g                          : logi  NA NA NA NA NA NA ...\n $ X.linoleic.acid_100g                      : logi  NA NA NA NA NA NA ...\n $ X.arachidonic.acid_100g                   : chr  \"\" \"\" \"\" \"\" ...\n $ X.gamma.linolenic.acid_100g               : chr  \"\" \"\" \"\" \"\" ...\n $ X.dihomo.gamma.linolenic.acid_100g        : chr  \"\" \"\" \"\" \"\" ...\n $ omega.9.fat_100g                          : logi  NA NA NA NA NA NA ...\n $ X.oleic.acid_100g                         : logi  NA NA NA NA NA NA ...\n $ X.elaidic.acid_100g                       : logi  NA NA NA NA NA NA ...\n $ X.gondoic.acid_100g                       : logi  NA NA NA NA NA NA ...\n $ X.mead.acid_100g                          : int  NA NA NA NA NA NA NA NA NA NA ...\n $ X.erucic.acid_100g                        : logi  NA NA NA NA NA NA ...\n $ X.nervonic.acid_100g                      : int  NA NA NA NA NA NA NA NA NA NA ...\n  [list output truncated]\n\nreadr\nread_tsv() 함수는 readr 패키지에서 TSV 파일을 읽을 수 있는 함수입니다.\n\n\nelapse_readr <- system.time(\n  foodfact2 <- read_tsv(file = \"data/FoodFacts.tsv\")\n)\n\nelapse_readr\n\n\n   user  system elapsed \n  4.846   0.267   4.186 \n\ndim(foodfact2)\n\n\n[1] 35000   163\n\nstr(foodfact2)\n\n\nspec_tbl_df [35,000 × 163] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ code                                      : chr [1:35000] \"0000000003087\" \"0000000004530\" \"0000000004559\" \"0000000016087\" ...\n $ url                                       : chr [1:35000] \"http://world-en.openfoodfacts.org/product/0000000003087/farine-de-ble-noir-ferme-t-y-r-nao\" \"http://world-en.openfoodfacts.org/product/0000000004530/banana-chips-sweetened-whole\" \"http://world-en.openfoodfacts.org/product/0000000004559/peanuts-torn-glasser\" \"http://world-en.openfoodfacts.org/product/0000000016087/organic-salted-nut-mix-grizzlies\" ...\n $ creator                                   : chr [1:35000] \"openfoodfacts-contributors\" \"usda-ndb-import\" \"usda-ndb-import\" \"usda-ndb-import\" ...\n $ created_t                                 : num [1:35000] 1.47e+09 1.49e+09 1.49e+09 1.49e+09 1.49e+09 ...\n $ created_datetime                          : POSIXct[1:35000], format: \"2016-09-17 09:17:46\" ...\n $ last_modified_t                           : num [1:35000] 1.47e+09 1.49e+09 1.49e+09 1.49e+09 1.49e+09 ...\n $ last_modified_datetime                    : POSIXct[1:35000], format: \"2016-09-17 09:18:13\" ...\n $ product_name                              : chr [1:35000] \"Farine de blé noir\" \"Banana Chips Sweetened (Whole)\" \"Peanuts\" \"Organic Salted Nut Mix\" ...\n $ generic_name                              : chr [1:35000] NA NA NA NA ...\n $ quantity                                  : chr [1:35000] \"1kg\" NA NA NA ...\n $ packaging                                 : chr [1:35000] NA NA NA NA ...\n $ packaging_tags                            : chr [1:35000] NA NA NA NA ...\n $ brands                                    : chr [1:35000] \"Ferme t'y R'nao\" NA \"Torn & Glasser\" \"Grizzlies\" ...\n $ brands_tags                               : chr [1:35000] \"ferme-t-y-r-nao\" NA \"torn-glasser\" \"grizzlies\" ...\n $ categories                                : chr [1:35000] NA NA NA NA ...\n $ categories_tags                           : chr [1:35000] NA NA NA NA ...\n $ categories_en                             : chr [1:35000] NA NA NA NA ...\n $ origins                                   : chr [1:35000] NA NA NA NA ...\n $ origins_tags                              : chr [1:35000] NA NA NA NA ...\n $ manufacturing_places                      : chr [1:35000] NA NA NA NA ...\n $ manufacturing_places_tags                 : chr [1:35000] NA NA NA NA ...\n $ labels                                    : chr [1:35000] NA NA NA NA ...\n $ labels_tags                               : chr [1:35000] NA NA NA NA ...\n $ labels_en                                 : chr [1:35000] NA NA NA NA ...\n $ emb_codes                                 : chr [1:35000] NA NA NA NA ...\n $ emb_codes_tags                            : chr [1:35000] NA NA NA NA ...\n $ first_packaging_code_geo                  : logi [1:35000] NA NA NA NA NA NA ...\n $ cities                                    : logi [1:35000] NA NA NA NA NA NA ...\n $ cities_tags                               : logi [1:35000] NA NA NA NA NA NA ...\n $ purchase_places                           : chr [1:35000] NA NA NA NA ...\n $ stores                                    : chr [1:35000] NA NA NA NA ...\n $ countries                                 : chr [1:35000] \"en:FR\" \"US\" \"US\" \"US\" ...\n $ countries_tags                            : chr [1:35000] \"en:france\" \"en:united-states\" \"en:united-states\" \"en:united-states\" ...\n $ countries_en                              : chr [1:35000] \"France\" \"United States\" \"United States\" \"United States\" ...\n $ ingredients_text                          : chr [1:35000] NA \"Bananas, vegetable oil (coconut oil, corn oil and/or palm oil) sugar, natural banana flavor.\" \"Peanuts, wheat flour, sugar, rice flour, tapioca starch, salt, leavening (ammonium bicarbonate, baking soda), s\"| __truncated__ \"Organic hazelnuts, organic cashews, organic walnuts almonds, organic sunflower oil, sea salt.\" ...\n $ allergens                                 : chr [1:35000] NA NA NA NA ...\n $ allergens_en                              : logi [1:35000] NA NA NA NA NA NA ...\n $ traces                                    : chr [1:35000] NA NA NA NA ...\n $ traces_tags                               : chr [1:35000] NA NA NA NA ...\n $ traces_en                                 : chr [1:35000] NA NA NA NA ...\n $ serving_size                              : chr [1:35000] NA \"28 g (1 ONZ)\" \"28 g (0.25 cup)\" \"28 g (0.25 cup)\" ...\n $ no_nutriments                             : logi [1:35000] NA NA NA NA NA NA ...\n $ additives_n                               : num [1:35000] NA 0 0 0 0 0 0 1 0 0 ...\n $ additives                                 : chr [1:35000] NA \"[ bananas -> en:bananas  ]  [ vegetable-oil -> en:vegetable-oil  ]  [ oil -> en:oil  ]  [ coconut-oil -> en:coc\"| __truncated__ \"[ peanuts -> en:peanuts  ]  [ wheat-flour -> en:wheat-flour  ]  [ flour -> en:flour  ]  [ sugar -> en:sugar  ] \"| __truncated__ \"[ organic-hazelnuts -> en:organic-hazelnuts  ]  [ hazelnuts -> en:hazelnuts  ]  [ organic-cashews -> en:organic\"| __truncated__ ...\n $ additives_tags                            : chr [1:35000] NA NA NA NA ...\n $ additives_en                              : chr [1:35000] NA NA NA NA ...\n $ ingredients_from_palm_oil_n               : num [1:35000] NA 0 0 0 0 0 0 0 0 0 ...\n $ ingredients_from_palm_oil                 : logi [1:35000] NA NA NA NA NA NA ...\n $ ingredients_from_palm_oil_tags            : chr [1:35000] NA NA NA NA ...\n $ ingredients_that_may_be_from_palm_oil_n   : num [1:35000] NA 0 0 0 0 0 0 0 0 0 ...\n $ ingredients_that_may_be_from_palm_oil     : logi [1:35000] NA NA NA NA NA NA ...\n $ ingredients_that_may_be_from_palm_oil_tags: chr [1:35000] NA NA NA NA ...\n $ nutrition_grade_uk                        : logi [1:35000] NA NA NA NA NA NA ...\n $ nutrition_grade_fr                        : chr [1:35000] NA \"d\" \"b\" \"d\" ...\n $ pnns_groups_1                             : chr [1:35000] NA NA NA NA ...\n $ pnns_groups_2                             : chr [1:35000] NA NA NA NA ...\n $ states                                    : chr [1:35000] \"en:to-be-completed, en:nutrition-facts-to-be-completed, en:ingredients-to-be-completed, en:expiration-date-to-b\"| __truncated__ \"en:to-be-completed, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed,\"| __truncated__ \"en:to-be-completed, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed,\"| __truncated__ \"en:to-be-completed, en:nutrition-facts-completed, en:ingredients-completed, en:expiration-date-to-be-completed,\"| __truncated__ ...\n $ states_tags                               : chr [1:35000] \"en:to-be-completed,en:nutrition-facts-to-be-completed,en:ingredients-to-be-completed,en:expiration-date-to-be-c\"| __truncated__ \"en:to-be-completed,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:\"| __truncated__ \"en:to-be-completed,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:\"| __truncated__ \"en:to-be-completed,en:nutrition-facts-completed,en:ingredients-completed,en:expiration-date-to-be-completed,en:\"| __truncated__ ...\n $ states_en                                 : chr [1:35000] \"To be completed,Nutrition facts to be completed,Ingredients to be completed,Expiration date to be completed,Cha\"| __truncated__ \"To be completed,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Packaging-code-\"| __truncated__ \"To be completed,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Packaging-code-\"| __truncated__ \"To be completed,Nutrition facts completed,Ingredients completed,Expiration date to be completed,Packaging-code-\"| __truncated__ ...\n $ main_category                             : chr [1:35000] NA NA NA NA ...\n $ main_category_en                          : chr [1:35000] NA NA NA NA ...\n $ image_url                                 : chr [1:35000] NA NA NA NA ...\n $ image_small_url                           : chr [1:35000] NA NA NA NA ...\n $ energy_100g                               : num [1:35000] NA 2243 1941 2540 1552 ...\n $ energy-from-fat_100g                      : num [1:35000] NA NA NA NA NA NA NA NA NA NA ...\n $ fat_100g                                  : num [1:35000] NA 28.57 17.86 57.14 1.43 ...\n $ saturated-fat_100g                        : num [1:35000] NA 28.57 0 5.36 NA ...\n $ -butyric-acid_100g                        : logi [1:35000] NA NA NA NA NA NA ...\n $ -caproic-acid_100g                        : logi [1:35000] NA NA NA NA NA NA ...\n $ -caprylic-acid_100g                       : logi [1:35000] NA NA NA NA NA NA ...\n $ -capric-acid_100g                         : logi [1:35000] NA NA NA NA NA NA ...\n $ -lauric-acid_100g                         : logi [1:35000] NA NA NA NA NA NA ...\n $ -myristic-acid_100g                       : logi [1:35000] NA NA NA NA NA NA ...\n $ -palmitic-acid_100g                       : logi [1:35000] NA NA NA NA NA NA ...\n $ -stearic-acid_100g                        : logi [1:35000] NA NA NA NA NA NA ...\n $ -arachidic-acid_100g                      : logi [1:35000] NA NA NA NA NA NA ...\n $ -behenic-acid_100g                        : logi [1:35000] NA NA NA NA NA NA ...\n $ -lignoceric-acid_100g                     : logi [1:35000] NA NA NA NA NA NA ...\n $ -cerotic-acid_100g                        : logi [1:35000] NA NA NA NA NA NA ...\n $ -montanic-acid_100g                       : logi [1:35000] NA NA NA NA NA NA ...\n $ -melissic-acid_100g                       : logi [1:35000] NA NA NA NA NA NA ...\n $ monounsaturated-fat_100g                  : num [1:35000] NA NA NA NA NA NA NA NA NA NA ...\n $ polyunsaturated-fat_100g                  : num [1:35000] NA NA NA NA NA NA NA NA NA NA ...\n $ omega-3-fat_100g                          : logi [1:35000] NA NA NA NA NA NA ...\n $ -alpha-linolenic-acid_100g                : logi [1:35000] NA NA NA NA NA NA ...\n $ -eicosapentaenoic-acid_100g               : logi [1:35000] NA NA NA NA NA NA ...\n $ -docosahexaenoic-acid_100g                : logi [1:35000] NA NA NA NA NA NA ...\n $ omega-6-fat_100g                          : logi [1:35000] NA NA NA NA NA NA ...\n $ -linoleic-acid_100g                       : logi [1:35000] NA NA NA NA NA NA ...\n $ -arachidonic-acid_100g                    : logi [1:35000] NA NA NA NA NA NA ...\n $ -gamma-linolenic-acid_100g                : logi [1:35000] NA NA NA NA NA NA ...\n $ -dihomo-gamma-linolenic-acid_100g         : logi [1:35000] NA NA NA NA NA NA ...\n $ omega-9-fat_100g                          : logi [1:35000] NA NA NA NA NA NA ...\n $ -oleic-acid_100g                          : logi [1:35000] NA NA NA NA NA NA ...\n $ -elaidic-acid_100g                        : logi [1:35000] NA NA NA NA NA NA ...\n $ -gondoic-acid_100g                        : logi [1:35000] NA NA NA NA NA NA ...\n $ -mead-acid_100g                           : logi [1:35000] NA NA NA NA NA NA ...\n $ -erucic-acid_100g                         : logi [1:35000] NA NA NA NA NA NA ...\n $ -nervonic-acid_100g                       : logi [1:35000] NA NA NA NA NA NA ...\n  [list output truncated]\n - attr(*, \"spec\")=\n  .. cols(\n  ..   code = col_character(),\n  ..   url = col_character(),\n  ..   creator = col_character(),\n  ..   created_t = col_double(),\n  ..   created_datetime = col_datetime(format = \"\"),\n  ..   last_modified_t = col_double(),\n  ..   last_modified_datetime = col_datetime(format = \"\"),\n  ..   product_name = col_character(),\n  ..   generic_name = col_character(),\n  ..   quantity = col_character(),\n  ..   packaging = col_character(),\n  ..   packaging_tags = col_character(),\n  ..   brands = col_character(),\n  ..   brands_tags = col_character(),\n  ..   categories = col_character(),\n  ..   categories_tags = col_character(),\n  ..   categories_en = col_character(),\n  ..   origins = col_character(),\n  ..   origins_tags = col_character(),\n  ..   manufacturing_places = col_character(),\n  ..   manufacturing_places_tags = col_character(),\n  ..   labels = col_character(),\n  ..   labels_tags = col_character(),\n  ..   labels_en = col_character(),\n  ..   emb_codes = col_character(),\n  ..   emb_codes_tags = col_character(),\n  ..   first_packaging_code_geo = col_logical(),\n  ..   cities = col_logical(),\n  ..   cities_tags = col_logical(),\n  ..   purchase_places = col_character(),\n  ..   stores = col_character(),\n  ..   countries = col_character(),\n  ..   countries_tags = col_character(),\n  ..   countries_en = col_character(),\n  ..   ingredients_text = col_character(),\n  ..   allergens = col_character(),\n  ..   allergens_en = col_logical(),\n  ..   traces = col_character(),\n  ..   traces_tags = col_character(),\n  ..   traces_en = col_character(),\n  ..   serving_size = col_character(),\n  ..   no_nutriments = col_logical(),\n  ..   additives_n = col_double(),\n  ..   additives = col_character(),\n  ..   additives_tags = col_character(),\n  ..   additives_en = col_character(),\n  ..   ingredients_from_palm_oil_n = col_double(),\n  ..   ingredients_from_palm_oil = col_logical(),\n  ..   ingredients_from_palm_oil_tags = col_character(),\n  ..   ingredients_that_may_be_from_palm_oil_n = col_double(),\n  ..   ingredients_that_may_be_from_palm_oil = col_logical(),\n  ..   ingredients_that_may_be_from_palm_oil_tags = col_character(),\n  ..   nutrition_grade_uk = col_logical(),\n  ..   nutrition_grade_fr = col_character(),\n  ..   pnns_groups_1 = col_character(),\n  ..   pnns_groups_2 = col_character(),\n  ..   states = col_character(),\n  ..   states_tags = col_character(),\n  ..   states_en = col_character(),\n  ..   main_category = col_character(),\n  ..   main_category_en = col_character(),\n  ..   image_url = col_character(),\n  ..   image_small_url = col_character(),\n  ..   energy_100g = col_double(),\n  ..   `energy-from-fat_100g` = col_double(),\n  ..   fat_100g = col_double(),\n  ..   `saturated-fat_100g` = col_double(),\n  ..   `-butyric-acid_100g` = col_logical(),\n  ..   `-caproic-acid_100g` = col_logical(),\n  ..   `-caprylic-acid_100g` = col_logical(),\n  ..   `-capric-acid_100g` = col_logical(),\n  ..   `-lauric-acid_100g` = col_logical(),\n  ..   `-myristic-acid_100g` = col_logical(),\n  ..   `-palmitic-acid_100g` = col_logical(),\n  ..   `-stearic-acid_100g` = col_logical(),\n  ..   `-arachidic-acid_100g` = col_logical(),\n  ..   `-behenic-acid_100g` = col_logical(),\n  ..   `-lignoceric-acid_100g` = col_logical(),\n  ..   `-cerotic-acid_100g` = col_logical(),\n  ..   `-montanic-acid_100g` = col_logical(),\n  ..   `-melissic-acid_100g` = col_logical(),\n  ..   `monounsaturated-fat_100g` = col_double(),\n  ..   `polyunsaturated-fat_100g` = col_double(),\n  ..   `omega-3-fat_100g` = col_logical(),\n  ..   `-alpha-linolenic-acid_100g` = col_logical(),\n  ..   `-eicosapentaenoic-acid_100g` = col_logical(),\n  ..   `-docosahexaenoic-acid_100g` = col_logical(),\n  ..   `omega-6-fat_100g` = col_logical(),\n  ..   `-linoleic-acid_100g` = col_logical(),\n  ..   `-arachidonic-acid_100g` = col_logical(),\n  ..   `-gamma-linolenic-acid_100g` = col_logical(),\n  ..   `-dihomo-gamma-linolenic-acid_100g` = col_logical(),\n  ..   `omega-9-fat_100g` = col_logical(),\n  ..   `-oleic-acid_100g` = col_logical(),\n  ..   `-elaidic-acid_100g` = col_logical(),\n  ..   `-gondoic-acid_100g` = col_logical(),\n  ..   `-mead-acid_100g` = col_logical(),\n  ..   `-erucic-acid_100g` = col_logical(),\n  ..   `-nervonic-acid_100g` = col_logical(),\n  ..   `trans-fat_100g` = col_double(),\n  ..   cholesterol_100g = col_double(),\n  ..   carbohydrates_100g = col_double(),\n  ..   sugars_100g = col_double(),\n  ..   `-sucrose_100g` = col_logical(),\n  ..   `-glucose_100g` = col_logical(),\n  ..   `-fructose_100g` = col_logical(),\n  ..   `-lactose_100g` = col_double(),\n  ..   `-maltose_100g` = col_logical(),\n  ..   `-maltodextrins_100g` = col_logical(),\n  ..   starch_100g = col_double(),\n  ..   polyols_100g = col_logical(),\n  ..   fiber_100g = col_double(),\n  ..   proteins_100g = col_double(),\n  ..   casein_100g = col_logical(),\n  ..   `serum-proteins_100g` = col_logical(),\n  ..   nucleotides_100g = col_logical(),\n  ..   salt_100g = col_double(),\n  ..   sodium_100g = col_double(),\n  ..   alcohol_100g = col_logical(),\n  ..   `vitamin-a_100g` = col_double(),\n  ..   `beta-carotene_100g` = col_logical(),\n  ..   `vitamin-d_100g` = col_double(),\n  ..   `vitamin-e_100g` = col_double(),\n  ..   `vitamin-k_100g` = col_double(),\n  ..   `vitamin-c_100g` = col_double(),\n  ..   `vitamin-b1_100g` = col_double(),\n  ..   `vitamin-b2_100g` = col_double(),\n  ..   `vitamin-pp_100g` = col_double(),\n  ..   `vitamin-b6_100g` = col_double(),\n  ..   `vitamin-b9_100g` = col_double(),\n  ..   folates_100g = col_double(),\n  ..   `vitamin-b12_100g` = col_double(),\n  ..   biotin_100g = col_logical(),\n  ..   `pantothenic-acid_100g` = col_double(),\n  ..   silica_100g = col_logical(),\n  ..   bicarbonate_100g = col_logical(),\n  ..   potassium_100g = col_double(),\n  ..   chloride_100g = col_logical(),\n  ..   calcium_100g = col_double(),\n  ..   phosphorus_100g = col_double(),\n  ..   iron_100g = col_double(),\n  ..   magnesium_100g = col_double(),\n  ..   zinc_100g = col_double(),\n  ..   copper_100g = col_double(),\n  ..   manganese_100g = col_double(),\n  ..   fluoride_100g = col_logical(),\n  ..   selenium_100g = col_double(),\n  ..   chromium_100g = col_logical(),\n  ..   molybdenum_100g = col_logical(),\n  ..   iodine_100g = col_logical(),\n  ..   caffeine_100g = col_logical(),\n  ..   taurine_100g = col_logical(),\n  ..   ph_100g = col_logical(),\n  ..   `fruits-vegetables-nuts_100g` = col_logical(),\n  ..   `fruits-vegetables-nuts-estimate_100g` = col_double(),\n  ..   `collagen-meat-protein-ratio_100g` = col_logical(),\n  ..   cocoa_100g = col_logical(),\n  ..   chlorophyl_100g = col_logical(),\n  ..   `carbon-footprint_100g` = col_logical(),\n  ..   `nutrition-score-fr_100g` = col_double(),\n  ..   `nutrition-score-uk_100g` = col_double(),\n  ..   `glycemic-index_100g` = col_logical(),\n  ..   `water-hardness_100g` = col_logical()\n  .. )\n - attr(*, \"problems\")=<externalptr> \n\n\n기존 방법은 2.663초, readr 패키지는 4.186초가 소요되었습니다. 데이터의 용량이 그리 크지 않기 때문에 기존 방법의 수행속도가 다소 빠를 수 있지만, 대용량 데이터의 경우에는 readr 패키지의 속도가 빠릅니다.\n그리고 35,000건의 데이터 중에서 기존의 방법은 23,179만 읽었을 뿐입니다.\n\n\n\n\n솔루션\n\n파일의 용량이 클 경우에는 readr 패키지를 이용해서 데이터를 읽는 것이 시간을 절약할 수 있는 유용한 방법입니다.\n\n\n\n\n\n",
    "preview": "posts/2022-02-23-readr/img/readr.png",
    "last_modified": "2022-03-01T09:38:53+09:00",
    "input_file": {},
    "preview_width": 736,
    "preview_height": 853
  },
  {
    "path": "posts/2022-01-27-donate/",
    "title": "R 재단에 기부하기",
    "description": "우리가 사항하는 즐겨 사용하는 R을 위해서 R 재단에 기부하는 방법을 살펴봅니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-01-27",
    "categories": [
      "R-Foundation"
    ],
    "contents": "\n\nContents\n이크 메일을 놓쳤습니다.\n늦었지만 결제했습니다.\n기부를 하면요\n이렇게 기부해보세요\n앞으로는요\n\n이크 메일을 놓쳤습니다.\n개인적으로 사용하는 메일 주소가 3개 있습니다. 물론 사용에 있어 주부가 있습니다. 대체적으로 중요한 것들은 gmail을 사용합니다만, 가끔을 놓치는 메일이 발생합니다.\n작년 12월에 DonorBox에서 온 한통의 메일을 스팸으로 오인하고 휴지통에 버렸나봅니다. DonorBox는 개인 및 비영리 단체가 인터넷을 통해 기부금을 받을 수 있는 온라인 모금 소프트웨어를 제공하는 사이트입니다.\n12월말에 R 재단으로의 25유로 기부가 실패했다는 메시지가 들어 있었습니다. 아주 작지만 2020년부터 년간 기부하는 기부금 카드 결제가 실패한 것입니다.\n카드 유효기간이 도래해서 카드를 갱신했었는데 왠만한 결제 시스템의 카드정보를 변경했지만, 이 사이트의 결제정보는 변경하지 못했나 봅니다.\n늦었지만 결제했습니다.\n메일의 링크를 타고 들어간 후 카드정보를 변경하고 바로 결제하였습니다.\n일회성이 아니라 연간 기부하는 플랜입니다. 다음과 같은 정보를 확인할 수 있습니다. 재작년 크리스마스 이후부터 기부했었군요.\n\n\n\nFigure 1: 연간 기부 플랜 정보\n\n\n\n연체의 개념은 없지만 뒤늦게 기부했어도 R 재단으로부터 다음과 같은 영수증 메일을 받을 수 있었습니다.\n\n\n\nFigure 2: 연간 기부 플랜 정보\n\n\n\n기부를 하면요\nR Foundation Supporting Members & Donors 페이지의 Supporting Members 섹션에 간단하게 소개됩니다.\n\n\n\nFigure 3: 기부자 명단\n\n\n\n이렇게 기부해보세요\nR 재단 홈페이지{target=\"_blank}를 방문하여 다음 그림처럼 Donate 메뉴를 누릅니다. 그러면 기부할 수 있는 페이지로 이동합니다.\n\n\n\nFigure 4: r-project 홈페이지\n\n\n\nR 재단에 기부하는 방법은 두 가지가 있습니다.\n연간 25유로 기부로 R 재단의 후원 회원 되는 방법\n50 유로 이상부터 시작하는 일회성 및 정기 기부\n다음 그림을 선택하고 연간 기부 동참에 참여합니다. 메뉴를 보면,\n개인기부는 25 유로, 기관 기부는 250유로, 후원자는 500유로로 책정되어 있습니다.\n\n\n\nFigure 5: 연간 개인 기부\n\n\n\n저는 첫번째 방법을 사용했습니다. 25 유로면 한화로 3만5천원이 채 되지 않는 금액입니다.\n앞으로는요\n개인적으로 R을 사용하면서 얻은 이득이, 회사에서 R을 사용하면 얻은 효과를 생각하면 미미합니다. 겸연적기 까지 합니다. 이제는 카드 정보가 바뀌어서 기부금 납입에 연체가 발생하지 않도록 하겠습니다. 앞으로 기회가 되면 후원자가 되려 합니다. 그런 날을 꿈꿔봅니다.\n\n\n\n",
    "preview": "posts/2022-01-27-donate/img/invoice.png",
    "last_modified": "2022-01-27T21:46:02+09:00",
    "input_file": {},
    "preview_width": 2724,
    "preview_height": 1544
  },
  {
    "path": "posts/2022-01-23-serverside/",
    "title": "Shiny 애플리케이션 서버에서 구동하기",
    "description": "로컬 호스트에서 구동되는 Shiny 애플리케이션을 Remote 서버에서 구현할 때의 이슈와 대안을 살펴봅니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2022-01-23",
    "categories": [
      "Shiny"
    ],
    "contents": "\n\nContents\nstand-alone 그리고 server\nBitStat\n어떤 문제인가요?\npagedown 패키지로 PDF 파일 생성 이슈\n문서의 브라우저 팝업 이슈\n\npagedown 패키지 이슈 해결\n크롬 설치\n크롬을 위한 설정\n\n문서의 브라우저 팝업 이슈 해결\n이제는 server에서\n\nstand-alone 그리고 server\nShiny 애플리케이션을 제법 만들수 있게 되었습니다.\n누구나 그렇듯, RStudio에서 애플리케이션 개발 작업을 수행합니다. 그리고 간단하게 애플리케이션 실행 아이콘을 눌러 실행합니다. 애플리케이션이 실행되면 이제부터는 RStudio 콘솔은 사용할 수 없게 됩니다. 애플리케이션을 종료할 때까지 말입니다. 가끔은 이것이 불편할 수 있습니다. 그러나 RStudio에서 쉽게 개발하는 대신에 이러한 불편함을 인내해야 합니다.\n이런 방법은 RStudio에서 stand-alone(스탠드 얼론) 방식으로 가볍게 애플리케이션의 기능을 검증하고, 또 수정하고 실행하고 검증을 합니다.\n어느덧, 개발한 Shiny 애플리케이션을 server에 배포하는 경지(?)에 도달했습니다. 대중을 위한 서비스 오픈은 아니고, 개인 노트북에 데모 환경을 구축하게 되었습니다. docker에 Shiny server를 구축하고 애플리케이션을 포팅하니 기존에 개발했던 코드에서 몇몇 이슈가 발생합니다. stand-alone에서는 정상적으로 구현되는 기능에 오류가 발생하기 시작한 것입니다.\nstand-alone와 server는 시스템 환경이 다를 수 있으므로, Shiny server로 애플리케이션을 배포할 경우에는 반드시 server 환경에서도 테스트를 수행해야 합니다.\nBitStat\nBitStat라는 Shiny 애플리케이션을 개발하고 있습니다. 현재는 R 패키지로 배포되고 있습니다만, 언젠가는 shinyapps.io나 binder 등에 서비스를 배포할 것입니다.\n이것은 이 애플리케이션을 stand-alone에서 테스트하고 있다는 것을 의미하기도 합니다.\n며칠 전부터 노트북의 docker 환경에 shiny server를 구축하고, BitStat를 비롯하여 몇몇 애플리케이션을 포팅하고 있습니다. 그러던 중에 stand-alone 환경에서 정상적으로 수행되던 기능에 오류가 발생하는 사례가 나오기 시작했습니다.\n이번 글을 이런 사례에 대한 Troubleshooting에 대한 내용입니다.\n어떤 문제인가요?\n다음과 같은 두가지 문제가 발생했습니다.\npagedown 패키지로 PDF 파일 생성 시 이슈\n동적 생성한 HTML 문서나 PDF 문서의 브라우저 팝업\npagedown 패키지로 PDF 파일 생성 이슈\npagedown 패키지는 크롬 브라우저를 이용해서 웹 문서를 PDF 파일로 변환합니다. Mac OSX에서 stand-alone로 사용할 경우에는 Mac OSX에 설치된 크롬 브라우저를 사용하는데, docker 컨테이너의 Linux server에는 크롬 브라우저가 설치되어 있지 않습니다.\n문서의 브라우저 팝업 이슈\nShiny에서 정적 페이지를 브라우징할 때에는 utils::browseURL()를 사용합니다. 이것은 stand-alone에서는 웹브라우저에서 정상적으로 동작하지만, docker 컨테이너의 Linux server에서는 동작하지 않습니다.\n엄밀히 말하자면, 웹 서비스도 client-server 방식으로 동작합니다. stand-alone에서는 사용자 환경이 server 환경이자 client 환경이기 때문에 utils::browseURL()는 사용자의 기본 웹 브라우저에서 정적 페이지를 브라우징합니다. 그러나 cli 기반의 Linux server에서는 브라우징이 어렵겠지요.\nX 터미널로 접속한다면, Unix-alikes server에서는 xdg-open 유틸리티가 브라우저를 띄우겠지만, 이것은 웹 서비스 환경이 아닙니다. 웹 서비스 환경이라면 사용자의 컴퓨터(client)에서 브라우징되어야만 합니다. 이것은 결국 Javascript를 사용해야 한다는 의미입니다.\npagedown 패키지 이슈 해결\n크롬 설치\n다음처럼 curl을 이용해서 크롬을 다운로드한 후 설치하고 삭제합니다. 이를 위해서 앞에서 apt-get로 curl을 설치한 것입니다.\n\nRUN apt-get update && apt-get install -y \\\n    curl\n\nRUN curl -L http://bit.ly/google-chrome-stable -o google-chrome-stable.deb && \\\n    apt-get -y install ./google-chrome-stable.deb && \\\n    rm google-chrome-stable.deb\n\n크롬을 위한 설정\npagedown 패키지가 크롬을 사용한다는 것을 앞에서 언급했습니다. 그런데 pagedown::chrome_print()가 크롬을 호출할 때, 다음처럼 “Error in is_remote_protocol_ok: Cannot find headless Chrome after 20 attempts”라는 에러가 발생합니다.\n\noutput file: diagnosis_paged_temp.knit.md\n\n\nOutput created: diagnosis_paged_temp.html\nWarning: Error in is_remote_protocol_ok: Cannot find headless Chrome after 20 attempts\n  1: runApp\n\nExecution halted\n\n크롬은 ‘샌드박스(sandbox)’라는 보안 개념을 적용합니다. 크롬은 브라우저에서 여러 개의 독립된 탭을 띄우고, 별도의 웹 페이지가 실행됩니다. 별도로 독립된 프로세스가 가동하는 것으로 이들 프로세스는 샌드박스처럼 격리되어서 서로 관여하지 못합니다. 즉, 어느 탭의 웹 페이지에서 악성코드가 침투하거나, 버그 또는 장애로 해당 탭의 페이지가 먹통되어도 다른 탭의 웹 페이지는 정상적으로 동작합니다.\n앞에서의 에러는 크롬의 샌드박스 기능에 기인합니다. 이 에러는 해결하기 위해서는 컨테이너에서 pagedown::chrome_print()가 크롬을 호출할 때, 샌드박스의 기능을 비활성해야 합니다.\n이를 위한 몇 가지 솔루션이 있습니다.\n\npagedown::chrome_print() 함수 호출 수정\nextra_args 인수값에 c(“–no-sandbox”)를 기술하여 호출\ndlookr 패키지를 수정해야 함\n크롬 실행 시 옵션 정의\n–no-sandbox 옵션 추가\n\n여기서는 2번 솔루션을 적용합니다.\n다음과 같은 스크립트를 담은 google-chrome 파일을 ./shiny-docker 경로에 생성합니다.\n\n#!/bin/bash\n\n/usr/bin/google-chrome --no-sandbox $*\n\n그리고 호스트에서 이 파일의 권한을 설정합니다. shiny 계정으로 컨테이너를 실행하기 때문에 755 권한을 부여해야 합니다.\n\nchmod 755 google-chrome\n\n다음으로 다음과 같은 스크립트를 담은 Renviron 파일을 ./shiny-docker 경로에 생성합니다.\n\n\nPATH=\"/:${PATH}\"\n\n\n\n그리고 Dockerfile 파일에서 이들을 다음처럼 컨테이너에 복사합니다.\n\nCOPY google-chrome /usr/local/bin/\nCOPY Renviron /.Renviron\n\n문서의 브라우저 팝업 이슈 해결\n기존 로직은 이렇게 간단합니다.\n\n\nbrowseURL(paste(\".\", output_file, sep = \"/\"))\n\n\n\nutils::browseURL()을 사용하는 로직을 다음처럼 Javascript로 브라우저 창을 팝업하는 로직으로 변경합니다.\n먼저 Javascript 코드로 함수를 작성합니다.\n\n\n# define js function for opening urls in new tab/window\njs_code <- \"shinyjs.browseURL = function(url) {\n              window.open(url, '_blank');\n           }\"\n\n\n\n그리고, 작성한 Javascript 함수를 사용할 수 있게 Shiny ui 영역에 설정합니다.\n\nuseShinyjs(),\nextendShinyjs(text = js_code, functions = 'browseURL')\n\n마지막으로 Shiny server 영역에서 해당 Javascript 함수를 호출합니다.\n\n\n# Change for server side excute\nfile.copy(paste(tempdir(), output_file, sep = \"/\"),\n          paste(\"www\", output_file, sep = \"/\"),\n          overwrite = TRUE)\nURL <- paste(\".\", output_file, sep = \"/\")  \n\njs$browseURL(URL)\n\n\n\n이제는 server에서\n앞으로 Shiny 애플리케이션을 개발할 때에는, stand-alone이 아닌 Shiny server에서 동작한다는 전제하에 로직을 구현해야할 것 같습니다. 이것이 RStudio의 개발 환경 후 Shiny server의 운영 환경으로 배포할 때 문제가 발생하는 것을 사전에 방지해주니까요.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-01-26T23:08:37+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-11-24-rpackage/",
    "title": "R 패키지 유지보수하기",
    "description": "CRAN에 올라간 패키지를 유지보수하는 에피소드를 소개합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2021-11-24",
    "categories": [
      "R Packages"
    ],
    "contents": "\n\nContents\nR 패키지 단상\nCRAN R 패키지 현황\nMASS 패키지를 아시나요?\n한통의 이메일\n메일 수령의 원인\n\n패키지 체크\n패키지 체크 항목\n\n또한 통의 메일\n반가운 메일\n감사의 인사\n\nR 패키지 단상\n생명입니다. 그리고 생태계입니다.\n“오늘도 어떤 새 생명이 탄생하고, 어떤 생명은 죽음을 맞이합니다. 또 어떤 것은 강한 생명력으로 번성하여 주변의 생명을 위협하기도 합니다.”\n저는 R 패키지도 하나의 생명체라 생각합니다.\n“오늘도 어떤 새 패키지가 등록되고, 어떤 패키지는 기준을 만족치 못해 퇴출됩니다. 어떤 패지키는 버전업되어 다시 올라오고, 또 어떤 패키지는 R 사용자의 인기가 높아서 오늘도 많은 수가 다운로드됩니다. ggplot2 패키지가 lattice 패키지를 도태시켰으며, R 사용자의 입소문에 어떤 패키지는 다운로드 횟수가 증가합니다. 오늘도 CRAN 리파지토리에는 보이지 않는 생명체들이 아우성에 뜨거운 하루를 보내고 있습니다.”\nprivate 패키지를 만들어 사용하다가 공개용 패키지인 dlookr을 만들어서 CRAN에 등록한 지 3년이 넘었습니다. 그러나 아직도 패키지를 업데이트한 후 CRAN에 제출할 때마다 조마조마합니다. 여러 기능을 추가하다보니 패키지를 체크하여 무결성을 검증하는 과정에서 문제가 발생하는 일이 종종 있기 때문입니다.\n이번에는 패키지를 만들어서 CRAN에 등록하고 유지보수하는 과정의 에피소드를 소개하려 합니다.\nCRAN R 패키지 현황\nhttps://cloud.r-project.org/web/packages/available_packages_by_name.html 페이지에는 설치 가능한 패키지들의 목록이 정리되어 있습니다.\n\n\n\n시시때때로 새 패키지가 올라오고, 또 어떤 패키지는 퇴출되기에 시점마다 다르지만, 지금 이 글을 쓸 시점은 18423개의 패키지가 등록되어 있습니다.\n\n\nrequire(tidyverse)\n\navailable.packages(repos = \"https://cran.rstudio.com/\") %>% \n  nrow()\n\n\n[1] 18423\n\nCRAN 패키지는 세계 각지에 미러링 사이트를 두어서 패키지 설치의 로드를 분산하고 있습니다.\n그러나 당신이 RStudio를 사용한다면, 당신은 RStudio에서 제공하는 미러링 사이트인 https://cran.rstudio.com/로부터 패키지를 설치할 것입니다.\n\n\noptions(\"repos\")\n\n\n\n$repos\n                       CRAN \n\"https://cran.rstudio.com/\" \nattr(,\"RStudio\")\n[1] TRUE\n우리나라에서 운용중인 미러링 사이트도 이제는 네개입니다. 예전에는 국내에 할당된 미러링 사이트의 쿼더가 3개였던 것으로 기억합니다만.\n미러링 사이트는 원본 리파지토리와 싱크를 맞추는 시점에 따라 패키지의 개수가 다를 수도 있습니다. 그리고 소소한 이슈로 미미하게 차이가 발생하기도 합니다.\n국내 미러링 서버의 패키지 개수를 비교해보겠습니다.\n\n\nrepos_url <- c(\"https://ftp.harukasan.org/CRAN/\",\n               \"https://cran.yu.ac.kr/\",\n               \"https://cran.seoul.go.kr/\",\n               \"https://cran.biodisk.org/\")\n\npkgs <- repos_url %>% \n  purrr::map_int(function(x) {\n  available.packages(repos = x) %>% \n  nrow()\n})\n\nnames(pkgs) <- repos_url\n\npkgs\n\n\nhttps://ftp.harukasan.org/CRAN/          https://cran.yu.ac.kr/ \n                          18423                           18423 \n      https://cran.seoul.go.kr/       https://cran.biodisk.org/ \n                          18420                           18423 \n\nMASS 패키지를 아시나요?\nMASS는 “Modern Applied Statistics with S”1라는 책의 데이터와 분석 코드를 패키지화한 것입니다.\n\n\ncitation(\"MASS\")\n\n\n\nTo cite the MASS package in publications use:\n\n  Venables, W. N. & Ripley, B. D. (2002) Modern Applied\n  Statistics with S. Fourth Edition. Springer, New York. ISBN\n  0-387-95457-0\n\nA BibTeX entry for LaTeX users is\n\n  @Book{,\n    title = {Modern Applied Statistics with S},\n    author = {W. N. Venables and B. D. Ripley},\n    publisher = {Springer},\n    edition = {Fourth},\n    address = {New York},\n    year = {2002},\n    note = {ISBN 0-387-95457-0},\n    url = {http://www.stats.ox.ac.uk/pub/MASS4},\n  }\n\n“Modern Applied Statistics with S”는 R(S-PLUS) 세계에서는 바이블과 같은 서적입니다. 현재는 Fourth Edition인데, 저는 이 책의 Third Edition을 2001년 아마존에서 구입해서 국내로 배송받은 기억이 있습니다. 그 당시의 아마존은 도서와 CD등의 미디어 판매 사이트 수준이었는데, 격세지감을 느낍니다.\n“Modern Applied Statistics with S”의 공저자인 브라이언 리플리(B. D. Ripley)는 R재단의 R 코어팀 멤버입니다. 옥스퍼드 대학교의 통계학과 교수였습니다. 아주 유명한 분이죠. R(S-PLUS) 세계에서의 바이블로 꼽히는 “S Programming”도 MASS처럼 W. N. Venables와 공저했습니다. 그리고 RODBC 패키지의 개발자이자 유지보수 관리자입니다.\n\n\n\nFigure 1: R Foundation의 R Core팀 멤버\n\n\n\n한통의 이메일\n11월 17일 브라이언 리플리에게 한통의 메일을 받았습니다. 자동 메일일수도 있으나, 이 메일은 11월 30일까지 “당신의 패키지에 문제가 있으니, 11월 30 이전에 수정해라. 수정하지 않으면 CRAN에서 퇴출하겠다.”라는 의미입니다. 최후통첩인 셈입니다. 예전에 이런 메일을 처음 받고, 가볍게 생각했다가 dlookr이 CRAN에서 퇴출되었던 경험이 있습니다.\nR 패키지를 CRAN에 등록하고 유지보수하는 입장에서는 브라이언 리플리의 메일은 달갑지 않습니다만, 이것도 누군가 해야할 롤이니 감내해야할 부분입니다.\n\n\n\nFigure 2: 브라이언 리플리의 메일\n\n\n\n2001년도 RODBC2가 버전업되면서, 이전 버전에서 잘 작동하던 한글이 제대로 표현되지 않은 적이 있습니다. 의외로 MASS의 저자인 브라이언 리플리가 RODBC의 개발자이기에 문제의 해결을 요청하는 메일을 작성했다가 된통 야단을 맞은 적이 있습니다. 메일 박스를 찾아보았는데, 해당 메일은 없더군요. 느낌은 아직도 기억합니다. 시크하기로 유명한 분입니다.\n메일 수령의 원인\ndlookr은 데이터를 시각화할 때, R에서 제공하는 기본 폰트외의 추가폰트(https://choonghyunryu.github.io/posts/2021-01-27-rfonts/ 참고)를 사용합니다. 그런데 이 기능을 위해서 extrafont 패키지를 이용합니다.\nextrafont 패키지는 TTF 폰트를 핸들링하는 ‘ttf2pt1’라는 유틸리티 프로그램을 연동하는 기능을 수행합니다만, ttf2pt1의 라이센스 정책상 별도의 패키지인 Rttf2pt1 패키지에 담아서 배포하고 있습니다. 즉, extrafont 패키지는 Rttf2pt1 패키지에 담아서 배포하는 ttf2pt1 유틸리티를 실행해야합니다.\n그런데 ttf2pt1는 2003년도 이후부터 유지관리가 되지 않는 유틸리티입니다. 그래서 extrafont과 Rttf2pt1 패키지의 저자인 Winston Chang도 최신 컴파일러에서 Rttf2pt1를 컴파일하기 어려워지고 있어 CRAN에 유지관리가 어려우니 다른 대안을 찾으라고 권고하기도 했습니다.\n최근 애플이 인텔의 CPU 대신 자사에서 개발한 ARM 기반의 CPU를 탑재한 노트북을 판매하면서, CRAN에서는 MacOS ARM64용 패키지를 배포합니다. 그런데 dlookr이 이 환경에서 패키지를 빌드할 때, ttf2pt1 유틸리티 호출때 에러가 발생한 것입니다.\n\n\n\nFigure 3: ttf2pt1 유틸리티 호출때 발생하는 에러\n\n\n\n2021년 9월에 dlookr 0.5.1을 등록할 때는 발생하지 않던 오류였는데, 정기적인 헬스 체크에 신규로 빌드하는 MacOS ARM64에서 오류가 발생하였던 것입니다.\n패키지 체크\nCRAN의 개별 패키지 페이지에는 ‘CRAN checks’라는 섹션이 있습니다. 이 섹션의 정보가 앞서 예시한 삽화입니다. 그리고 서머리 테이블만 발췌하면 다음과 같습니다. 이 화면은 기존 0.5.1 버전의 MacOS ARM64용 빌드 오류를 패치한 0.5.2버전이 등록되는 과정에서의 체크 정보입니다.\n\n\n\nFigure 4: CRAN checks 서머리 테이블\n\n\n\n패키지 등록 요청이 받아지면, CRAN에서는 자동으로 수일에 거쳐 15개 환경에서의 패키지 바이너리가 빌드됩니다. 이 과정에서 패키지의 무결성을 검증하는 여러 체크항목이 수행되는데, 그 결과에 따라서 Status에 다음과 같은 상태값이 부여됩니다.\nOK\n모든 체크 항목을 정상적으로 통과\n\nNOTE\n체크항목 중에서 심각하지 않은 이슈 발견\nCRAN 등록 유지\n\nWARN\n패키지 빌드 과정에서 오류가 발생하거나, 심각한 이슈 발견\n일정 기간 경화 후 CRAN에서 퇴출\n\n패키지 체크 항목\nCRAN에 패키지를 등록하기 전에 패키지 개발자는 정합성 체크를 수행해야 합니다.\n다음은 패키지를 빌드한 후, 정합성을 체크하는 명령어입니다. 쉘(명령행)에서 수행하는 명령어로 dlookr 0.5.2 버전을 예로 들었습니다.\n\nR CMD build dlookr\nR CMD check --as-cran dlookr_0.5.2.tar.gz\n\nCRAN에서 패키지의 정합성을 체크하는 체크 룰을 갈수록 엄격해집니다. 이번에 패치버전을 체크할 때에는 새로운 체크 룰이 생겨서, 기존에는 정상으로 간주하던 것에서 경로가 발생하기도 했습니다.\n다음은 많은 패키지들에서 발생하는 NOTE 사례입니다. CRAN은 R 패키지 배포본의 파일 크기를 5MB 이내로 제한합니다. 만약 배포본의 크기가 5MB보다 클 경우에 나타나는 NOTE 메시지입니다.\n\n\n\nFigure 5: 솔라리스용 바이너리의 NOTE 메시지\n\n\n\n사실 패키지 개발자는 15개 환경에서 체크하기에는 무리입니다. 해당 운영체제 환경을 모두 보유할 개발자는 아무도 없을 것입니다. 다행히 R-hub(https://builder.r-hub.io/)는 CRAN에서 배포하는 cross-patform 환경에서 패키지를 빌드하고 체크를 할 수 있는 서비스를 지원합니다. 그리고 rhub 패키지는 해당 서비스와 연동해서 여러 플랫폼에서 패키지를 빌드하고 체크할 수 있도록 도와줍니다.\n그러나 저는 여러번 시도해보았지만 해당 서비스의 가상 host에서 dlookr을 빌드할 수 있는 환경을 제대로 만들어 주지 못했습니다. 그래서 저는 맥북프로의 MacOS에서 패키지를 개발하고, parallels로 가상의 MS-Windows와 docker의 가상 CentOS Linux에서 이상이 없는지 체크하고, 이상없을 경우에 CRAN에 제출합니다.\nVignettes 등의 도움말은 PDF 파일로도 생성되는데, 솔라리스 운영체제에서는 이 PDF 파일 크기가 다른 운영체제와는 달리 필요 이상으로 크게 만들어집니다. 저도 이런 이슈로 dlookr의 첨부 이미지 파일을 줄이거나 해상도를 낮춰 Vignettes 등의 도움말 파일의 용량을 줄였습니다만, 이번 0.5.2 버전에 나눔스퀘어 폰트를 포함하면서 솔라리스 운영체제에서 파일 용량이 5MB를 초과했나봅니다. 0.5.3 버전에서 용량을 줄여야겠습니다.\nCRAN의 체크 항목은 패키지에서 참조(Depends 및 Import)하는 패키지의 개수도 20개 이내로 제한합니다. dlookr은 이미 20개의 타 패키지를 사용합니다. 그래서 추가하고 싶은 기능이 있으나, 패키지의 개수를 초과해서 보류하고 있습니다. 이것은 중요한 체크 항목입니다. 참조하는 패키지가 많은수록 패키지의 안정성은 떨어질 수 밖에 없습니다. 만약 어떤 패키지가 퇴출된다면, 그 패키지를 사용하는 다른 패키지도 퇴출되는 운영에 처합니다. 그래서 이 경우에는 Reverse imports 패키지의 유지 운영 담당자에게도 사전에 메일이 발송됩니다.\n“당신의 B 패키지가 A라는 패키지를 사용하고 있는데, 이 패키지는 YYYY-MM-DD에 CRAN에서 퇴출되니, 그 전에 대안을 찾아 너의 패키지를 수정해라.” 이런 내용의 메일입니다. 저도 한번 받아보았는데, Reverse imports 패키지 개발자 중의 한 사람이 해당 패키지 개발자에게 문제제기를 하더라구요. 그리고 그 개발자가 패치를 만들어서 문제가 해결되었습니다.\n그런데 문제가 완전히 해결되지 않았던 것입니다. 그 패키지가 extrafont였습니다. github의 해당 이슈에서 extrafont 개발자인 “Winston Chang”은 일단 불을 껐지만 안정적으로 유지보수할 수 없음을 이야기하면서 showtext 패키지를 추천했습니다. 그래서 0.5.2 버전에서는 extrafont 패키지를 제거하고 showtext 패키지를 추가했습니다. 그런데 아쉽게도 sysfonts 패키지까지 덤으로 추가해야 해서 20개 참조 패키지 수를 다 채워버렸습니다.\nReverse imports 패키지는 해당 패키지를 사용하는 다른 패키지들을 의미합니다. ggplot2나 dplyr 패키지의 Reverse imports 패키지의 수는 어마하게 많습니다. dlookr도 그 중 하나입니다. 이처럼 CRAN 패키지를 제출한 패키지 개발자는 사명감을 갖고 유지보수해야 합니다.\n또한 통의 메일\n올초 dlookr 패키지의 개발 시 어처구니없는 실수를 한 적이 있습니다. 시각화 매커니즘을 대대적으로 수정하면서, R의 기본 폰트가 아닌 외부 폰트를 사용하도록 로직을 수정했습니다.\nR에는 몇개의 특수한 목적의 미리 정의된 이름의 함수가 있습니다. .onAttach() 함수는 패키지가 어태치되면서 자동으로 수행하라는 의미입니다. 쉽게 풀면, library(dlookr)을 수행하면 자동으로 로직이 실행되는 함수입니다. 이 함수에 사용자의 운용체제의 폰트 경로에서 특정 TTF 폰트를 R에 로드하는 로직을 넣었는데, 오타로 인해서 제가 테스트한 환경이 아닌 몇몇 플랫폼에서 에러가 발생한 것입니다. dlookr 특정 함수의 오류가 아니라, 아예 패키지가 로드되지 못하게 된 것입다.\n역시나 브라이언 리플리에게 다음과 같은 메일이 왔습니다.\n\n\n\nFigure 6: 브라이언 리플리의 메일\n\n\n\n그런데 그는 오타를 넘어 운영체제에서 폰트 경로를 찾는 다음 로직에 대해서도 문제를 제기했습니다.\n\n\nget_ttf_paths <- function() {\n  if (grepl(\"^darwin\", R.version$os)) {\n    paths <- c(\"/Library/Fonts/\", \"/System/Library/Fonts\", \n               \"~/Library/Fonts/\")\n    return(paths[file.exists(paths)])\n  }\n  else if (grepl(\"^linux-gnu\", R.version$os)) {\n    paths <- c(\"/usr/share/fonts/\", \"/usr/X11R6/lib/X11/fonts/TrueType/\", \n               \"~/.fonts/\")\n    return(paths[file.exists(paths)])\n  }\n  else if (grepl(\"^freebsd\", R.version$os)) {\n    paths <- c(\"/usr/local/share/fonts/truetype/\", \"/usr/local/lib/X11/fonts/\", \n               \"~/.fonts/\")\n    return(paths[file.exists(paths)])\n  }\n  else if (grepl(\"^mingw\", R.version$os)) {\n    return(paste(Sys.getenv(\"SystemRoot\"), \"\\\\Fonts\", sep = \"\"))\n  }\n  else if (grepl(\"^SunOS\", R.version$os)) {\n    paths <- c(\"/usr/share/fonts/\", \"/usr/X11/lib/X11/fonts/TrueType/\", \n               \"~/.fonts/\")\n  }        \n  else {\n    #msg <- \"Don't know where to look for truetype fonts.\"\n    #packageStartupMessage(msg)\n    return(character(0))\n  }\n}  \n\n\n\n이 코드에서 darwin은 MacOS, mingw은 MS-WIndows, SunOS는 솔라리스 운영체제를 의미합니다. 그리고 Linux와 FreeBSD 까지 체크한, 나름 최선의 로직이라 생각했는데 그의 관점에서는 성에 차지 않았나봅니다.\n아무튼 여전히 시크한 메일이지만, 그의 철학에 얼굴이 화끈거리는 창피함에 숙연해졌습니다.\n시크하지만, 미워할 수 없는 브라이언 리플리입니다.\n반가운 메일\n브라이언 리플리의 메일은 대부분 경고성 메일인 반면, 우베 리게스(Uwe Ligges)의 메일은 긍정적인 측면의 메일입니다. 우베 리게스는 독일 도르트문트 대학교의 통계학 교수입니다. 이분도 R Core팀 멤버이지요.\n\n\n\nFigure 7: R Foundation의 R Core팀 멤버\n\n\n\n우베 리게스의 메일도 다음처럼 대부분 자동 발송메일입니다.\n\n\n\nFigure 8: 우베 리게스의 메일\n\n\n\n제출된 패키지가 CRAN의 체크를 패스해서, CRAN 리파지토리에 등록된다는 기분 좋은 메일입니다. 이 메일을 받으면, 이미 패키지 소스는 CRAN에 등록됩니다. 그리고 순차적으로 며칠동안 15개의 환경의 바이너리가 만들어지게 됩니다.\n다음 메일은 MS-Windows에서 정상적으로 빌드되었다는 메시지입니다.\n\n\n\nFigure 9: 우베 리게스의 메일\n\n\n\n그런데 우베 리게스의 메일은 수동 메일도 있습니다. 정상적으로 등록된 패키지가 내부 체크에서 오류가 발생하였을 때, 질문하면 친절하게 솔루션을 안내해 주기도 합니다. 그리고 지난 버전의 오류를 해결했는지 본인과 패키지 관리 담당자에게 설명하라고도 합니다.\n감사의 인사\n브라인언 리플리나 우베 리게스는 그 많은 CRAN의 패키지의 안정적인 운영을 위해서 기꺼이 자신의 시간을 할애했을 것입니다. 이들 뿐만 아니라, CRAN Core팀 및 전 세계의 R 패키지 개발자들은 이 풍성한 R 생태계를 위해서 지금도 R 스크립트를 만지고 있거나, 누군가의 메일에 답장을 쓰며, 누군가에게는 안정적인 패키지 운영을 위해 안내 메일을 작성하고 있겠죠. 그리고 프로그램화된 자동 메일은 누군가에서 기쁜 소식과 반갑지 않은 소식을 발송하고 있을 겁니다.\nR 생태계의 발전을 위해 불철주야 활동하는 보이지 않는 그들에게 감사의 인사를 전합니다.\n\n현재는 Fourth Edition인데 이전에는 Edition은 “Modern Applied Statistics with S-PLUS”라는 이름의 서적이었습니다.↩︎\nR에서 ODBC(Open Database Connectivity)통해 데이터베이스의 데이터를 핸들링할 수 있는 패키지↩︎\n",
    "preview": "posts/2021-11-24-rpackage/img/rcoreteam.png",
    "last_modified": "2021-11-26T16:52:09+09:00",
    "input_file": {},
    "preview_width": 1826,
    "preview_height": 1452
  },
  {
    "path": "posts/2021-10-31-rmarkdown/",
    "title": "R Markdown의 이해",
    "description": "재현가능한 연구의 개념과 Sweave, knitr, rmarkdown을 통해 R Markdown을 이해합니다.",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2021-10-31",
    "categories": [
      "Reproducible Research"
    ],
    "contents": "\n\nContents\n재현가능한 연구\n마크업 언어\n마크다운\nLaTeX\nPandoc\n\nSweave\nSweave 매커니즘\nSweave 예제\n\nknitr 패키지\nR 마크다운 파일\nknitr 매커니즘\nknitr 예제\n\nrmarkdown 패키지\n\n재현가능한 연구\n‘재현가능한 연구’(Reproducible Research)라는 용어가 생소할 수 있으나 R 세계에서는 제법 회자되는 용어로, 공개한 연구의 결과물이 재현되고 검증될 수 있도록 하는 것을 의미합니다. 이 방법은 결국 연구가 발전하는 방향으로 재창조되는 결실을 가져옵니다. 이를 위해서는 데이터 분석의 방법을 실험 데이터에 연결하여 재현 검증될 수 있는 체계를 구축해야 합니다.\n재현가능한 연구를 이해하기 쉬운 사례로 설명하겠습니다. 데이터 입출력 및 분석 로직과 분석 결과를 표현하는 R 스크립트를 R 마크다운 문서에 정리합니다. R 마크다운 문서에는 연구에 대한 자세한 설명과 결과 해석이 포함되어 있고, 별도의 공간에 파일로 저장된 원시 데이터도 경로를 통해 문서와 연결되어 있습니다. 이 R 마크다운 문서와 데이터 파일을 RStudio 프로젝트나 R 패키지에 포함하여 배포하면 누구나 동일한 결과를 재현할 수 있게 됩니다.\n공개한 연구 자료에는 데이터, 분석을 위한 R 코드 및 과정의 설명과 결과의 해석 모두 포함되어야 합니다. 그러므로 재현가능한 연구는 공유의 가치를 인정하는 오픈소스 정신이 깃들여 있는 셈입니다.\nCRAN Task View: Reproducible Research에는 재현가능한 연구를 지원하는 여러 R 패키지를 소개하고 있습니다. 페이지에 방문하면 아마 많은 패키지와 다양한 기능의 소개에 놀랄 것입니다. 또한 방대한 기능에 무엇인 재현가능한 연구인지 이해하기 어려울 수 있습니다.\n이 글에서는 R 패키지가 지원하는 여러 기능 중에서 다음 두 가지 사례를 통해서 재현가능한 연구를 소개하겠습니다.\nLaTeX에 포함된 R 코드를 수행하여 PDF 문서를 만드는 사례와 관련 리소스\nR 마크다운 문서로 분석 결과가 포함된 HTML 보고서를 만드는 방법과 관련 리소스\n마크업 언어\n여러분은 웹 브라우저에 표현되는 컨텐츠(웹 페이지나 웹 어플리케이션)는 HTML로 구현된다는 것은 알고 계실겁니다. 그런데 HTML은 “Hyper Text Markup Langaue”의 약자라는 것은 아시나요? 그리고 영어의 정확한 의미를 아시나요?\n‘Text’(텍스트)는 의미의 전달을 목적으로, 알파벳, 한글, 숫자, 기호 등의 문자 세트로 문서나 글을 표현한 것입니다. 우리는 텍스트를 순차적으로 읽으면서 전달하는 의미를 이해하게 됩니다.\n‘Hyper Text’(하이퍼 텍스트)는 사전적으로 “텍스트를 뛰어넘는다”는 의미입니다. 웹 페이지는 본문에 다른 페이지로 이동하는 링크 기능을 이용해서 다른 페이지나, 페이지의 다른 영역으로 이동하는 링크 기능으로 페이지 내의 다른 문장의 영역으로 넘나들 수 있습니다. HTML에서는 이 링크를 하이퍼 링크(hyper link)라고 부릅니다. 즉, 하이퍼 링크를 통해서 텍스트를 비순차적으로 읽으면서, 순차적 전달보다 의미를 좀 더 쉽게 이해할 수 있습니다. 이것이 하이퍼 텍스트의 개념입니다.\n통상적으로 ‘텍스트 파일’은 서식이 없는 텍스트로 구성되어서, 어떤 에디터로도 쉽게 읽을 수 있는 파일을 의미합니다. 그러나 텍스트에 서식이 포함된 즉, 타이틀이나 주석, 볼드와 이탤릭처럼 폰트 페이스 등의 서식을 사용하거나 설명을 위해 이미지 삽화가 포함된 파일은 텍스트 에디터로는 읽을 수 없습니다. 별도의 프로그램으로만 열어볼 수 있습니다. HTML의 하이퍼 텍스트는 이러한 서식을 포함하고 있기 때문에 에디터가 아닌 웹 브라우저를 통해 읽어야 합니다.\n‘Markup Langaue’(마크업 언어)는 텍스트의 특정 위치에 표시를 하는 것을(마크업) 통해서 문서를 구조적으로 표현하는 컴퓨터 언어를 의미합니다. 물론 HTML에서의 구조적인 표현은 서식의 표현이나 페이지 간의 이동이나 페이지 내에서의 이동 등을 의미합니다.\n마크업 언어는 태그(tag) 등을 이용해서 문서나 데이터의 구조를 설명합니다. 즉 마크업이라는 작업에 태그를 이용하는 것입니다. 그래서 마크업 언어의 문법적 특징은 태그에 나타나게 됩니다.\n몇개의 HTML 태그를 예시하면, 하이퍼 링크 태그인 <a><\/a>는 하이퍼 링크를 정의하며, 이미지 태그인 <img />로 웹 페이지에 이미지를 삽입합니다.\n마크다운\n마크업 언어의 하나인 마크다운(Markdown)은 읽기 쉽고 쓰기 쉬운 양식을 작성하기 위해서 만들어졌습니다. 사용법이 매우 쉽기 때문에 빠르게 서식이 있는 문서를 만들 수 있습니다.\n\n\n\n\n\n\n\n“마크다운(Markdown)은 일반 텍스트 기반의 경량 마크업 언어입니다. 일반 텍스트로 서식이 있는 문서를 작성하는 데 사용되며, 일반 마크업 언어에 비해 문법이 쉽고 간단한 것이 특징입니다. HTML과 리치 텍스트(RTF)1 등 서식 문서로 쉽게 변환되기 때문에 응용 소프트웨어와 함께 배포되는 README 파일이나 온라인 게시물 등에 많이 사용됩니다.”2\n\n\nLaTeX\n저작을 위한 전문 시스템인 LaTeX(레이택)은 일반 R 사용자가 사용하기에는 다소 여렵지만 출력물의 퀄리티가 높아서 선호되기도 합니다.\n\n\n\n\n\n\n\nLaTeX(레이택)은 논문이나 출판물 등의 문서를 작성하는 데 쓰이는 전문 조판 시스템입니다. LaTeX은 수식과 다이어그램을 많이 사용하고, 도표와 이미지 삽입이 빈번한 이공계열에서 논문을 작성에 사용하고 있습니다. 또한 서적 등을 출판하는 출판업계에서도 사용하고 있습니다.\n\n\nPandoc\nRStudio를 설치하면 Pandoc(판독)도 함께 설치됩니다. 그 이유는 RStudio 기능에 재현가능한 연구를 위한 여러 문서 작성 기능이 있는데, Pandoc이 약방의 감초처럼 사용되기 때문입니다.\n\n\n\n\n\n\n\n“Pandoc은 마크업 형식을 다른 마크업 형식으로 변환해주는 라이브러리입니다. Pandoc은 Markdown, HTML, LaTeX 및 Word docx 등 수많은 마크업과 워드 프로세싱 형식 간에 변환을 수행합니다.”3\n\n\nSweave\nR 재단의 멤버십 회원인 오스트리아의 Friedrich Leisch가 2002년에 만든, utils 패키지의 Sweave(Leisch 2002) 함수는 자동으로 리포트는 생성해 주는 함수입니다.\nSweave는 S를 이용한 데이터 분석과 LaTeX 조판을 통합하여 데이터 분석 보고서를 작성합니다. S는 상용 S+와 오픈소스 R을 의미합니다. 아마 요즘에 개발되었다면 함수 이름을 Rweave로 작명했을 것입니다. 왜냐하면 2000년대 초반에는 R보다는 S+의 명성이 더 컸기 때문입니다.\nSweave라는 이름은 “S” 와 “Weave”를 결합한 복합어입이다. 그러므로 이 함수는 “에스 위브”라 읽어야 합니다. “Weave”는 날실(직물의 길이 방향, 즉 세로로 놓인 실)과 씨실(직물의 너비 방향, 즉 가로로 놓인 실)로 직물을 짜는 것을 의미하는 동사입니다. 즉 직물을 짜듯, S(S+와 R)의 코드를 LaTeX 조판 코드와 잘 결합하여 완성된 데이터 분석 보고서(직물)를 생성한다는 의미입니다.\n정리하자면, Sweave는 R 코드의 실행 결과(데이터 분석의 출력 정보, 표, 그래프 등)를 LaTeX 파일에 자동 삽입합니다. 그리고 이 LaTeX 파일로 PDF을 조판하여 최종 보고서를 생성하는 것입니다.\nSweave의 단점은 LaTeX과 결합하여 PDF 파일만 생성한다는 점입니다. 2000년대 초반에는 S 사용자의 많은 수가 LaTeX을 사용할 수 있는 통계학자였고, 학술지 등에 LaTeX으로 만들어진 논문을 투고해야 했기에 이 조합은 매우 유용하였습니다. 그러나 최근의 데이터 과학 필드에서는 HTML 기반으로 보고서를 작성하는 경우가 많기 때문에, HTML을 포함한 다양한 포맷의 보고서를 생성할 수 없다는 것은 단점일 수 밖에 없습니다.\nSweave 매커니즘\nS 코드 청크(Chunk, 덩어리)는 특정 작업을 수행하는 S 스크립트(이하 R 스크립트로 표현합니다)를 의미합니다. LaTeX으로 만들어진 날실 곧곧에 R 코드인 씨실을 넣기 위해서 코드 청크를 사용합니다.\nLaTeX 파일에 삽입하는 코드 청크는 다음의 포맷을 따릅니다.\n\n<<Sweave 옵션 영역>>=\nR 코드 삽입 영역\n@    \n\nSweave는 LaTeX 파일을 읽어 ‘<<>>=’ 과 ‘@’ 사이의 R 코드를 실행한 후, 실행 결과를 해당 위치의 LaTeX 파일에 삽입합니다. ‘<<’과’>>=’ 사이에 기술할 수 있는 Sweave 옵션은 다양한 출력을 제어할 수 있습니다. 예를 들면 플롯의 크기를 조정하거나 출력 텍스트의 서식을 제어할 수 있습니다. 경우에 따라서 실행 결과와 함께 R 코드도 출력할 수도 있습니다.\nR 코드의 실행 결과가 삽입된 LaTeX 파일은 LaTeX 시스템의 pdfLaTex이나 XeLaTeX 프로그램을 통해서 PDF 파일로 변환됩니다. Sweave 파일은 확장자로 “Rnw”를 사용해야 합니다. 그런데 이 확장자는 대소문자를 구별하지 않습니다만 관행적으로 첫 문자를 대문자로 표현합니다.\nRStudio는 이 몇 단계 과정을 한번에 엮어 실행합니다. 그래서 바로 PDF 파일을 만들어줍니다.\nSweave 예제\nSweave(Leisch 2002)에 삽입된 짧은 예제 파일을 살펴보겠습니다. example-1.Rnw 파일의 LaTeX 스크립트 중간에 R 코드가 삽입된 코드 청크 영역을 주의깊게 살펴보십시요.\n\n\\documentclass[a4paper]{article}\n\n\\begin{document}\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a \\LaTeX{} document:\n\n<<>>=\ndata(airquality)\nkruskal.test(Ozone  ̃ Month, data = airquality)\n@\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n\\begin{center}\n<<fig=TRUE,echo=FALSE>>=\nboxplot(Ozone  ̃ Month, data = airquality)\n@\n\\end{center}\n\n\\end{document}\n\n\nSweave 파일은 Sweave에 의해서 다음과 같은 완전한, example-1.tex이라는 이름의 LaTex 파일로 변환됩니다. 시각화 결과는 example-1-002.pdf 파일로 따로 저장되게 됩니다.\n\n\\documentclass[a4paper]{article}\n\n\\usepackage{Sweave}\n\\begin{document}\n\nIn this example we embed parts of the examples from the\n\\texttt{kruskal.test} help page into a \\LaTeX{} document:\n\n\\begin{Sinput}\n> data(airquality)\n> kruskal.test(Ozone ~ Month, data = airquality)\n\\end{Sinput}\n\\begin{Soutput}\n      Kruskal-Wallis rank sum test\ndata: Ozone by Month\nKruskal-Wallis chi-squared = 29.2666, df = 4, p-value = 6.901e-06\n\\end{Soutput}\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n\\begin{center}\n\\includegraphics{example-1-002}\n\\end{center}\n\n\\end{document}\n\n\n마지막으로 LaTeX 파일을 pdfLaTex으로 변환하면 다음과 같은 PDF 문서가 만들어집니다.\nPDF 문서 내용knitr 패키지\n2010년대부터 R 필드의 패러다임 전환이 시작됩니다. 데이터 과학(data science)이 발전하면서 기존 통계학자들의 기여가 많았던 R 개발 진영에 데이터 과학자, 데이터 엔지니어, 프로그램 개발자들이 대거 활동하기 시작하면서 R 분석의 변화를 맞습니다.\n데이터 모델링 분야에서는 개별 함수들을 이용한 방법에서 Max Kuhn의 caret(2007) 패키지로, 데이터 시각화 분야에서는 R 그래픽 함수와 lattice 패키지에서 Hadley Wickham의 ggplot2(2007)로 진화합니다. 데이터 엔지니어링에서도 Hadley Wickham의 dplyr(2014)가 기존 데이터 프레임 연산을 대체합니다.\ncaret 패키지를 제외한 ggplot2, dplyr 패키지에 협업 가능한 유용한 패키지들이 에코 패키지로 결합하여 tidyverse 패키지군이 태동합니다. 이들은 RStudio라는 회사를 구심점으로 많은 유용한 패키지를 개발합니다. 특히 Hadley Wickham을 중심으로 많은 데이터 과학자와 데이터 엔지니어, 웹 개발자들이 두각을 나타냅니다.\n재현가능한 연구 분야도 발전하게 됩니다. Sweave의 단점을 보완하고 여러 유용한 기능을 추가한 knitr 패키지가 2012년 Yihui Xie에 의해 개발됩니다. 이제 데이터 분석 결과를 LaTeX 시스템 기반의 PDF로 조판하는 시대를 넘어서 HTML 기반의 WEB 채널로 배포되고 공유하는 시대가 도래한 것입니다.\n\n\n\n\n\n\n\n“Sweave에서 영감받은 knitr 패키지는 다른 애드온 패키지를 하나의 패키지로 결합4하여, R을 이용해서 동적으로 리포트를 생성하기 위해서 개발되었습니다.”5  이제 재현 가능을 연구를 위한 보고서 작성이 PDF를 넘어 HTML로 확장되고, 그 성능도 향상되기 시작합니다.\n\n\nknitr도 “knit”와 “R”을 결합한 복합어입이다. 그래서 “니트 알”로 발음해야 합니다. “knit”는 스웨터와 같은 옷을 뜨게질하는 것을 의미합니다. Sweave의 “Weave”가 직물을 직조하는 것과 동일한 의미입니다. 패키지 이름을 작명하는 것에도 위트가 숨어 있습니다. 마크다운 문서 곳곳에 다른 색상의 R이라는 털실로 뜨게질해서 무늬가 있는 스웨터(보고서)를 만드는 역할을 표현한 것입니다.\nR 마크다운 파일\n‘R 마크다운’(R Markdwon) 파일은 재현 개능한 연구를 위해서, 마크다운 문서에 R 코드 청크를 삽입한 파일입니다. 확장자로 “Rmd”를 사용하는데, 확장자는 Sweave 파일처럼 대소문자를 구별하지 않습니다.\nknitr 매커니즘\nknitr는 R 마크다운 파일의 R 코드 청크를 실행해서 그 결과를 마크다운 문서와 결합한 마크다운 파일을 생성합니다. 마크다운 파일의 확장자는 “md”입니다.\nknitr의 R 코드 청크는 다음의 포맷을 따릅니다.\n\n\n```{r, knitr 옵션 영역}\nR 코드 삽입 영역\n```    \n\n\nknitr는 R 마크다운 파일을 읽어 백틱(`) 문자 세개로 구성된 ‘```{r}’ 과 ‘```’ 사이의 R 코드를 실행한 후, 실행 결과를 해당 위치의 파일에 삽입하여 마크다운 문서를 생성합니다. 옵션은 다양한 출력을 제어할 수 있습니다. 예를 들면 플롯과 출력 텍스트의 서식을 제어할 수 있는 것은 물론 결과를 캐싱해서, 다음 작업에는 연산을 수행하지 않고 결과를 캐시에서 가져올 수도 있습니다.\nSweave가 Sweave 파일을 LaTeX 파일까지만 생성하고, pdfLaTeX 유틸리티가 LaTeX 파일을 PDF 파일로 변환하는 것처럼, knitr는 R 마크다운 파일을 마크다운 파일까지만 생성합니다. 그리고 pandoc이 마크다운 파일을 원하는 포맷의 문서 파일로 변환합니다. 두 방법의 차이점은 pdfLaTeX가 PDF 파일만 생성하는 것에 반해서, pandoc은 PDF를 포함해서 HTML, 워드, 파워포인트 파일까지 다양한 파일을 생성합니다.\nknitr는 Sweave 파일도 처리할 수 있습니다. 이 경우는 knitr 패키지의 Sweave2knitr 함수가 Sweave 파일을 R 마크다운로 변환한 다음에 작업합니다. 그러나 호환성이 다소 떨어지고, kintr의 다양한 기능으로 Sweave이 아니라 knitr을 사용하는 것을 권장합니다.\nknitr 예제\nSweave(Leisch 2002)에 삽입된 예제를 example-1.Rmd이라는 이름의 다음과 같은 R 마크다운 파일로 생성하였습니다.\n\n\nIn this example we embed parts of the examples from the\n*kruskal.test* help page into a LaTeX document:\n\n```{r}\ndata(airquality)\nkruskal.test(Ozone ~ Month, data = airquality)\n```\nwhich shows that the location parameter of the Ozone\ndistribution varies significantly from month to month.\nFinally we include a boxplot of the data:\n\n```{r, echo=FALSE}\nboxplot(Ozone ~ Month, data = airquality)\n```  \n\n\n\n그리고 knitr 패키지의 knit 함수를 이용해서 마크다운 파일로 변경합니다.\n\n\nlibrary(knitr)\n\nknit(\"example-1.Rmd\")\n\n\n\n생성된 마크다운 파일인 example-1.md는 다음과 같습니다.\n생성된 마크다운 파일knitr 패키지의 pandoc 함수는 Pandoc을 랩핑한 함수입니다. 다음 명령을 마크다운 문서를 HTML로 변환하는 R 스크립트입니다. 메시지를 보면 Pandoc이 호출되어 HTML을 생성함을 유추할 수 있습니다.\n\npandoc(\"example-1.md\", format = \"html\")\nExecuting: pandoc -t html -o 'example-1.html' 'example-1.md'\n[1] \"example-1.html\"\n\n생성된 HTML 파일인 example-1.html이 웹 브라우저에 표현된 내용은 다음과 같습니다.\n생성된 HTML 파일의 랜더링 화면\nRStudio를 사용하면 이런 몇 단계의 작업을 메뉴 하나로 함축해 놓았기 때문에, 그냥 메뉴 아이콘을 눌러 작업할 수 있습니다. 그래서 단계적으로 함수를 호출하는 것을 모르는 R 사용자가 대부분입니다. 지극히 정상입니다. 다만 RStudio를 사용하지 않는 R 사용자는 단계적으로도 작업을 할 수 있도록, 함수를 호출하는 방법을 숙지해야 합니다.\n\nrmarkdown 패키지\nSweave와 knitr는 문서에 삽입된 R 코드 청크를 실행하고 그 결과를 LaTeX이나 마크다운 문서에 삽입하는 작업을 수행합니다. 그러나 이 작업은 최종 문서를 만드는 작업의 일부입니다. R 마크다운 파일이 문서가 되기 위해서는 다음의 워크플로우를 거쳐야 합니다. 이 작업을 수행해 주는 것이 rmarkdown 패키지입니다.\n\n\n\n\n\n\n\n\n\n\nrmarkdown는 마크다운과 R 코드를 연동하여 재현 가능한 문서를 작성할 수 있는 R 패키지입니다. 마크다운의 마크업 태그는 파일에 저장된 코드를 실행한 결과와 매시업되어 사용자가 의도한 저작 콘텐츠를 고품질의 문서로 자동으로 생성해 줍니다.  R Markdown 문서는 재현 가능하며 수십 가지의 정적 및 동적 출력 형식을 지원합니다.\n\n\nR 마크다운은 보고서만 만드는 도구가 아닙니다. R 마크다운을 이용하면 다음 그림6처럼 블로그와 같은 웹 페이지를 만들 수도 있으며, 서적도 출판할 수 있습니다.\nDocument your analyses, make a website, make slides… the world is your oyster!R 마크다운을 소개하는 이 웹 페이지도 rmarkdown 기반으로 만들었습니다. (정확히 말하자면 distill 패키지를 이용한 R 마크다운 파일의 웹 페이지 생성)\n많은 문서들이 웹 페이지로 생산되고, 배포 공유되는 세상입니다. R 마크다운을 이용해서 재현 가능한 문서를 생산하는 것을 적극 추천합니다. 특히 RStudio는 R 마크다운을 생산하고 배포하는 기능을 쉽게 메뉴화했습니다. 어렵게 느끼지 말고, R 기반의 데이터 분석 경험을 적극 자산화하고 공유하시기 바랍니다.\n\n\n\nLeisch, Friedrich. 2002. “Sweave: Dynamic Generation of Statistical Reports Using Literate Data Analysis.” In Compstat 2002 - Proceedings in Computational Statistics, edited by Wolfgang Härdle and Bernd Rönz, 575–80. Physica Verlag, Heidelberg. http://www.stat.uni-muenchen.de/ leisch/Sweave.\n\n\nhttps://ko.wikipedia.org/wiki/서식_있는_텍스트_포맷. 서식 있는 텍스트 포맷 또는 리치 텍스트 포맷(Rich Text Format, 줄여서 RTF)은 마이크로소프트사가 1987년에 개발한 규격인 사유의 문서 파일 형식이며 크로스 플랫폼 문서 교환을 위하여 만들어졌다.↩︎\nhttps://ko.wikipedia.org/wiki/마크다운 발췌↩︎\nhttps://pandoc.org/MANUAL.html 발췌↩︎\nknitr ≈ Sweave + cacheSweave + pgfSweave + weaver + animation::saveLatex() + R2HTML::RweaveHTML() + highlight::HighlightWeaveLatex() + 0.2 * brew + 0.1 * SweaveListingUtils + more↩︎\nknitr 홈페이지 https://yihui.org/knitr/ 발췌↩︎\nhttps://www.williamrchase.com/slides/ggplot_intro.html#57↩︎\n",
    "preview": "posts/2021-10-31-rmarkdown/img/markdown.png",
    "last_modified": "2021-10-31T15:07:10+09:00",
    "input_file": {},
    "preview_width": 300,
    "preview_height": 185
  },
  {
    "path": "posts/2021-10-24-renewal/",
    "title": "distill을 이용한 블로깅",
    "description": "\"그동안 blogdown 패키지를 이용해서 블로그를 운영해왔으나, 이번에 distill 패키지를 이용해서 블로그를 리뉴얼했습니다. 사실 그동안 블로그 운영이 활발하지 않았습니다. 이번 리뉴얼을 계기로 좀 더 분발해 보겠습니다.\"",
    "author": [
      {
        "name": "유충현",
        "url": "https://choonghyunryu.github.io"
      }
    ],
    "date": "2021-10-24",
    "categories": [
      "Reproducible Research"
    ],
    "contents": "\n\nContents\n들어가기\n블로깅\ndistill\n에필로그\n\n들어가기\n재현가능한 연구(Reproducible Research)라는 용어가 생소할 수 있으나 R world에서는 제법 회자되는 용어입니다. 공개한 연구의 결과물이 재현되고 검증될 수 있도록 하는 것을 의미합니다. 이 방법은 결국 연구가 발전하는 방향으로 재창조되는 결실을 가져옵니다. 이를 위해서는 데이터 분석의 방법을 실험 데이터에 연결하여 재현 검증될 수 있는 체계를 구축해야 합니다.\n재현가능한 연구를 이해하기 쉬운 사례로 설명하겠습니다. 데이터 입출력 및 분석 방법의 로직과 분석 결과를 표현하는 R 스크립트를 R 마크다운 문서에 정리합니다. 물론 연구에 대한 자세한 설명과 결과 해석이 문서에 포함되어 있고, 원시 데이터도 별도로 포함합니다. 이 연구 결과를 RStudio 프로젝트나 R 패키지로 배포하면 누구나 동일한 결과를 재현할 수 있게 됩니다.\n공개한 연구 자료에는 데이터, 분석을 위한 R 코드 및 과정의 설명과 결과의 해석 모두 포함되어야 합니다. 그러므로 재현가능한 연구는 오픈소스 정신이 깃들여 있는 셈입니다.\nCRAN Task View: Reproducible Research 페이지를 보면 R에서 재현가능한 연구를 지원하는 에코 시스템이 어마어마하게 많고 다양한 기능을 가지고 있음에 놀랄 것입니다. 다수의 R 패키지가 여러 기능을 통해서 재현가능한 연구를 지원합니다만, R 마크다운 문서가 정적이거나 동적인 HTML 문서의 보고서로 만들어지거나 혹은 LaTeX을 이용해서 PDF 문서로 만들어지는 것이 눈에 잡히는 재현가능한 연구의 도구입니다.\n광의적으로는 rmarkdown, knitr, blogdown, bookdown, distill도 재현가능한 연구를 지원하는 패키지입니다.\n블로깅\n개인적으로는 utils 패키지의 Sweave1 함수로 LaTeX 기반의 PDF 문서를 조판하는 것을 오래전부터 즐겨 왔습니다. 서적을 출판할 때에도 이용했구요. 그러나 이 방법은 LaTeX 문법을 배워야하는 허들이 있었습니다. 그러나 요즘은 rmarkdown도 LaTeX로 만든 PDF 파일 못지 않을 품질의 PDF 파일을 만들어줍니다. 사용자 입장에서는 진입장벽이 낮아진 것입니다.\n어느 때부터 blogdown 패키지가 R 마크다운 문서로 블로그를 쉽게 만들 수 있음을 알고, blogdown 패키지를 사용해서 블로그를 운영해왔습니다. 만족해서 사용하고 있었지만, distill을 사용하기 시작하면서 distill로 이사하게 되었습니다.\n두 패키지를 비교하자면, blogdown은 일반 블로그의 특징을 잘 표현하지만, 지식을 공유하고 재현가능한 연구 기반의 포스팅이 산만한 감이 없지 않습니다. 그러나 distill은 R 기반의 연구(응용) 결과의 공유를 모토로하는 재현가능한 연구 기반의 포스팅에 최적화되어 있습니다.\ndistill\ndistill 홈페이지인 https://rstudio.github.io/distill/ (Allaire et al. 2018)를 방문하면, distill의 기능을 익힐 수 있습니다. 헥스로고는 다음과 같이 물방울 이미지를 포함하고 있습니다.\n\n\n\n다음에 distill의 재현가능한 연구에 최적화된 대표적인 기능을 정리해 봅니다.\nRefrences의 표시\n페이지 하단에 참조물을 표현합니다.\nBibtex, YAML format의 bibliography을 지원합니다.\n\nCitation의 표시\n페이지 하단에 Citation을 표현합니다.\n페이지 YAML 헤더를 읽어 Citation을 만들어줍니다.\n\n이 포스트는 기술적인 노하우나 경험을 담고 있지는 않지만, 이해의 측면에서 distill의 재현가능한 연구에 최적화된 대표적인 기능을 표현해 보았습니다.\n에필로그\n이번 포스트는 이제부터 블로그 운영에 distill 패키지를 사용한다는 소식을 전하기 위해서 작성했습니다. 앞으로 distill 패키지를 비롯한, 재현가능한 연구를 지원하는 R 패키지들을 소개하는 포스트를 올려보겠습니다.\n\n\n\nAllaire, JJ, Rich Iannone, Alison Presmanes Hill, and Yihui Xie. 2018. “Distill for r Markdown.” https://rstudio.github.io/distill.\n\n\nhttps://en.wikipedia.org/wiki/Sweave의 첫 문장은 Sweave의 정의를 설명합니다. 그 내용은 다음과 같습니다. Sweave는 통계 프로그래밍 언어 R의 기능으로 R 코드를 LaTeX 또는 LyX 문서에 통합할 수 있습니다. 목적은 “데이터 또는 분석이 변경되는 경우 자동으로 업데이트될 수 있는 동적 보고서를 생성하는 것”입니다.↩︎\n",
    "preview": "posts/2021-10-24-renewal/img/logo.png",
    "last_modified": "2021-10-23T20:28:43+09:00",
    "input_file": {},
    "preview_width": 240,
    "preview_height": 277
  },
  {
    "path": "posts/2021-10-23-powermockup/",
    "title": "Wireframe 도구 PowerMockup 소개",
    "description": "\"웹앱이나 앱을 개발할 때 UI/UX를 설계를 목적으로 화면 단위의 레이아웃을 디자인하는 것을 와이어프레임(wireframe)이라 합니다.   와이어프레임이 좀더 정적으로 구체화되면 목업(mockup)이라고 합니다. 용어는 차이가 있으나 두 개 모두 화면을 설계한다는 것에는 차이가 없습니다.\"",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-10-23",
    "categories": [
      "UI Design"
    ],
    "contents": "\n\nContents\n들어가기\n파워목업 소개\n파워목업 다운로드\n파워목업 장점\n파워목업 실행하기\n파워목업에 대한 기대\n\n들어가기\n이번에는 R이나 데이터 분석과 무관한 주제에 대해서 이야기합니다.\n개인적으로 데이터분석 웹앱인 BitStat를 Shiny 기반의 오픈소스로 개발하는 일을 시작하였습니다. 처음에는 암묵적으로 떠올린 개념으로 화면을 개발했으나, 복잡도가 늘어나고 화면의 일관성을 유지하기 위해서 개발에 앞서 와이어프레임을 그려보려 합니다.\nWireframe 도구를 찾던 중 PowerMockup를 소개하는 글을 포스팅하면 무료로 사용할 수 있는 라이센스를 획득할 수 있는 방법이 있어, 부득이하게 제품 소개글을 게시합니다.\n파워목업 소개\n웹앱이나 앱을 개발할 때 UI/UX를 설계를 목적으로 화면 단위의 레이아웃을 디자인하는 것을 와이어프레임(wireframe)이라 합니다. 와이어프레임이 좀더 정적으로 구체화되면 목업(mockup)이라고 합니다. 용어는 차이가 있으나 두 개 모두 화면을 설계한다는 것에는 차이가 없습니다.\n예전에 잠깐 발사믹(Balsmiq mockups)를 사용한 경험이 있어 구글링을 통해서 몇 개의 대표적인 와이어프레임(목업) 도구를 리스트업 했습니다. 그중 파워목업(PowerMockup)을 발견했습니다.\n이 툴의 장점은 파워포인트의 애드인(add-in)으로서, 제가 많은 시간 사용하고 있는 파워포인트 안에서 동작한다는 점입니다. 그러니 자연스럽게 파워포인트 문서로 와이어프레임을 디자인합니다. 두번째 장점은 블로그에 소개하면 유로 라이센스를 제공받을 수 있다는 점입니다.\n파워목업 다운로드\n파워목업 홈페이지인 https://www.powermockup.com/를 방문하면, 다음 그림처럼 프로그램을 다운로드할 수 있는 화면이 떡하니 나타납니다.\n\n\n\n정품을 구매하더라도 트라이얼 버전을 다운로드해야 합니다.\n사실 트라이얼 버전을 다운로드해서 사용하기에는 어려움이 많습니다. 그 많은 위젯 중에서 몇개만 지원해서 현실적으로 와이어프레임을 디자인하는 것이 불가능하다고 보여집니다.\n파워목업 장점\n파워목업은 파워포인트 안에서 작동하므로 위젯 등의 도형을 드래그 드롭으로 끌어다 디자인하면 됩니다. 쉽습니다.\n파워목업의 특징 설명 페이지파워목업이 내세우는 장점에도 쉬운 사용성을 첫 째로 꼽고 있습니다.\n파워목업 선택의 당위성개인적으로 파워목업의 장점은 파워포인트 애드인이라는 점입니다. 아무래도 프로젝트를 위한 많은 문서들을 파워포인트로 만들기 때문에, 사용하기에 익숙하다는 점. 그리고 최종 산출물이 파워포인트 문서 파일로 저장된다는 점입니다.\n파워목업 실행하기\n다운로드 후 설치를 하였고, 파워포인트를 실행했습니다. 오른쪽의 파워목업의 도형들이 나열되어 있습니다. 이 중에서 필요한 도형을 끌어다가 디자인하면 됩니다.\n파워포인트에서의 파워목업레이아웃의 예제도 있습니다. 처음부터 하나씩 도형을 엮어 디자인할 수도 있으나, 예제를 먼저 가져다 놓고 수정하는 방법도 있겠죠. 그러나 예제의 사례는 몇 가지가 안되더군요.\n파워목업 디자인 예제체크박스그룹을 가져다 디자인하는 사례입니다. 오른쪽 다이얼로그 박스를 수정하면 다이얼로그 박스의 디자인이 변경됩니다. 도든 도형들이 이렇게 동작하지 않지만, 사용자가 세세하게 컨트롤하는 수고를 덜어주는 기능이라 유용했습니다.\n체크박스그룹 예제파워목업에 대한 기대\n아직은 트라이얼 버전이라 기획하고 있는 와이어프레임을 그릴 수 는 없습니다. 그러나 라이센스를 획득하고 잠겼던 도형들이 열리면 BitStat의 와이어프레임 문서들이 만들어지고, 와이어프레임의 수만큼 앱의 화면도 늘어갈겁니다.\n\n\n\n",
    "preview": "posts/2021-10-23-powermockup/img/homepage.png",
    "last_modified": "2021-10-23T00:24:02+09:00",
    "input_file": {},
    "preview_width": 773,
    "preview_height": 441
  },
  {
    "path": "posts/2021-10-19-includehtml/",
    "title": "Shiny 웹앱에 정적 HTML 문서 넣기",
    "description": "\"Shiny 웹앱을 개발할 때, 기능에 대한 설명을 위한 도움말 기능의 구현이 필요할 수 있다. 아마도 Rmarkdown으로 HTML 기반의 웹 문서를 떠올릴 것이다. 이 글에서 해당 솔루션을 제시한다.\"",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-10-19",
    "categories": [
      "R-Programming"
    ],
    "contents": "\n\nContents\n프롤로그\nhtmlOutput\n웹 문서 랜더링하기\nincludeHTML\niframe에 HTML 삽입하기\n\n에필로그\n\n프롤로그\n통계 기반의 데이터 분석을 UI/UX로 지원하는 Shiny 웹앱 기반으로 작성하고 있다. BitStat라고 이름을 명명한 프로젝트는 어느 정도 가시적인 앱의 형상이 구현되었고, 이제는 필요한 기능을 추가하는 일을 본격적으로 진행할 때다. 그러기에 앞서서 초보자가 BitStat의 기능을 쉽게 익힐 수 있도록 도움말 기능을 추가하였다.\n도움말 기능을 추가하면서 겪은 몇 가지 문제점에 대해서 정리하였다. Shiny 웹앱에 정적 HTML 문서를 넣고자하는 R 사용자에게 도움이 되었으면하는 바램이다.\nhtmlOutput\n아시다시피 Shiny에서 웹 페이지를 출력하는 출력 위젯은 htmlOutput이다. Shiny 웹앱에 정적 HTML 문서를 넣고자하는 R 사용자는 BitStat에서 정적 웹 문서를 만들고, 그 문서를 htmlOutput에 넣어주는 것을 제`일 먼저 떠올렸을 것이다.\n어느 누구도 이의를 제기하지 않을 명확한 방법이다.\n다음은 “데이터 준비”라는 도움말을 만들어 놓은 HTML 문서를 삽입하기 위해서 출력 위젯인 htmlOutput를 “date_prepare”라는 아이디로 정의한 예다.\n\n\noutput$ui_help <- renderUI({\n  tagList(\n    tabBox(\n      width = 12,\n      tabPanel(\n        title = translate(\"데이터\"),\n        tabsetPanel(\n          tabPanel(\n            title = translate(\"데이터 준비\"),\n            htmlOutput(\"date_prepare\", style = \"height: 700px;\")\n          ),\n          tabPanel(\n            title = translate(\"데이터 진단\")\n          ),\n          tabPanel(\n            title = translate(\"데이터 변환\")\n          )          \n        )\n      ),\n      \n      tabPanel(\n        title = translate(\"기술통계\"),\n        tabsetPanel(\n          tabPanel(\n            title = translate(\"탐색적 데이터분석\")\n          )\n        )\n      )\n    ) \n  )  \n})\n\n\n\n웹 문서 랜더링하기\n이 작업을 수행하면서 웹 문서를 랜더링하여 htmlOutput에 넣는 방법에는 두 가지 솔루션이 있음을 알았다.\nincludeHTML\nincludeHTML는 HTML 파일을 읽어 HTML 컨텐츠를 삽입한다. 함수 이름이 직관적이라 쉽게 그 기능을 유추할 수 있을 것이다.\nhelp 디렉토리의 “date_prepare.html” HTML 파일을 로드하기 위해서 렌더링 파트를 완성했다.\n\n\noutput$date_prepare <- renderUI({\n  includeHTML(\"help/date_prepare.html\")\n})\n\n\n\n렌더링된 결과는 다음 그림과 같다. 원하는대로 웹 문서가 잘 삽입되었다.\n\n\n\nhtmltools 패키지의 includeHTML 함수는 유용한 함수임에는 틀림없다. 이 함수는 정적 HTML 페이지를 Rmarkdown 문서에 포함시키는 것을 쉽게 준다. 그러나 Shiny에서 웹 페이지를 추가할 때는 예기치 못한 부작용(side effect)이 발생했다.\n사이드바의 도움말 메뉴를 실행해서 도움말을 출력한 이후에, 다른 메뉴의 기능이 작동하지 않았다. 메뉴를 클릭해도 어떠한 이벤트가 발생하지 않았다. 사이드바의 메뉴 모양도 미묘하게 바뀌어 있었다. 화면처럼 선택한 사이드바인 도움말에 사각형의 보더가 표현되었다.\nShiny의 근간이 되는 Bootstrap의 CSS, Javascript와 삽입된 HTML 페이지의 CSS, Javascript가 충돌한 것으로 여겨진다. 그러므로 includeHTML 함수로 Shiny 웹앱에 정적 HTML을 삽입할 경우에는 간단한 페이지로 제한할 필요가 있다.\niframe에 HTML 삽입하기\n충돌을 피하기 위해서 iframe에 HTML을 삽입하였다.\n\n\naddResourcePath(\"tmpuser\", getwd())\n\noutput$date_prepare <- renderUI({\n  tags$iframe(\n    seamless = \"seamless\",\n    src = \"tmpuser/help/date_prepare.html\",\n    width = \"100%\",\n    height = \"100%\"\n  )\n})\n\n\n\n여기서 중요한 것은 addResourcePath 함수로 워킹 디렉토리를 명시적으로 정의하지 않으면, iframe에서 HTML 파일을 찾을 수 없다.1\n랜더링된 결과는 includeHTML로 출력한 HTML의 화면과 미미한 차이가 있다. iframe 영역이 명확하게 구분된다는 점이다. 그러나 부작용없이 정상적으로 다른 모든 기능이 구동되었다.\n\n\n\n에필로그\n정적 웹 페이지를 Shiny 웹앱에 포함시킬 경우에는 iframe을 확보한 후 iframe에 포함시키는 것이 부작용 없이, 기 정의한 다른 기능과 문제없이 동작하게 된다.\n\nhttps://stackoverflow.com/questions/24875943/display-html-file-in-shiny-app을 참고하였음↩︎\n",
    "preview": "posts/2021-10-19-includehtml/img/includehtml.png",
    "last_modified": "2021-10-23T00:25:43+09:00",
    "input_file": {},
    "preview_width": 3416,
    "preview_height": 1802
  },
  {
    "path": "posts/2021-10-17-glmnet/",
    "title": "glm predict 시 발생하는 에러 사례",
    "description": "\"이진분류에 lasso regression을 이용하기 위해서 glmnet 패키지를 이용하여 모델을 적합한 후 predict할 경우에 발생할 수 있는 오류와 해결 방법을 제시한다.\"",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-10-17",
    "categories": [
      "Statistics"
    ],
    "contents": "\n\nContents\n프롤로그\nalookr과 이진분류 모델\n\n오류 사례의 재현\n데이터 분할\n불균형 클래스 핸들링\n데이터 정제\n모델 평가를 위한 test set 추출\n이진분류 모델 적합\n모델의 평가\n에러의 문제 탐색\n에러의 문제 해결\n\n\n프롤로그\n분석가가 주관적으로 이진분류 모델을 적합하고, 적합한 모델로 예측(predict)할 경우에는 분석가의 의도가 실려, 데이터 분석 시에 문제가 발생하는 경우는 적다.\n그러나, autoML의 경우에는 문제가 발생할 수도 있다. 물론 여러 사례에 대해서 미리 로직으로 문제를 회피하면 이런 이슈를 사전에 방지할 수 있다. alookr 패키지를 개발하면서 lasso 모델을 이용한 이진분류를 개발하고, 몇 가지 사례로 검증하면서 놓쳤던 에러 발생 사례를 최근에 발견하고 이를 해결하는 방법을 제시한다.\nalookr과 이진분류 모델\nalookr은 이진분류 모델의 auto-ML을 위해서 만들어진 패키지다. 아직은 개발해야 할 기능이 많지만, 느리게 목표를 향해 나가고 있다. logistic regression부터 시각해서 하나하나 모델을 추가해 나가고 있는 중, 몇 개월 전에 추가한 lasso regression 사례에서 오류가 발생한 것이다. 물론 특정 사례에서 발생하였다.\n오류 사례의 재현\n데이터 분할\n원시 데이터에서 결측치를 보정하고 training set과 test set을 분할한다.\n\n\nlibrary(dlookr)\nlibrary(alookr)\nlibrary(dplyr)\nlibrary(mlbench)\n\ndata(BreastCancer)\n\n# 결측치를 포함한 변수\ndiagnose(BreastCancer) %>%\n  filter(missing_count > 0)\n\n\n# A tibble: 1 x 6\n  variables   types  missing_count missing_percent unique_count\n  <chr>       <chr>          <int>           <dbl>        <int>\n1 Bare.nuclei factor            16            2.29           11\n# … with 1 more variable: unique_rate <dbl>\n\n# 결측치의 대체 수행\nbreastCancer <- BreastCancer %>%\n  mutate(Bare.nuclei = imputate_na(BreastCancer, Bare.nuclei, Class,\n                         method = \"mice\", no_attrs = TRUE, print_flag = FALSE))\n\n# 기본 인수로 training set과 test set로 분할\nsb <- breastCancer %>%\n  split_by(target = Class)\n\n\n\n불균형 클래스 핸들링\n\n\n# training set 돗수분포표 - 불균형 클래스 데이터\ntable(sb$Class)\n\n\n\n   benign malignant \n      458       241 \n\n# training set 상대돗수분포표 - 불균형 클래스 데이터\nprop.table(table(sb$Class))\n\n\n\n   benign malignant \n0.6552217 0.3447783 \n\n# 집계 - 불균형 클래스 데이터\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  29556 \n + split data            \n    - train set count :  489 \n    - test set count  :  210 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\n# SMOTE 샘플링\ntrain_smote <- sb %>%\n  sampling_target(seed = 1234L, method = \"ubSMOTE\")\n\n# 돗수분포표\ntable(train_smote$Class)\n\n\n\n   benign malignant \n      692       519 \n\n데이터 정제\n\n\n# training set의 정제\ntrain <- train_smote %>%\n  cleanse\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\nNo variables that unique value is one.\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• Id = 416(0.343517753922378)\n\n── Checking character variables ─────────────────────── categorical data ──\nNo character variables.\n\n모델 평가를 위한 test set 추출\n\n\n# test set 추출\ntest <- sb %>%\n  extract_set(set = \"test\")\n\n\n\n이진분류 모델 적합\nlasso 모델링을 위해서 alookr 패키지가 아닌, R 스크립트로 모델을 적합한다.\n\n\nlibrary(glmnet)\nlibrary(dplyr)\n\ntrain_X <- train %>% \n  select(-Class) %>% \n  data.matrix \n  \nlabel <- train %>% \n  transmute(Class = ifelse(Class == \"malignant\", 1, 0)) %>% \n  pull \n  \nmodel_lasso <- glmnet::glmnet(x = train_X, y = label, family = \"binomial\")\n\n\n\n모델의 평가\n모델의 평가를 위해서 앞에서 만들어 놓은 test 데이터셋을 이용한다. 그런데, 에러가 발생하였다.\n물론 alookr에서도 다음과 유사한 스크립트로 예측 로직을 구현하였다.\n\npred <- predict(model_lasso, \n                test %>% \n                  select(-Class) %>% \n                  data.matrix)\n\nError in h(simpleError(msg, call)) : error in evaluating the argument 'x' in selecting \na method for function 'as.matrix': Cholmod error 'X and/or Y have wrong dimensions' \nat file ../MatrixOps/cholmod_sdmult.c, line 90\n\n에러의 문제 탐색\n“wrong dimensions”이라는 에러 메시지에 착안하여, train셋과 test 셋의 차원을 비교해 본다. 변수의 개수가 다르다.\n\n\ndim(train_X)\n\n\n[1] 1211    9\n\ndim(test %>% \n      select(-Class))\n\n\n[1] 210  10\n\n변수 이름을 살펴보니, test 셋에서는 train 셋에 없는 “Id” 변수가 포함되어 있다. 앞에서 cleanse() 함수로 train 셋을 정제할 때 식별자인 “Id”가 제거되었던 것이다.\n\n\ncolnames(train_X)\n\n\n[1] \"Cl.thickness\"    \"Cell.size\"       \"Cell.shape\"     \n[4] \"Marg.adhesion\"   \"Epith.c.size\"    \"Bare.nuclei\"    \n[7] \"Bl.cromatin\"     \"Normal.nucleoli\" \"Mitoses\"        \n\nnames(test %>% \n      select(-Class))\n\n\n [1] \"Id\"              \"Cl.thickness\"    \"Cell.size\"      \n [4] \"Cell.shape\"      \"Marg.adhesion\"   \"Epith.c.size\"   \n [7] \"Bare.nuclei\"     \"Bl.cromatin\"     \"Normal.nucleoli\"\n[10] \"Mitoses\"        \n\n에러의 문제 해결\nalookr에서는 lasso외에 6개의 이진분류 모델이 있으며, 값을 예측할 때 에러가 발생하지 않았다. 아마도 함수 내부에서 필요한 변수를 취했을 것이다. 그러나 glmnet 패키지는 사용자가 기술한 데이터셋의 변수를 모두 사용하는 것 같다. 그래서 train 셋에 있는 변수만 취하는 로직으로 수정해서 문제를 해결하였다.\n\n\npred <- predict(model_lasso, \n                test %>% \n                  select(matches(model_lasso$beta %>% row.names())) %>% \n                  data.matrix)\n\n\n\ncleanse() 함수로 train 셋을 정제하는 것처럼 test 셋도 정제하면 문제가 없었을 것이다. 그러나 auto-ML을 위해서는 사용자의 주관적인 개입이 적게 때문에 이 부분의 로직은 포함되어 있지 않았다.\n아무튼 glmnet 패키지는 예측할 때(predict 함수를 호출할 때) train 셋과 test 셋의 변수 개수가 같아야 한다는 것을 잊지 말자.\n\n\ntest <- sb %>%\n  extract_set(set = \"test\") %>% \n  cleanse\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\nNo variables that unique value is one.\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• Id = 205(0.976190476190476)\n\n── Checking character variables ─────────────────────── categorical data ──\nNo character variables.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-18T10:37:21+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-10-introduce_bitstat/",
    "title": "BitStat 패키지 소개",
    "description": "\"부족함이 많이 있다. 하지만 이 책이 훌륭한 통계적 알고리즘의 개발과 성능 좋은 한국형 통계분석기 개발에 일말의 보탬이 되었으면 좋겠다. 한 것 없이  길고 고독한 시간이었다. 이제 밖에 나가 쏟아지는 별들을 가슴에 담고 들어오리라. 통계분석기도 만들어 봐야겠다. 언제까지나 SAS만 돌릴 수 없을 테니까.\"",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-10-10",
    "categories": [
      "Statistics"
    ],
    "contents": "\n\nContents\n프롤로그\nBitStat 패키지 소개\n기능적 특징\n다국어 지원 (i18n)\n데이터 분석 보고서 지원\n도움말 및 튜토리얼 지원\n\n구조적 특징\nR & shiny\nServer Hosting & Local Hosting\ndlookr & alookr\n\n세션 자료 소개\n\n프롤로그\n“부족함이 많이 있다. 하지만 이 책이 훌륭한 통계적 알고리즘의 개발과 성능 좋은 한국형 통계분석기 개발에 일말의 보탬이 되었으면 좋겠다. 한 것 없이 길고 고독한 시간이었다. 이제 밖에 나가 쏟아지는 별들을 가슴에 담고 들어오리라. 통계분석기도 만들어 봐야겠다. 언제까지나 SAS만 돌릴 수 없을 테니까.”\n1994년 9월, 대학교 3학년 시절. C로 배우는 통계학을 출판하기에 앞서 작성한 저자 서문의 일부다.\n사실 책을 출판할 계획은 아니었었다. 그런데 단행본으로 출판되면서 나의 인생 궤적에서 하나의 설명 변수로 자리 잡은 사건이었다.\n그리고 27년이 흘러, 2021년 9월 30년이 채 안된 시점에 드디어 염원하던 통계분석기를 만들기 시작했다.\nBitStat 패키지 소개\n한국형 통계분석기는 의미가 없어진지 오래다. 디지털, 오픈소스 시대에는 국경이 무너져 버린 지 오래기 때문이다. 그냥 통계분석기다. 한국형이라는 그럴싸한 수식어는 필요없는 허구이기 때문이다.\nBitStat 패키지가 염원하던 통계분석기의 이름이다. 어제 온라인 미트업을 통해 BitStat 패키지 소개 세션을 가졌다. 염원하던 통계분석기가 BitStat 패키지로 세상에 소개된 것이다.\n소개 페이지 원문을 간단하게 공유해 본다.\n기능적 특징\n다국어 지원 (i18n)\nBitStat은 메뉴와 메시지 등 UI 레벨에서 한글과 영문을 지원합니다.\n\n\n\n영문 지원 BitStat데이터 분석 보고서 지원\nBitStat은 PDF와 HTML 포맷의 각종 보고서 작성을 지원합니다.\nPDF 포맷 보고서\nPDF 포맷 보고서는 정적 컨텐츠를 제공하며, 데이터 분석 프로젝트의 산출물로 활용이 가능합니다.\n \nHTML 포맷 보고서\nhtml 포맷 보고서는 동적 컨텐츠를 제공하며, 독립적으로 심층적인 데이터 분석의 수행이 가능합니다.\n \n도움말 및 튜토리얼 지원\nrmarkdown 기반의 도움말과 튜토리얼은 사용자가 데이터 분석을 쉽게 수행할 수 있도록 가이드를 제공합니다.\n구조적 특징\nR & shiny\nR 패키지로 개발되며, shiny 웹앱으로 UI/UX를 구현하였습니다.\nR 패키지\nR 패키지로 배포가 가능합니다.\nshiny\nshiny 웹앱을 구성합니다. 현재는 프로토타이핑 버전으로 Look & Feal은 shinydashboard로 구현되었습니다.\nServer Hosting & Local Hosting\nServer\nBitStat는 데이터 분석 Server로서 클라우드 환경에 배포되어 데이터 분석을 지원합니다.\nDesktop\nBitStat는 사용자 컴퓨터에서 구동되어 데이터 분석 수행을 지원합니다.\ndlookr & alookr\nBitStat는 dlookr, alookr 패키지의 기능을 최적화 하여 지원합니다.\ndlookr\ndlookr 패키지는 데이터 진단, 탐색적 데이터 분석, 통계적 데이터 변환 기능을 UI/UX 기반과 보고서 기반으로 최적화 및 자동화 합니다.\n\n\n\nhttps://choonghyunryu.github.io/dlookr/\nalookr\nalookr 패키지는 분석 데이터 분할, 이진 분류 모델 적합 및 모델 검증을 자동화하는 AutoML 기능을 지원합니다.\n현재 alookr 패키지는 기능의 만들어지는 과정이기 때문에, BitStat 패키지와 병행해서 기능이 진보될 것입니다.\n\n\n\nhttps://choonghyunryu.github.io/alookr/\n세션 자료 소개\nShiny Hands-on과 같이 진행된 세션이었고, https://choonghyunryu.github.io/openstat_project/about.html에 소개 자료와 Shiny Hands-on 내용이 포함되어 있다.\n\n\n\n",
    "preview": "posts/2021-10-10-introduce_bitstat/img/bitstat_kor.jpg",
    "last_modified": "2021-10-23T09:18:44+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-19-ggplot_aes/",
    "title": "ggplot과 동적 데이터",
    "description": "\"R 데이터 분석에서의 시각화는 ggplot2 패키지다.\" 이 명제는 부정할 수 없는 현실이다. lattice 패키지가 S-PLUS의 Trellis 그래프를 R에 구현하였기에 늘 감사한 마음으로 애용했었다. 그런데 언젠가부터 ggplot2 패키지가 lattice 패키지를 대체하게 되었고, 이제는 ggplot2 패키지 이전과 이후로 나눌 정도로 표준이 되어 버렸다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-09-19",
    "categories": [
      "Visualization"
    ],
    "contents": "\n\nContents\n다룰 이야기\n산점도를 그려 보자\n산점도 함수를 만들어 보자\n첫 번째 함수의 정의\n두 번째 함수의 정의\n\n변수의 이름에 공백이 있는 경우\n세 번째 함수의 정의\n\n덤 하나 더\n시사점\n\n다룰 이야기\n“R 데이터 분석에서의 시각화는 ggplot2 패키지다.” 이 명제는 부정할 수 없는 현실이다. lattice 패키지가 S-PLUS의 Trellis 그래프를 R에 구현하였기에 늘 감사한 마음으로 애용했었다. 그런데 언젠가부터 ggplot2 패키지가 lattice 패키지를 대체하게 되었고, 이제는 ggplot2 패키지 이전과 이후로 나눌 정도로 표준이 되어 버렸다.\n이미 정해져 있는 데이터로 시각화하는 과정을 반복하다 보면, 어느 새 함수와 같은 자신만의 서브 루틴을 만들게 된다. 이것은 정해져 있는 데이터가 아닌, 데이터의 구조와 변수의 이름이 다른, 함수 입장에서는 동적 데이터다.\n\n이미 알고 있는 데이터는 ggplot2 문법의 R 스크립트에 변수를 미학이라고 어색하게 번역되는 Aesthetic의 줄임말인 aes() 함수를 통해서 좌표계의 기하학적인 요소에 매핑을 하게 된다. 그러나 함수의 인수로 넘어온 변수를 매핑하는데는 몇 가지 이슈가 발생한다. 이번 글에서는 이 이슈를 트러블 슈팅하는 방법을 살펴본다.\n\n산점도를 그려 보자\nggplot2 패키지는 geom_point() 함수로 산점도를 그린다.\n\n\nlibrary(ggplot2)\n\np <- ggplot(mtcars, aes(wt, mpg))\np + geom_point()\n\n\n\n\n우리는 이미 mtcars라는 데이터 프레임과 변수 wt와 mpg를 알고 있다. 그렇기 때문에 aes() 함수에 차량의 무게인 wt(Weight) 변수를 x-축에 지정하고, 연비인 mpg(Miles/gallon) 변수를 y-축에 지정한 산점도를 그릴 수 있었다. 명시적으로 변수 이름으로 좌표에 매핑하여 산점도를 그린 것이다.\n산점도 함수를 만들어 보자\n산점도를 그리는 일이 빈번해 졌다. Copy & Paste 신공을 펼쳐서 산점도를 그리는 R 스크립트를 여러 번 복사하고 변수 이름만 바꾸어 산점도를 그릴 수도 있지만 산점도를 그리는 함수를 만들면 좀 더 쉽게 산점도를 그릴 수 있을 것 같다.\n첫 번째 함수의 정의\n함수를 정의하는 방법을 사용해서 plot_scatter()을 정의한 후\n\n\nplot_scatter <- function(data, x, y) {\n  library(ggplot2)\n\n  p <- ggplot(data = data, aes(x = x, y = y))\n  p + geom_point()\n}\n\n\n\n앞서 시각화했던 차량의 무게와 연비와의 관계를 살펴보기 위한 산점도 함수를 그리려 시도하였다.\n\n\nplot_scatter(data = mtcars, x = wt, y = mpg)\n\n\n\n그러나 호출한 함수에서는 다음과 같은 오류가 발생하였다.\n\nError in FUN(X[[i]], ...) : object 'wt' not found\n\n두 번째 함수의 정의\naes_string() 함수는 인수값인, 좌표계에 매핑할 변수의 이름을 심볼이 아닌 문자열로 지정하는 함수다. 함수 이름에 string이 포함된 것으로 쉽게 유추 가능하다.\n이 함수를 사용해서 plot_scatter2()를 정의한 후\n\n\nplot_scatter2 <- function(data, x, y) {\n  library(ggplot2)\n\n  p <- ggplot(data = data, aes_string(x = x, y = y))\n  p + geom_point()\n}\n\n\n\n앞서 시각화했던 차량의 무게와 연비와의 관계를 살펴보기 위한 산점도 함수를 그리려 시도하였다. 이번에는 에러 없이 정상적인 산점도를 얻었다. 인수값은 심볼이 아닌 문자열이기 때문에 “wt”, “mpg”를 사용하게 된다.\n\n\nplot_scatter2(data = mtcars, x = \"wt\", y = \"mpg\")\n\n\n\n\n변수의 이름에 공백이 있는 경우\n변수 이름에 공백을 포함하는 것은 단언컨데 지양해야 할 방법이다. 굳이 어절을 공백으로 나눌 필요가 있다면, 공백 대신 언더라인(_)을 사용하는 것이 좋다. 그러나 권장되지 않더라도 이미 공백이 포함된 데이터를 R에서 사용한다면, 당신은 가끔 부작용(side effect)에 당황할 수도 있다.\n세 번째 함수의 정의\n공공데이터인 국민건강보험공단의 검강검진 정보로 변수의 이름에 공백이 포함된 사례를 살펴본다.\n\n\nlibrary(readr)\n\nhealth <- read_csv(\"data/국민건강보험공단_건강검진정보_20191231.csv\", \n                   locale = locale(encoding = \"cp949\"),\n                   show_col_types = FALSE)\n\nspec(health)\n\n\ncols(\n  기준년도 = col_double(),\n  `가입자 일련번호` = col_double(),\n  시도코드 = col_double(),\n  성별코드 = col_double(),\n  `연령대 코드(5세단위)` = col_double(),\n  `신장(5Cm단위)` = col_double(),\n  `체중(5Kg 단위)` = col_double(),\n  허리둘레 = col_double(),\n  `시력(좌)` = col_double(),\n  `시력(우)` = col_double(),\n  `청력(좌)` = col_double(),\n  `청력(우)` = col_double(),\n  `수축기 혈압` = col_double(),\n  `이완기 혈압` = col_double(),\n  `식전혈당(공복혈당)` = col_double(),\n  `총 콜레스테롤` = col_double(),\n  트리글리세라이드 = col_double(),\n  `HDL 콜레스테롤` = col_double(),\n  `LDL 콜레스테롤` = col_double(),\n  혈색소 = col_double(),\n  요단백 = col_double(),\n  혈청크레아티닌 = col_double(),\n  `(혈청지오티)AST` = col_double(),\n  `(혈청지오티)ALT` = col_double(),\n  `감마 지티피` = col_double(),\n  흡연상태 = col_double(),\n  음주여부 = col_double(),\n  `구강검진 수검여부` = col_double(),\n  치아우식증유무 = col_double(),\n  `결손치 유무` = col_character(),\n  치아마모증유무 = col_character(),\n  `제3대구치(사랑니) 이상` = col_character(),\n  치석 = col_double(),\n  `데이터 공개일자` = col_double()\n)\n\n변수 이름을 보면 공백을 포함한 것도 문제지만 괄호를 포함한 변수도 많다. 변수 이름에 공백을 포함하는 것도 문제지만 괄호처럼 특수문자를 포함하는 것도 문제다.\n괄호가 포함된 변수 이름으로 산점포를 그리는 예제다. 그러나 에러가 발생한다.\n\n\nplot_scatter2(data = health, x = \"시력(좌)\", y = \"시력(우)\")\n\n\n\n\nError in 시력(좌) : could not find function \"시력\"\n\n변수에 포함된 괄호가 함수를 호출할 때의 괄호와 충돌이 발생해서, R 엔진이 시력이라는 함수를 찾다가 오류가 발생한 경우다. 그러므로 변수의 이름에는 괄호와 같은 특수문자를 사용하지 말자.\n다음은 공백이 포함된 변수 이름으로 산점포를 그리는 예제다. 역시 에러가 발생한다.\n\n\nplot_scatter2(data = health, x = \"총 콜레스테롤\", y = \"감마 지티피\")\n\n\n\n\nError in parse(text = elt) : <text>:1:3: unexpected symbol\n1: 총 콜레스테롤\n      ^ \n\n세번째 사용자 정의 함수는 aes() 함수를 사용한다. aes() 함수는 인수값에 심볼을 지정해야 하기 때문에 sym() 함수로 문자열을 심볼로 바꾼 다음에 rlang 패키지의 인젝션 연산자 !!로 싱글 객체를 넘겨준다.\n\n\nplot_scatter3 <- function(data, x, y) {\n  library(tidyverse)\n\n  p <- ggplot(data = data, aes(x = !!sym(x), y = !!sym(y)))\n  p + geom_point()\n}\n\n\n\n괄호가 포함된 변수에 대해서도 정상적으로 수행되며,\n\n\nplot_scatter3(data = health, x = \"시력(좌)\", y = \"시력(우)\") +\n  ylim(0, 2.0) +\n  theme_grey(base_family = \"NanumSquare\")\n\n\n\n\n공백이 포함된 변수에 대해서도 정상적으로 수행되며,\n\n\nplot_scatter3(data = health, x = \"총 콜레스테롤\", y = \"감마 지티피\") +\n  ylim(0, 500) +\n  theme_grey(base_family = \"NanumSquare\")\n\n\n\n\n덤 하나 더\n선형모형을 수행하는 함수를 하나 만들었다.\n\n\nlinear <- function(data, x, y) {\n  formula_str <- sprintf(\"%s ~ %s\", y, x)  \n\n  lm(formula_str, data = data) %>% \n    broom::tidy()\n}\n\n\n\n잘 알려져 있는 붓꽃 데이터로 단순선형모형을 적합해 보자.\n\n\nlinear(data = iris, x = \"Sepal.Length\", y = \"Sepal.Width\")\n\n\n# A tibble: 2 x 5\n  term         estimate std.error statistic  p.value\n  <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    3.42      0.254      13.5  1.55e-27\n2 Sepal.Length  -0.0619    0.0430     -1.44 1.52e- 1\n\n이번에는 앞서 문제가 되었던 두 가지 사례의 변수로 단순선형모형을 적합해 보자.\n\n\nlinear(data = health, x = \"시력(좌)\", y = \"시력(우)\") \n\n\n\n\nError in 시력(우) : could not find function \"시력\"\n\n\n\nlinear(data = health, x = \"총 콜레스테롤\", y = \"감마 지티피\")\n\n\n\n\nError in str2lang(x) : <text>:1:4: unexpected symbol\n1: 감마 지티피\n       ^ \n\n두 사례 모두 산점도와 유사한 에러가 발생하였다.\n이런 문제를 해결하기 위해서 다음과 같이 함수를 수정하였다. R에서 억음부호(Backticks) `는 공백 문자 및 특수 문자가 포함된, 표준 변수 이름이 아닌 변수 이름(non-standard)을 지정할 때 사용한다. 그러므로 sprintf() 함수의 포뮬러를 만드는 문자열에 억음부호를 넣으면 문제가 쉽게 해결된다.\n\n\nlinear2 <- function(data, x, y) {\n  formula_str <- sprintf(\"`%s` ~ `%s`\", y, x)  \n\n  lm(formula_str, data = data) %>% \n    broom::tidy()\n}\n\n\n\n\n\nlinear2(data = health, x = \"시력(좌)\", y = \"시력(우)\") \n\n\n# A tibble: 2 x 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)    0.411    0.0498      8.27 4.28e-16\n2 `시력(좌)`     0.587    0.0497     11.8  3.34e-30\n\n\n\nlinear2(data = health, x = \"총 콜레스테롤\", y = \"감마 지티피\")\n\n\n# A tibble: 2 x 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)       11.5     11.7        0.984  0.326 \n2 `총 콜레스테롤`    0.135    0.0598     2.26   0.0242\n\n시사점\n\nR에서 변수명을 정의할 때에는 공백이나 특수문자를 넣지 말자. 원하지 않는 부작용이 발생할 수 있다. 부작용을 회피하는 스크립트를 작성하는 것 보다, 근본적으로 문제가 되는 변수명을 지양하고 표준 변수명을 사용한다. 그리고 가급적 한글 변수명을 사용하지 않는 것도 권장한다.\n\n\n\n\n",
    "preview": "posts/2021-09-19-ggplot_aes/2021-09-19-ggplot_aes_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-11T10:35:37+09:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 768
  },
  {
    "path": "posts/2021-08-04-dlookr_report/",
    "title": "dlookr과 데이터분석 리포트",
    "description": "0.5.0 버전에서는 데이터 진단, EDA, 데이터 변환을 지원하는 리포트의 개선이 가장 두드러진 변화다. 이번 포스트에서 변화된 새로운 리포트에 대해서 소개한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-08-04",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n다룰 이야기\n문제 의식\nPDF 파일 출력 보고서\nhtml 파일 출력 보고서\n개선 포인트\n\n개선된 보고서\n웹 기반의 동적 보고서\nPDF 파일 기반의 정적 보고서\n테마의 도입\n\n데이터 진단 보고서\n동적 보고서\n정적 보고서\n\nEDA와 데이터변환 보고서의 스크린 샷\nEDA 정적 보고서의 커버\n데이터 변환 보고서의 커버\n\n마무리\n\n다룰 이야기\n7월말에 dlookr 0.5.0 버전이 CRAN에 등록되었다. 아직 Windows 버전의 바이너리 버전이 빌드되기 전이지만, 며칠 새 빌드될 것이다.\n0.5.0 버전에서는 데이터 진단, EDA, 데이터 변환을 지원하는 리포트의 개선이 가장 두드러진 변화다. 이번 포스트에서 변화된 새로운 리포트에 대해서 소개한다. 단, 기능보다는 리포트 포맷 등 물리적으로 변경된 외형 중심으로 소개하기로 한다. 기능적 소개는 다음에 다루도록 하겠다.\n\n이제 dlookr 패키지는 동적 분석이 가능한 웹 기반의 보고서와 PDF 파일과 html 파일로 생성되는 정적 보고서를 제공한다. 인터렉티브하게 데이터 분석을 수행하기 위한 목적의 동적 리포트는 좀 더 직관적인 정보를 제공해 주며, 데이터 분석의 결과를 보관하기 위한 목적의 정적 보고서는 좀 더 팬시한 컨텐츠를 제공한다.\n\n문제 의식\n기존 버전의 dlookr 패키지의 보고서는 몇 가지 문제점을 가지고 있었다.\nPDF 파일 출력 보고서\nPDF 파일로 제공되는 보고서는 LaTeX 기반으로 생성되기 때문에 출력 결과물의 미려함 이면에 보고서를 출력하지 못하는 상황이 있었다. LaTeX 기반으로 동작하기 때문에 LaTeX이 설치되지 않은 환경에서는 사용할 수 없다는 문제점과 비영어권 문자를 지원하지 못한다는 점이다. 한글 문서의 출력을 위해서 kotex을 지원했지만, 사용자 환경에 따라 동작하지 못하는 경우가 발생하였다.\n개인적으로 베트남 출장 시에 베트남 언어를 사용할 경우에는 베트남 언어를 지원하게 dlookr 패키지를 수정하여 사용하였지만, 일반 사용자는 영문 이외의 데이터가 포함된 데이터의 보고서를 출력하는 데 크고 작은 이슈가 발생했었다.\nhtml 파일 출력 보고서\nhtml 파일의 경우에는 영문 이외의 문자를 포함한 데이터도 문제 없이 리포팅하였다. 다만, 모든 컨텐츠가 PDF 파일 출력의 것과 동일하였기 때문에, 웹 인터페이스의 장점을 살릴 수 없었다. 즉, PDF 파일 출력이 여의치 않을 경우에 사용하는 보고서가 되었다.\n개선 포인트\n기존 보고서의 문제점을 개선하기 위해서 다음과 같은 개선 포인트를 선정하여 새 버전의 리포트를 기획하였다.\n비 영어 문화권의 언어를 지원하야 한다.\n보고서의 디자인에 사용자 정의 기능을 부여한다.\n인터렉티브한 분석을 위해서 웹 버전의 보고서는 동적 기능을 부여한다.\n정적 기능의 보고서의 조판 기능을 개선한다.\n개선된 보고서\n웹 기반의 동적 보고서\n동적 보고서는 reactable 패키지를 이용하여 구현하였다. reactable 패키지는 React Table라는 JavaScript 라이브러리를 인터페이스하는 R 패키지다. 이를 이용해서 인터렉티브한 데이터 테이블을 웹 페이지에 구현할 수 있었다.\nreactable 패키지를 이용한 데이터 테이블의 인터렉티브는 메뉴를 조작할 때, 실시간으로 정보를 구현하는 것이 아니라, 보고서를 빌드할 때, 미리 드릴링을 위한 시각화 결과와 집계표를 미리 생성한다는 특징이 있다. 그러므로 보고서를 생성할 때, 어느 정도의 시간이 소요된다.\nPDF 파일 기반의 정적 보고서\n정적 보고서는 pagedown 패키지를 이용하여 구현하였다. pagedown 패키지는 JavaScript 라이브러리 paged.js를 사용하여 HTML 문서의 콘텐츠를 개별 페이지로 분할해 준다. 그리고 분할된 웹 페이지를 크롬과 인터넷익스플르로 엣지를 이용해서 PDF 파일로 변환해 준다.\n\n결국은 이번 버전 업에서 JavaScript가 중요한 역할을 수행한 셈이다. 이제는 패키지의 개발의 데이터 분석가가 아닌, 어플리케이션 개발자의 근성을 일깨워 주고 있다. 그만큼 건드려야 할 기술 요소가 확장되 가고 있음을 절실히 느낀다.\n\n테마의 도입\n아주 복잡한 의미의 테마가 아니지만, 테마의 개념을 도입했다. bule 테마와 orange 테마는 각각 푸른색과 오렌지 색의 룩앤필을 제공해 준다. bule 테마는 R의 고유 로고인 푸른색의 R을 위한 기본 색상이고, orange 테마는 회사의 CI에서 따온 것이다. dlookr 패키지를 회사에서도 사용할 수 있으니…\n테마와 보고서의 Skelton은 unhcRstyle 패키지를 많이 참조하였다. 가능하면 dependency 패키지에 넣으려 했지만, dependency 패키지가 20개를 초과하는 바람에 unhcRstyle 패키지의 골격을 녹여 넣었다. CRAN의 엄격한 패키지 심사는 dependency 패키지를 20개 이내로 제한하고 있기 때문이다.\n데이터 진단 보고서\n동적 보고서\ndiagnose_web_report()의 기본 인수는 orange 테마의 웹 보고서를 생성한다.\n\n\n# create dataset\nheartfailure2 <- dlookr::heartfailure\nheartfailure2[sample(seq(NROW(heartfailure2)), 20), \"sodium\"] <- NA\nheartfailure2[sample(seq(NROW(heartfailure2)), 5), \"smoking\"] <- NA\nheartfailure2[sample(seq(NROW(heartfailure2)), 2), \"time\"] <- 0\nheartfailure2[sample(seq(NROW(heartfailure2)), 1), \"creatinine\"] <- -0.3\n \ndiagnose_web_report(heartfailure2)\n\n\n\n일부 페이지의 모습으로 상단에 메뉴가 표시되어 있어, 해당 영역으로 이동할 수 있다.\n\n\n\n테마, 타이틀과 타이틀 색상, 그리고 로고를 바꾸어 보았다.\n\n\ndiagnose_web_report(heartfailure2, title = \"데이터 진단\",  subtitle = \"심장병데이터\", \n                    logo_img = \"/Users/choonghyunryu/Documents/01_Personal/00_bitr/r2bit.png\",\n                    title_color = \"orange\", theme = \"blue\")\n\n\n\nbule 테마의 사례의 특정 화면이다. 테이블의 탭을 눌러 시각화 정보나 통계 요약정보를 조회할 수 있다.\n\n\n\n정적 보고서\ndiagnose_paged_report()의 기본 인수는 orange 테마의 웹 보고서를 생성한다.\n\n\ndiagnose_paged_report(heartfailure2)\n\n\n\nPDF 파일 기반의 정적 보고서의 표지다. 로고, 타이블과 서브 타이틀, 표지 이미지 등을 사용자가 변경할 수도 있다.\n\n\n\nPDF 파일 기반의 정적 보고서의 표지다. A4 용지에 출력이 가능한 사이즈다.\n\n\n\nEDA와 데이터변환 보고서의 스크린 샷\nEDA 정적 보고서의 커버\n로고와 타이틀의 이름을 바꾼 EDA 정적 보고서\n\n\n\n데이터 변환 보고서의 커버\n\n\n\n마무리\n보고서 기능보다는 리포트 포맷 등 물리적으로 변경된 외형 중심으로 dlookr 0.5.0의 보고서를 살펴 보았다. 다음에는 데이터 분석의 기능 중심으로 보고서를 살펴 보기로 한다.\n0.5.0 버전에서는 사용자가 어느 정도 보고서의 포맷과 조판을 수정할 수 있는 기능을 부여했지만, 향후에는 좀더 디테일한 부분까지 편집할 수 있는 기능을 구현하도록 하겠다.\n\n\n\n",
    "preview": "posts/2021-08-04-dlookr_report/img/diag_web_title.jpg",
    "last_modified": "2021-10-11T10:33:10+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-30-numeric_precision/",
    "title": "수치 정밀도에 대해서",
    "description": "R의 수치연산에서 32-비트의 정수와 IEC 60559 스팩의 부동 소수점(배정밀도)의 산술연산을 사용한다. R이 설치된 모든 머신에서 공통적으로 적용된다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-06-30",
    "categories": [
      "R-Programming"
    ],
    "contents": "\n\nContents\nR의 수치정밀도\n.Machine\n정수\n부동 소수점\nRound-Off 에러\n\n\nR의 수치정밀도\nR의 수치연산에서 32-비트의 정수와 IEC 60559 스팩의 부동 소수점(배정밀도)의 산술연산을 사용한다. R이 설치된 모든 머신에서 공통적으로 적용된다.\n\n\n\n.Machine\n.Machine은 R이 설치된 머신에서의 수치 정밀도에 대한 정보를 제공해 준다.\nNumerical Characteristics of the Machine\n\n\nlibrary(dplyr)\n\n.Machine %>% \n  unlist %>% \n  prettyNum()\n\n\n               double.eps            double.neg.eps \n           \"2.220446e-16\"            \"1.110223e-16\" \n              double.xmin               double.xmax \n          \"2.225074e-308\"           \"1.797693e+308\" \n              double.base             double.digits \n                      \"2\"                      \"53\" \n          double.rounding              double.guard \n                      \"5\"                       \"0\" \n        double.ulp.digits     double.neg.ulp.digits \n                    \"-52\"                     \"-53\" \n          double.exponent            double.min.exp \n                     \"11\"                   \"-1022\" \n           double.max.exp               integer.max \n                   \"1024\"              \"2147483647\" \n              sizeof.long           sizeof.longlong \n                      \"8\"                       \"8\" \n        sizeof.longdouble            sizeof.pointer \n                     \"16\"                       \"8\" \n           longdouble.eps        longdouble.neg.eps \n           \"1.084202e-19\"            \"5.421011e-20\" \n        longdouble.digits       longdouble.rounding \n                     \"64\"                       \"5\" \n         longdouble.guard     longdouble.ulp.digits \n                      \"0\"                     \"-63\" \nlongdouble.neg.ulp.digits       longdouble.exponent \n                    \"-64\"                      \"15\" \n       longdouble.min.exp        longdouble.max.exp \n                 \"-16382\"                   \"16384\" \n\n정수\n정수는 0과 음수 및 양수로 구성된다. 그러므로 32-비트로 표현할 수 있는 정수의 범위는 \\(-2 ^ {31} \\sim 2 ^ {31} - 1\\)이다.\n정수의 최대값은 “integer.max”로 표현된다.\n\n\n# 정수 최대값\n.Machine[[\"integer.max\"]]\n\n\n[1] 2147483647\n\n# 정수 최대값의 수식\n2 ^ 31 - 1\n\n\n[1] 2147483647\n\n정수의 최대값을 초과하는 2147483648는 실수인 “numeric”으로 형변환되어 처리된다.\n\n\n2147483647L %>% \n  is()\n\n\n[1] \"integer\"             \"double\"              \"numeric\"            \n[4] \"vector\"              \"data.frameRowLabels\"\n\n2147483648L %>% \n  is()\n\n\n[1] \"numeric\" \"vector\" \n\n부동 소수점\n부동 소수점의 최소값과 최대값의 범위는 각각 “double.xmin”와 “double.xmax”에 정의되어 있다.\n\n\n# 부동 소수점 최소값\n.Machine[[\"double.xmin\"]]\n\n\n[1] 2.225074e-308\n\n# 부동 소수점 최대값\n.Machine[[\"double.xmax\"]]\n\n\n[1] 1.797693e+308\n\nR은 부동 소수점의 최소값과 최대값의 범위를 초과하는 값은 수학에서의 무한대와는 다르지만, 무한대인 Inf(Infinite)로 표현한다.\n\n\n1.797693e+308\n\n\n[1] 1.797693e+308\n\n1.797693e+308 < Inf\n\n\n[1] TRUE\n\n1.797694e+308 < Inf\n\n\n[1] FALSE\n\n1.797694e+308\n\n\n[1] Inf\n\nRound-Off 에러\n컴퓨터는 수치를 이진수로 표현한다. 그래서 10진수의 소수점 이하의 자리수가 이진수로 컴퓨터에서 유효자리수로 처리될때, Round-Off 에러가 발생한다.\n\\(\\sqrt4\\)는 2이지만, \\(\\sqrt5\\)는 소수점으로 제한된 비트 안에 채우기 위해서 오차가 발생한다. 그래서 예제의 \\(\\sqrt5 ^ 2\\)이 정확한 5로 인식되지 않는다.\n\n\nsqrt(4)\n\n\n[1] 2\n\nsqrt(4) ^ 2\n\n\n[1] 4\n\nsqrt(4) ^ 2 == 4\n\n\n[1] TRUE\n\n4 - sqrt(4) ^ 2 \n\n\n[1] 0\n\nsqrt(5)\n\n\n[1] 2.236068\n\nsqrt(5) ^ 2 \n\n\n[1] 5\n\nsqrt(5) ^ 2 == 5\n\n\n[1] FALSE\n\n5 - sqrt(5) ^ 2 \n\n\n[1] -8.881784e-16\n\nR에서 컴퓨터로 정확하게 표현할 수 있는 부동 소수점은 “double.eps”에 정의되어 있다. 이것은 \\((1 + \\epsilon) \\ne 1\\)을 만족하는 가장 작은 양의 부동소수점 \\(\\epsilon\\)을 의미한다.\n\n\n.Machine[[\"double.eps\"]]\n\n\n[1] 2.220446e-16\n\n.Machine[[\"double.eps\"]] + 1 != 1\n\n\n[1] TRUE\n\n2.220446e-17 + 1 != 1\n\n\n[1] FALSE\n\n\n\n\n",
    "preview": "posts/2021-06-30-numeric_precision/img/iec.gif",
    "last_modified": "2021-10-23T10:30:11+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-07-optimal_binning/",
    "title": "스코어카드와 최적 비닝",
    "description": "금융권에서 사용하는 스코어카드(scorecard) 기법에서는 연속형 변수를 범주형 변수로 변환하는 작업을 빈번히 사용한다. 이번에는 일반적인 비닝 방법이 아닌 최적 비닝(optimal binning) 방법을 사용한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-02-07",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n다룰 이야기\n최적 비닝(optimal binning)\n데이터\n데이터 준비\n데이터 진단\n종속변수\n\n스코어카드 개념\n준비\n최적비닝 수행\n비닝 결과의 요약\n비닝의 진단\n비닝 결과의 활용\n\n비닝 성능 진단 메트릭\n오즈\nWOE\nInformation Value\nKolmogorov-Smirnov statistics(K-S 통계량)\nROC 곡선과 AUC\n로렌츠 곡선과 지니계수\n카이스퀘어 통계량\n\n결언\n\n다룰 이야기\n앞서 비닝에 대해서 다루었다. 특히 dlookr 패키지로 연속형 변수를 비닝하는 여러 솔루션을 다루었다.\n금융권에서 사용하는 스코어카드(scorecard) 기법에서는 연속형 변수를 범주형 변수로 변환하는 작업을 빈번히 사용한다. 이번에는 일반적인 비닝 방법이 아닌 최적 비닝(optimal binning) 방법을 사용한다.\n\ndlookr 패키지에는 일반적인 비닝과 스코어카드 모델(Scorecard modeling)을 위한 최적 비닝 기능을 지원한다. 여기서는 최적 비닝의 개념과 최적 비닝을 수행하고, 비닝 결과의 성능을 검증하는 방법을 다룬다.\n\n최적 비닝(optimal binning)\n최적 비닝은 비닝하고자 하는 연속형 변수(indicator)를 비닝할 때, 이진 클래스(binary class)의 종속 변수(target variable)를 고려한다. 즉, 이진분류(binary classification predict) 모델을 적용한다. 쉽게 말하자면 종속 변수의 positive 클래스의 예측력이 높은 수준(levels, 범주)과 그렇지 않은 수준으로 비닝한다. 이때 범주화(이산화)된 결과는 순서형 범주, 즉 ordered factor가 된다.\n이산화된 범주의 수준(여기서는 bins)의 positive 클래스 예측력은 단조 증감 추세(monotonic trend)를 가져야 한다. 즉, ordered factor로 표현된 bins의 순서대로 예측력이 단조증가(monotonic increase) 하거나 단조감소(monotonic decrease)해야 한다. 정확히 말하자면 뒤에서 다룰 WoE가 단조 증감추세를 가져야 한다.\n단조 증가 사례\n\n\n\n단조 감소 사례\n\n\n\n데이터\n\n\n\n데이터 준비\nkaggle의 https://www.kaggle.com/tug004/breast-cancer-dataset에 breast_cancer.csv는 유방암 진단 데이터를 제공한다. 이 데이터로 스코어카드의 기초 측도(measures)와 최적 비닝을 살펴 본다.\n먼저 데이터를 다운로드한 후 R로 가져오자.\n\n\nlibrary(dplyr)\nlibrary(dlookr)\n\nfname <- \"breast_cancer.csv\"\n\nread.csv(fname) %>% \n  select(-X) -> breast_cancer \n\n\n\n데이터 진단\n\n\nbreast_cancer %>% \n  diagnose() %>% \n  as.data.frame()\n\n\n                 variables     types missing_count missing_percent\n1                       id   integer             0               0\n2                diagnosis character             0               0\n3              radius_mean   numeric             0               0\n4             texture_mean   numeric             0               0\n5           perimeter_mean   numeric             0               0\n6                area_mean   numeric             0               0\n7          smoothness_mean   numeric             0               0\n8         compactness_mean   numeric             0               0\n9           concavity_mean   numeric             0               0\n10     concave.points_mean   numeric             0               0\n11           symmetry_mean   numeric             0               0\n12  fractal_dimension_mean   numeric             0               0\n13               radius_se   numeric             0               0\n14              texture_se   numeric             0               0\n15            perimeter_se   numeric             0               0\n16                 area_se   numeric             0               0\n17           smoothness_se   numeric             0               0\n18          compactness_se   numeric             0               0\n19            concavity_se   numeric             0               0\n20       concave.points_se   numeric             0               0\n21             symmetry_se   numeric             0               0\n22    fractal_dimension_se   numeric             0               0\n23            radius_worst   numeric             0               0\n24           texture_worst   numeric             0               0\n25         perimeter_worst   numeric             0               0\n26              area_worst   numeric             0               0\n27        smoothness_worst   numeric             0               0\n28       compactness_worst   numeric             0               0\n29         concavity_worst   numeric             0               0\n30    concave.points_worst   numeric             0               0\n31          symmetry_worst   numeric             0               0\n32 fractal_dimension_worst   numeric             0               0\n   unique_count unique_rate\n1           569 1.000000000\n2             2 0.003514938\n3           456 0.801405975\n4           479 0.841827768\n5           522 0.917398946\n6           539 0.947275923\n7           474 0.833040422\n8           537 0.943760984\n9           537 0.943760984\n10          542 0.952548330\n11          432 0.759226714\n12          499 0.876977153\n13          540 0.949033392\n14          519 0.912126538\n15          533 0.936731107\n16          528 0.927943761\n17          547 0.961335677\n18          541 0.950790861\n19          533 0.936731107\n20          507 0.891036907\n21          498 0.875219684\n22          545 0.957820738\n23          457 0.803163445\n24          511 0.898066784\n25          514 0.903339192\n26          544 0.956063269\n27          411 0.722319859\n28          529 0.929701230\n29          539 0.947275923\n30          492 0.864674868\n31          500 0.878734622\n32          535 0.940246046\n\n데이터를 진단할 결과를 보면 결측치도 없고, 연속형 변수의 중복값도 많지 않고, 대체적으로 양호하다.\n종속변수\ndiagnosis는 종속변수다. B는 benign(양성 종양), M는 malignant(악성 종양)을 의미한다. positive를 malignant(악성 종양)인 M으로 negative를 benign(양성 종양)인 B로 정의하고 이야기를 풀어 나간다.\n\n\ntable(breast_cancer$diagnosis)\n\n\n\n  B   M \n357 212 \n\n스코어카드 개념\n스코어카드 모델에서는 모델링으로 만들어진 최종 스코어를 비닝한 등급으로 모델의 성능을 평가한다. 이 과정이 최적 비닝과 유사하고, 그 성능 평가 지표도 동일하기에 모델의 성능을 평가하는 방법에 국한해서 설명한다. (스코어카드 모델링은 다루지는 않는다.)\n준비\n먼저 관점의 정리가 필요하다. 역학 분석에 응용되는 사례-대조 연구(case-control studies)에서 사례군(case)은 보통 환자를 의미한다. 대조군은 환자가 아닌 군을 의미한다. 흡연이 폐암에 미치는 영향을 분석할 때 다음과 같은 분할표를 만들 수 있다.\n\n폐암발생\n폐암 미발생\n흡연\na\nb\n비흡연\nc\nd\n이것을 이진분류로 바라보면 폐암 발생여부는 예측하려는 변수인 종속변수(target variable)고, 흡연 여부는 종속변수를 설명하는 예측변수(predictor variable)다. 폐암발생 여부가 관심사이고, 이를 예측하려는 모델이므로 폐암발생은 positive, 폐암 미발생은 negative 클래스다. 즉, 관심 대상이 되는 클래스가 positive 클래스다.\n만약 환경과 질병의 인과관계를 규명하는 연구에서는 질병발생이 positive 클래스이고 질병 미발생이 negative 클래스일 것이다. 그러나 신약의 효과 규명하는 연구에서는 완치가 positive 클래스이고 미완치가 negative 클래스일 것이다. 즉, positive와 negative는 좋고 나쁨, 우열의 관계는 아니다. 간혹 positive와 negative를 양성과 음성으로 번역하여 표현하는 경우도 있으나, 개인적으로 마음에 들지 않아 그냥 포지티브(positive)와 네거티브(negative)라 칭하고 있다.\n연체여부, 부도여부 등을 설명하는 스코어카드도 종속변수와 예측변수로 일반화할 수 있다. 개념은 동일하다. 그러나 필드에서 부르는 클래스의 용어가 다르다. 그래서 용어가 어느 정도 혼란을 줄 수도 있겠다.\n질병 발생과 같은 의료분야 연구나, 연체, 부도와 같은 신용평점 모델에서의 종속변수의 클래스를 표현하는 용어는 positive 클래스, negative 클래스 보다는 good(우량), bad(불량)과 같은 표현을 한다. 폐암발생, 연체, 부도는 bad(불량)로 폐암 미발생, 연체 미발생은 good(우량)으로 표현한다. 관점(관심 대상이 되는 클래스)을 어떻게 보느냐에 따라 달라질 수 있는 여지가 없다.\n최적비닝 수행\n\ndlookr에서는 최적 비닝에서 이진 클래스를 good(우량), bad(불량)으로 부르지 않고, positive와 negative로 칭한다. 그러므로 스코어카드에 익숙한 사용자는 positive를 good(우량)으로, negative를 bad(불량)으로 생각하면 될 것이다. 그래서 이 글에서는 편의상 양성종양을 우량으로, 악성종양은 불량으로 부른다. 그러므로 양성종양을 관심 클래스로 보고, 양성종양을 positive, 악성종양을 negative로 클래스로 보자.\n\n스코어카드 개념을 살펴보기 위해서 먼저 dlookr 패키지로 유방암 데이터 중에서 종양의 반지름 평균인 radius_mean 변수를 최적비닝해 보자. 유방암 데이터는 여러 관측 측도로 악성 종양을 예측하는 사례의 데이터이므로 종속변수인 diagnosis를 고려해서 예측변수 radius_mean를 최적 비닝한다.\n다음은 dlookr의 binning_by() 함수로 최적비닝을 수행한다. dlookr의 binning_by() 함수는 종속 변수가 0과 1의 두가지로 코딩된 경우에 동작한다. positive 클래스가 1, negative 클래스가 0이어야 한다. 그러나 factor일 경우에는 자동으로 변환하여 비닝을 수행한다. 메시지를 보면 악성종양(M)이 1, 양성종양(B)이 0으로 변환되었다.\n\n\nbreast_cancer %>% \n  binning_by(diagnosis, radius_mean) -> bin_radius_mean\n\n\n` diagnosis ` ~ ` radius_mean `\n<environment: 0x7f86def9c838>\n\n비닝을 통해서 연속형 변수가 5개 수준을 갖는 범주형 변수로 변환되었다.\n\n\nbin_radius_mean\n\n\nbinned type: optimal\nnumber of bins: 5\nx\n[6.98,11.8] (11.8,13.1]   (13.1,15]   (15,16.8] (16.8,28.1] \n        150         115         132          54         118 \n\n분석하려는 관점이 다를 경우는 변수를 변환해서 사용해야 한다. 양성종양을 positive, 악성종양을 negative로 클래스로 보기로 했으므로, positive를 good(우량)으로, negative를 bad(불량)으로 정의하기 위해서 diagnosis_new이라는 파생변수를 만들어 최적 비닝을 수행한다.\n\n\nbreast_cancer %>% \n  mutate(diagnosis_new = ifelse(diagnosis == 'M', 0, 1)) %>% \n    binning_by(diagnosis_new, radius_mean) -> bin_radius_mean\n\n\n` diagnosis_new ` ~ ` radius_mean `\n<environment: 0x7f86e9d37b30>\n\npositive 클래스와 negative 클래스가 바뀌어도 결과는 동일하다.\n\n최적비닝만을 목적으로 한다면, 굳이 클래스를 변환하지 않더라도 비닝된 결과는 동일하므로 이진 factor일 경우에는 함수가 자동으로 0과 1로 변환하도록 해도 무방하다. 즉, 최적 비닝을 수행하는 용도라면 positive 클래스와 negative 클래스를 고민할 필요가 없다.\n\n\n\nbin_radius_mean\n\n\nbinned type: optimal\nnumber of bins: 5\nx\n[6.98,11.8] (11.8,13.1]   (13.1,15]   (15,16.8] (16.8,28.1] \n        150         115         132          54         118 \n\n비닝 결과의 요약\n비닝 결과로 다음 분할표를 유추할 수 있다.\n\n우량(양성종양)\n불량(악성종양)\n합계\n[6.98,11.8]\na\nb\na + b = 150\n(11.8,13.1]\nc\nd\nc + d = 115\n(13.1,15]\ne\nf\ne + f = 132\n(15,16.8]\ng\nh\ng + h = 54\n(16.8,28.1]\ni\nj\ni + j = 118\n분할표의 각 셀의 분포를 알기 위해서는 다음의 명령을 수행한다.\n\n\ntable(bin_radius_mean, breast_cancer$diagnosis)\n\n\n               \nbin_radius_mean   B   M\n    [6.98,11.8] 147   3\n    (11.8,13.1] 105  10\n    (13.1,15]    94  38\n    (15,16.8]    10  44\n    (16.8,28.1]   1 117\n\n\n우량\n불량\n합계\n[6.98,11.8]\n147\n3\n150\n(11.8,13.1]\n105\n10\n115\n(13.1,15]\n94\n38\n132\n(15,16.8]\n10\n44\n54\n(16.8,28.1]\n1\n117\n118\n이제 비닝의 성능을 살펴볼 기초 자료(분할표)가 정리되었다.\n비닝의 진단\nsummary() 함수는 비닝된 결과에 대해서 비닝의 성능을 평가할 수 있는 여러 정보를 계산해 준다. 이번에 다룰 내용이 이 정보들에 대한 가벼운 소개다.\n\n\nsummary(bin_radius_mean)\n\n\n── Binning Table ──────────────────────── Several Metrics ── \n          Bin CntRec CntPos CntNeg RatePos RateNeg     Odds      WoE\n1 [6.98,11.8]    150    147      3 0.41176 0.01415 49.00000  3.37067\n2 (11.8,13.1]    115    105     10 0.29412 0.04717 10.50000  1.83023\n3   (13.1,15]    132     94     38 0.26331 0.17925  2.47368  0.38456\n4   (15,16.8]     54     10     44 0.02801 0.20755  0.22727 -2.00275\n5 (16.8,28.1]    118      1    117 0.00280 0.55189  0.00855 -5.28332\n6       Total    569    357    212 1.00000 1.00000  1.68396       NA\n       IV     JSD     AUC\n1 1.34023 0.11657 0.00291\n2 0.45197 0.04973 0.02636\n3 0.03233 0.00402 0.15012\n4 0.35957 0.03868 0.20406\n5 2.90100 0.18344 0.55111\n6 5.08509 0.39243 0.93457\n\n── General Metrics ───────────────────────────────────────── \n• Gini index                       :  0.86914\n• IV (Jeffrey)                     :  5.08509\n• JS (Jensen-Shannon) Divergence   :  0.39243\n• Kolmogorov-Smirnov Statistics    :  0.72862\n• HHI (Herfindahl-Hirschman Index) :  0.21617\n• HHI (normalized)                 :  0.02022\n• Cramer's V                       :  0.79818 \n\n── Significance Tests ──────────────────── Chisquare Test ── \n        Bin A       Bin B statistics      p_value\n1 [6.98,11.8] (11.8,13.1]   6.255713 1.237934e-02\n2 (11.8,13.1]   (13.1,15]  15.846398 6.869719e-05\n3   (13.1,15]   (15,16.8]  43.166738 5.026802e-11\n4   (15,16.8] (16.8,28.1]  19.324617 1.102754e-05\n\n비닝 결과의 활용\n진단한 결과를 분석해서, 비닝된 변수를 사용하기로 결정되었다면 extract() 함수로 비닝 결과를 추출한다.\n\n\nbreast_cancer %>% \n  mutate(bin_radius_mean = extract(bin_radius_mean)) %>% \n  select(bin_radius_mean) %>% \n  head(10)\n\n\n   bin_radius_mean\n1      (16.8,28.1]\n2      (16.8,28.1]\n3      (16.8,28.1]\n4      [6.98,11.8]\n5      (16.8,28.1]\n6      (11.8,13.1]\n7      (16.8,28.1]\n8        (13.1,15]\n9      (11.8,13.1]\n10     (11.8,13.1]\n\n비닝 성능 진단 메트릭\n최적 비닝의 성능은 최적 비능의 알고리즘보다는 데이터의 분포에 영향을 받는다. 비닝하려는 연속형 변수의 크기가 이진 클래스의 종속변수와 관계가 없다면, 최적 비닝 알고리즘은 비닝을 수행하지 않는다. 예를 들어 나이(나이는 이산형 변수이지만, 년단위가 아닌 시간이나 일 단위의 나이로 확장해 보자.)와 성별의 관계를 보면, 나이가 많을수록(연속형 변수 값이 클수록) 남성이나 여성으로 분류되지는 않는다. 그러므로 성별이라는 종속변수를 대상으로 한 나이 변수의 비닝은 의미없다.\nbinning_by() 함수는 최적 비닝의 유의성을 검사한 후 의미있는 경우에만 최적 비닝을 수행한다. 다음 가상 데이터에 대한 최적 비닝은 수행되지 않는다. 변수의 비닝이 의미없기 때문이다.\n\n\nset.seed(10)\nx <- sample(100)\n\nset.seed(20)\ny <- sample(0:1, size = 100, replace = TRUE)\n\ndata.frame(x, y) %>% \n  binning_by(y, x) \n\n\n` y ` ~ ` x `\n<environment: 0x7f86db7fac30>\n[1] \"No significant splits\"\nattr(,\"class\")\n[1] \"optimal_bins\" \"character\"   \n\n앞선 예제에서 summary() 함수는 비닝된 결과의 성능을 진단하는 여러 지표를 요약함을 알았다. 그러나 다음에 보여주는 성능 지표에 대한 설명은 summary() 함수의 결과를 해석하는 것이 아니라 최대한으로 이론적인 설명과 계산하는 방법에 초점을 맞춘다. 그러면, binning_by() 함수로 만들어진 optimal_bins 클래스 객체에 대한 이해가 필요하다.\n\n\nclass(bin_radius_mean)\n\n\n[1] \"optimal_bins\" \"bins\"         \"ordered\"      \"factor\"      \n\n이 클래스 객체 여러 속성(attributes)을 가지고 있는데, 대표적인 속성이 performance라는 성능 테이블이다. performance을 이용해서 summary() 함수를 구현한 것이다.\n\n\nattributes(bin_radius_mean)\n\n\n$levels\n[1] \"[6.98,11.8]\" \"(11.8,13.1]\" \"(13.1,15]\"   \"(15,16.8]\"  \n[5] \"(16.8,28.1]\"\n\n$class\n[1] \"optimal_bins\" \"bins\"         \"ordered\"      \"factor\"      \n\n$type\n[1] \"optimal\"\n\n$breaks\n[1]  6.981 11.750 13.080 15.040 16.840 28.110\n\n$raw\n  [1] 17.990 20.570 19.690 11.420 20.290 12.450 18.250 13.710 13.000\n [10] 12.460 16.020 15.780 19.170 15.850 13.730 14.540 14.680 16.130\n [19] 19.810 13.540 13.080  9.504 15.340 21.160 16.650 17.140 14.580\n [28] 18.610 15.300 17.570 18.630 11.840 17.020 19.270 16.130 16.740\n [37] 14.250 13.030 14.990 13.480 13.440 10.950 19.070 13.280 13.170\n [46] 18.650  8.196 13.170 12.050 13.490 11.760 13.640 11.940 18.220\n [55] 15.100 11.520 19.210 14.710 13.050  8.618 10.170  8.598 14.250\n [64]  9.173 12.680 14.780  9.465 11.310  9.029 12.780 18.940  8.888\n [73] 17.200 13.800 12.310 16.070 13.530 18.050 20.180 12.860 11.450\n [82] 13.340 25.220 19.100 12.000 18.460 14.480 19.020 12.360 14.640\n [91] 14.620 15.370 13.270 13.450 15.060 20.260 12.180  9.787 11.600\n[100] 14.420 13.610  6.981 12.180  9.876 10.490 13.110 11.640 12.360\n[109] 22.270 11.340  9.777 12.630 14.260 10.510  8.726 11.930  8.950\n[118] 14.870 15.780 17.950 11.410 18.660 24.250 14.500 13.370 13.850\n[127] 13.610 19.000 15.100 19.790 12.190 15.460 16.160 15.710 18.450\n[136] 12.770 11.710 11.430 14.950 11.280  9.738 16.110 11.430 12.900\n[145] 10.750 11.900 11.800 14.950 14.440 13.740 13.000  8.219  9.731\n[154] 11.150 13.150 12.250 17.680 16.840 12.060 10.900 11.750 19.190\n[163] 19.590 12.340 23.270 14.970 10.800 16.780 17.470 14.970 12.320\n[172] 13.430 15.460 11.080 10.660  8.671  9.904 16.460 13.010 12.810\n[181] 27.220 21.090 15.700 11.410 15.280 10.080 18.310 11.710 11.810\n[190] 12.300 14.220 12.770  9.720 12.340 14.860 12.910 13.770 18.080\n[199] 19.180 14.450 12.230 17.540 23.290 13.810 12.470 15.120  9.876\n[208] 17.010 13.110 15.270 20.580 11.840 28.110 17.420 14.190 13.860\n[217] 11.890 10.200 19.800 19.530 13.650 13.560 10.180 15.750 13.270\n[226] 14.340 10.440 15.000 12.620 12.830 17.050 11.320 11.220 20.510\n[235]  9.567 14.030 23.210 20.480 14.220 17.460 13.640 12.420 11.300\n[244] 13.750 19.400 10.480 13.200 12.890 10.650 11.520 20.940 11.500\n[253] 19.730 17.300 19.450 13.960 19.550 15.320 15.660 15.530 20.310\n[262] 17.350 17.290 15.610 17.190 20.730 10.600 13.590 12.870 10.710\n[271] 14.290 11.290 21.750  9.742 17.930 11.890 11.330 18.810 13.590\n[280] 13.850 19.160 11.740 19.400 16.240 12.890 12.580 11.940 12.890\n[289] 11.260 11.370 14.410 14.960 12.950 11.850 12.720 13.770 10.910\n[298] 11.760 14.260 10.510 19.530 12.460 20.090 10.490 11.460 11.600\n[307] 13.200  9.000 13.500 13.050 11.700 14.610 12.760 11.540  8.597\n[316] 12.490 12.180 18.220  9.042 12.430 10.250 20.160 12.860 20.340\n[325] 12.200 12.670 14.110 12.030 16.270 16.260 16.030 12.980 11.220\n[334] 11.250 12.300 17.060 12.990 18.770 10.050 23.510 14.420  9.606\n[343] 11.060 19.680 11.710 10.260 12.060 14.760 11.470 11.950 11.660\n[352] 15.750 25.730 15.080 11.140 12.560 13.050 13.870  8.878  9.436\n[361] 12.540 13.300 12.760 16.500 13.400 20.440 20.200 12.210 21.710\n[370] 22.010 16.350 15.190 21.370 20.640 13.690 16.170 10.570 13.460\n[379] 13.660 11.080 11.270 11.040 12.050 12.390 13.280 14.600 12.210\n[388] 13.880 11.270 19.550 10.260  8.734 15.490 21.610 12.100 14.060\n[397] 13.510 12.800 11.060 11.800 17.910 11.930 12.960 12.940 12.340\n[406] 10.940 16.140 12.850 17.990 12.270 11.360 11.040  9.397 14.990\n[415] 15.130 11.890  9.405 15.500 12.700 11.160 11.570 14.690 11.610\n[424] 13.660  9.742 10.030 10.480 10.800 11.130 12.720 14.900 12.400\n[433] 20.180 18.820 14.860 13.980 12.870 14.040 13.850 14.020 10.970\n[442] 17.270 13.780 10.570 18.030 11.990 17.750 14.800 14.530 21.100\n[451] 11.870 19.590 12.000 14.530 12.620 13.380 11.630 13.210 13.000\n[460]  9.755 17.080 27.420 14.400 11.600 13.170 13.240 13.140  9.668\n[469] 17.600 11.620  9.667 12.040 14.920 12.270 10.880 12.830 14.200\n[478] 13.900 11.490 16.250 12.160 13.900 13.470 13.700 15.730 12.450\n[487] 14.640 19.440 11.680 16.690 12.250 17.850 18.010 12.460 13.160\n[496] 14.870 12.650 12.470 18.490 20.590 15.040 13.820 12.540 23.090\n[505]  9.268  9.676 12.220 11.060 16.300 15.460 11.740 14.810 13.400\n[514] 14.580 15.050 11.340 18.310 19.890 12.880 12.750  9.295 24.630\n[523] 11.260 13.710  9.847  8.571 13.460 12.340 13.940 12.070 11.750\n[532] 11.670 13.680 20.470 10.960 20.550 14.270 11.690  7.729  7.691\n[541] 11.540 14.470 14.740 13.210 13.870 13.620 10.320 10.260  9.683\n[550] 10.820 10.860 11.130 12.770  9.333 12.880 10.290 10.160  9.423\n[559] 14.590 11.510 14.050 11.200 15.220 20.920 21.560 20.130 16.600\n[568] 20.600  7.760\n\n$performance\n          Bin CntRec CntPos CntNeg CntCumPos CntCumNeg RatePos\n1 [6.98,11.8]    150    147      3       147         3 0.41176\n2 (11.8,13.1]    115    105     10       252        13 0.29412\n3   (13.1,15]    132     94     38       346        51 0.26331\n4   (15,16.8]     54     10     44       356        95 0.02801\n5 (16.8,28.1]    118      1    117       357       212 0.00280\n6       Total    569    357    212        NA        NA 1.00000\n  RateNeg RateCumPos RateCumNeg     Odds   LnOdds      WoE      IV\n1 0.01415    0.41176    0.01415 49.00000  3.89182  3.37067 1.34023\n2 0.04717    0.70588    0.06132 10.50000  2.35138  1.83023 0.45197\n3 0.17925    0.96919    0.24057  2.47368  0.90571  0.38456 0.03233\n4 0.20755    0.99720    0.44811  0.22727 -1.48160 -2.00275 0.35957\n5 0.55189    1.00000    1.00000  0.00855 -4.76217 -5.28332 2.90100\n6 1.00000         NA         NA  1.68396  0.52115       NA 5.08509\n      JSD     AUC\n1 0.11657 0.00291\n2 0.04973 0.02636\n3 0.00402 0.15012\n4 0.03868 0.20406\n5 0.18344 0.55111\n6 0.39243 0.93457\n\n$target\n  [1] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0\n [33] 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1\n [65] 0 0 1 1 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0\n [97] 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0\n[129] 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1\n[161] 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 1 0 1 1 1 0 1\n[193] 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1 1 0\n[225] 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0\n[257] 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1\n[289] 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n[321] 1 0 1 0 1 1 1 1 0 0 0 1 1 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0\n[353] 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0 0 1 0 0 1 1 1 1 1 0 1 1 1 1\n[385] 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n[417] 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1\n[449] 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0\n[481] 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1\n[513] 0 1 0 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1\n[545] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1\n\n오즈\n오즈(odds)는 해당 bins별 불량 1건당 우량이 몇 건인지를 나타내는 불량 클래스 대비 우량 클래스 비율을 나타낸다. 그러므로 오즈는 1을 기준으로 1을 초과하면 우량 비율이, 1 미만이면 불량 비율이 높다는 것을 의미한다. 그 값이 1에서 멀어질수록 해당 bins에서 우량 또는 불량의 구분이 명확하다는 것을 의미한다. n개의 bins에 대한 odds의 산식은 다음과 같다. \\(Npos\\)와 \\(Nneg\\)는 각각 우량 건수와 불량 건수를 의미한다.\n\\(odds_i = Npos_i / Nneg_i, i = 1, 2, \\cdots, n\\)\n[6.98,11.8]에서의 오즈가 49로 가장 크다. 반대로 (16.8,28.1]에서의 오즈가 0에 가깝다.\n\n\nbin_radius_mean %>% \n  attr(\"performance\") %>% \n  select(Bin, CntPos, CntNeg, Odds)\n\n\n          Bin CntPos CntNeg     Odds\n1 [6.98,11.8]    147      3 49.00000\n2 (11.8,13.1]    105     10 10.50000\n3   (13.1,15]     94     38  2.47368\n4   (15,16.8]     10     44  0.22727\n5 (16.8,28.1]      1    117  0.00855\n6       Total    357    212  1.68396\n\nWOE\nWeight of Evidence는 각 bins가 우량과 불량을 판별하는데 얼마나 많은 정보를 주고 있는가를 나타낸다. bins별 WOE의 값이 커지면 커질수록 해당 범주형 변수의 예측능력은 더욱더 높아짐을 의미한다.\n\\(WOE_i = log \\left( \\frac {Npos_i/\\sum_{i=1}^N Npos_i}{Nneg_i / \\sum_{i=1}^N Nneg_i} \\right), i = 1, 2, \\cdots, n\\)\n\n\nbin_radius_mean %>% \n  attr(\"performance\") %>% \n  select(Bin, CntPos, CntNeg, Odds, WoE)\n\n\n          Bin CntPos CntNeg     Odds      WoE\n1 [6.98,11.8]    147      3 49.00000  3.37067\n2 (11.8,13.1]    105     10 10.50000  1.83023\n3   (13.1,15]     94     38  2.47368  0.38456\n4   (15,16.8]     10     44  0.22727 -2.00275\n5 (16.8,28.1]      1    117  0.00855 -5.28332\n6       Total    357    212  1.68396       NA\n\n비닝이 의미가 있기 위해서는 ordered factor로 표현된 bins의 순서대로 WOE는 단조증가 하거나 단조감소해야 한다. 이 최적 비닝 예제에서 bins의 순서대로 WOE 분포는 다음 플롯처럼 단조 감소한다.\n\n\nplot(bin_radius_mean, type = \"WoE\")\n\n\n\n\nInformation Value\nInformation Value(IV)는 모형의 예측력에 변수가 기여하는 정도를 가늠하는 측도다. 이 값은 bins별로 측정하는 것이 아닌, 변수 레벨로 측정하는 측도이므로 다음의 식으로 구한다. bins별로 계산후 합산하여 변수의 IV를 계산한다.\n\\(IV_i = \\left( {Npos_i/\\sum_{i=1}^N Npos_i} \\right) - \\left( {Nneg_i / \\sum_{i=1}^N Nneg_i} \\right) \\times WOE_i ,i = 1, 2, \\cdots, n\\)\n\\(IV = \\sum_{i=1}^NIV_i, i = 1, 2, \\cdots, n\\)\n통상적으로 IV에 따른 변수의 예측력은 다음과 같다.\n\\(IV <= 0.02\\) : 예측불능\n\\(0.02 < IV <= 0.1\\) : 약한 예측력\n\\(0.1 < IV <= 0.3\\) : 중간 예측력\n\\(0.3 < IV\\) : 강한 예측력\n\n\nbin_radius_mean %>% \n  attr(\"performance\") %>% \n  select(Bin, CntPos, CntNeg, Odds, WoE, IV)\n\n\n          Bin CntPos CntNeg     Odds      WoE      IV\n1 [6.98,11.8]    147      3 49.00000  3.37067 1.34023\n2 (11.8,13.1]    105     10 10.50000  1.83023 0.45197\n3   (13.1,15]     94     38  2.47368  0.38456 0.03233\n4   (15,16.8]     10     44  0.22727 -2.00275 0.35957\n5 (16.8,28.1]      1    117  0.00855 -5.28332 2.90100\n6       Total    357    212  1.68396       NA 5.08509\n\nKolmogorov-Smirnov statistics(K-S 통계량)\n스코어카드 모델의 변별력을 나타내는 지표로서 구간별 우량 누적비율에서 불량 누적비율을 뺀 값 중 가장 큰 값을 의미한다. 비닝된 변수의 변별력을 평가할 수도 있다. K-S 통계량이 클수록 우량 혹은 negative를 판별하는 모형으로서 의미가 있으며, 다음 수식으로 구한다.\nK-S 통계량 = \\(max |우량 누적비율_i - 불량 누적비율_i|, i = 1, 2, \\cdots, n\\)\n다음처럼 K-S 통계량이 0.2 이상일 경우에 모형(혹은 비닝된 변수)의 변별력이 확보되는 것으로 판단한다.\n모형 변별력\nK-S 통계량\nMinimum\n0.2\nAverage\n0.3\nVery Good\n0.4\nExcellent\n0.5\n\n\nbin_radius_mean %>% \n  attr(\"performance\") %>% \n  filter(Bin != \"Total\") %>% \n  summarise(KS = max(abs(RateCumPos - RateCumNeg)))\n\n\n       KS\n1 0.72862\n\nROC 곡선과 AUC\nROC(Receiver Operating Characteristic) 곡선은 수평축(x-축)에 우량 누적구성비, 수직축(y-축)에는 불량 누적구성비를 나타낸 곡선으로서 우량과 불량의 Trade-Off 관계를 보여주기 때문에 Trade-Off 곡선이라고도 한다. 모형의 변별력을 나타내는 지표로 사용하며, ROC 곡선 아래의 면적(AUC; Area Under Roc)이 넓을수록 모형의 변별력이 높다. (비닝된 변수의 성능이 높다.)\n\nAUC는 ROC 곡선의 아래쪽 영역으로 이 값이 클수록 모형의 변별력이 큰 것을 의미한다. 다음 플롯으로 AUC의 규모를 파악할 수 있지만, 각 bins별 AUC의 합으로 계산할 수도 있다.\n\n\n\nlibrary(ggplot2)\n\n#  시작점인 원점(RateCumNeg = 0, RateCumPos = 0)을 추가하여 그린다.\ndata.frame(RateCumNeg = 0, RateCumPos = 0) %>% \n  bind_rows(bin_radius_mean %>% \n              attr(\"performance\") %>% \n              filter(Bin != \"Total\") %>% \n              select(RateCumNeg, RateCumPos)) %>% \n  ggplot(aes(x = RateCumNeg, y = RateCumPos)) +\n  geom_point(color = \"red\") +\n  geom_line(color = \"red\") +\n  ylim(0, 1)\n\n\n\n\nbins별 AUC의 합은 다음과 같이 계산한다. 아니면 Total 레코드의 값을 가져와도 된다.\n\n\nbin_radius_mean %>% \n  attr(\"performance\") %>% \n  filter(Bin != \"Total\") %>% \n  summarise(sum(AUC)) %>% \n  pull\n\n\n[1] 0.93456\n\n로렌츠 곡선과 지니계수\n로렌츠(Lorenz) 곡선은 수평축(x-축)에 불량 누적구성비, 수직축(y-축)에는 우량 누적구성비를 나타낸 곡선이다. 로렌츠 곡선은 동일한 불량누적 구성비에서 해당 우량 누적구성비가 낮으면 모형의 변별력이 높다.\n\n\n#  시작점인 원점(RateCumNeg = 0, RateCumPos = 0)을 추가하여 그린다.\ndata.frame(RateCumNeg = 0, RateCumPos = 0) %>% \n  bind_rows(bin_radius_mean %>% \n              attr(\"performance\") %>% \n              filter(Bin != \"Total\") %>% \n              select(RateCumNeg, RateCumPos)) %>% \n  ggplot(aes(y = RateCumNeg, x = RateCumPos)) +\n  geom_point(color = \"red\") +\n  geom_line(color = \"red\") +\n  geom_abline(intercept = 0, slope = 1) +\n  annotate(\"text\", x = 0.625, y = 0.375, label = \"A\", size = 7) + \n  annotate(\"text\", x = 0.875, y = 0.063, label = \"B\", size = 7) + \n  xlim(0, 1)\n\n\n\n\n선형직선과 로렌츠 곡선 사이의 면적을 이용해서 지니계수(Gini Coefficient)를 구한다. 그림에서 선형직선과 로렌츠 곡선 사이의 면적 A, 지니계수 아래의 면적을 B라고 하면 지니계수는 \\(\\frac{A}{A + B}\\) 인데 \\(2 \\times AUC - 1\\)로 구한다. 그림에서 면적 A가 넓으면 지니계수도 커지는데 면적 A가 넓다는 것은 불량누적비가 높아질 때, 우량누적비의 증가량은 낮게 증가한다는 의미다.\n다음처럼 지니계수가 0.25 이상일 경우에 모형(혹은 비닝된 변수)의 변별력이 확보되는 것으로 판단한다.\n모형 변별력\n지니계수\nMinimum\n0.25\nAverage\n0.35\nVery Good\n0.5\nExcellent\n0.6\n카이스퀘어 통계량\n예제의 bins의개수는 \\(bin_1, bin_2, \\cdots, bin_5\\) 5개다. 이것을 순차적으로 2개의 bins로 카이스퀘어 검정을 수행할 수 있다. 즉, 4개의 쌍\\((bin_1, bin_2), (bin_2, bin_3), (bin_3, bin_4), (bin_4, bin_5)\\)에 대해서 다음의 가설을 수립한다.\n\\(H_0\\) : \\((bin_i, bin_{i+1})\\) 변수와 종속변수(악성종양 여부)와는 독립이다.\\(H_1\\) : \\((bin_i, bin_{i+1})\\) 변수와 종속변수(악성종양 여부)와는 독립이 아니다.\n앞의 summary() 함수의 결과를 보면 네 개 검정의 p-value가 모두 0.05보다 작았다. 즉, 나누어진 bins들은 유의하게 비닝된 것이다.\n결언\n스코어카드를 만들 때 사용하는 최적 비닝 방법과 대표적인 스코어카드의 성능 지표에 대해서 살펴 보았다. 스코어카드를 개발하거나, 비닝 필요한 경우 dlookr의 최적 비닝을 활용해 보기 바란다.\n\n\n\n",
    "preview": "posts/2021-02-07-optimal_binning/img/increase.png",
    "last_modified": "2021-10-11T10:17:15+09:00",
    "input_file": {},
    "preview_width": 1422,
    "preview_height": 697
  },
  {
    "path": "posts/2021-02-06-binning/",
    "title": "비닝(Binning)",
    "description": "모든 경우는 아니지만 연속형 변수를 비닝하면, 데이터의 분포에 따라서 이상치(outliers)로 발생할 수 있는 이슈를 회피하거나 과적합(over fitting)을 완화시켜주기도 한다. 그래서 흔하지 않지만, 가끔은 연속형 변수를 범주형 변수로 변환할 필요가 있다. 특히 금융권에서 사용하는 스코어카드 기법에서는 연속형 변수를 범주형 변수로 변환하는 작업을 빈번히 사용한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-02-06",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n다룰 이야기\n비닝(Binning)\ncut() 함수로 비닝하기\n\ndlookr을 이용한 비닝\n예제 데이터 생성\n비닝하기\n비닝 결과의 추출\ndplyr 패키지와의 협업\n그 밖의 기능\n\n\n다룰 이야기\n연속형 변수가 범주화되면 디테일한 데이터의 정보가 사라진다. 예를 들면 시험성적이 96점인 학생의 A 학점은 90점으로 A 학점을 받은 학생과 동일하게 인식되지만, 두 학생의 성적 우열은 엄연히 존재한다.\n모든 경우는 아니지만 연속형 변수를 비닝하면, 데이터의 분포에 따라서 이상치(outliers)로 발생할 수 있는 이슈를 회피하거나 과적합(over fitting)을 완화시켜주기도 한다. 그래서 흔하지 않지만, 가끔은 연속형 변수를 범주형 변수로 변환할 필요가 있다. 특히 금융권에서 사용하는 스코어카드 기법에서는 연속형 변수를 범주형 변수로 변환하는 작업을 빈번히 사용한다.\n\ndlookr 패키지에는 일반적인 비닝과 스코어링 모델(Scoring Modeling)을 위한 Optimal Binning 기능을 지원한다. 여기서는 일반적인 방법으로서의 비닝에 대해서만 다룬다.\n\n비닝(Binning)\n비닝(Binning)을 쉽게 이해하려면 히스토그램을 떠올리면 된다. 다음은 iris 데이터 중 연속형 변수를 히스토그램으로 표현한 것이다. 연속형 변수를 일정한 간격으로 나누어 계급(class)을 만든 후 해당하는 계급의 구간(class interval)에 몇 개의 관측치가 포함되는지 집계한 것이 도수분포표(frequency table)다. 그리고 계급 구간별로 도수를 막대의 높이로 표현한 것이 히스토그램(histogram)이다. 원래의 데이터를 데이터가 포함된 계급에 매핑하는 범주화하는 이 작업도 비닝의 한 방법이다.\n\n\niris$Sepal.Length\n\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7\n [17] 5.4 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4\n [33] 5.2 5.5 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6\n [49] 5.3 5.0 7.0 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1\n [65] 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7\n [81] 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7\n [97] 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4\n[113] 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1\n[129] 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\nhist_info <- hist(iris$Sepal.Length, col = \"lightblue\")\n\n\n\n\n히스토그램을 그리는 hist() 함수는 유용한 정보를 list로 반환한다. 정보의 반환을 return() 함수가 아닌 invisible() 함수를 사용해서 별도의 이름에 할당한 후 그 정보를 사용할 수 있다. 특히 계급을 정의할 수 있는 계급의 경계는 breaks로, 계급 구간에 포함된 원 데이터의 개수인 도수(frequency)는 counts로 반환되었다. density는 상대도수(relative frequency)를 의미한다.\n\n\nhist_info\n\n\n$breaks\n[1] 4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.5 8.0\n\n$counts\n[1]  5 27 27 30 31 18  6  6\n\n$density\n[1] 0.06666667 0.36000000 0.36000000 0.40000000 0.41333333 0.24000000\n[7] 0.08000000 0.08000000\n\n$mids\n[1] 4.25 4.75 5.25 5.75 6.25 6.75 7.25 7.75\n\n$xname\n[1] \"iris$Sepal.Length\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\ncut() 함수로 비닝하기\nR에서는 cut() 함수로 비닝한다. 그런데 사용자가 계급의 경계인 breaks를 지정해 주어야 한다. 앞에서 수행한 히스토그램의 정보를 활용해 보자.\n\n\nbins <- cut(iris$Sepal.Length, breaks = hist_info$breaks)\n\nbins\n\n\n  [1] (5,5.5] (4.5,5] (4.5,5] (4.5,5] (4.5,5] (5,5.5] (4.5,5] (4.5,5]\n  [9] (4,4.5] (4.5,5] (5,5.5] (4.5,5] (4.5,5] (4,4.5] (5.5,6] (5.5,6]\n [17] (5,5.5] (5,5.5] (5.5,6] (5,5.5] (5,5.5] (5,5.5] (4.5,5] (5,5.5]\n [25] (4.5,5] (4.5,5] (4.5,5] (5,5.5] (5,5.5] (4.5,5] (4.5,5] (5,5.5]\n [33] (5,5.5] (5,5.5] (4.5,5] (4.5,5] (5,5.5] (4.5,5] (4,4.5] (5,5.5]\n [41] (4.5,5] (4,4.5] (4,4.5] (4.5,5] (5,5.5] (4.5,5] (5,5.5] (4.5,5]\n [49] (5,5.5] (4.5,5] (6.5,7] (6,6.5] (6.5,7] (5,5.5] (6,6.5] (5.5,6]\n [57] (6,6.5] (4.5,5] (6.5,7] (5,5.5] (4.5,5] (5.5,6] (5.5,6] (6,6.5]\n [65] (5.5,6] (6.5,7] (5.5,6] (5.5,6] (6,6.5] (5.5,6] (5.5,6] (6,6.5]\n [73] (6,6.5] (6,6.5] (6,6.5] (6.5,7] (6.5,7] (6.5,7] (5.5,6] (5.5,6]\n [81] (5,5.5] (5,5.5] (5.5,6] (5.5,6] (5,5.5] (5.5,6] (6.5,7] (6,6.5]\n [89] (5.5,6] (5,5.5] (5,5.5] (6,6.5] (5.5,6] (4.5,5] (5.5,6] (5.5,6]\n [97] (5.5,6] (6,6.5] (5,5.5] (5.5,6] (6,6.5] (5.5,6] (7,7.5] (6,6.5]\n[105] (6,6.5] (7.5,8] (4.5,5] (7,7.5] (6.5,7] (7,7.5] (6,6.5] (6,6.5]\n[113] (6.5,7] (5.5,6] (5.5,6] (6,6.5] (6,6.5] (7.5,8] (7.5,8] (5.5,6]\n[121] (6.5,7] (5.5,6] (7.5,8] (6,6.5] (6.5,7] (7,7.5] (6,6.5] (6,6.5]\n[129] (6,6.5] (7,7.5] (7,7.5] (7.5,8] (6,6.5] (6,6.5] (6,6.5] (7.5,8]\n[137] (6,6.5] (6,6.5] (5.5,6] (6.5,7] (6.5,7] (6.5,7] (5.5,6] (6.5,7]\n[145] (6.5,7] (6.5,7] (6,6.5] (6,6.5] (6,6.5] (5.5,6]\n8 Levels: (4,4.5] (4.5,5] (5,5.5] (5.5,6] (6,6.5] ... (7.5,8]\n\n\n\ntable(bins)\n\n\nbins\n(4,4.5] (4.5,5] (5,5.5] (5.5,6] (6,6.5] (6.5,7] (7,7.5] (7.5,8] \n      5      27      27      30      31      18       6       6 \n\npretty() 함수는 적당하게 계급의 경계를 구해준다. 이 함수를 이용해서 비닝해 보자.\n\n\npretty(iris$Sepal.Length) \n\n\n[1] 4 5 6 7 8\n\ncut(iris$Sepal.Length, breaks = pretty(iris$Sepal.Length))\n\n\n  [1] (5,6] (4,5] (4,5] (4,5] (4,5] (5,6] (4,5] (4,5] (4,5] (4,5]\n [11] (5,6] (4,5] (4,5] (4,5] (5,6] (5,6] (5,6] (5,6] (5,6] (5,6]\n [21] (5,6] (5,6] (4,5] (5,6] (4,5] (4,5] (4,5] (5,6] (5,6] (4,5]\n [31] (4,5] (5,6] (5,6] (5,6] (4,5] (4,5] (5,6] (4,5] (4,5] (5,6]\n [41] (4,5] (4,5] (4,5] (4,5] (5,6] (4,5] (5,6] (4,5] (5,6] (4,5]\n [51] (6,7] (6,7] (6,7] (5,6] (6,7] (5,6] (6,7] (4,5] (6,7] (5,6]\n [61] (4,5] (5,6] (5,6] (6,7] (5,6] (6,7] (5,6] (5,6] (6,7] (5,6]\n [71] (5,6] (6,7] (6,7] (6,7] (6,7] (6,7] (6,7] (6,7] (5,6] (5,6]\n [81] (5,6] (5,6] (5,6] (5,6] (5,6] (5,6] (6,7] (6,7] (5,6] (5,6]\n [91] (5,6] (6,7] (5,6] (4,5] (5,6] (5,6] (5,6] (6,7] (5,6] (5,6]\n[101] (6,7] (5,6] (7,8] (6,7] (6,7] (7,8] (4,5] (7,8] (6,7] (7,8]\n[111] (6,7] (6,7] (6,7] (5,6] (5,6] (6,7] (6,7] (7,8] (7,8] (5,6]\n[121] (6,7] (5,6] (7,8] (6,7] (6,7] (7,8] (6,7] (6,7] (6,7] (7,8]\n[131] (7,8] (7,8] (6,7] (6,7] (6,7] (7,8] (6,7] (6,7] (5,6] (6,7]\n[141] (6,7] (6,7] (5,6] (6,7] (6,7] (6,7] (6,7] (6,7] (6,7] (5,6]\nLevels: (4,5] (5,6] (6,7] (7,8]\n\ndlookr을 이용한 비닝\n예제 데이터 생성\nISLR 패키지의 Carseats는 유아용 차량 시트 판매에 대한 정보를 담고 있다. 400개의 관측치로 11개의 변수를 가지고 있다.\nIncome은 소득 수준을 나타내며, 그 단위는 1000 US 달러다.\n\n\n# Generate data for the example\ncarseats <- ISLR::Carseats\n\ndim(carseats)\n\n\n[1] 400  11\n\nnames(carseats)\n\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\"\n [5] \"Population\"  \"Price\"       \"ShelveLoc\"   \"Age\"        \n [9] \"Education\"   \"Urban\"       \"US\"         \n\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\ncarseats[sample(seq(NROW(carseats)), 5), \"Urban\"] <- NA\n\ncarseats$Income\n\n\n  [1]  73  48  35 100  64 113 105  81 110 113  78  94  35  28 117  NA\n [17]  32  74 110  76  90  29  46  31 119  32 115 118  74  99  94  58\n [33]  32  38  54  84  76  41  73  60  NA  53  69  42  79  63  90  98\n [49]  52  93  32  90  40  64 103  81  82  91  93  71 102  32  45  88\n [65]  67  26  92  61  69  59  81  NA  45  NA  68 111  87  71  NA  67\n [81] 100  72  83  36  NA 103  84  67  42  66  22  46 113  30  97  25\n [97]  42  82  77  47  69  93  22  91  96 100  33 107  79  65  62 118\n[113]  99  29  87  35  75  53  88  94 105  89  NA 103  NA  78  68  48\n[129] 100 120  NA  69  87  98  31  94  75  42 103  62  60  42  84  88\n[145]  68  63  83  54  NA 120  84  58  78  36  69  72  34  58  90  60\n[161]  28  21  74  64  64  58  67  73  89  41  39  NA 102  91  24  89\n[177] 107  72  71  25 112  83  60  74  33 100  51  32  37 117  37  42\n[193]  26  70  98  93  28  61  80  88  92  83  78  82  80  22  67 105\n[209]  54  21  41 118  69  84 115  83  33  44  61  79 120  44 119  45\n[225]  82  25  33  64  73 104  60  69  80  76  62  32  34  28  24 105\n[241]  80  63  46  25  30  43  56  NA  52  67 105 111  97  24 104  81\n[257]  40  NA  38  36 117  42  77  26  29  35  93  82  57  69  26  56\n[273]  33 106  93 119  69  48 113  57  86  NA  96 110  46  26 118  44\n[289]  NA  77 111  70  66  84  76  35  44  83  63  40  NA  93  77  52\n[305]  98  29  32  92  80 111  65  68 117  81  33  21  36  30  72  45\n[321]  70  39  50 105  65  69  30  38  66  54  NA  63  33  60 117  70\n[337]  35  38  24  44  29 120 102  42  80  68 107  39  NA  27 101 115\n[353] 103  67  31 100 109  73  96  62  86  25  55  75  21  30  56 106\n[369]  NA 100  41  81  NA  71  47  46  60  61  88 111  64  65  28 117\n[385]  37  73 116  73  89  42  75  63  42  51  58 108  23  26  79  37\n\n비닝하기\ndlookr 패키지의 binning() 함수는 연속형 변수를 범주형 변수로 비닝한다. Income을 비닝해 보자. 소득이 10개 소득 구간의 범주로 비닝되었다.\n\n\nlibrary(dlookr)\n\nbin <- binning(carseats$Income)\n\nbin\n\n\nbinned type: quantile\nnumber of bins: 10\nx\n            [21,30]             (30,38]       (38,47.43333] \n                 39                  38                  37 \n(47.43333,61.46667]       (61.46667,69]       (69,76.53333] \n                 38                  45                  31 \n      (76.53333,84]           (84,95.2]          (95.2,107] \n                 41                  35                  40 \n          (107,120]                <NA> \n                 36                  20 \n\nsummary() 함수는 범주화된 구간의 도수분포표(frequency table)를 구해준다.\n\n\nsummary(bin)\n\n\n                levels freq   rate\n1              [21,30]   39 0.0975\n2              (30,38]   38 0.0950\n3        (38,47.43333]   37 0.0925\n4  (47.43333,61.46667]   38 0.0950\n5        (61.46667,69]   45 0.1125\n6        (69,76.53333]   31 0.0775\n7        (76.53333,84]   41 0.1025\n8            (84,95.2]   35 0.0875\n9           (95.2,107]   40 0.1000\n10           (107,120]   36 0.0900\n11                <NA>   20 0.0500\n\nplot() 함수는 범주화된 구간에 대한 분포를 시각화한다. 플롯 상단은 변형된 히스토그램이다. 일반적으로 히스토그램은 구간의 너비는 동일하고, 높이로 그 밀도를 가늠한다. 그러나 이 변형된 히스토그램은 구간의 너비가 실제 구간의 길이와 동일하게 표현된다. 그러므로 높이가 밀도를 의미하지 않는다. “밀도는 사각형의 너비 \\(\\times\\) 사각형의 높이다.” 즉, 기본 히스토그램보다 더 많은 정보를 표현한다.\n기본에 우리가 알고 있는 히스토그램은 아래의 막대 그래프와 유사하다. 즉, 두 플롯을 이용해서 데이터의 분포를 좀 더 직관적으로 파악할 수 있는 시각화 도구다.\n\n\nplot(bin)\n\n\n\n\n앞에서 소득을 소득 구간으로 비닝할 때 사용한 비닝 방법은 “quantile”이다. 이 방법은 분위수의 간격이 동일하게 구간을 나눈다. 그러므로 구간이 10개이므로 첫째 구간은 0~10% 분위수 구간이다. 즉, 10% 분위수는 30이다. 연산 과정에서 정확하게 딱 떨어지지는 않지만, 범주별로 동일한(거의) 도수를 갖게 된다.\n이번에는 각 구간의 너비(interval)가 동일하게 비닝한다. 간단하게 type 인수값을 “equal”로 지정한다.\n\n\nbin_equal <- binning(carseats$Income, type = \"equal\")\n\nbin_equal\n\n\nbinned type: equal\nnumber of bins: 10\nx\n    [21,30.9]   (30.9,40.8]   (40.8,50.7]   (50.7,60.6]   (60.6,70.5] \n           39            44            35            30            53 \n  (70.5,80.4]   (80.4,90.3]  (90.3,100.2] (100.2,110.1]   (110.1,120] \n           45            41            35            27            31 \n         <NA> \n           20 \n\nsummary(bin_equal)\n\n\n          levels freq   rate\n1      [21,30.9]   39 0.0975\n2    (30.9,40.8]   44 0.1100\n3    (40.8,50.7]   35 0.0875\n4    (50.7,60.6]   30 0.0750\n5    (60.6,70.5]   53 0.1325\n6    (70.5,80.4]   45 0.1125\n7    (80.4,90.3]   41 0.1025\n8   (90.3,100.2]   35 0.0875\n9  (100.2,110.1]   27 0.0675\n10   (110.1,120]   31 0.0775\n11          <NA>   20 0.0500\n\nplot(bin_equal)\n\n\n\n\n이번에는 각 구간의 너비(interval)가 동일하게 비닝한다. 간단하게 type 인수값을 “equal”로 지정한다.\n\n\nbin_equal <- binning(carseats$Income, type = \"equal\")\n\nbin_equal\n\n\nbinned type: equal\nnumber of bins: 10\nx\n    [21,30.9]   (30.9,40.8]   (40.8,50.7]   (50.7,60.6]   (60.6,70.5] \n           39            44            35            30            53 \n  (70.5,80.4]   (80.4,90.3]  (90.3,100.2] (100.2,110.1]   (110.1,120] \n           45            41            35            27            31 \n         <NA> \n           20 \n\nsummary(bin_equal)\n\n\n          levels freq   rate\n1      [21,30.9]   39 0.0975\n2    (30.9,40.8]   44 0.1100\n3    (40.8,50.7]   35 0.0875\n4    (50.7,60.6]   30 0.0750\n5    (60.6,70.5]   53 0.1325\n6    (70.5,80.4]   45 0.1125\n7    (80.4,90.3]   41 0.1025\n8   (90.3,100.2]   35 0.0875\n9  (100.2,110.1]   27 0.0675\n10   (110.1,120]   31 0.0775\n11          <NA>   20 0.0500\n\nplot(bin_equal)\n\n\n\n\n이번에는 내부적으로 base 패키지의 pretty() 함수를 사용하는 “pretty” 방법이다. 간단하게 type 인수값을 “pretty”로 지정한다. 결과는 데이터에 따라 다르지만, 이번 데이터 사례는 “equal” 방법과 동일한 결과로 비닝한다.\n\n\nbin_pretty <- binning(carseats$Income, type = \"pretty\")\n\nbin_pretty\n\n\nbinned type: pretty\nnumber of bins: 10\nx\n  [20,30]   (30,40]   (40,50]   (50,60]   (60,70]   (70,80]   (80,90] \n       39        44        35        30        53        45        41 \n (90,100] (100,110] (110,120]      <NA> \n       35        27        31        20 \n\nsummary(bin_pretty)\n\n\n      levels freq   rate\n1    [20,30]   39 0.0975\n2    (30,40]   44 0.1100\n3    (40,50]   35 0.0875\n4    (50,60]   30 0.0750\n5    (60,70]   53 0.1325\n6    (70,80]   45 0.1125\n7    (80,90]   41 0.1025\n8   (90,100]   35 0.0875\n9  (100,110]   27 0.0675\n10 (110,120]   31 0.0775\n11      <NA>   20 0.0500\n\nplot(bin_pretty)\n\n\n\n\n“kmenas” 방법은 Kmeans 클러스터링 기법을 이용해서 비닝한다. 하나의 연속형 변수로 Kmeans 클러스터링 후, 같은 클러스터에 포함된 값들을 하나의 구간으로 나눈다. 하나의 변수로 클러스터링하기 때문에 구간이 단조(monotonic, 주어진 순서를 보존하면서 상승/하강) 증가하게 형성된다.\n\n\nbin_kmeans <- binning(carseats$Income, type = \"kmeans\")\n\nbin_kmeans\n\n\nbinned type: kmeans\nnumber of bins: 10\nx\n   [21,33.5]  (33.5,42.5]  (42.5,51.5]  (51.5,60.5]  (60.5,67.5] \n          57           40           23           28           34 \n (67.5,75.5]    (75.5,85]    (85,96.5] (96.5,108.5]  (108.5,120] \n          42           45           38           38           35 \n        <NA> \n          20 \n\nsummary(bin_kmeans)\n\n\n         levels freq   rate\n1     [21,33.5]   57 0.1425\n2   (33.5,42.5]   40 0.1000\n3   (42.5,51.5]   23 0.0575\n4   (51.5,60.5]   28 0.0700\n5   (60.5,67.5]   34 0.0850\n6   (67.5,75.5]   42 0.1050\n7     (75.5,85]   45 0.1125\n8     (85,96.5]   38 0.0950\n9  (96.5,108.5]   38 0.0950\n10  (108.5,120]   35 0.0875\n11         <NA>   20 0.0500\n\nplot(bin_kmeans)\n\n\n\n\ntype 인수값을 “bclust”로 지정하면 bagged clustering을 수행하여 비닝한다.\n\n\nbin_bclust <- binning(carseats$Income, type = \"bclust\")\n\nbin_bclust\n\n\nbinned type: bclust\nnumber of bins: 10\nx\n  [21,31.5] (31.5,36.5]   (36.5,49]   (49,62.5] (62.5,70.5] \n         42          27          48          39          45 \n(70.5,76.5]   (76.5,85]     (85,95]  (95,107.5] (107.5,120] \n         27          41          35          40          36 \n       <NA> \n         20 \n\nsummary(bin_bclust)\n\n\n        levels freq   rate\n1    [21,31.5]   42 0.1050\n2  (31.5,36.5]   27 0.0675\n3    (36.5,49]   48 0.1200\n4    (49,62.5]   39 0.0975\n5  (62.5,70.5]   45 0.1125\n6  (70.5,76.5]   27 0.0675\n7    (76.5,85]   41 0.1025\n8      (85,95]   35 0.0875\n9   (95,107.5]   40 0.1000\n10 (107.5,120]   36 0.0900\n11        <NA>   20 0.0500\n\nplot(bin_bclust)\n\n\n\n\n비닝 결과의 추출\n비닝한 결과를 서머리하거나 시각화한 후 비닝한 결과에 확신이 생긴다면, extract() 함수로 비닝한 데이터를 추출한다. 마지막 예제, bagged clustering 기법의 비닝 결과를 채택하였다고 가정한다.\n\n\ncarseats$Income_bin <- extract(bin_bclust)\n\n\n\n비닝된 모든 관측치를 살펴보기에는 건수가 크니 앞의 20건만 살펴보자.\n\n\nhead(carseats$Income_bin, n = 10)\n\n\n [1] (70.5,76.5] (36.5,49]   (31.5,36.5] (95,107.5]  (62.5,70.5]\n [6] (107.5,120] (95,107.5]  (76.5,85]   (107.5,120] (107.5,120]\n10 Levels: [21,31.5] < (31.5,36.5] < (36.5,49] < ... < (107.5,120]\n\ndplyr 패키지와의 협업\ndlookr 패키지는 dplyr 패키지와 궁합이 잘 맞는다. 다음은 Income을 비닝 후 ShelveLoc 변수의 수준(levels)별로 비닝 구간의 분포를 비교한다.\n\n\nlibrary(dplyr)\n\ncarseats %>%\n mutate(Income_bin = binning(carseats$Income) %>% \n                     extract()) %>%\n group_by(ShelveLoc, Income_bin) %>%\n summarise(freq = n()) %>%\n arrange(ShelveLoc, desc(freq)) \n\n\n# A tibble: 33 x 3\n# Groups:   ShelveLoc [3]\n   ShelveLoc Income_bin           freq\n   <fct>     <ord>               <int>\n 1 Bad       (76.53333,84]          12\n 2 Bad       (47.43333,61.46667]    11\n 3 Bad       (61.46667,69]          11\n 4 Bad       (107,120]              11\n 5 Bad       (38,47.43333]          10\n 6 Bad       (84,95.2]              10\n 7 Bad       (95.2,107]             10\n 8 Bad       (30,38]                 7\n 9 Bad       [21,30]                 5\n10 Bad       (69,76.53333]           5\n# … with 23 more rows\n\ndlookr 패키지의 target_by(), relate(), plot.relate() 함수를 이용하면, dplyr 구문 형태로 쉽게 그 분포를 시각화 할 수 있다.\n\n\ncarseats %>%\n  mutate(Income_bin = binning(carseats$Income) %>% \n           extract()) %>%\n  target_by(ShelveLoc) %>% \n  relate(Income_bin) %>% \n  plot()\n\n\n\n\n그 밖의 기능\nbinning() 함수는 비닝 결과를 조정하는 여러 인수를 지원한다. 하나씩 살펴보자.\nBins 개수 지정하기\nnbins 인수는 비닝된 범주의 개수를 지정한다. 이 인수를 사용하면 알고리즘을 통해 관측치의 개수를 고려해서 적당한 범주로 나눈다. 그러나 경우에 따라서 분석가가 원하는 범주의 개수와 다를 수 있다. 이 경우에 nbins 인수로 범주의 개수를 지정할 수 있다.\n\n\nbin_bclust <- binning(carseats$Income, type = \"bclust\", nbins = 5)\n\nbin_bclust\n\n\nbinned type: bclust\nnumber of bins: 5\nx\n    [21,49]   (49,72.5]   (72.5,85]  (85,106.5] (106.5,120] \n        117          92          60          72          39 \n       <NA> \n         20 \n\nsummary(bin_bclust)\n\n\n       levels freq   rate\n1     [21,49]  117 0.2925\n2   (49,72.5]   92 0.2300\n3   (72.5,85]   60 0.1500\n4  (85,106.5]   72 0.1800\n5 (106.5,120]   39 0.0975\n6        <NA>   20 0.0500\n\nplot(bin_bclust)\n\n\n\n\nordered factor 또는 factor\nbinning() 함수로 만들어진 범주는 R의 ordered factor로 반환된다. 만약 분석가가 ordered factor가 아닌 factor를 원한다면 ordered 인수값에 FALSE를 지정한다. 기본값은 TRUE이기 때문에 앞에서 수행한 모든 예제는 순서 범주형인 ordered factor로 반환되었다.\n\n\nbin_bclust <- binning(carseats$Income, type = \"bclust\")\n\nextract(bin_bclust) %>% \n  head()\n\n\n[1] (67.5,76.5] (39.5,50.5] (27.5,39.5] (95,109.5]  (56.5,67.5]\n[6] (109.5,120]\n10 Levels: [21,27.5] < (27.5,39.5] < (39.5,50.5] < ... < (109.5,120]\n\nbin_bclust <- binning(carseats$Income, type = \"bclust\", ordered = FALSE)\n\nextract(bin_bclust) %>% \n  head()\n\n\n[1] (69.5,79.5] (47.5,56.5] (27.5,35.5] (95,107.5]  (56.5,69.5]\n[6] (107.5,120]\n10 Levels: [21,27.5] (27.5,35.5] (35.5,47.5] ... (107.5,120]\n\n범주 라벨 지정하기\n범주 라벨 이름을 breaks를 이용해서 (34.5,49]와 같이 표현할 수도 았지만, 분석가가 labels 인수를 사용해서 다음처럼 의미를 부여할 수도 있다.\n\n\nbin <- binning(carseats$Income, nbins = 3, labels = c(\"Low\", \"Middle\", \"High\"))\nbin\n\n\nbinned type: quantile\nnumber of bins: 3\nx\n   Low Middle   High   <NA> \n   129    128    123     20 \n\nsummary(bin)\n\n\n  levels freq   rate\n1    Low  129 0.3225\n2 Middle  128 0.3200\n3   High  123 0.3075\n4   <NA>   20 0.0500\n\nplot(bin)\n\n\n\n\nBreaks를 이쁘게 다듬기\n연속형 변수에서 breaks는 일반적으로 소수점으로 만들어진다. 이 경우에는 개별 breaks의 소수점 아래 자리수가 다를 수 있다. 다른 자리수로 만든 범주는 시각적으로 해석하기에 거추장스럽다. 이 경우에는 approxy.lab 인수를 TRUE로 지정하면 비닝 breaks를 근사치로 변환해서 깔끔한 형태로 바꿔준다. 기본값이 TRUE이기 때문에 별도로 지정하지 않아도 된다.\n\n\nbin <- binning(carseats$Income, type = \"bclust\", approxy.lab = FALSE)\nbin\n\n\nbinned type: bclust\nnumber of bins: 10\nx\n   [21,30.5]  (30.5,37.5]  (37.5,50.5]  (50.5,62.5]  (62.5,70.5] \n          39           34           45           38           45 \n (70.5,76.5]    (76.5,85]    (85,96.5] (96.5,107.5]  (107.5,120] \n          27           41           38           37           36 \n        <NA> \n          20 \n\nbin <- binning(carseats$Income, type = \"bclust\", approxy.lab = TRUE)\nbin\n\n\nbinned type: bclust\nnumber of bins: 10\nx\n  [21,31.5] (31.5,37.5] (37.5,50.5] (50.5,59.5] (59.5,65.5] \n         42          31          45          23          31 \n(65.5,75.5]   (75.5,85]     (85,95]  (95,107.5] (107.5,120] \n         52          45          35          40          36 \n       <NA> \n         20 \n\n\n\n\n",
    "preview": "posts/2021-02-06-binning/2021-02-06-binning_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-11T10:15:19+09:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-01-27-rfonts/",
    "title": "추가 폰트 사용하기",
    "description": "R로 그린 플롯에서 우리는 기하학적인 요소로 데이터의 현황을 이해한다. 그런데 라벨이나 제목, 범례, 축의 구간 단위에 표현하는 텍스트도 데이터를 이해하는 데 중요한 역할을 한다. 플롯의 텍스트는 폰트로 그리는데, R에서의 폰트와  폰트를 시각화하는 매커니즘을 다룬다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-01-27",
    "categories": [
      "Visualization"
    ],
    "contents": "\n\nContents\n다룰 이야기\ndlookr\ndlookr 0.3.X vs 0.4.0\n\nR의 폰트 체계\nR의 기본 폰트\n폰트 패밀리와 폰트 페이스\nR에서 폰트 인식하기\n그래픽 전역변수에 폰트 패밀리 지정하기\n\nggplot2에서의 폰트의 사용\nPDF 디바이스\n그래픽 디바이스\nPDF 파일로 플롯 내보내기\nPDF 디바이스 기본 폰트\nPDF 디바이스에 폰트 출력하기\nPDF 디바이스에 폰트 포함하기\n\n마무리 하며\n\n다룰 이야기\nR로 그린 플롯에서 우리는 기하학적인 요소로 데이터의 현황을 이해한다. 그런데 라벨이나 제목, 범례, 축의 구간 단위에 표현하는 텍스트도 데이터를 이해하는 데 중요한 역할을 한다. 플롯의 텍스트는 폰트로 그리는데, R에서의 폰트와 폰트를 시각화하는 매커니즘을 다룬다.\n\nR로 데이터를 시각화하는 것은 참 쉽다. 영어를 사용하는 문화권의 분석가에 한해서다. 한글을 표현하고 싶거나, 한글이 포함된 플롯을 파일로 출력하는 것은 생각보다 쉽지 않다. 기본 폰트가 아닌 확장 폰트를 사용하는 것도 쉬운 작업은 아니다. 기본 환경 그대로 편하게 그릴 것이냐? 기본 환경을 확장해서 어렵게 그릴 것이냐?는 여러분의 수고가 확장된 플롯에서 얻는 효용보다 작을 때 고민하게 바란다.\n\ndlookr\n나는 데이터 시각화에서 한글을 사용하지 않는다. 데이터 분석에서의 데코레이션은 사치이기 때문이다. 그러나 플롯을 분석가가 아닌 일반 대중에서 설명하기 위함이나 단행본을 집필할 경우에는 예외적이지만 그래서 한글의 사용을 제한적으로 사용한다.\n내가 개발한 dlook 패키지는 데이터의 진단과 EDA에 대한 전문 패키지라서 데이터의 시각화를 위한 여러 함수를 제공한다. 그러나 한글 지원에 인색하다. 국내 로컬용 패키지가 아닌 글로벌용 패키지인지라 한글에 대한 지원은 세 개의 레포트 출력 함수에 국한되어 있다.\n“보기 좋은 떡이 먹기도 좋다.”라는 속담이 갑자기 생각난 것은 아니지만, 이제 dlookr 패키지로 생성한 플롯을 분석가가 아닌 일반 대중을 위한 플롯으로 확장할 때가 온 것 같다는 생각이 스쳤다. 그래서 기존 플롯을 ggplot2 패키지를 이용한 플롯으로 변경함과 동시에 출판을 염두에 둔 심미적인 요소를 가미하였다. 그리고 0.4.0 버전의 마이너 릴리즈로 배포하였다.\ndlookr 0.3.X vs 0.4.0\ndlookr 패키지의 0.3.14 버전과 0.4.0 버전의 몇 가지 시각화의 사례를 비교한다.\noptimal binning\n\n\nlibrary(dlookr)\n\n# Generate data for the example\ncarseats <- ISLR::Carseats\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\ncarseats[sample(seq(NROW(carseats)), 5), \"Urban\"] <- NA\n\n# optimal binning using name\nbin <- binning_by(carseats, US, Advertising)\n\n\n` US ` ~ ` Advertising `\n<environment: 0x7f9b8fd6c2d0>\n\nbin\n\n\nbinned type: optimal\nnumber of bins: 3\nx\n[-1,0]  (0,6] (6,29] \n   144     69    187 \n\n# summary optimal_bins class\nsummary(bin)\n\n\n── Binning Table ──────────────────────── Several Metrics ── \n     Bin CntRec CntPos CntNeg RatePos RateNeg    Odds      WoE\n1 [-1,0]    144     19    125 0.07364 0.88028  0.1520 -2.48101\n2  (0,6]     69     54     15 0.20930 0.10563  3.6000  0.68380\n3 (6,29]    187    185      2 0.71705 0.01408 92.5000  3.93008\n4  Total    400    258    142 1.00000 1.00000  1.8169       NA\n       IV     JSD     AUC\n1 2.00128 0.20093 0.03241\n2 0.07089 0.00869 0.01883\n3 2.76272 0.21861 0.00903\n4 4.83489 0.42823 0.06028\n\n── General Metrics ───────────────────────────────────────── \n• Gini index                       :  -0.87944\n• IV (Jeffrey)                     :  4.83489\n• JS (Jensen-Shannon) Divergence   :  0.42823\n• Kolmogorov-Smirnov Statistics    :  0.80664\n• HHI (Herfindahl-Hirschman Index) :  0.37791\n• HHI (normalized)                 :  0.06687\n• Cramer's V                       :  0.81863 \n\n── Significance Tests ──────────────────── Chisquare Test ── \n   Bin A  Bin B statistics      p_value\n1 [-1,0]  (0,6]   87.67064 7.731349e-21\n2  (0,6] (6,29]   34.73349 3.780706e-09\n\noptimal binning 클래스 bin을 시각화 한다.\n\n\n# visualize all information for optimal_bins class\nplot(bin)\n\n\n\n기존 0.3.14 버전에서는 다음과 같이 시각화된다.\n\n\n\n그러나 0.4.0 버전에서는 좀 더 깨끗하게 출력된다. 왼쪽 아래의 barchart도 Bad Rate에서 Percentage of Positive로 마이너하게 바꿨다.\n\n\n\n이상치 분석\n내친 김에 하나 더 비교해 보자.\n다음 예제에서는 이상치 비율이 5% 이상인 수치변수를 찾은 다음 이상치의 평균을 전체 평균으로 나눈 결과를 내림차순으로 반환한다.\n\n\nlibrary(nycflights13)\nlibrary(dplyr)\n\ndiagnose_outlier(flights) %>% \n  filter(outliers_ratio > 5) %>% \n  mutate(rate = outliers_mean / with_mean) %>% \n  arrange(desc(rate)) %>% \n  select(-outliers_cnt)\n\n\n# A tibble: 2 x 6\n  variables outliers_ratio outliers_mean with_mean without_mean  rate\n  <chr>              <dbl>         <dbl>     <dbl>        <dbl> <dbl>\n1 arr_delay           8.28         121.       6.90       -3.69  17.5 \n2 dep_delay          12.8           93.1     12.6         0.444  7.37\n\n그리고 그중 하나를 시각화한다.\n\n\nflights %>%\n  plot_outlier(arr_delay)\n\n\n\n기존 0.3.14 버전에서는 다음과 같이 시각화된다.\n\n\n\n그러나 0.4.0 버전에서는 좀 더 깨끗하게 출력된다.\n\n\n\n어떤가? 기존 플롯보다 많이 깔끔해졌다. 어떤 것이 달라졌는가?\n내가 노안이 깊어진 것은 인정한다. 그래서 폰트의 크기를 키운 것은 아니다. 기하학의 도형 시각화 못지 않게 텍스트의 역할이 좀 더 커진 것이다. 상대적으로 기하학의 도형 시각화 공간이 줄어든 것은 사실이다.\n또 어떤 것이 달라졌는가? 폰트 패밀리(font family)를 Arial Narrow로 변경했다. 글자 크리가 커진 반면 Arial Narrow 폰트로 출력되는 텍스트의 폭을 좀더 줄이면서, 텍스트도 좀 더 이쁘게 출력했다.\nR의 폰트 체계\nArial Narrow 폰트로 시각화하는 것은 기존의 R 그래픽 환경을 수정해 주어야 한다. 이 작업은 R의 폰트 체계를 이해해야 한다.\nR의 기본 폰트\n\n세리프(serif)는 타이포그래피에서 글자의 획 일부 끝이 돌출된 모양을 의미한다. 그리고 세리프가 있는 글자체를 세리프체, 없는 글자체를 산세리프체(sans-serif)라 한다. sans는 없음을 뜻하는 프랑스어의 접두어다. 세리프체는 명조체, 산세리프체는 고딕체(돋움체)를 연상하면 이해하기 쉽다.\n\nR의 폰트 중에서 “sans”, “serif”, “mono”는 시스템(Windows, Linix, macOS) 독립적이라, 공통적으로 사용할 수 있다. 그 중 산세리프체인 “sans”가 기본 폰트다. “serif”는 세리프체이며, “mono”는 모노스페이스(monospace)체다. 이 폰트체는 모든 공백문자를 포함한 모든 글자/문자의 너비가 같다. 즉, 알파벳 i와 G의 너비가 같은 고정폭 폰트체다.\n엄밀하게 말하자면, “sans”, “serif”, “mono”는 폰트가 아니라 폰트 유형을 의미한다. 고딕, 명조, 고정폭 폰트며, 크로스플랫폼(cross-platform) 하에서 운용하기 위해 추상화 해 놓은 폰트 이름이다. 실제로는 운영체제(operating system)에서 추상화한 폰트 이름에 매핑된 실제 폰트가 사용된다. 그러므로 시각화에 표현되는 텍스트의 모양은 운영체제와 사용자의 환경에 따라 다를 수 있다.\n다음의 명령을 MS Windows, Linix, macOS 환경에서 수행해 보았다.\n\n\nplot(1:8, 1:8, type = \"n\", bty = \"n\", xaxt = \"n\", yaxt = \"n\", \n     xlab = \"\", ylab = \"\", pty = \"s\", mar = c(0, 0, 0, 0))\ntext(2, 7, \"Font Test : English, sans\", adj = 0, cex = 2,  family = \"sans\")\ntext(2, 6, \"Font Test : English, serif\", adj = 0, cex = 2, family = \"serif\")\ntext(2, 5, \"Font Test : English, mono\", adj = 0, cex = 2, family = \"mono\")\ntext(2, 4, \"폰트 테스트 : 한글, sans\", adj = 0, cex = 2, family = \"sans\")\ntext(2, 3, \"폰트 테스트 : 한글, serif\", adj = 0, cex = 2, family = \"serif\")\ntext(2, 2, \"폰트 테스트 : 한글, mono\", adj = 0, cex = 2, family = \"mono\")\n\n\n\nMS Windows에서는 다음과 같은 결과를 얻는다.\n\n\n\n결과를 보면, 한글의 경우에는 “sans”, “serif”, “mono”가 같은 폰트로 보인다. 버전에 따라 차이가 있지만, 최근에 사용되는 버전의 MS Windows에서는 영문 폰트로 지정한 한글의 경우 맑은 고딕이 기본으로 설정되어 출력된다.\nMS Windows에서만 지원하는 windowsFonts() 함수로 R에서 사용할 수 있는 기본 폰트를 조회했다. “sans”는 Arial, “serif”는 Times New Roman, “mono”는 Courier New 폰트로 매핑되어 있음을 알 수 있다. 그러므로 넷째 줄에 출력된 한글 폰트 테스트는 “sans”에 해당하는 Arial 폰트를 사용하는데, Arial 폰트는 영문 폰트라 한글 글리프(Glyph)가 정의되어 있지 않다. 그러므로 맑은 고딕 폰트의 글리프를 가져다 출력하는 것이다.\n\n글리프(Glyph)는 폰트에서 글자나 문자 한 자 한 자의 모양을 정의한 자형(字形)을 의미한다. 즉, 시각적인 문자의 표현이다.\n\n\n> windowsFonts()\n$serif\n[1] \"TT Times New Roman\"\n\n$sans\n[1] \"TT Arial\"\n\n$mono\n[1] \"TT Courier New\"\n\nLinux에서는 다음과 같은 결과를 얻는다.\n\n\n\n얼핏 보아도 세리프와 산세리프의 모양이 다른 것을 알 수 있다. 나의 CentOS Linux 도커 환경에서, X11Fonts() 함수로 R에서 사용할 수 있는 기본 폰트를 조회했다. “sans”는 helvetica, “serif”는 times, “mono”는 courier 폰트로 매핑되어 있음을 알 수 있다.\n\n> X11Fonts()\n$serif\n[1] \"-*-times-%s-%s-*-*-%d-*-*-*-*-*-*-*\"\n\n$sans\n[1] \"-*-helvetica-%s-%s-*-*-%d-*-*-*-*-*-*-*\"\n\n$mono\n[1] \"-*-courier-%s-%s-*-*-%d-*-*-*-*-*-*-*\"\n\n$Times\n[1] \"-adobe-times-%s-%s-*-*-%d-*-*-*-*-*-*-*\"\n\n$Helvetica\n[1] \"-adobe-helvetica-%s-%s-*-*-%d-*-*-*-*-*-*-*\"\n\n$CyrTimes\n[1] \"-cronyx-times-%s-%s-*-*-%d-*-*-*-*-*-*-*\"\n\n$CyrHelvetica\n[1] \"-cronyx-helvetica-%s-%s-*-*-%d-*-*-*-*-*-*-*\"\n\n$Arial\n[1] \"-monotype-arial-%s-%s-*-*-%d-*-*-*-*-*-*-*\"\n\n$Mincho\n[1] \"-*-mincho-%s-%s-*-*-%d-*-*-*-*-*-*-*\"\n\n“sans”, “serif”, “mono”별로 어떤 한글 폰트가 매핑되어 있는지 살펴본다.\n/etc/fonts/conf.d 디렉토리의 40-nonlatin.conf, 65-nonlatin.conf 파일에는 라틴계열이 언어가 아닌 비영어권의 폰트 설정이 정의되어 있다. 65-nonlatin.conf 파일을 열어 “sans”, “serif”, “mono”별로 매핑된 한글 폰트 목록을 조회해 보았다. “serif”는 NanumMyeongjo, UnBatang, Baekmuk Batang, 즉, 나눔명조, 은바탕, 백묵바탕 순으로 매핑되어 있다. 그러므로 나눔명조 폰트가 출력된 것이다. “sans”는 NanumGothic, UnDotum, Baekmuk Gulim, 즉, 나눔고딕, 은돋움, 백묵돋움, 백묵굴림 순으로 매핑되어 있다. 그러므로 나눔고딕 폰트가 출력된 것이다. 마찬가지로 “mono”는 나눔고딕코딩 폰트가 출력되었다.\n\n[root@9ead55835a3e conf.d]# pwd\n/etc/fonts/conf.d\n[root@9ead55835a3e conf.d]# grep \"serif\\|mono\\|ko\" 65-nonlatin.conf\n        <family>serif<\/family>\n            <family>NanumMyeongjo<\/family> <!-- hangul (ko) -->\n            <family>UnBatang<\/family> <!-- hangul (ko) -->\n            <family>Baekmuk Batang<\/family> <!-- hangul (ko) -->\n        <family>sans-serif<\/family>\n            <family>NanumGothic<\/family> <!-- hangul (ko) -->\n            <family>UnDotum<\/family> <!-- hangul (ko) -->\n            <family>Baekmuk Dotum<\/family> <!-- hangul (ko) -->\n            <family>Baekmuk Gulim<\/family> <!-- hangul (ko) -->\n        <family>monospace<\/family>\n            <family>NanumGothicCoding<\/family> <!-- hangul (ko) -->\n            <family>NanumGothic<\/family> <!-- hangul (ko) -->\n            <family>UnDotum<\/family> <!-- hangul (ko) -->\n            <family>Baekmuk Dotum<\/family> <!-- hangul (ko) -->\n            <family>Baekmuk Gulim<\/family> <!-- hangul (ko) -->\n[root@9ead55835a3e conf.d]#     \n\nmacOS에서는 다음과 같은 결과를 얻는다.\n\n\n\n다른 운영체제와 달리 macOS에서는 한글이 출력되지 않았다. 이것이 다른 운영체제와 다른 macOS 운영체제에서의 R의 특징이다. 그러나 quartzFonts() 함수로 영문의 경우에는 “sans”는 Helvetica, “serif”는 Times-Roman, “mono”는 Courier 폰트로 매핑되어 있음을 알 수 있다.\n주) 이 글은 일부 Linux와 MS Windows 환경에서 실행한 결과를 포함하지만, 전체적으로는 macOS 운영체제에서 테스트되고 기술하였음을 미리 알려 둔다. 그러나 다른 운영체제에서도 어렵지 않게 수행할 수 있을 것이다.\n\n> quartzFonts()\n$serif\n[1] \"Times-Roman\"      \"Times-Bold\"       \"Times-Italic\"     \"Times-BoldItalic\"\n\n$sans\n[1] \"Helvetica\"             \"Helvetica-Bold\"        \"Helvetica-Oblique\"     \"Helvetica-BoldOblique\"\n\n$mono\n[1] \"Courier\"             \"Courier-Bold\"        \"Courier-Oblique\"     \"Courier-BoldOblique\"\n\nmacOS에서는 한글 기본 폰트를 설정하기 위해서는 다음처럼 quartzFont(), quartzFonts() 함수를 사용한다. 물론 다른 운영체제에서도 가능하다. 다만, MS Windows에서는 windowsFont(), windowsFonts() 함수를, Linux에서는 X11Font(), X11Fonts() 함수를 이용하면 된다.\n다음 예제는 “sans”, “serif”, “mono”별로 애플고딕, 애플명조, 나눔고딕코딩 폰트를 매핑한다. 그런데 이번에는 영문 폰트가 기존의 것과 다르다. 그 이유는 한글 폰트는 영문 글리프도 정의했기 때문이다. 영문 글리프의 개수는 한글에 비에 매우 적기 때문에, 대부분의 한글 폰트는 영문 글리프도 함께 정의한다. 그 이유는 한글과 영문을 혼용할 때 좀더 잘 어울리도록 영문 글리프를 새로이 정의하기 때문이다.\n\n\nquartzFonts(sans = quartzFont(rep(\"AppleGothic\", 4)),\n            serif = quartzFont(rep(\"AppleMyungjo\", 4)),\n            mono = quartzFont(c(\"D2Coding\", \"D2CodingBold\", \"D2Coding\", \"D2CodingBold\")))\n\nplot(1:8, 1:8, type = \"n\", bty = \"n\", xaxt = \"n\", yaxt = \"n\", \n     xlab = \"\", ylab = \"\", pty = \"s\", mar = c(0, 0, 0, 0))\ntext(2, 7, \"Font Test : English, sans\", adj = 0, cex = 2,  family = \"sans\")\ntext(2, 6, \"Font Test : English, serif\", adj = 0, cex = 2, family = \"serif\")\ntext(2, 5, \"Font Test : English, mono\", adj = 0, cex = 2, family = \"mono\")\ntext(2, 4, \"폰트 테스트 : 한글, sans\", adj = 0, cex = 2, family = \"sans\")\ntext(2, 3, \"폰트 테스트 : 한글, serif\", adj = 0, cex = 2, family = \"serif\")\ntext(2, 2, \"폰트 테스트 : 한글, mono\", adj = 0, cex = 2, family = \"mono\")\n\n\n\n폰트의 매핑을 변경하고 출력한 결과를 보니, 한글이 정상적으로 출력되었다.\n\n\n\n\n예제에서는 폰트 매커니즘을 설명하기 위해서 기본 폰트인 “sans”, “serif”, “mono”를 변경하였다. 그러나 이를 변경할 경우에 어떤 side-effect를 발생할 지 모르니 수정하지 않는 것을 권장한다. 그래서 앞서 바꾼 설정을 다시 돌려 놓아야 한다.\n\n\n\nquartzFonts(sans = quartzFont(c(\"Helvetica\", \"Helvetica-Bold\", \"Helvetica-Oblique\", \"Helvetica-BoldOblique\")),\n            serif = quartzFont(c(\"Times-Roman\", \"Times-Bold\", \"Times-Italic\", \"Times-BoldItalic\")),\n            mono = quartzFont(c(\"Courier\", \"Courier-Bold\", \"Courier-Oblique\", \"Courier-BoldOblique\")))\n\n\n\n폰트 패밀리와 폰트 페이스\n\n폰트 패밀리(font family)는 폰트의 고유한 특징을 유지하면서 파생된 폰트들의 집합이다. 대표적으로 글자의 굵기에 따라 기본체(Regular), 굵은체(Bold), 가는체(Light)로 나눌 수 있으며, 기울어진 모양의 이탤릭체(Italic) 등이 폰트 패밀리다.\n\n배포되는 폰트에 따라서 여러 유형의 폰트를 정의한다. 그러나 R은 폰트 패밀리에 다음과 같은 4개의 폰트만 지원한다. 그러므로 4개를 초과하는 폰트를 가진 폰트 패밀리라도 오직 4가지만 사용할 수 있다.  그리고 이것을 폰트 페이스(font faces)라 부른다.\nRegular : 기본 폰트체\nBold : 굵은체\nItalic : 이탤릭체\nBoldItalic : 굵은 이탤릭체\nquartzFont() 함수에서 인수로 사용한 4개의 텍스트가 바로 Regular, Bold, Italic, BoldItalic을 의미한다. 애플고딕 폰트는 오직 기본 폰트체만 배포하기 때문에 모두 동일한 폰트를 지정했다. 그리고 D2Coding 폰트는 기본체와 굵은체 두 가지만 배포하기 때문에 이탤릭체에 두 개를 반복 지정한 것이다.\n다음의 Raleway 폰트 패밀리는 18개의 폰트를 포함하고 있다. 그러나 이 폰트 패밀리도 R에서는 4개 밖에 사용할 수 없다.\n\n\n\nRaleway 폰트 패밀리의 폰트들은 572개 혹은 672개의 글리프로 정의되어 있는 반면에, 나눔고딕 Bold는 20,452개의 글리프로 구성되어 있다. 글리프의 개수가 적은 라틴문자 계열의 폰트는 이처럼 많은 유형의 폰트를 개발할 수 있는 여력이 있다는 장점이 있다.\n\n대부분의 한글 폰트 패밀리는 이탤릭체나 굵은이탤릭체를 정의하지 않는다. 그 이유는 이탤릭체의 사용 빈도가 그리 높지 않기 때문이다. 경제성의 논리다. MS 오피스에서는 기본체(Regular)만 있더라도 애뮬레이터하여, 굵은체, 이탤릭체, 굵은이탤릭체를 표현하지만, R에서는 폰트 패밀리에서 제공하는 폰트 페이스(스타일)만 지원한다.\n\n폰트 패밀리에서 제공하는 여러 폰트 페이스를 사용하기 위해서는 운영체제별로 quartzFont(), quartzFonts() 함수, windowsFont(), windowsFonts() 함수, X11Font(), X11Fonts() 함수를 이용해서 폰트 패밀리와 폰트 페이스를 정의해야 한다. 그렇지 않으면 오직 기본체(Regular)만 사용할 수 있다.\n\n\nquartzFonts(d2code = quartzFont(c(\"D2Coding\", \"D2CodingBold\", \"D2Coding\", \"D2CodingBold\")))\n\npar(mar = c(0, 0, 0, 0))\nplot(1:10, 1:10, type = \"n\", bty = \"n\", xaxt = \"n\", yaxt = \"n\", \n     xlab = \"\", ylab = \"\", pty = \"s\", mar = c(0, 0, 0, 0))\ntext(2, 9, \"Font Test : English, regular\", adj = 0, cex = 2,  family = \"sans\", font = 1)\ntext(2, 8, \"Font Test : English, bold\", adj = 0, cex = 2, family = \"sans\", font = 2)\ntext(2, 7, \"Font Test : English, italic\", adj = 0, cex = 2, family = \"sans\", font = 3)\ntext(2, 6, \"Font Test : English, bold italic\", adj = 0, cex = 2, family = \"sans\", font = 4)\ntext(2, 5, \"폰트 테스트 : 한글, regular\", adj = 0, cex = 2, family = \"d2code\", font = 1)\ntext(2, 4, \"폰트 테스트 : 한글, bold\", adj = 0, cex = 2, family = \"d2code\", font = 2)\ntext(2, 3, \"폰트 테스트 : 한글, italic\", adj = 0, cex = 2, family = \"d2code\", font = 3)\ntext(2, 2, \"폰트 테스트 : 한글, bold italic\", adj = 0, cex = 2, family = \"d2code\", font = 4)\n\n\n\n\n영문은 sans 폰트로 4개의 폰트 페이스를 표현하지만, 한글 D2Coding폰트는 이탤릭 계열의 페이스를 지원하지 못하였다. 그 이유는 D2Coding 폰트는 이탤릭 계열의 폰트를 배포하지 않았기 때문이다.\nR에서 폰트 인식하기\n폰트와 폰트 패밀리의 이름을 알면 R에서 여러 서체로 텍스트를 꾸밀 수 있다. 그러나 우리는 폰트와 폰트 패밀리의 이름을 모두 외울 수도 없고, 정확한 이름을 찾기도 간단치 않다. 앞서 사용한 애플고딕, 애플명조, D2Coding은 서체 관리자에서 조회해서 알아낸 것이다.\nextrafont 패키지의 font_import() 함수는 사용자 시스템에 설치된 트루타입(TrueType) 폰트를 읽어서 R에서 사용할 수 있도록 폰트의 이름과 폰트의 페이스를 등록해 준다. extrafont 패키지는 폰트 정보를 읽어 extrafontdb 패키지에 폰트 데이터베이스를 생성한다. 그리고 폰트 데이터베이스를 통해서 폰트를 쉽게 가져다 쓸 수 있게 도와준다.\n나눔고딕 폰트 정보를 가져오는 예제다. “NanumGothic”에 매치되는 폰트를 가져온다. 만약 pattern 인수를 사용하지 않으면 모든 트루타입 폰트를 가져오느라 시간이 걸릴 것이다. 그래서 나눔고딕 폰트만 가져오려는 것이다. 그런데 font_import() 함수가 정규표현식을 완전하게 지원하지 않아서, 나눔고딕코딩 폰트를 제외하고 가져올 방법이 없었다.\n\nfont_import() 함수의 pattern 인수는 폰트 이름이 아니라, 폰트 파일 이름의 패턴을 검색한다. 예를 들면 MS Windows에서는 Arial Narrow는 검색되지 않을 것이다. 그 이유는 MS Windows에서는 Arial Narrow 폰트의 파일 이름은 “ARIALN”로 표현되기 때문이다. 그러므로 폰트의 파일 이름을 인지하는 것이 필요하다. 아니면 시간이 걸리더라도 pattern 인수를 사용하지 않고, 모든 폰트를 읽어들이는 방법도 고려할 수 있다.\n\n\n\nlibrary(extrafont)\n\n# 사용자 및 시스템 폰트 디렉토리에서 폰트 가져와 등록하기\nfont_import(pattern = \"NanumGothic\", prompt = FALSE)\n\n\n\nextrafontdb 패키지에 등록된 폰트 데이터베이스는 extrafont 패키지의 fonttable() 함수로 조회할 수 있다.\n\n\nlibrary(dplyr)\nlibrary(extrafont)\n\nfonttable() %>% \n  filter(stringr::str_detect(FamilyName, \"NanumGothic\"))\n\n\n  package                       afmfile\n1      NA            NanumGothic.afm.gz\n2      NA        NanumGothicBold.afm.gz\n3      NA      NanumGothicCoding.afm.gz\n4      NA NanumGothicCoding-Bold.afm.gz\n5      NA       NanumGothicLight.afm.gz\n                                                       fontfile\n1            /Users/choonghyunryu/Library/Fonts/NanumGothic.ttf\n2        /Users/choonghyunryu/Library/Fonts/NanumGothicBold.ttf\n3      /Users/choonghyunryu/Library/Fonts/NanumGothicCoding.ttf\n4 /Users/choonghyunryu/Library/Fonts/NanumGothicCoding-Bold.ttf\n5       /Users/choonghyunryu/Library/Fonts/NanumGothicLight.ttf\n                FullName        FamilyName               FontName\n1            NanumGothic       NanumGothic            NanumGothic\n2        NanumGothicBold       NanumGothic        NanumGothicBold\n3      NanumGothicCoding NanumGothicCoding      NanumGothicCoding\n4 NanumGothicCoding Bold NanumGothicCoding NanumGothicCoding-Bold\n5      NanumGothic Light NanumGothic Light       NanumGothicLight\n   Bold Italic Symbol afmsymfile\n1 FALSE  FALSE  FALSE         NA\n2  TRUE  FALSE  FALSE         NA\n3 FALSE  FALSE  FALSE         NA\n4  TRUE  FALSE  FALSE         NA\n5 FALSE  FALSE  FALSE         NA\n\n역시 나눔고딕코딩 폰트 정보도 가져왔다. 그래서 정규표현이 잘 지원되는 str_detect() 함수로 나눔고딕 정보만 조회한다. R에서 지원하지 않는 폰트 페이스인 Light는 폰트이름과 같은 폰트 패밀리로 등록되었고, Regular와 Bold가 NanumGothic 폰트 패밀리도 등록되었다. Bold 변수, Italic 변수로 폰트 페이스를 구분한다.\n\n\nfonttable() %>% \n  filter(stringr::str_detect(FamilyName, \"NanumGothic(?!Coding)\")) %>% \n  select(FamilyName:Italic)\n\n\n         FamilyName         FontName  Bold Italic\n1       NanumGothic      NanumGothic FALSE  FALSE\n2       NanumGothic  NanumGothicBold  TRUE  FALSE\n3 NanumGothic Light NanumGothicLight FALSE  FALSE\n\n이 결과는 다음 스크립트와 동일한 역할을 수행한다.\n\n\nquartzFonts(NanumGothic = quartzFont(c(\"NanumGothic\", \"NanumGothicBold\", \"NanumGothic\", \"NanumGothicBold\")))\n\n\n\nLinux와 macOS에서는 정확한 폰트의 이름만 알면, text() 함수의 family 인수처럼 기본 폰트를 변경할 수 있다. 그러나 MS Windows에서는 windowsFont(), windowsFonts() 함수로 Window 디바이스에서 폰트를 인식할 수 있도록 R에게 알려 주어야 한다. font_import() 함수가 모든 것을 해결해 주지 않는다.\n비트맵 아키텍처를 사용하는 MS Windows 환경에서는 font_import() 함수를 수행한 후에, 추가로 다음 스크립트를 수행해야 해당 폰트를 사용할 수 있다. loadfonts() 함수는 windowsFont(), windowsFonts() 함수의 사용을 쉽게 해주는 역할을 한다.\n\n\nloadfonts(device = \"win\", quiet = TRUE)\n\n\n\n\nextrafont::font_import() 함수는 한번만 수행하면 해당 폰트가 등록되며, extrafont::loadfonts() 함수는 R 세션마다 매번 수행해야 한다. MS Windows 사용자는 주의해야 할 부분이다.\n\n그래픽 전역변수에 폰트 패밀리 지정하기\n앞의 작업은 한글 출력을 위한 완벽한 작업은 아니다. 폰트 패밀리에 한글 폰트를 매핑한 것에 지나지 않는다. 그러므로 폰트를 사용하기 위해서는 text()와 같은 저수준 그래픽 함수(low level graphics function)를 사용할때마다 폰트 패밀리를 지정해야 한다. 매우 번거로운 작업이다. 그래서 par() 함수로 그래픽 전역변수에 폰트 패밀리를 등록한다.\n특히 macOS 환경에서 모든 한글이 포함되는 플롯을 그리는 작업에서 한글을 정상적으로 출력하기 위해서는 전역 그래픽 환경변수에 폰트 패밀리(font family)를 설정해 주어야 한다.\n애플명조를 사용하고 싶다면 다음처럼 par() 함수로 폰트 패밀리에 AppleMyungjo를 지정한다. par() 함수로 설정된 환경변수는 전역변수이기 때문에 R 세션이 종료될 때까지 설정이 유지된다. 즉, R을 종료할 때까지 모든 텍스트는 애플명조 폰트를 사용한다. 다만, 예제의 text() 함수처럼 폰트 패밀리를 지정할 수 있는 인수를 지원한다면, “Courier”나 “NanumGothic” 폰트를 사용할 수도 있다.\n\n\npar(family = \"AppleMyungjo\", mar = c(0, 0, 0, 0))\n\nplot(1:8, 1:8, type = \"n\", bty = \"n\", xaxt = \"n\", yaxt = \"n\", \n     xlab = \"\", ylab = \"\", pty = \"s\", mar = c(0, 0, 0, 0))\ntext(2, 7, \"Font Test : English\", adj = 0, cex = 2)\ntext(2, 6, \"Font Test : English\", adj = 0, cex = 2)\ntext(2, 5, \"Font Test : English\", adj = 0, cex = 2, family = \"Courier\")\ntext(2, 4, \"폰트 테스트 : 한글\", adj = 0, cex = 2)\ntext(2, 3, \"폰트 테스트 : 한글\", adj = 0, cex = 2)\ntext(2, 2, \"폰트 테스트 : 한글\", adj = 0, cex = 2, family = \"NanumGothic\")\n\n\n\n\n다음은 시스템 폰트 디렉토리에 있는 사용자가 설치한 나눔고딕 폰트로 플롯을 출력한다.\n\n\n# 전역 그래픽 환경변수의 폰트 패밀리에 시스템 폰트 디렉토리에 있는 폰트 패밀리 지정\npar(family = \"NanumGothic\")\nplot(rnorm(100), type = \"b\", pch = 16, main = list(\"정규난수의 플로팅 Test\", cex = 2))\n\n\n\n# 전역 그래픽 환경변수의 폰트 패밀리에 시스템 폰트 디렉토리에 없는 폰트 패밀리 지정\n# 폰트 패밀리 이름을 정확하게 입력해야 한다.\npar(family = \"NanumGothic123\")\nplot(rnorm(100), type = \"b\", pch = 16, main = list(\"정규난수의 플로팅 Test\", cex = 2))\n\n\n\n\n마지막 예제처럼 폰트 혹은 폰트 패밀리의 이름을 오타로 잘못 기술하거나, 없는 폰트 패밀리의 이름을 기술하면 다음처럼 경고와 함께 모든 텍스트는 출력되지 않는다. 물론 “NanumGothic”라는 폰트 패밀리는 나의 macOS R 환경에는 존재하지 않는다.\n\n> warnings() %>% head(n = 3)\nWarning messages:\n1: In doTryCatch(return(expr), name, parentenv, handler) :\n  no font could be found for family \"NanumGothic123\"\n2: In doTryCatch(return(expr), name, parentenv, handler) :\n  no font could be found for family \"NanumGothic123\"\n3: In doTryCatch(return(expr), name, parentenv, handler) :\n  no font could be found for family \"NanumGothic123\"\n\nggplot2에서의 폰트의 사용\nggplot2는 R 그래픽의 전역 변수와는 다른 매커니즘이다. 그 근간은 R 그래픽를 따르지만, 독립된 환경을 구사한다. 그러므로 par() 함수로 폰트 패밀리를 지원할 수 없다.\n\n\nlibrary(ggplot2)\n\npar(family = \"NanumGothic\")\n\nggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"차량 클래스별 도수\", y = \"도수\") +\n  theme(plot.title = element_text(size = 20))\n\n\n\n\n결과 플롯을 보면 한글 폰트인 나눔고딕 폰트가 적용되지 못했다. ggplot2 플롯은 테마에서 폰트 관련 설정을 정의한다. 그래서 ggplot의 기본 테마인 theme_gray 테마의 base_family로 폰트 패밀리를 지정해야 한다.\n\n\nggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"차량 클래스별 도수\", y = \"도수\") +\n  theme_gray(base_family = \"NanumGothic\")\n\n\n\n\ntheme_gray 테마의 base_family 설정은 모든 텍스트 요소에 일괄적으로 적용된다. 메인 타이틀과 y-축 라벨의 폰트 패밀리를 달리 지정할 수 있을까? 개별 텍스트 요소에 다른 설정을 적용하기 위해서는 theme() 함수를 사용한다.\n다음은 theme() 함수로 메인 타이틀은 앞에서 “mono”에 매핑한 D2Coding 폰트의 굵은 폰트를 y-축 라벨은 나눔고딕 폰트를 지정한다. 그리고 폰트의 크기도 기본 설정에서 변경하였다.\n\n\nggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"차량 클래스별 도수\", y = \"도수\") +\n  theme(plot.title = element_text(family = \"d2code\", size = 20, face = 2),\n        axis.title.y = element_text(family = \"NanumGothic\", size = 12))\n\n\n\n\nggplot에서도 par() 함수처럼 폰트 설정을 전역변수로 설정하는 방법이 없을까? 매번 플롯을 그릴 때마 폰트 패밀리를 지정해야 할까?\ntheme_set() 함수로 par() 함수처럼 ggplot의 그래픽 환경을 설정할 수 있다.\n\n\ntheme_set(theme_gray(base_family = \"NanumGothic\"))\nggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"차량 클래스별 도수\", y = \"도수\") \n\n\n\n\n이전에 theme_set() 함수로 폰트 패밀리를 지정했기 때문에 한글이 나눔고딕체로 정상 출력된다.\n\n\nggplot(mpg, aes(class, hwy, fill = class)) + \n  geom_boxplot(alpha  = 0.5) + \n  labs(title = \"차량 클래스별 연비\", y = \"연비 (highway miles per gallon)\") + \n  theme(legend.position = \"none\")\n\n\n\n\nPDF 디바이스\n그래픽 디바이스\nR의 그래픽 디바이스(graphics devices)는 플롯이 출력되는 모든 대상을 의미한다. 앞에서 다룬 MS Windows의 windows, Linux(UNIX-like)의 X11, macOS의 quartz는 각각의 운영체제에서의 그래픽 윈도우를 의미한다. 즉, 기본적으로 플롯이 출력되는 화면이다. 이것은 R의 그래픽 디바이스다.\n프린터나, 메타파일, PNG, JPEG, BMP, TIFF와 같은 비트맵(bitmap) 포맷의 그래픽 파일도 그래픽 디바이스다. 또한 SVG(Scalar Vector Graphics), PDF, PostScript 파일도 그래픽 디바이스다.\nPDF 파일로 플롯 내보내기\nPNG, JPEG, BMP, TIFF와 같은 비트맵 포맷의 파일로 윈도우에 표시된 플롯을 내보내는 방법은 쉽기도 하지만 텍스트 표현에 특별한 이슈가 없다. 그 이유는 윈도우에 표시된 플롯을 비트맵 형식으로 전환하기 때문에, 윈도우에 이상 없이 출력된 플롯은 모두 파일로 출력이 가능하다.\n그러나 PostScript 파일, 특히 PDF 파일의 매커니즘은 비트맵 형식의 그래픽 파일과는 다르다. 이 파일 포맷은 폰트로 텍스트를 표현하기 때문에 해당 그래픽 디바이스에 없는 폰트로 만들어진 플롯을 출력할 경우는 다음과 같은 에러가 발생한다.\n\n\npdf(\"mpg.pdf\")\nggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"차량 클래스별 도수\", y = \"도수\") +\n  theme_gray(base_family = \"NanumGothic\")\ndev.off()\n\n\n\n\nError in grid.Call.graphics(C_text, as.graphicsAnnot(x$label), x$x, x$y,  : \n  invalid font type\nIn addition: There were 50 or more warnings (use warnings() to see the first 50)\n\n\n> warnings() %>% head(n = 3)\nWarning messages:\n1: In grid.Call(C_stringMetric, as.graphicsAnnot(x$label)) :\n  font family 'NanumGothic' not found in PostScript font database\n2: In grid.Call(C_stringMetric, as.graphicsAnnot(x$label)) :\n  font family 'NanumGothic' not found in PostScript font database\n3: In grid.Call(C_stringMetric, as.graphicsAnnot(x$label)) :\n  font family 'NanumGothic' not found in PostScript font database\n\nPDF 디바이스 기본 폰트\nPDF 디바이스의 기본 폰트는 다음과 같다. 여기서 한글은 \"Korea1\"과 \"Korea1deb\" 두 개다. 일본어 폰트와 중국어 폰트도 보인다. 또한 이들 폰트들은 비상업용 폰트로 폰트 라이선스에서 자유롭다.\n\n\nlibrary(dplyr)\n\npdfFonts() %>% names()\n\n\n  [1] \"serif\"                       \"sans\"                       \n  [3] \"mono\"                        \"AvantGarde\"                 \n  [5] \"Bookman\"                     \"Courier\"                    \n  [7] \"Helvetica\"                   \"Helvetica-Narrow\"           \n  [9] \"NewCenturySchoolbook\"        \"Palatino\"                   \n [11] \"Times\"                       \"URWGothic\"                  \n [13] \"URWBookman\"                  \"NimbusMon\"                  \n [15] \"NimbusSan\"                   \"URWHelvetica\"               \n [17] \"NimbusSanCond\"               \"CenturySch\"                 \n [19] \"URWPalladio\"                 \"NimbusRom\"                  \n [21] \"URWTimes\"                    \"ArialMT\"                    \n [23] \"Japan1\"                      \"Japan1HeiMin\"               \n [25] \"Japan1GothicBBB\"             \"Japan1Ryumin\"               \n [27] \"Korea1\"                      \"Korea1deb\"                  \n [29] \"CNS1\"                        \"GB1\"                        \n [31] \"Arial Narrow\"                \"NanumGothic\"                \n [33] \"NanumGothicCoding\"           \"NanumGothic Light\"          \n [35] \"Lato Black\"                  \"Lato\"                       \n [37] \"Lato Hairline\"               \"Lato Heavy\"                 \n [39] \"Lato Light\"                  \"Lato Medium\"                \n [41] \"Lato Semibold\"               \"Lato Thin\"                  \n [43] \".SF Compact Rounded\"         \".Keyboard\"                  \n [45] \".New York\"                   \".SF Compact\"                \n [47] \"System Font\"                 \".SF NS Mono\"                \n [49] \".SF NS Rounded\"              \"Academy Engraved LET\"       \n [51] \"Andale Mono\"                 \"AppleMyungjo\"               \n [53] \"Arial Black\"                 \"Arial\"                      \n [55] \"Arial Rounded MT Bold\"       \"Arial Unicode MS\"           \n [57] \"Bodoni Ornaments\"            \"Bodoni 72 Smallcaps\"        \n [59] \"Comic Sans MS\"               \"Courier New\"                \n [61] \"Georgia\"                     \"IBM Plex Sans\"              \n [63] \"IBM Plex Sans ExtraLight\"    \"IBM Plex Sans Light\"        \n [65] \"IBM Plex Sans Medium\"        \"IBM Plex Sans SemiBold\"     \n [67] \"IBM Plex Sans Thin\"          \"Impact\"                     \n [69] \"Khmer Sangam MN\"             \"Komika Axis\"                \n [71] \"Komika Title - Paint\"        \"Lao Sangam MN\"              \n [73] \"LG Smart_H Light\"            \"LG Smart_H Regular\"         \n [75] \"LG Smart_0521_HS98 Regular\"  \"LG Smart_H SemiBold\"        \n [77] \"Liberation Sans Narrow\"      \"Luminari\"                   \n [79] \"MARU Buri Beta\"              \"Microsoft Sans Serif\"       \n [81] \"NanumBarunGothic\"            \"NanumBarunGothic Light\"     \n [83] \"NanumBarunGothic UltraLight\" \"Nanum Brush Script\"         \n [85] \"NanumSquare Bold\"            \"NanumSquare ExtraBold\"      \n [87] \"NanumSquare Light\"           \"NanumSquare\"                \n [89] \"NanumSquareRound Bold\"       \"NanumSquareRound ExtraBold\" \n [91] \"NanumSquareRound Light\"      \"NanumSquareRound Regular\"   \n [93] \"Noto Sans Adlam\"             \"Noto Sans Avestan\"          \n [95] \"Noto Sans Bamum\"             \"Noto Sans Bassa Vah\"        \n [97] \"Noto Sans Batak\"             \"Noto Sans Bhaiksuki\"        \n [99] \"Noto Sans Brahmi\"            \"Noto Sans Buginese\"         \n[101] \"Noto Sans Buhid\"             \"Noto Sans Carian\"           \n[103] \"Noto Sans CaucAlban\"         \"Noto Sans Chakma\"           \n[105] \"Noto Sans Cham\"              \"Noto Sans Coptic\"           \n[107] \"Noto Sans Cuneiform\"         \"Noto Sans Cypriot\"          \n[109] \"Noto Sans Duployan\"          \"Noto Sans EgyptHiero\"       \n[111] \"Noto Sans Elbasan\"           \"Noto Sans Glagolitic\"       \n[113] \"Noto Sans Gothic\"            \"Noto Sans HanifiRohg\"       \n[115] \"Noto Sans Hanunoo\"           \"Noto Sans Hatran\"           \n[117] \"Noto Sans ImpAramaic\"        \"Noto Sans InsPahlavi\"       \n[119] \"Noto Sans InsParthi\"         \"Noto Sans Kaithi\"           \n[121] \"Noto Sans Kayah Li\"          \"Noto Sans Kharoshthi\"       \n[123] \"Noto Sans Khojki\"            \"Noto Sans Khudawadi\"        \n[125] \"Noto Sans Lepcha\"            \"Noto Sans Limbu\"            \n[127] \"Noto Sans Linear A\"          \"Noto Sans Linear B\"         \n[129] \"Noto Sans Lisu\"              \"Noto Sans Lycian\"           \n[131] \"Noto Sans Lydian\"            \"Noto Sans Mahajani\"         \n[133] \"Noto Sans Mandaic\"           \"Noto Sans Manichaean\"       \n[135] \"Noto Sans Marchen\"           \"Noto Sans MeeteiMayek\"      \n[137] \"Noto Sans Mende Kikakui\"     \"Noto Sans Meroitic\"         \n[139] \"Noto Sans Miao\"              \"Noto Sans Modi\"             \n[141] \"Noto Sans Mongolian\"         \"Noto Sans Mro\"              \n[143] \"Noto Sans Multani\"           \"Noto Sans Nabataean\"        \n[145] \"Noto Sans Newa\"              \"Noto Sans NewTaiLue\"        \n[147] \"Noto Sans N'Ko\"              \"Noto Sans Ogham\"            \n[149] \"Noto Sans Ol Chiki\"          \"Noto Sans OldHung\"          \n[151] \"Noto Sans OldNorArab\"        \"Noto Sans Old Permic\"       \n[153] \"Noto Sans OldPersian\"        \"Noto Sans OldSouArab\"       \n[155] \"Noto Sans Old Turkic\"        \"Noto Sans Osage\"            \n[157] \"Noto Sans Osmanya\"           \"Noto Sans Pahawh Hmong\"     \n[159] \"Noto Sans Palmyrene\"         \"Noto Sans PauCinHau\"        \n[161] \"Noto Sans PhagsPa\"           \"Noto Sans Phoenician\"       \n[163] \"Noto Sans PsaPahlavi\"        \"Noto Sans Rejang\"           \n[165] \"Noto Sans Runic\"             \"Noto Sans Samaritan\"        \n[167] \"Noto Sans Saurashtra\"        \"Noto Sans Sharada\"          \n[169] \"Noto Sans Shavian\"           \"Noto Sans Siddham\"          \n[171] \"Noto Sans SoraSomp\"          \"Noto Sans Sundanese\"        \n[173] \"Noto Sans Syloti Nagri\"      \"Noto Sans Syriac\"           \n[175] \"Noto Sans Tagalog\"           \"Noto Sans Tagbanwa\"         \n[177] \"Noto Sans Tai Le\"            \"Noto Sans Tai Tham\"         \n[179] \"Noto Sans Tai Viet\"          \"Noto Sans Takri\"            \n[181] \"Noto Sans Thaana\"            \"Noto Sans Tifinagh\"         \n[183] \"Noto Sans Tirhuta\"           \"Noto Sans Ugaritic\"         \n[185] \"Noto Sans Vai\"               \"Noto Sans Wancho\"           \n[187] \"Noto Sans WarangCiti\"        \"Noto Sans Yi\"               \n[189] \"Noto Serif Ahom\"             \"Noto Serif Balinese\"        \n[191] \"Party LET\"                   \"Public Sans Black\"          \n[193] \"Public Sans\"                 \"Public Sans ExtraBold\"      \n[195] \"Public Sans ExtraLight\"      \"Public Sans Light\"          \n[197] \"Public Sans Medium\"          \"Public Sans SemiBold\"       \n[199] \"Public Sans Thin\"            \"Raleway Black\"              \n[201] \"Raleway\"                     \"Raleway ExtraBold\"          \n[203] \"Raleway ExtraLight\"          \"Raleway Light\"              \n[205] \"Raleway Medium\"              \"Raleway SemiBold\"           \n[207] \"Raleway Thin\"                \"Roboto Condensed\"           \n[209] \"Roboto Condensed Light\"      \"Source Code Pro Black\"      \n[211] \"Source Code Pro\"             \"Source Code Pro ExtraLight\" \n[213] \"Source Code Pro Light\"       \"Source Code Pro Medium\"     \n[215] \"Source Code Pro Semibold\"    \"Source Sans Pro Black\"      \n[217] \"Source Sans Pro\"             \"Source Sans Pro ExtraLight\" \n[219] \"Source Sans Pro Light\"       \"Source Sans Pro SemiBold\"   \n[221] \"Tahoma\"                      \"Times New Roman\"            \n[223] \"Trattatello\"                 \"Trebuchet MS\"               \n[225] \"Verdana\"                     \"Webdings\"                   \n[227] \"Wingdings\"                   \"Wingdings 2\"                \n[229] \"Wingdings 3\"                 \"NanumMyeongjo YetHangul\"    \n\n한글 폰트인 \"Korea1\"는 HY신명조인 HYSMyeongJoStd-Medium-Acro, \"Korea1deb\"는 HY중고딕인 HYGothic-Medium-Acro에 매핑되어 있다.\n\n\npdfFonts()[c(\"Korea1\", \"Korea1deb\")] %>% \n  purrr::map_chr(function(x) x[[\"family\"]])\n\n\n                      Korea1                    Korea1deb \n\"HYSMyeongJoStd-Medium-Acro\"       \"HYGothic-Medium-Acro\" \n\nPDF 디바이스에 폰트 출력하기\nPDF 디바이스에 기본 폰트가 아닌 다른 폰트를 설정하기 위해서는 다음처럼 extrafont 패키지의 font_import() 함수와 loadfonts() 함수를 사용해야 한다. 이 예제는 나눔스퀘어 폰트를 PDF 파일로 출력하기 위한 과정이다.\n\n\n# 사용자 및 시스템 폰트 디렉토리에서 폰트 가져와 등록하기\nfont_import(pattern = \"NanumSquare\", prompt = FALSE)\n\n# 등록한 폰트를 PDF 디바이스에 로드하기\nloadfonts(\"pdf\", quiet = TRUE)\n\n\n\n일단, 원하는 폰트가 PDF 디바이스에 로드되었으면, PDF 디바이스에 폰트를 출력하는 것은 다음과 같이 두 가지 케이스로 분류해야 한다.\n기본 폰트\npdf() 함수로 PDF 디바이스에 플롯 내보내기\n\n추가 폰트\nPDF 디바이스에 추가 폰트를 로드한 후,\n폰트를 로드하지 않으면, 에러 발생\n\ncairo_pdf() 함수로 PDF 디바이스에 플롯 내보내기\npdf() 함수를 사용하면 추가 폰트가 아닌 대체 폰트가 출력될 수 있음\n\n\n영문 추가 폰트 출력하기\n폰트 디렉토리에서 dlookr 패키지의 플롯에서 사용하고 있는 Arial Narrow 패키지 정보를 가져온다.\n\n\n# 사용자 및 시스템 폰트 디렉토리에서 폰트 가져와 등록하기\nfont_import(pattern = \"Arial Narrow\", prompt = FALSE)\n\n\n\n\nArial Narrow 폰트는 MS Windows에는 설치되어 있다. 그러나 상업용 라이센스가 있는 폰트라 타 운영체제에서는 이 폰트와 유사한 Free 폰트인 LiberationSansNarrow 폰트 1.07.4 버전을 사용하면 된다.\n\n\n\np_arial_narrow <- ggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"Frequency by class of cars\", y = \"Frequency\") +\n  theme(text=element_text(family = \"Arial Narrow\"))\n\np_arial_narrow\n\n\n\n\n시각화 결과를 보면 Arial Narrow 폰트의 특징을 파악하기 어렵다. 그러나 다음 시각화의 결과를 보면 이름처럼 폭이 좁아진 모습을 판단할 수 있을 것이다.\n\n\npar(mar = c(0, 0, 0, 0))\nplot(1:10, 1:10, type = \"n\", bty = \"n\", xaxt = \"n\", yaxt = \"n\", \n     xlab = \"\", ylab = \"\", pty = \"s\", mar = c(0, 0, 0, 0))\ntext(2, 9, \"Font Test : regular - sans\", adj = 0, cex = 2,  \n     family = \"sans\", font = 1)\ntext(2, 8, \"Font Test : regular - Arial Narrow\", adj = 0, cex = 2, \n     family = \"Arial Narrow\", font = 1)\ntext(2, 7, \"Font Test : bold - sans\", adj = 0, cex = 2, \n     family = \"sans\", font = 2)\ntext(2, 6, \"Font Test : bold - Arial Narrow\", adj = 0, cex = 2, \n     family = \"Arial Narrow\", font = 2)\ntext(2, 5, \"Font Test : italic - sans\", adj = 0, cex = 2, \n     family = \"sans\", font = 3)\ntext(2, 4, \"Font Test : italic - Arial Narrow\", adj = 0, cex = 2, \n     family = \"Arial Narrow\", font = 3)\ntext(2, 3, \"Font Test : bold italic - sans\", adj = 0, cex = 2, \n     family = \"sans\", font = 4)\ntext(2, 2, \"Font Test : bold italic - Arial Narrow\", adj = 0, cex = 2, \n     family = \"Arial Narrow\", font = 4)\n\n\n\n\nextrafont 패키지의 font_import() 함수로 폰트 테이블 등록된 Arial Narrow 폰트는 윈도우(X11, Quartz) 디바이스에는 출력되지만 MS Windows의 윈도우 디바이스나 PDF 디바이스에도 등록해 주어야 사용이 가능하다.\n\n\nloadfonts(device = \"pdf\", quiet = TRUE)\n\npdfFonts() %>% \n  names() %>% \n  grep(\"Nanum|Arial\", ., value = TRUE)\n\n\n [1] \"ArialMT\"                     \"Arial Narrow\"               \n [3] \"NanumGothic\"                 \"NanumGothicCoding\"          \n [5] \"NanumGothic Light\"           \"Arial Black\"                \n [7] \"Arial\"                       \"Arial Rounded MT Bold\"      \n [9] \"Arial Unicode MS\"            \"NanumBarunGothic\"           \n[11] \"NanumBarunGothic Light\"      \"NanumBarunGothic UltraLight\"\n[13] \"Nanum Brush Script\"          \"NanumSquare Bold\"           \n[15] \"NanumSquare ExtraBold\"       \"NanumSquare Light\"          \n[17] \"NanumSquare\"                 \"NanumSquareRound Bold\"      \n[19] \"NanumSquareRound ExtraBold\"  \"NanumSquareRound Light\"     \n[21] \"NanumSquareRound Regular\"    \"NanumMyeongjo YetHangul\"    \n\n그러나 pdf() 함수로 플롯을 PDF 디바이스로 내보낸 결과를 보면, Arial Narrow 폰트가 아니다.\n\n\npdf(\"arial_narrow.pdf\", family = \"Arial Narrow\", width = 5, height = 5)\nplot(mtcars$mpg, mtcars$wt, \n     main = \"Fuel Efficiency of 32 Cars\",\n     xlab = \"Weight (x1000 lb)\",\n     ylab = \"Miles per Gallon\")\ndev.off()\n\n\n\n\n\n\n출력된 폰트는 PDF 디바이스 폰트에 등록된 첫번 째 폰트인 “serif”, 즉 실제로는 Times Roman 폰트다.\n\n\npdfFonts() %>% names() %>% head(3)\n\n\n[1] \"serif\" \"sans\"  \"mono\" \n\npdfFonts()[\"serif\"]\n\n\n$serif\n$family\n[1] \"Times\"\n\n$metrics\n[1] \"Times-Roman.afm\"      \"Times-Bold.afm\"      \n[3] \"Times-Italic.afm\"     \"Times-BoldItalic.afm\"\n[5] \"Symbol.afm\"          \n\n$encoding\n[1] \"default\"\n\nattr(,\"class\")\n[1] \"Type1Font\"\n\nPDF 디바이스에 추가 폰트를 출력하기 위해서는 반드시 cariro_pdf() 함수를 사용해야 한다.\n\n\ncairo_pdf(\"cairo_arrial_narrow.pdf\", family = \"Arial Narrow\", width = 5, height = 5)\nplot(mtcars$mpg, mtcars$wt, \n     main = \"Fuel Efficiency of 32 Cars\",\n     xlab = \"Weight (x1000 lb)\",\n     ylab = \"Miles per Gallon\")\ndev.off()\n\n\n\n\n\n\n한글 추가 폰트 출력하기\n한글이 포함된 플롯을 네 가지 방법으로 PDF 디바이스로 내보낸다.\n폰트 패밀리를 지정하지 않고, pdf() 함수를 사용하면 한글이 출력되지 않는다. 그러나 같은 방법으로 “Korea1deb” 폰트 패밀리를 지정하면 한글이 정상적으로 출력되지만 함께 출력되는 영문 문자의 배열이 흐트러져 이상하게 출력된다. cairo_pdf() 함수에서는 PDF 디바이스의 기본 폰트인 “Korea1deb” 폰트 패밀리를 인식하지 못한다. 그러나 추가 폰트인 “NanumGothic”는 정상적으로 잘 출력된다. 그러므로 추가 폰트를 사용한 경우에는 네번 째 방법을 이용한다. 플롯의 비교를 위해서 출력된 PDF 파일을 2 \\(times\\) 2 구조로 배열 해 보았다.\n\nPDF 디바이스의 기본 폰트인 “Korea1deb” 폰트 패밀리가 Cairo PDF 디바이스에 적용되지 않는 것처럼, 윈도우 디바이스에서도 사용할 수 없다. 왜냐하면 PDF 디바이스 전용 폰트이기 때문이다.\n\n\n\npdf(\"font_test1.pdf\", width = 5, height = 5)\nplot(mtcars$mpg, mtcars$wt, \n     main = \"pdf(), no font family\\n 한글 출력 테스트\",\n     xlab = \"Weight (x1000 lb)\",\n     ylab = \"Miles per Gallon\")\ndev.off()\n\npdf(\"font_test2.pdf\", family = \"Korea1deb\", width = 5, height = 5)\nplot(mtcars$mpg, mtcars$wt, \n     main = \"pdf(), Korea1deb font family\\n 한글 출력 테스트\",\n     xlab = \"Weight (x1000 lb)\",\n     ylab = \"Miles per Gallon\")\ndev.off()\n\ncairo_pdf(\"font_test3.pdf\", family = \"Korea1deb\", width = 5, height = 5)\nplot(mtcars$mpg, mtcars$wt, \n     main = \"cairo_pdf(), Korea1deb font family\\n 한글 출력 테스트\",\n     xlab = \"Weight (x1000 lb)\",\n     ylab = \"Miles per Gallon\")\ndev.off()\n\ncairo_pdf(\"font_test4.pdf\", family = \"NanumGothic\", width = 5, height = 5)\nplot(mtcars$mpg, mtcars$wt, \n     main = \"cairo_pdf(), Korea1deb font family\\n 한글 출력 테스트\",\n     xlab = \"Weight (x1000 lb)\",\n     ylab = \"Miles per Gallon\")\ndev.off()\n\n\n\n\n\n\nggplot2 플롯도 동일한 방법으로 비교해 본다. ggplot2 플롯 사례에서도 동일한 결과가 나타난다.\n\n\np <- ggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"pdf(), no font family\\n 한글 출력 테스트\", y = \"Frequency\") \nggsave(\"ggplot_korean1.pdf\", p, width = 5, height = 5)\n\np <- ggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"pdf(), Korea1deb font family\\n 한글 출력 테스트\", y = \"Frequency\") +\n  theme(text=element_text(family = \"Korea1deb\"))\nggsave(\"ggplot_korean2.pdf\", p, width = 5, height = 5)\n\np <- ggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"cairo_pdf(), no font family\\n 한글 출력 테스트\", y = \"Frequency\") +\n  theme(text=element_text(family = \"Korea1deb\")) \ncairo_pdf(\"ggplot_korean3.pdf\", width = 5, height = 5)\np\ndev.off()\n\n\nquartz_off_screen \n                2 \n\np <- ggplot(mpg, aes(class)) +\n  geom_bar(fill = \"steelblue\", alpha  = 0.5) + \n  labs(title = \"cairo_pdf(), no font family\\n 한글 출력 테스트\", y = \"Frequency\") +\n  theme(text=element_text(family = \"NanumGothic\"))\ncairo_pdf(\"ggplot_korean4.pdf\", width = 5, height = 5)\np\ndev.off()\n\n\nquartz_off_screen \n                2 \n\n\n\n\nggplot 플롯은 ggsave() 함수로 외부 파일로 내보낼 수 있다. 이중 PDF 디바이스로 내보내는 다음 예제도 pdf() 함수를 사용하는 것과 같은 결과를 얻는다. 그 이유는 ggsave() 함수가 pdf() 함수를 랩핑했기 때문이다. 그러므로 추가 폰트를 사용한 ggplot 플롯은 ggsave() 함수를 사용하지 말고 cairo_pdf() 함수를 사용해야 한다.\n\n\nggsave(\"pmg_arrial_narrow.pdf\", plot = p_arial_narrow,  family = \"Arial Narrow\",\n       width = 6, height = 4)\n\n\n\nPDF 디바이스에 폰트 포함하기\n만약에 PDF 디바이스로 출력된 폰트가 없는 사용자에게 PDF 파일을 공유하면, 파일을 공유받은 사용자의 환경에 해당 폰트가 없어서 파일의 출력에 문제가 발생할 수 있다. 그래서 다음처럼 PDF 디바이스 파일에 폰트를 포함하기도 한다.\n\n\nembed_fonts(\"ggplot_korean4.pdf\", outfile = \"ggplot_korean4_emb.pdf\")\n\n\n\n두 파일의 용량을 비교해보면 폰트를 포함한 PDF 파일의 용량이 다소 크다.\n\n\nsystem(\"ls -al ggplot_korean4*\", intern = TRUE)\n\n\n[1] \"-rw-r--r--  1 choonghyunryu  staff  13825 Oct 11 17:40 ggplot_korean4.pdf\"    \n[2] \"-rw-r--r--  1 choonghyunryu  staff  15348 Oct 11 17:40 ggplot_korean4_emb.pdf\"\n\n마무리 하며\ndlookr 패키지의 최초 마이너 버전 업그레이드의 가장 큰 특징은 ggplot 플롯에 추가 폰트인 Arial Narrow 폰트를 사용한 점이다. 이 작업을 진행하면서 발생했던 이슈들을 정리해 보았다.\n혹자에게는 복잡한 작업이 필요치 않을 수 있지만, 한글을 사용하는 우리의 R 사용자는 반드시 한글 출력 문제에 부딪칠 수 밖에 없는 숙명이 있다. 어찌 정리하다보니 길어졌다. 많은 이야기를 쏟아내다보니 산만하고 복잡해진 감이 없지 않다. 그러나 이 기회에 한글 출력을 확실하게 해결하리라는 바램으로 글을 마친다.\n\n\n\n",
    "preview": "posts/2021-01-27-rfonts/img/binning_by.png",
    "last_modified": "2021-10-11T17:40:04+09:00",
    "input_file": {},
    "preview_width": 1027,
    "preview_height": 620
  },
  {
    "path": "posts/2021-01-05-morpheme/",
    "title": "Mecab으로 한글 띄어쓰기 검사기 만들기",
    "description": "한글 띄어쓰기는 정말 어려운 작업이다. 한글은 영어처럼 독립된 단어 기반의 언어가 아니라 조사, 접두어, 접미어 등이 결합되어 구현되기 때문에 정확한 띄어쓰기 구사를 장담하기 어렵다. 그래서 여러 맞춤법 검사 엔진이 개발되어 배포되거나, 포탈 등의 채널에서 서비스로 제공되고 있다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2021-01-05",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n다룰 이야기\nMecab\n한글 띄어쓰기 검사기 만들기\n사용자 정의함수 만들기\n테스트 데이터\n한글 띄어쓰기 검사기 성능 검사\n\nTO-DO\n\n다룰 이야기\n한글 띄어쓰기는 정말 어려운 작업이다. 한글은 영어처럼 독립된 단어 기반의 언어가 아니라 조사, 접두어, 접미어 등이 결합되어 구현되기 때문에 정확한 띄어쓰기 구사를 장담하기 어렵다. 그래서 여러 맞춤법 검사 엔진이 개발되어 배포되거나, 포탈 등의 채널에서 서비스로 제공되고 있다.\n\n오늘은 포털 등의 전문적인 띄어쓰기 검사 엔진보다는 못하지만, 제법 쓸만한 띄어쓰기 검사기의 프로토타입을 소개한다. 텍스트 분석을 위해서, 다들 형태소 분석기 하나 정도는 애용하고 있을 것이다. 개인적으로는 Mecab 형태소 분석기를 애용하고 있는데, 이를 이용해서 띄어쓰기 검사를 수행하는 R 함수를 정의하고자 한다.\n\nMecab\nMecab 형태소 분석기는 일본에서 만든 오픈소스 형태소 분석기다. 검색 엔진에 사용하기 위해서 만들어졌으며, 특정 언어에 국한되지 않고 범용적으로 설계되었기 때문에 한글을 지원하는 은전한닢이라는 오픈소스 프로젝트로 한글화되어 사용되고 있다.\n이 형태소 분석기는 형태소 분석의 속도도 빠르고 성능도 나쁘지 않다. 무엇보다도 R에서도 Mecab의 사용을 지원하는 RMeCab 패키지(이 패키지도 일본에서 개발되었음)가 일찌감치 공개되었기 때문에 Mecab 형태소 분석기를 사용하고 있는 것이다. 즉, Mecab <- 은전한닢 <- RMeCab의 구조로 형태소 분석을 수행하고 있다.\n과거에는 Mecab이 MS-Windows를 지원하지 않았는데, 최근에는 MS-Windows에서도 Mecab을 사용할 수 있는 방법이 인터넷에 공유되고 있으니, MS-Windows 환경의 사용자도 Mecab 형태소 분석기 사용을 고려해보는 것을 추천한다.\n한글 띄어쓰기 검사기 만들기\n사용자 정의함수 만들기\n다음처럼 간단하게 한글 띄어쓰기 검사기를 만들 수 있다. 너무 간단해서 의아하게 생각할 수도 있겠다.\n\n\ngetSpacing <- function(x) {\n  mor <- unlist(RMeCab::RMeCabC(x))\n\n  ## 조사/어미/접미사/마침표,물음표,느낌표,컴마\n  idx <- grep(\"^J|^E|^XS|SF|SE|NNBC|SC|VCP\", names(mor))\n\n  for (i in rev(idx)) {\n    mor[i-1] <- paste(mor[i-1], mor[i], sep = \"\")\n  }\n\n  mor <- mor[-idx]\n\n  paste(mor, collapse = \" \")\n}\n\n\n\n개념은 간단하다. RMeCab은 검색엔진에서의 활용을 목적으로 만들어졌기 때문에, 한글 형태소 품사(Part Of Speech, POS) 태그에서\n다른 형태소 분석이 보다 품사를 좀더 세밀하게 분해하는 특징이 있다.\n원문을 분해할 때, 특정 품사의 음절을 자음과 모음을 분리하지 않는다.\n띄어쓰기가 전혀 없는, 한 어절로 만든 문장에서 잘 작동한다.\n그래서 나누어진 토큰을 공백으로 묶어줄 때 조사, 어미, 문장 기호(마침표, 물음표, 느낌표 등)은 공백을 주지 않고 앞의 토큰에 붙여주면 된다.\n물론 프로토타이핑으로 예외 처리 로직이 없기 때문에 100% 정확하지는 않지만 기대 이상의 성능을 보여준다.\n테스트 데이터\n띄어쓰기 검사기로 조회해서, 네이버의 맞춤법 검사기에 띄어쓰기 검사도 포함되었음을 소개하는 포스트를 읽었다. 그 포스트의 본문 중 다음과 같은 일부 문장을 테스트 데이터로 가져왔다.\n출처 : https://overthink.tistory.com/314\n\n우리는 평소 타이핑을 하게 될 일이 굉장히 많이 있습니다.\n취준생분들이라면 자소서를 작성하실 때의 경우가 있죠.\n또한 블로그를 작성하시거나 업무상의 연락으로 메일을 작성할 때가 있습니다.\n이럴 때에 맞춤법 또는 띄어쓰기가 굉장히 신경이 쓰이곤 합니다.\n네이버에서는 간단하게 띄어쓰기를 검사할 수 있는 방법이 있습니다.\n\n이 문장을 네이버 맞춤법 검사에 넣어보니 띄어쓰기가 올바르게 되어 있음을 알 수 있다. 다만 취준생분들이라면이 표준어가 아님을 알려준다.\n\n\n\n\n\nphrase <- \"우리는 평소 타이핑을 하게 될 일이 굉장히 많이 있습니다.\n취준생분들이라면 자소서를 작성하실 때의 경우가 있죠.\n또한 블로그를 작성하시거나 업무상의 연락으로 메일을 작성할 때가 있습니다.\n이럴 때에 맞춤법 또는 띄어쓰기가 굉장히 신경이 쓰이곤 합니다.\n네이버에서는 간단하게 띄어쓰기를 검사할 수 있는 방법이 있습니다.\"\n\n\n\nMecab의 품사 태깅 결과는 다음과 같다.\n\n\nunlist(RMeCab::RMeCabC(phrase))\n\n\n        NP         JX        NNG        NNG        JKO         VV \n    \"우리\"       \"는\"     \"평소\"   \"타이핑\"       \"을\"       \"하\" \n        EC     VV+ETM        NNG        JKS        MAG        MAG \n      \"게\"       \"될\"       \"일\"       \"이\"   \"굉장히\"     \"많이\" \n        VA         EF         SF        NNG        XSN        XSN \n      \"있\"   \"습니다\"        \".\"   \"취준생\"       \"분\"       \"들\" \n       VCP         EC        NNG        JKO        NNG        XSV \n      \"이\"     \"라면\"   \"자소서\"       \"를\"     \"작성\"       \"하\" \n    EP+ETM        NNG        JKG        NNG        JKS         VA \n      \"실\"       \"때\"       \"의\"     \"경우\"       \"가\"       \"있\" \n        EF         SF        MAJ        NNG        JKO        NNG \n      \"죠\"        \".\"     \"또한\"   \"블로그\"       \"를\"     \"작성\" \n       XSV         EP         EC        NNG        XSN        JKG \n      \"하\"       \"시\"     \"거나\"     \"업무\"       \"상\"       \"의\" \n       NNG        JKB        NNG        JKO        NNG    XSV+ETM \n    \"연락\"     \"으로\"     \"메일\"       \"을\"     \"작성\"       \"할\" \n       NNG        JKS         VA         EF         SF     VA+ETM \n      \"때\"       \"가\"       \"있\"   \"습니다\"        \".\"     \"이럴\" \n       NNG        JKB        NNG        MAJ        NNG        JKS \n      \"때\"       \"에\"   \"맞춤법\"     \"또는\" \"띄어쓰기\"       \"가\" \n       MAG        NNG        JKS         VV         EC      VX+EF \n  \"굉장히\"     \"신경\"       \"이\"     \"쓰이\"       \"곤\"   \"합니다\" \n        SF        NNP        JKB         JX         XR        XSA \n       \".\"   \"네이버\"     \"에서\"       \"는\"     \"간단\"       \"하\" \n        EC        NNG        JKO        NNG    XSV+ETM        NNB \n      \"게\" \"띄어쓰기\"       \"를\"     \"검사\"       \"할\"       \"수\" \n        VV        ETM        NNG        JKS         VA         EF \n      \"있\"       \"는\"     \"방법\"       \"이\"       \"있\"   \"습니다\" \n        SF \n       \".\" \n\n한글 띄어쓰기 검사기 성능 검사\n그리고 앞에서 정의한 한글 띄어쓰기 검사기 결과는 다음과 같다. 네이버 맞춤법 검사기와 동일한 결과다.\n\n\ngetSpacing(phrase)\n\n\n[1] \"우리는 평소 타이핑을 하게 될 일이 굉장히 많이 있습니다. 취준생분들이라면 자소서를 작성하실 때의 경우가 있죠. 또한 블로그를 작성하시거나 업무상의 연락으로 메일을 작성할 때가 있습니다. 이럴 때에 맞춤법 또는 띄어쓰기가 굉장히 신경이 쓰이곤 합니다. 네이버에서는 간단하게 띄어쓰기를 검사할 수 있는 방법이 있습니다.\"\n\n이번에는 극단적인 사례를 만들어서 실험해 본다. 원 문장의 모든 공백을 없애서 한 어절로 만들었다.\n\n\nphrase <- \"우리는평소타이핑을하게될일이굉장히많이있습니다.취준생분들이라면자소서를작성하실때의경우가있죠.또한블로그를작성하시거나업무상의연락으로메일을작성할때가있습니다.이럴때에맞춤법또는띄어쓰기가굉장히신경이쓰이곤합니다.네이버에서는간단하게띄어쓰기를검사할수있는방법이있습니다.\"\n\n\n\n네이버의 경우에는 띄어쓰기가 전혀 안된 하나의 어절에서도 어느 정도 띄어쓰기를 분리하지만, 이 사례에서는 전혀 작동하지 못했다.\n\n\n\n그러나 한글 띄어쓰기 검사기인 getSpacing()는 정확하게 띄어쓰기를 수행해 준다.\n\n\ngetSpacing(phrase)\n\n\n[1] \"우리는 평소 타이핑을 하게 될 일이 굉장히 많이 있습니다. 취준생분들이라면 자소서를 작성하실 때의 경우가 있죠. 또한 블로그를 작성하시거나 업무상의 연락으로 메일을 작성할 때가 있습니다. 이럴 때에 맞춤법 또는 띄어쓰기가 굉장히 신경이 쓰이곤 합니다. 네이버에서는 간단하게 띄어쓰기를 검사할 수 있는 방법이 있습니다.\"\n\nMecab의 가장 유용한 장점은 이처럼 띄어쓰기가 전혀 안된 하나의 어절에서도 잘 작동한다는 점이다. 그래서 STT(Speech to Text)처럼 띄어쓰기가 취약한 텍스트 데이터를 띄어쓰기가 전혀 안된 하나의 어절로 변환한 후 형태소 분석을 수행하여 성과를 낸 경험도 있다.\n그러나 중요한 점은, STT 데이터처럼 극단적이지 않을 경우에는, 띄어쓰기가 안된 문장보다는 정확하지 않더라도 어느 정도 띄어쓰기가 된 문장의 결과가 성능이 좀 더 높게 나온다.\n\n\n\n“무궁화 꽃이 피었습니다.” 사례에서는 띄어쓰기가 안된 문장인 “무궁화꽃이피었습니다.”에서 무궁화 꽃이를 분리하지 못했다.\n\n\ngetSpacing(\"무궁화 꽃이 피었습니다. 무궁화꽃이피었습니다.\")\n\n\n[1] \"무궁화 꽃이 피었습니다. 무궁화꽃이 피었습니다.\"\n\nTO-DO\n유용하지만, 베타버전인 네이버 맞춤법 검사기는 500 음절의 문장만 지원한다. 또한 네이버 맞춤법(띄어쓰기) 검사기의 대체재를 쉽게 구할 수도 있다. 그러나 연구 목적이나 분석 목적으로 애플리케이션에서의 사용자 입력이 아닌, R의 분석 코드 안에서 띄어쓰기 검사기가 필요한 경우가 있을 것이다. 그래서 몇 개의 예외처리 기능을 추가한 R 패키지를 만들어 보려 한다.\npython의 KoNLPy는 Macab을 포함해서 여러 공개된 형태소 분석기를 지원하고 있으니, KoNLPy을 이용한 python 한글 띄어쓰기 검사기를 만들어볼까 한다.\n\n\n\n",
    "preview": "posts/2021-01-05-morpheme/img/naver_01.png",
    "last_modified": "2021-10-11T10:08:31+09:00",
    "input_file": {},
    "preview_width": 1354,
    "preview_height": 954
  },
  {
    "path": "posts/2020-12-31-woolsan/",
    "title": "겸제 정선의 울산바위",
    "description": "고등학교 역사 시간에 조선시대 산수화가인 `겸제 정선`의 `진경산수화(眞景山水畵)`에 대해서 배운 적이 있다. 요즘은 인터넷 검색을 통해서 쉽게 겸제의 작품을 감상할 수 있지만 그 시절은 유면 화가의 작품을 교과서가 아닌 곳에서 쉽게 접할 수 없었다. 나는 아직도 그 시절 교과서에 실린 `인왕제색도`의 감동은 아직도 잊지 못한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2020-12-31",
    "categories": [
      "Gallery"
    ],
    "contents": "\n\nContents\n겸제 정선과 진경 산수화\n울산바위 수묵화\n울산바위 1\n울산바위 2\n울산바위 3\n울산바위 4\n\n\n\n\n\n겸제 정선과 진경 산수화\n\n고등학교 역사 시간에 조선시대 산수화가인 겸제 정선의 진경산수화(眞景山水畵)에 대해서 배운 적이 있다. 요즘은 인터넷 검색을 통해서 쉽게 겸제의 작품을 감상할 수 있지만 그 시절은 유면 화가의 작품을 교과서가 아닌 곳에서 쉽게 접할 수 없었다. 나는 아직도 그 시절 교과서에 실린 인왕제색도의 감동은 아직도 잊지 못한다.\n\nCNN(Convolutional Neural Networks)을 이용한 neural style transfer 기법으로 겸제의 인왕제색도풍으로 내 카메라 앵글에 잡혔던 울산바위를 수묵화로 바꿔보았다.\n울산바위 수묵화\n울산바위 1\n2018년도 봄에 와이프와 울산바위 등반 전, 바로 아래서 울산바위를 바라보다 앵글에 담은 사진이다.\n\n\n\n겸재의 붓 터치로 화선지에는 이런 그림이 그려졌을 것이다. 이 날은 구름이 많은 날씨였다. 한국의 수묵화는 여백의 미가 있는지라, 주 대상이 아닌 구름을 여백으로 승화시키지 못했다. 학습을 잘못해서 구름이 봉우리 느낌으로 덧칠해져 있다.\n\n\n\n울산바위 2\n등반 후 정상부근의 봉우리를 담은 사진이다.\n\n\n\n겸재의 붓 터치로 화선지에는 이런 그림이 그려졌을 것이다. 이 그림 역시 구름이 봉우리 느낌으로 덧칠해져 있다.\n\n\n\n울산바위 3\n울산바위 등반 전, 흔들바위 못 미쳐서 바라본 울산바위를 앵글에 담은 사진이다.\n\n\n\n겸재의 붓 터치로 화선지에는 이런 그림이 그려졌을 것이다. 이 그림 역시 구름이 봉우리 느낌으로 덧칠해져 있다.\n\n\n\n울산바위 4\n2016년도 봄에 작은 아들과 흔들바위에서 울산바위를 바라보며 앵글에 담았다.\n\n\n\n겸재의 붓 터치로 화선지에는 이런 그림이 그려졌을 것이다. 앵글에 담긴 풍경에 하늘 여백이 거의 없어 그나마 제대로 각색되어 그려졌다.\n\n\n\n\n\n\n",
    "preview": "posts/2020-12-31-woolsan/img/woolsan_title.png",
    "last_modified": "2021-10-11T10:27:39+09:00",
    "input_file": {},
    "preview_width": 1000,
    "preview_height": 350
  },
  {
    "path": "posts/2020-07-12-ggplot2_exam/",
    "title": "몇 개의 ggplot tips",
    "description": "R에서 데이터를 조작할 때, 가장 까다로운 것이 범주형 데이터를 표현하는 factor다. 이번 이야기에서는 시각화를 위해서 factor를 다루는 방법과 몇몇 ggplot2 패키지를 사용할 때의 몇 가지 팁을 소개한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2020-07-12",
    "categories": [
      "Visualization"
    ],
    "contents": "\n\nContents\n다룰 이야기\n데이터\n\n시각화\n조작하지 않은 플롯\nx-축의 tick 마크 라벨 회전\n데이터 크기별로 정렬하여 출력\n일부 범주의 정보만 출력하기\n축을 바꿔서 출력하기\n\n\n다룰 이야기\nR에서 데이터를 조작할 때, 가장 까다로운 것이 범주형 데이터를 표현하는 factor다. 이번 이야기에서는 시각화를 위해서 factor를 다루는 방법과 몇몇 ggplot2 패키지를 사용할 때의 몇 가지 팁을 소개한다.\n플롯에서 factor의 경우에는, 데이터의 표현이 levels 순서로 이루어진다. 그러나 표현 순서를 변경하고 싶다.\n솔루션) factor의 levels을 표현하는 순서에 맞게 변경한다.\n\nfactor의 모든 levels이 아닌 상위.하위 몇 개의 levels에 해당하는 것만 시각화하고 싶다.\n솔루션) 필터링 기능의 함수를 사용하며 몇 개의 levels에 해당하는 데이터만 추출한다.\n\n또한 다음처럼 ggplot에서 응용할 수 있는 몇 가지 팁도 제시한다.\n플롯에서 x-축의 tick 마크에 출력할 라벨 이름이 길 경우에는 서로 겹쳐서 읽기 어려운 경우가 있다.\n솔루션 1) 라벨을 회전시켜 겹치지 않게 출력한다.\n솔루션 2) x-축과 y-축을 바꿔서 출력한다.\n\nx-축과 y-축을 바꿔서 출력하고 싶다.\n솔루션) coord_flip() 함수를 적용하여 축을 바꾼다.\n\ny-축에 데이터를 배치하는 순서를 역순으로 변경하고 싶다.\n솔루션 1) aes() 함수 안에서 forcats::fct_reorder() 함수를 이용한다. 혹은,\n솔루션 2) aes() 함수 안에서 ggplot2::reorder() 함수를 이용한다.\n\n데이터\nggplot2 패키지에 포함된 txhousing는 Texas A&M University(TAMU)의 The Real Estate Center에서 제공한 Texas의 주택 시장에 대한 정보를 담고 있다. 이 데이터로 이야기를 풀어나가려 한다.\ncity\nName of MLS area\n\nyear,month,date\nDate\n\nsales\nNumber of sales\n\nvolume\nTotal value of sales\n\nmedian\nMedian sale price\n\nlistings\nTotal active listings\n\ninventory\n“Months inventory”:\namount of time it would take to sell all current listings at current pace of sales.\n\n\n\n\nstr(ggplot2::txhousing)\n\n\ntibble [8,602 × 9] (S3: tbl_df/tbl/data.frame)\n $ city     : chr [1:8602] \"Abilene\" \"Abilene\" \"Abilene\" \"Abilene\" ...\n $ year     : int [1:8602] 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ month    : int [1:8602] 1 2 3 4 5 6 7 8 9 10 ...\n $ sales    : num [1:8602] 72 98 130 98 141 156 152 131 104 101 ...\n $ volume   : num [1:8602] 5380000 6505000 9285000 9730000 10590000 ...\n $ median   : num [1:8602] 71400 58700 58100 68600 67300 66900 73500 75000 64500 59300 ...\n $ listings : num [1:8602] 701 746 784 785 794 780 742 765 771 764 ...\n $ inventory: num [1:8602] 6.3 6.6 6.8 6.9 6.8 6.6 6.2 6.4 6.5 6.6 ...\n $ date     : num [1:8602] 2000 2000 2000 2000 2000 ...\n\n시각화\n조작하지 않은 플롯\n도시별 일별 부동산 판매 건수의 분포를 박스플롯을 그려 살펴보자. 그런데 도시 개수가 적지 않아, x-축의 도시 이름이 겹쳐서 출력되었다. 어떤 도시의 부동산 판매 건수가 많은지 혹은 적은지 알 수 없다.\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\ntxhousing %>% \n  filter(!is.na(sales)) %>% \n  ggplot(aes(x = city, y = sales, fill = city)) +\n  geom_boxplot() +\n  theme(legend.position = \"none\")\n\n\n\n\nx-축의 tick 마크 라벨 회전\n도시 이름이 겹쳐서 출력되었다. 어떤 도시의 부동산 판매 건수가 많은지 혹은 적은지 알 수 없다.\n만약 x-축이 연속형 변수일 경우에는 라벨이 겹치지 않을 정도로 몇 개의 tick 마크만 출력하면 문제가 해결된다. 그러나 이 경우처럼 x-축이 범주형 변수일 경우에는 이 방법을 적용할 수 없다.\nx-축의 tick 마크 라벨을 회전하면 겹쳐지는 것을 방지할 수 있다. 다음처럼 theme() 함수를 이용해서 axis.text.x의 출력 각도 90도로 조정한다.\n\n\ntxhousing %>% \n  filter(!is.na(sales)) %>% \n  ggplot(aes(x = city, y = sales, fill = city)) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))\n\n\n\n\n이제 도시 이름이 겹치지 않아서 Houstion에서의 부동산 거래가 활발함을 할 수 있다. 그러나 한가지 문제점이 있다. 도시 이름이 회전하다보니, 그 이름을 읽어서 인식하기가 다소 어려워 졌다. 또한 도시 이름이 긴 것이 많아 전체 시각화 결과에서 x-축의 영역이 필요 이상으로 커졌다.\n이번에는 각도를 45도로 조정해 보자.\n\n\ntxhousing %>% \n  filter(!is.na(sales)) %>% \n  ggplot(aes(x = city, y = sales, fill = city)) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))\n\n\n\n\n출력된 플롯에서 x-축의 영역이 줄어들었고, 좀더 가로 모양에 가까워져서 이름을 읽어서 인식하기가 다소 쉬워 졌다. 그러나 도시간의 간격이 좀 줄어들어, 도시 이름간을 분리하는데 다소 간섭이 발생한다.\n데이터 크기별로 정렬하여 출력\n도시별 부동산 거래 규모를 파악하는 시나리오는“어느 도시가 거래량이 많은지, 혹은 어느 도시가 거래량이 적은지?”를 파악하는 것이다. 지극히 일반적인 시나리오다. 만약 관심있는 도시가 정해져 있고, 그 도시의 순서가 의미가 있다면 관심있는 도시 순서로 표현되길 원할 것이다.\n앞에서 정의한 시나리오는 거래량이 많은 도시부터 시작해서 적은 도시의 순서(내림차순)로 표현하거나, 반대로 거래량이 적은 도시부터 시작해서 많은 도시의 순서(오름차순)로 표현하는 방법이다. 만약 수치 변수가 positive 지표일 경우에는 내림차순으로, negative일 경우에는 오름차숨으로 표현하는 것이 바람직하다. 거래량은 부정적인 지표가 아니므로 내림차순으로 표현하는 것이 바람직하다.\n범주형 데이터인 factor나 ordered factor는 정의된 levels 순서로 데이터가 표현된다. 그러므로 levels을 거래량의 내림차순에 해당하는 도시 이름으로 변경해주면 도시별 박스 플롯이 내림차순으로 표현된다.\nforcats 패키지의 fct_reorder() 함수는 지정한 수치변수의 정렬 순서에 따라 factor의 levels 순서를 변경한다. 그러므로 다음과 같이 mutate() 함수에 적용하여 거래량의 내림차순 순으로 도시 이름의 순서를 변경해 준다.\n\n\ntxhousing %>% \n  filter(!is.na(sales)) %>% \n  mutate(city = forcats::fct_reorder(city, sales, .desc = TRUE)) %>% \n  ggplot(aes(x = city, y = sales, fill = city)) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n일부 범주의 정보만 출력하기\nTop-n을 분석할 경우, 예를 들면 부동산 거래량 상위 10개 도시만 시각화는 경우, 부동산 거래량 하위 7개 도시만 시각화는 경우에는 전체 도시가 아닌 일부 도시를 필터링하여 출력해야 한다. 예제 데이터는 46개의 도시 정보를 포함하고 있다. 이 경우 46개의 도시에 대한 거래량을 이해하는 것보다 규모가 크거나 작은 몇 개의 도시에 대한 정보를 재빨리 파악하는 것이 중요하다.\nfilter() 함수는 조건을 만족하는 사례만 취하는 함수다. 다음처럼 조건에 상위, 혹은 하위 Top-n 개의 도시를 추려서 시각화하면 원하는 결과를 얻을 수 있다.\n\n\nn <- 12\ntxhousing %>% \n  filter(!is.na(sales)) %>% \n  mutate(city = forcats::fct_reorder(city, sales, .desc = TRUE)) %>% \n  filter(city %in% levels(city)[1:n]) %>% \n  ggplot(aes(x = city, y = sales, fill = city)) +\n  geom_boxplot() +\n  theme(legend.position = \"none\") +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))\n\n\n\n\n축을 바꿔서 출력하기\n일반화된 법칙은 아니지만 경우에 따라서, x-축과 y-축을 바꿔서 출력하면 몇 가지 잇점을 얻을 수 있다.\nx-축에 표현할 범주형 변수의 levels 개수가 많아서 겹쳐 출력할 경우에 겹치는 문제를 해결할 수 있다. 그리고 박스 플롯처럼 분포를 파악하는 시각화의 경우에는 위 아래로 시선을 훑으면서 해석하는 것보다는 왼쪽에서 오른쪽으로 시선을 훑는 것이 정보를 이해하는데 수월하다. 유사한 기능을 수행하는 히스토그램이나 밀도 그래프(density plot)는 횡으로 나열하여 표현한 것도 이런 이유다. 그러나 절대적이지는 않다. 세로로 표현해도 해석하는 데 문제는 없다. 다만, 세심한 배려가 시각화 결과를 해석하는 분석가나 정보를 재공받는 플롯 해석가에게 해석에 존더 집중할 수 있는 환경을 만들어 줄 수 있다는 점을 고려해야 한다.\nggplot의 경우에는 coord_flip() 함수를 적용해서 x-축과 y-축을 바꿔서 출력할 수 있다.\n\n\nn <- 12\ntxhousing %>% \n  filter(!is.na(sales)) %>% \n  mutate(city = forcats::fct_reorder(city, sales, .desc = TRUE)) %>% \n  filter(city %in% levels(city)[1:n]) %>% \n  ggplot(aes(x = city, y = sales, fill = city)) +\n  geom_boxplot() +\n  coord_flip() +\n  theme(legend.position = \"none\")\n\n\n\n\n유클리드 공간을 표현하는 직교좌표계인 데카르트 좌표계(Cartesian coordinate system)는 우리가 흔히 사용하는 좌표계다. x-축과 y-축이 교차하는 좌표계에 플롯을 그릴 경우에는, x-축의 경우는 왼쪽에서 오른쪽의 방향성이 있고, y-축의 경우에는 아래에서 윗쪽으로의 방향성이 있다. 앞의 경과에서 Houstion이 맨 아래에 표현된 이유다.\n사람의 공간 인지 능력은 아래서 시작해서 위로 시선을 옮기는 접근보다 위에서 아래로 시선을 옮겨가면서 파악하는 것이 수월하다. 그런데 앞의 결과는 축이 바뀌면서 y-축에 표현되는 정보가 내림차순이 아닌 오름차순으로 바뀌었다.\nforcats 패키지의 fct_reorder()를 이용하면 순서를 바꿔 값이 큰 Houstion을 맨 위에 표현할 수 있다. 이 경우 xlab() 함수를 이용해서 x-축의 레이블을 변경해 준다. x-축과 y-축이 바뀌었으므로, xlab() 함수는 실제로 x-축의 레이블을 변경시키며, 시각적으로는 우리가 익숙하게 인식하는 y-축 방향의 레이블을 변경한다. 만약 ylab() 함수를 사용하면 sales에 해당하는, 우리가 익숙하게 인식하는 y-축 방향의 레이블이 변경되므로 주의해야 한다.\n\n\nn <- 12\ntxhousing %>% \n  filter(!is.na(sales)) %>% \n  mutate(city = forcats::fct_reorder(city, sales, .desc = TRUE)) %>% \n  filter(city %in% levels(city)[1:n]) %>% \n  ggplot(aes(x = forcats::fct_reorder(city, desc(city)), y = sales, fill = city)) +\n  geom_boxplot() +\n  coord_flip() +\n  xlab(\"city\") + \n  ggtitle(\"Boxplot of sales by cities\") +\n  theme(legend.position = \"none\")\n\n\n\n\nforcats 패키지의 fct_reorder()는 ggplot의 reorder() 함수로 대체할 수도 있다.\n\n\nn <- 12\ntxhousing %>% \n  filter(!is.na(sales)) %>% \n  mutate(city = forcats::fct_reorder(city, sales, .desc = TRUE)) %>% \n  filter(city %in% levels(city)[1:n]) %>% \n  ggplot(aes(x = reorder(city, desc(city)), y = sales, fill = city)) +\n  geom_boxplot() +\n  coord_flip() +\n  xlab(\"city\") + \n  ggtitle(\"Boxplot of sales by cities\") +  \n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2020-07-12-ggplot2_exam/2020-07-12-ggplot2_exam_files/figure-html5/box-1.png",
    "last_modified": "2021-10-11T10:24:51+09:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-27-r-alookr/",
    "title": "alookr - Binary classification modeling",
    "description": "`alookr`은 이진분류 모델의 개발 과정에서 데이터 정제, 데이터의 traning set과 test set으로의 분리, 모델 적합, 적합된 모델의 평가 및 최적의 모델을 선정한다. 이 패키지는 `dplyr` 패키지와 협업하여 이진분류 데이터 분석 프로세스를 유연하게 처리해 준다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2020-03-27",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n개요\nalookr 설치\n사용 방법\n데이터 정제\n예제 데이터 생성하기\n데이터 정제\n상관관계가 높은 변수의 진단 및 제거\n\ntraining set과 test set 데이터 분할\nData: 신용카드 연체 데이터\n데이터 분할\n데이터셋의 비교\ntraining set과 test set의 추출\n\n모델 적합, 평가 및 예측(Modeling and Evaluate, Predict)\nData: Wisconsin Breast Cancer 데이터\n데이터 전처리\n데이터 분할\nsampling_target()를 이용한 불균형 클래스 핸들링\ncleanse()를 이용하여 분류 모델을 위한 데이터 정제\nextract_set()을 이용하여 모델 평가를 위한 test set 추출\nrun_models()을 이용한 이진분류 모델 수행\n모델의 평가\nplot_performance()이용한 ROC 곡선의 시각화\n예측\n\n\n개요\nalookr은 이진분류 모델의 개발 과정에서 데이터 정제, 데이터의 traning set과 test set으로의 분리, 모델 적합, 적합된 모델의 평가 및 최적의 모델을 선정한다. 이 패키지는 dplyr 패키지와 협업하여 이진분류 데이터 분석 프로세스를 유연하게 처리해 준다.\n주요 기능:\n데이터를 traning set과 test set으로 나눈다.\n여러 대표적인 이진분류 모델을 적합한다.\n적합한 이진분류 모델을 평가하고 최적의 모델을 선정한다.\n이진분류 모델의 전 프로세스를 지원한다.\nalookr이라는 이름은 데이터 분석 과정에서 looking at the analytics process에서 유래하여 작명하였다.\nalookr 설치\nCRAN에 등록된, 릴리즈된 패키지는 다음과 같이 설치한다.:\n\n\ninstall.packages(\"alookr\")\n\n\n\n혹은 GitHub에 등록된 vignettes이 없는 개발버전은 다음처럼 설치한다.:\n\n\ndevtools::install_github(\"choonghyunryu/alookr\")\n\n\n\n혹은 GitHub에 등록된 vignettes을 포함한 개발버전은 다음처럼 설치한다.:\n\n\ninstall.packages(c(\"ISLR\"))\ndevtools::install_github(\"choonghyunryu/alookr\", build_vignettes = TRUE)\n\n\n\n사용 방법\nalookr에는 몇 가지 vignette 파일을 포함하고 있는데, 이 포스트는 이를 기초로 작성하였다.\n제공되는 vignette는 다음과 같다.\nCleansing the dataset\nSplitting the dataset\nSplit the data into a training set and a test set\n\nClassification Modeling\nModeling and Evaluate, Predict\n\n\n\nbrowseVignettes(package = \"alookr\")\n\n\n\n데이터 정제\n예제 데이터 생성하기\nalookr 패키지의 기초적인 사용 방법을 설명하기 위해서 data_exam라는 가상의 데이터를 생성한다. 이 데이터는 5개의 변수를 포함한다.\n예제를 위한 데이터는 다음과 같은 변수를 포함한다.:\nid : character\nyear: character\ncount: numeric\nalpha : character\nflag : character\n\n\n# 샘플 데이터 생성\nset.seed(123L)\nid <- sapply(1:1000, function(x)\n  paste(c(sample(letters, 5), x), collapse = \"\"))\n\nyear <- \"2018\"\n\nset.seed(123L)\ncount <- sample(1:10, size = 1000, replace = TRUE)\n\nset.seed(123L)\nalpha <- sample(letters, size = 1000, replace = TRUE)\n\nset.seed(123L)\nflag <- sample(c(\"Y\", \"N\"), size = 1000, prob = c(0.1, 0.9), replace = TRUE)\n\ndata_exam <- data.frame(id, year, count, alpha, flag, stringsAsFactors = FALSE)\n\n# 데이터의 구조\nstr(data_exam)\n\n\n'data.frame':   1000 obs. of  5 variables:\n $ id   : chr  \"osncj1\" \"rvket2\" \"nvesi3\" \"chgji4\" ...\n $ year : chr  \"2018\" \"2018\" \"2018\" \"2018\" ...\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: chr  \"o\" \"s\" \"n\" \"c\" ...\n $ flag : chr  \"N\" \"N\" \"N\" \"N\" ...\n\n# 데이터의 집계\nsummary(data_exam)\n\n\n      id                year               count       \n Length:1000        Length:1000        Min.   : 1.000  \n Class :character   Class :character   1st Qu.: 3.000  \n Mode  :character   Mode  :character   Median : 6.000  \n                                       Mean   : 5.698  \n                                       3rd Qu.: 8.000  \n                                       Max.   :10.000  \n    alpha               flag          \n Length:1000        Length:1000       \n Class :character   Class :character  \n Mode  :character   Mode  :character  \n                                      \n                                      \n                                      \n\n데이터 정제\ncleanse()는 이진분류 모델 적합을 수행하기 전에 데이터 정제를 수행한다.\ncleanse()의 기능은 다음과 같다.:\n모든 관측치가 동일한 값을 갖는 변수의 제거\n유일한 값의 비중이 높은 변수의 제거\ncharacter 변수를 factor로 변경\n결측치의 비중이 높은 변수의 제거\ncleanse()을 이용한 데이터의 정제\n다음 예제는 data_exam 데이터 프레임을 정제한다.:\n\n\nlibrary(alookr)\n\n# 데이터 정제\nnewDat <- cleanse(data_exam)\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n── Checking character variables ─────────────────────── categorical data ──\n• alpha\n• flag\n\n# 정제된 데이터의 구조\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  3 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n $ flag : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 2 1 1 1 1 1 ...\n\nremove variables whose unique value is one : year 변수는 오직 “2018”라는 하나의 값만 가지고 있다.. 이런 변수는 모형적합에 필요치 않다. 그래서 제거된다.\nremove variables with high unique rate : 범주형 변수에서 수준(levle)의 개수가 너무 많으면, 분류모델의 변수로 적당하지 않다. 이 경우에는 식별자(identifier)일 가능성이 높다. 그래서 범주형이나 문자열 변수에서 유일값의 비율로 정의된, 수준의 개수/관측치의 개수가 큰 변수를 제거한다.\n레벨의 개수가 1000인 변수 id의 유일값의 비율은 1이다. 그래서 이 변수는 제거 대상이다.\n변수 alpha의 유일값의 비율은 0.026이며, 이 변수로 제거된다.\n\nconverts character variables to factor : character 변수인 변수 flag는 factor로 변경된다.\n다음 예제처럼 유일값의 비율의 임계치를 변경하여, 범주형 변수의 제거를 조정할 수 있다.:\n\n\n# 데이터 정제\nnewDat <- cleanse(data_exam, uniq_thres = 0.03)\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n── Checking character variables ─────────────────────── categorical data ──\n• alpha\n• flag\n\n# 정제된 데이터의 구조\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  3 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n $ flag : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 2 1 1 1 1 1 ...\n\n변수 alpha가 제거되지 않았다.\n데이터 정제에 유일값의 비율을 적용하지 않으려면, 다음처럼 uniq 인수값을 FALSE로 지정하면 된다.:\n\n\n# 데이터 정제\nnewDat <- cleanse(data_exam, uniq = FALSE)\n\n\n── Checking character variables ─────────────────────── categorical data ──\n• id\n• year\n• alpha\n• flag\n\n# 정제된 데이터의 구조\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  5 variables:\n $ id   : Factor w/ 1000 levels \"ablnc282\",\"abqym54\",..: 594 715 558 94 727 270 499 882 930 515 ...\n $ year : Factor w/ 1 level \"2018\": 1 1 1 1 1 1 1 1 1 1 ...\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n $ flag : Factor w/ 2 levels \"N\",\"Y\": 1 1 1 1 2 1 1 1 1 1 ...\n\ncharacter 변수를 factor로 변경하지 않으려면, 다음처럼 char 인수값을 FALSE로 지정한다.:\n\n\n# 데이터 정제\nnewDat <- cleanse(data_exam, char = FALSE)\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n# 정제된 데이터의 구조\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  3 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: chr  \"o\" \"s\" \"n\" \"c\" ...\n $ flag : chr  \"N\" \"N\" \"N\" \"N\" ...\n\n결측치가 포함된 변수를 제거하려면, missing 인수값을 FALSE로 지정한다. 다음 예제는 결측치가 포함된 flag 변수를 제거한다.\n\n\ndata_exam$flag[1] <- NA \n\n# 데이터 정제\nnewDat <- cleanse(data_exam, missing = TRUE)\n\n\n── Checking missing value ────────────────────────────────── included NA ──\n• flag\n\n── Checking unique value ─────────────────────────── unique value is one ──\n• year\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• id = 1000(1)\n\n── Checking character variables ─────────────────────── categorical data ──\n• alpha\n\n# 정제된 데이터의 구조\nstr(newDat)\n\n\n'data.frame':   1000 obs. of  2 variables:\n $ count: int  3 3 10 2 6 5 4 6 9 10 ...\n $ alpha: Factor w/ 26 levels \"a\",\"b\",\"c\",\"d\",..: 15 19 14 3 10 18 22 11 5 20 ...\n\n상관관계가 높은 변수의 진단 및 제거\n선형모형(linear model)에서 독립변수간에 강한 상관관계(correlation)가 있으면, 다중공선성(multicollinearity)이 발생한다. 따라서 상관관계가 존재하는 변수쌍에서 하나의 변수를 제거하는 것이 좋다.\n선형모형이 아니더라도 상관관계가 큰 변수쌍에서 하나의 변수를 제거하면, 모델 적합과정에서 연산 오버헤드를 줄일 수도 있다. 그리고 모델을 해석하는 것도 쉬워진다.\ntreatment_corr()을 이용한 데이터 정제\ntreatment_corr() 은 상관관계가 높은 변수의 쌍을 진단하거나 제거한다. treatment_corr() 은 수치형 변수에 대해서는 피어슨(pearson)의 상관계수를 범주형 변수에 대해서는 스피어만(spearman)의 상관계수를 계산한다.\n다음 예제는 상관관계가 높은 변수를 진단하고, 제거한다.:\n\n\n# 수치형 변수\nx1 <- 1:100\nset.seed(12L)\nx2 <- sample(1:3, size = 100, replace = TRUE) * x1 + rnorm(1)\nset.seed(1234L)\nx3 <- sample(1:2, size = 100, replace = TRUE) * x1 + rnorm(1)\n\n# 범주형 변수\nx4 <- factor(rep(letters[1:20], time = 5))\nset.seed(100L)\nx5 <- factor(rep(letters[1:20 + sample(1:6, size = 20, replace = TRUE)], time = 5))\nset.seed(200L)\nx6 <- factor(rep(letters[1:20 + sample(1:3, size = 20, replace = TRUE)], time = 5))\nset.seed(300L)\nx7 <- factor(sample(letters[1:5], size = 100, replace = TRUE))\n\nexam <- data.frame(x1, x2, x3, x4, x5, x6, x7)\nstr(exam)\n\n\n'data.frame':   100 obs. of  7 variables:\n $ x1: int  1 2 3 4 5 6 7 8 9 10 ...\n $ x2: num  2.55 4.55 9.55 12.55 10.55 ...\n $ x3: num  0.194 2.194 4.194 6.194 3.194 ...\n $ x4: Factor w/ 20 levels \"a\",\"b\",\"c\",\"d\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ x5: Factor w/ 13 levels \"c\",\"e\",\"f\",\"g\",..: 1 5 3 2 4 7 6 8 9 8 ...\n $ x6: Factor w/ 15 levels \"c\",\"d\",\"f\",\"g\",..: 1 2 3 4 3 5 6 7 8 9 ...\n $ x7: Factor w/ 5 levels \"a\",\"b\",\"c\",\"d\",..: 2 2 1 4 5 1 4 3 1 5 ...\n\nhead(exam)\n\n\n  x1        x2         x3 x4 x5 x6 x7\n1  1  2.554297  0.1939687  a  c  c  b\n2  2  4.554297  2.1939687  b  h  d  b\n3  3  9.554297  4.1939687  c  f  f  a\n4  4 12.554297  6.1939687  d  e  g  d\n5  5 10.554297  3.1939687  e  g  f  e\n6  6  6.554297 10.1939687  f  l  h  a\n\n# default case\nexam_01 <- treatment_corr(exam)\nhead(exam_01)\n\n\n         x2         x3 x6 x7\n1  2.554297  0.1939687  c  b\n2  4.554297  2.1939687  d  b\n3  9.554297  4.1939687  f  a\n4 12.554297  6.1939687  g  d\n5 10.554297  3.1939687  f  e\n6  6.554297 10.1939687  h  a\n\n# not removing variables\ntreatment_corr(exam, treat = FALSE)\n\n# Set a threshold to detecting variables when correlation greater then 0.9\ntreatment_corr(exam, corr_thres = 0.9, treat = FALSE)\n\n# verbose FALSE 모드\nexam_02 <- treatment_corr(exam, verbose = FALSE)\nhead(exam_02)\n\n\n         x2         x3 x6 x7\n1  2.554297  0.1939687  c  b\n2  4.554297  2.1939687  d  b\n3  9.554297  4.1939687  f  a\n4 12.554297  6.1939687  g  d\n5 10.554297  3.1939687  f  e\n6  6.554297 10.1939687  h  a\n\nremove variables whose strong correlation : 상관관계가 높은 변수 x1, x4, x5가 제거된다.\ntraining set과 test set 데이터 분할\nData: 신용카드 연체 데이터\nISLR 패키지의 Default 1천명의 고객에 대한 정보를 포함하고 있는 시뮬레이션된 데이터다. 이 데이터는 어떤 고객이 신용카드의 부채를 상환하지 못하는지를 예측하는 목적의 데이터다.\n이 데이터 프레임은 1천개의 관측치와 다음의 4개 변수를 포함한다.:\ndefault : factor. 채무불이행 여부를 나낸다. “Y”는 불이행(연체), “N”은 이행을 의미한다.\nstudent: factor. 고객이 학생인지의 여부다. Y“는 학생,”N\"은 학생이 아니다.\nbalance: numeric. 고객의 월별 결제 후 신용카드 계자에 남아 있는 평균 잔액\nincome : numeric. 고객의 소득\n\n\n# 신용카드 연체 데이터\nhead(ISLR::Default)\n\n\n  default student   balance    income\n1      No      No  729.5265 44361.625\n2      No     Yes  817.1804 12106.135\n3      No      No 1073.5492 31767.139\n4      No      No  529.2506 35704.494\n5      No      No  785.6559 38463.496\n6      No     Yes  919.5885  7491.559\n\n# 데이터 구조\nstr(ISLR::Default)\n\n\n'data.frame':   10000 obs. of  4 variables:\n $ default: Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ student: Factor w/ 2 levels \"No\",\"Yes\": 1 2 1 1 1 2 1 2 1 1 ...\n $ balance: num  730 817 1074 529 786 ...\n $ income : num  44362 12106 31767 35704 38463 ...\n\n# 데이터 집계\nsummary(ISLR::Default)\n\n\n default    student       balance           income     \n No :9667   No :7056   Min.   :   0.0   Min.   :  772  \n Yes: 333   Yes:2944   1st Qu.: 481.7   1st Qu.:21340  \n                       Median : 823.6   Median :34553  \n                       Mean   : 835.4   Mean   :33517  \n                       3rd Qu.:1166.3   3rd Qu.:43808  \n                       Max.   :2654.3   Max.   :73554  \n\n데이터 분할\nsplit_by()는 data.frame이나 tbl_df 객체를 분할하여 training set과 test set로 나눈다.\nsplit_by()을 이용한 데이터 분할\nsplit_by()는 split_df 클래스 객체를 반환하는데, 여기에는 training set과 test set를 분리하기 위한 분할 정보 및 기준이 포함된다.\n\n\nlibrary(alookr)\nlibrary(dplyr)\n\n# 데이터의 분할\nsb <- ISLR::Default %>%\n  split_by(default, seed = 6534)\n\nsb\n\n\n# A tibble: 10,000 x 5\n# Groups:   split_flag [2]\n   default student balance income split_flag\n   <fct>   <fct>     <dbl>  <dbl> <chr>     \n 1 No      No         730. 44362. train     \n 2 No      Yes        817. 12106. train     \n 3 No      No        1074. 31767. train     \n 4 No      No         529. 35704. train     \n 5 No      No         786. 38463. test      \n 6 No      Yes        920.  7492. train     \n 7 No      No         826. 24905. test      \n 8 No      Yes        809. 17600. train     \n 9 No      No        1161. 37469. train     \n10 No      No           0  29275. train     \n# … with 9,990 more rows\n\nsplit_df 클래스의 주요한 속성(attributes)은 다음과 같다.:\nsplit_seed : integer. 데이터 분할에 사용된 random seed\ntarget : character. target 변수의 이름\nbinary : logical. target 변수의 binary class 여부\nminority : character. minority class 이름(수준, level)\nmajority : character. majority class 이름(수준, level)\nminority_rate : numeric. minority class 비율\nmajority_rate : numeric. majority class 비율\n\n\n# 모든 속성의 이름\nattr_names <- names(attributes(sb))\nattr_names\n\n\n [1] \"names\"         \"row.names\"     \"groups\"        \"class\"        \n [5] \"split_seed\"    \"target\"        \"binary\"        \"minority\"     \n [9] \"majority\"      \"minority_rate\" \"majority_rate\"\n\nsb_attr <- attributes(sb)\n\n# 세 번째 속성인 row.names는 그 길이가 매우 길기 때문에 출력에서 제외한다.\nsb_attr[!attr_names %in% \"row.names\"]\n\n\n$names\n[1] \"default\"    \"student\"    \"balance\"    \"income\"     \"split_flag\"\n\n$groups\n# A tibble: 2 x 2\n  split_flag       .rows\n  <chr>      <list<int>>\n1 test           [3,000]\n2 train          [7,000]\n\n$class\n[1] \"split_df\"   \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n$split_seed\n[1] 6534\n\n$target\n  default \n\"default\" \n\n$binary\n[1] TRUE\n\n$minority\n[1] \"Yes\"\n\n$majority\n[1] \"No\"\n\n$minority_rate\n   Yes \n0.0333 \n\n$majority_rate\n    No \n0.9667 \n\nsummary()는 split_by()에 의해서 분할된 두 데이터를 개괄적으로 집계한다.\n\n\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  6534 \n + split data            \n    - train set count :  7000 \n    - test set count  :  3000 \n + target variable    :  default \n    - minority class  :  Yes (0.033300)\n    - majority class  :  No (0.966700)\n\n데이터셋의 비교\ntraining set과 test set은 비슷해야 한다. 두 데이터가 유사하지 않으면, 예측모델의 성능이 저하될 수 있다.\nalookr은 training set과 test set의 유사성을 비교하는 기능을 제공한다.\n만약 두 데이터 세트가 유사하지 않으면, 원 데이터에서 training set과 test set을 다시 분할해야 한다.\ncompare_target_category()를 이용한 범주형 변수의 비교\nsplit_df 클래스에 포함된training set과 test set 정보에 근거해서 두 데이터 셋에서의 범주형 변수를 비교한다.\n\n\nsb %>%\n  compare_target_category()\n\n\n# A tibble: 4 x 5\n  variable level train  test abs_diff\n  <chr>    <fct> <dbl> <dbl>    <dbl>\n1 default  No    96.7  96.7   0.00476\n2 default  Yes    3.33  3.33  0.00476\n3 student  No    70.0  71.8   1.77   \n4 student  Yes   30.0  28.2   1.77   \n\n# character형 변수도 포함한다. 그러나, 예제에서는 character형 변수가 없기 때문에 결과는 동일하다.\nsb %>%\n  compare_target_category(add_character = TRUE)\n\n\n# A tibble: 4 x 5\n  variable level train  test abs_diff\n  <chr>    <fct> <dbl> <dbl>    <dbl>\n1 default  No    96.7  96.7   0.00476\n2 default  Yes    3.33  3.33  0.00476\n3 student  No    70.0  71.8   1.77   \n4 student  Yes   30.0  28.2   1.77   \n\n# 주변합(marginal)을 출력한다.\nsb %>%\n  compare_target_category(margin = TRUE)\n\n\n# A tibble: 6 x 5\n  variable level    train   test abs_diff\n  <chr>    <fct>    <dbl>  <dbl>    <dbl>\n1 default  No       96.7   96.7   0.00476\n2 default  Yes       3.33   3.33  0.00476\n3 default  <Total> 100    100     0.00952\n4 student  No       70.0   71.8   1.77   \n5 student  Yes      30.0   28.2   1.77   \n6 student  <Total> 100    100     3.54   \n\n# student 변수만 비교한다.\nsb %>%\n  compare_target_category(student)\n\n\n# A tibble: 2 x 5\n  variable level train  test abs_diff\n  <chr>    <fct> <dbl> <dbl>    <dbl>\n1 student  No     70.0  71.8     1.77\n2 student  Yes    30.0  28.2     1.77\n\nsb %>%\n  compare_target_category(student, margin = TRUE)\n\n\n# A tibble: 3 x 5\n  variable level   train  test abs_diff\n  <chr>    <fct>   <dbl> <dbl>    <dbl>\n1 student  No       70.0  71.8     1.77\n2 student  Yes      30.0  28.2     1.77\n3 student  <Total> 100   100       3.54\n\ncompare_target_category()는 tbl_df 객체를 반환하는데 다음과 같은 변수를 포함한다.:\nvariable : character. 범주형 변수의 이름\nlevel : factor. 범주형 변수의 수준\ntrain : numeric. training set에서의 해당 수준의 상대 돗수\ntest : numeric. test set에서의 해당 수준의 상대 돗수\nabs_diff : numeric. 두 상대 돗수 차의 절대치\ncompare_target_numeric()를 이용한 수치형 변수의 비교\nsplit_df 클래스에 포함된training set과 test set 정보에 근거해서 두 데이터 셋에서의 수치형 변수를 비교한다.\n\n\nsb %>%\n  compare_target_numeric()\n\n\n# A tibble: 2 x 7\n  variable train_mean test_mean train_sd test_sd train_z test_z\n  <chr>         <dbl>     <dbl>    <dbl>   <dbl>   <dbl>  <dbl>\n1 balance        836.      834.     487.    477.    1.72   1.75\n2 income       33446.    33684.   13437.  13101.    2.49   2.57\n\n# balance 변수만 비교한다.\nsb %>%\n  compare_target_numeric(balance)\n\n\n# A tibble: 1 x 7\n  variable train_mean test_mean train_sd test_sd train_z test_z\n  <chr>         <dbl>     <dbl>    <dbl>   <dbl>   <dbl>  <dbl>\n1 balance        836.      834.     487.    477.    1.72   1.75\n\ncompare_target_numeric()는 tbl_df 객체를 반환하는데 다음과 같은 변수를 포함한다.:\nvariable : character. 수치형 변수의 이름\ntrain_mean : numeric. training set에서의 산술평균\ntest_mean : numeric. test set에서의 산술평균\ntrain_sd : numeric. training set에서의 표준편차\ntest_sd : numeric. test set에서의 표준편차\ntrain_z : numeric. training set에서의 산술평균/표준편차\ntest_z : numeric. test set에서의 산술평균/표준편차n\ncompare_plot()를 이용한 시각화 비교\nsplit_df 클래스에 포함된 training set과 test set을 비교할 수 있는 플롯을 시각화한다.\n\n\n# income 변수만 비교 시각화 출력\nsb %>%\n  compare_plot(\"income\")\n\n\n\n\n\n\n# 모든 변수의 비교 플롯 출력\nsb %>%\n  compare_plot()\n\n\n\n\ncompare_diag()를 이용한 training set과 test set의 진단\nsplit_df 클래스에 포함된 training set과 test set 간의 유사성을 진단한다.\n\n\ndefaults <- ISLR::Default\ndefaults$id <- seq(NROW(defaults))\n\nset.seed(1)\ndefaults[sample(seq(NROW(defaults)), 3), \"student\"] <- NA\nset.seed(2)\ndefaults[sample(seq(NROW(defaults)), 10), \"balance\"] <- NA\n\nsb_2 <- defaults %>%\n  split_by(default)\n\nsb_2 %>%\n  compare_diag()\n\n\n$missing_value\n# A tibble: 3 x 4\n  variables train_misscount train_missrate test_missrate\n  <chr>               <int>          <dbl>         <dbl>\n1 student                 3         0.0429       NA     \n2 balance                 8         0.114        NA     \n3 balance                 2        NA             0.0667\n\n$single_value\n# A tibble: 0 x 3\n# … with 3 variables: variables <chr>, train_uniq <lgl>,\n#   test_uniq <lgl>\n\n$uniq_rate\n# A tibble: 0 x 5\n# … with 5 variables: variables <chr>, train_uniqcount <int>,\n#   train_uniqrate <dbl>, test_uniqcount <int>, test_uniqrate <dbl>\n\n$missing_level\n# A tibble: 1 x 4\n  variables n_levels train_missing_nlevel test_missing_nlevel\n  <chr>        <int>                <int>               <int>\n1 student          3                    0                   1\n\nsb_2 %>%\n  compare_diag(add_character = TRUE)\n\n\n$missing_value\n# A tibble: 3 x 4\n  variables train_misscount train_missrate test_missrate\n  <chr>               <int>          <dbl>         <dbl>\n1 student                 3         0.0429       NA     \n2 balance                 8         0.114        NA     \n3 balance                 2        NA             0.0667\n\n$single_value\n# A tibble: 0 x 3\n# … with 3 variables: variables <chr>, train_uniq <lgl>,\n#   test_uniq <lgl>\n\n$uniq_rate\n# A tibble: 0 x 5\n# … with 5 variables: variables <chr>, train_uniqcount <int>,\n#   train_uniqrate <dbl>, test_uniqcount <int>, test_uniqrate <dbl>\n\n$missing_level\n# A tibble: 1 x 4\n  variables n_levels train_missing_nlevel test_missing_nlevel\n  <chr>        <int>                <int>               <int>\n1 student          3                    0                   1\n\nsb_2 %>%\n  compare_diag(uniq_thres = 0.0005)\n\n\n$missing_value\n# A tibble: 3 x 4\n  variables train_misscount train_missrate test_missrate\n  <chr>               <int>          <dbl>         <dbl>\n1 student                 3         0.0429       NA     \n2 balance                 8         0.114        NA     \n3 balance                 2        NA             0.0667\n\n$single_value\n# A tibble: 0 x 3\n# … with 3 variables: variables <chr>, train_uniq <lgl>,\n#   test_uniq <lgl>\n\n$uniq_rate\n# A tibble: 2 x 5\n  variables train_uniqcount train_uniqrate test_uniqcount\n  <chr>               <int>          <dbl>          <int>\n1 default                NA             NA              2\n2 student                NA             NA              2\n# … with 1 more variable: test_uniqrate <dbl>\n\n$missing_level\n# A tibble: 1 x 4\n  variables n_levels train_missing_nlevel test_missing_nlevel\n  <chr>        <int>                <int>               <int>\n1 student          3                    0                   1\n\ntraining set과 test set의 추출\ntraining set과 test set를 비교하여 두 데이터 세트가 유사하다고 판단되면 split_df 객체에서 데이터를 추출한다.\nextract_set()을 이용한 training set과 test set 데이터 추출\n다음은 extract_set() 함수로 split_df 객체에서 training set과 test set을 추출한다.\n\n\ntrain <- sb %>%\n  extract_set(set = \"train\")\n\ntest <- sb %>%\n  extract_set(set = \"test\")\n\ndim(train)\n\n\n[1] 7000    4\n\ndim(test)\n\n\n[1] 3000    4\n\nsampling_target()을 이용한 모델 적합용 데이터 추출\n이진분류 모형의 target 변수(variable, class)에서 majority class와 minority class의 비율이 비슷하지 않은, 특히 minority class의 비율이 매우 작은 target class를 불균형 class(imbalanced class)라고 한다.\ntarget 변수가 불균형 class인 경우 대다수 클래스인 majority class의 특성이 모델에 적극적으로 반영된다. 이 경우에는 적합된 모델이 minority class를 대부분을 majority class로 예측하는 오류가 발생함을 의미한다. 따라서 training set을 균형잡힌 클래스로 만들어야합니다.\nsampling_target()은 split_df의 training set에서 샘플링을 수행하여 불균형 class 문제를 해결한다.\n\n\n# under-sampling with random seed\nunder <- sb %>%\n  sampling_target(seed = 1234L)\n\nunder %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No        233\n2 Yes       233\n\n# under-sampling with random seed, and minority class frequency is 40%\nunder40 <- sb %>%\n  sampling_target(seed = 1234L, perc = 40)\n\nunder40 %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No        349\n2 Yes       233\n\n# over-sampling with random seed\nover <- sb %>%\n  sampling_target(method = \"ubOver\", seed = 1234L)\n\nover %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No       6767\n2 Yes      6767\n\n# over-sampling with random seed, and k = 10\nover10 <- sb %>%\n  sampling_target(method = \"ubOver\", seed = 1234L, k = 10)\n\nover10 %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No       6767\n2 Yes      2330\n\n# SMOTE with random seed\nsmote <- sb %>%\n  sampling_target(method = \"ubSMOTE\", seed = 1234L)\n\nsmote %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No        932\n2 Yes       699\n\n# SMOTE with random seed, and perc.under = 250\nsmote250 <- sb %>%\n  sampling_target(method = \"ubSMOTE\", seed = 1234L, perc.under = 250)\n\nsmote250 %>%\n  count(default)\n\n\n# A tibble: 2 x 2\n  default     n\n  <fct>   <int>\n1 No       1165\n2 Yes       699\n\nsampling_target()에서 method 인수는 샘플링 방법을 지정한다. “ubUnder”는 under sampling을, “ubOver”는 over sampling, “ubSMOTE”는 SMOTE(Synthetic Minority Over-sampling TEchnique)을 수행한다.\n모델 적합, 평가 및 예측(Modeling and Evaluate, Predict)\nData: Wisconsin Breast Cancer 데이터\nmlbench 패키지의 BreastCancer는 유방암 데이터다. 이 데이터는 여러 종류의 양성 또는 악성 종양을 식별하는 목적으로 만들어졌다.\n이 데이터 프레임은 699개 관측치에, 다음과 같은 11개의 변수(하나는 문자형 변수, 9개는 순서형 또는 범주형, 1 개의 target 변수)로 구성되었다.:\nId : character. Sample code 번호\nCl.thickness : ordered factor. 종양 덩어리의 두께\nCell.size : ordered factor. 세포 크기의 균일성\nCell.shape : ordered factor. 세포 모양의 균일성\nMarg.adhesion : ordered factor. 부분적 유착정도\nEpith.c.size : ordered factor. 단일 상피세포의 크기\nBare.nuclei : factor. 노출된 핵들\nBl.cromatin : factor. 자극성이 적은 염색질(크로마틴)\nNormal.nucleoli : factor. 일반적인 소핵들\nMitoses : factor. Mitoses (유사분열)\nClass : factor. target 변수. benign(양성) malignant(악성)\n\n\nlibrary(mlbench)\ndata(BreastCancer)\n\n# class of each variables\nsapply(BreastCancer, function(x) class(x)[1])\n\n\n             Id    Cl.thickness       Cell.size      Cell.shape \n    \"character\"       \"ordered\"       \"ordered\"       \"ordered\" \n  Marg.adhesion    Epith.c.size     Bare.nuclei     Bl.cromatin \n      \"ordered\"       \"ordered\"        \"factor\"        \"factor\" \nNormal.nucleoli         Mitoses           Class \n       \"factor\"        \"factor\"        \"factor\" \n\n데이터 전처리\n다음과 같은 전처리를 수행한다.:\n결측치가 포함된 변수를 찾아내고, 대체(imputate)한다.\n데이터를 training set과 test set으로 나눈다.\n불균형 클래스 문제를 해결하기 위해서, training set에서 샘플링을 수행한다.\n분류 모델을 수행하기 위해서 데이터를 정제한다.\ndlookr::imputate_na()를 이용한 결측치 처리\n결측치가 포함된 변수를 찾아서, dlookr 패키지의 imputate_na()를 사용하여 누락된 값을 대체한다.\n\n\nlibrary(dlookr)\nlibrary(dplyr)\n\n# 결측치를 포함한 변수\ndiagnose(BreastCancer) %>%\n  filter(missing_count > 0)\n\n\n# A tibble: 1 x 6\n  variables   types  missing_count missing_percent unique_count\n  <chr>       <chr>          <int>           <dbl>        <int>\n1 Bare.nuclei factor            16            2.29           11\n# … with 1 more variable: unique_rate <dbl>\n\n# 결측치의 대체 수행\nbreastCancer <- BreastCancer %>%\n  mutate(Bare.nuclei = imputate_na(BreastCancer, Bare.nuclei, Class,\n                         method = \"mice\", no_attrs = TRUE, print_flag = FALSE))\n\n\n\n데이터 분할\nsplit_by()을 이용한 training set과 test set의 분할\nalookr 패키지의 split_by()는 원시 데이터에서 training set과 test set을 분할한다.\nsplit_by()의 ratio 인수는 training set의 비율을 지정하며, split_df 클래스 객체를 생성한다.\n\n\nlibrary(alookr)\n\n# 기본 인수로 training set과 test set로 분할\nsb <- breastCancer %>%\n  split_by(target = Class)\n\n# 클래스 이름을 조회\nclass(sb)\n\n\n[1] \"split_df\"   \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n# ratio = 0.6로 training set과 test set을 분할\ntmp <- breastCancer %>%\n  split_by(Class, ratio = 0.6)\n\n\n\nsummary()는 split_df 객체에 대한 다음과 같은 유용한 정보를 표시한다.:\nrandom seed : random seed는 내부적으로 데이터를 분리하는 데 사용되는 random seed다.\nsplit data : 분할 데이터의 정보\ntraining set count : training set 수\ntest set count : test set 수\n\ntarget variable : target 변수 이름\nminority class : minority class의 이름과 비율\nmajority class : majority class의 이름과 비율\n\n\n\n# 집계 정보\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  2618 \n + split data            \n    - train set count :  489 \n    - test set count  :  210 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\n# 집계 정\nsummary(tmp)\n\n\n** Split train/test set information **\n + random seed        :  15164 \n + split data            \n    - train set count :  419 \n    - test set count  :  280 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\ntraining set에서 누락된 수준(level) 확인\n범주형 변수의 경우 training set와 test set가 분리되면서, 특정 수준이 training set에서 누락될 수 있다.\n이 경우 모형을 적합할(fitting) 때는 문제가 없지만, 작성한 모형으로 예측할 때 오류가 발생한다. 따라서 누락된 수준을 방지하기 위해 전처리가 수행된다.\n다음 예제에서는 다행히도 training set에서 누락된 수준을 포함하는 범주형 변수가 없다.\n\n\n# 결측 레벨이 포함된 training set의 범주형 변수 목록\nnolevel_in_train <- sb %>%\n  compare_target_category() %>% \n  filter(train == 0) %>% \n  select(variable) %>% \n  unique() %>% \n  pull\n\nnolevel_in_train\n\n\ncharacter(0)\n\n# training set의 범주형 변수 중 결측 수준이 포함된 경우, 재 분할\nwhile (length(nolevel_in_train) > 0) {\n  sb <- breastCancer %>%\n    split_by(Class)\n\n  nolevel_in_train <- sb %>%\n    compare_category() %>% \n    filter(train == 0) %>% \n    select(variable) %>% \n    unique() %>% \n    pull\n}\n\n\n\nsampling_target()를 이용한 불균형 클래스 핸들링\n불균형 클래스 데이터의 문제\n불균형 클래스(수준) 데이터는 target 변수의 도수(frequency)에서 한 수준의 도수가 상대적으로 매우 작다는 것을 의미한다. 그러나 대다수의 분류 모델의 positive class 비율은 상대적으로 작습니다. 예를 들어 스팸 예측 모델에서 관심있는 스팸 메일(positive class)의 비율은 스팸이 아닌 메일에 비해 매우 작다.\n불균형 클래스 데이터는 기계 학습 분류에서 일반적으로 발생하는 문제다.\ntable() 및 prop.table()은 불균형 클래스 데이터를 진단하는 데 유용한 함수다. 그러나 alookr 패키지의 summary()는 더 간단하며, 좀 더 자세한 정보를 제공한다.\n\n\n# training set 돗수분포표 - 불균형 클래스 데이터\ntable(sb$Class)\n\n\n\n   benign malignant \n      458       241 \n\n# training set 상대돗수분포표 - 불균형 클래스 데이터\nprop.table(table(sb$Class))\n\n\n\n   benign malignant \n0.6552217 0.3447783 \n\n# 집계 - 불균형 클래스 데이터\nsummary(sb)\n\n\n** Split train/test set information **\n + random seed        :  2618 \n + split data            \n    - train set count :  489 \n    - test set count  :  210 \n + target variable    :  Class \n    - minority class  :  malignant (0.344778)\n    - majority class  :  benign (0.655222)\n\n불균형 클래스 데이터 핸들링\n대부분의 기계 학습 알고리즘은 각 클래스의 샘플 수가 거의 같을 때 가장 잘 작동된다. 또한 대부분의 알고리즘은 정확도를 최대화( maximize accuracy)하고 오류를 줄이도록 설계되었다. 따라서 우리는 반드시 불균형 클래스 문제를 처리해한다.\nsampling_target()는 샘플링을 수행하여 불균형 클래스 데이터 문제를 해결한다.\nResampling - minority class의 오버 샘플링\n오버 샘플링(over sampling)은 minority class의 사본을 더 추가하는 방법으로 수행한다.\n오버 샘플링은 sampling_target() 함수의 method 인수값을 “ubOver”로 지정하여 수행한다.\n\n\n# 오버 샘플링\ntrain_over <- sb %>%\n  sampling_target(method = \"ubOver\")\n\n# 돗수분포표\ntable(train_over$Class)\n\n\n\n   benign malignant \n      322       322 \n\nResampling - majority class의 언더 샘플링\n언더 샘플링(under sampling)은 majority class의 관측치의 일부를 제거하는 방법으로 수행한다.\n언더 샘플링은 sampling_target() 함수의 method 인수값을 “ubUnder”로 지정하여 수행한다.\n\n\n# 언더 샘플링\ntrain_under <- sb %>%\n  sampling_target(method = \"ubUnder\")\n\n# 돗수분포표\ntable(train_under$Class)\n\n\n\n   benign malignant \n      167       167 \n\nGenerate synthetic samples - SMOTE\nSMOTE(Synthetic Minority Oversampling Technique)는 최근접 이웃 알고리즘(nearest neighbors algorithm)을 사용하여 새로운 데이터를 합성하여 생생한다.\nSMOTE 샘플링은 sampling_target() 함수의 method 인수값을 “ubSMOTE”로 지정하여 수행한다.\n\n\n# SMOTE 샘플링\ntrain_smote <- sb %>%\n  sampling_target(seed = 1234L, method = \"ubSMOTE\")\n\n# 돗수분포표\ntable(train_smote$Class)\n\n\n\n   benign malignant \n      668       501 \n\ncleanse()를 이용하여 분류 모델을 위한 데이터 정제\ncleanse()는 분류 모델링을 위한 데이터를 정제한다.\n이 함수는 분류 모델을 적합할 때 유용하며, 다음의 기능을 수행한다.:\n오직 유일한 하나의 값을 갖는 변수를 제거한다.\n문자형 또는 범주형 변수에 대해서, 관측치 수와 비교하여 고유값의 수가 많은 변수를 제거한다.\n이 경우는 대개 식별자 또는 식별자와 유사한 변수들이다.\n\n그리고 character 변수를 factor 변수로 변환한다.\n이 예제에서 cleanse()는 고유값 비율이 높은 변수 ID를 제거했다.\n\n\n# training set의 정제\ntrain <- train_smote %>%\n  cleanse\n\n\n── Checking unique value ─────────────────────────── unique value is one ──\nNo variables that unique value is one.\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• Id = 425(0.363558597091531)\n\n── Checking character variables ─────────────────────── categorical data ──\nNo character variables.\n\nextract_set()을 이용하여 모델 평가를 위한 test set 추출\n\n\n# test set 추출\ntest <- sb %>%\n  extract_set(set = \"test\")\n\n\n\nrun_models()을 이용한 이진분류 모델 수행\nrun_models()는 split_by()로 작성된 split_df 객체를 사용하여 잘 알려진, 대표적인 이진분류(binary classification) 모델링을 수행한다.\nrun_models()는 모델을 적합할 때, 병렬처리로 수행된다. 다만, MS-Windows 운영체제와 RStudio 환경에서는 지원하지 않는다.\n현재 지원하는 알고리즘은 다음과 같다.:\nlogistic : 로지스틱 회귀(logistic regression), stats 패키지의 glm()\nrpart : Recursive Partitioning Trees, rpart 패키지의 rpart()\nctree : Conditional Inference Trees, party 패키지의 ctree()\nrandomForest :Classification with Random Forest, randomForest 패키지의 randomForest()\nranger : A Fast Implementation of Random Forests, ranger 패키지의 ranger()\nrun_models()은 model_df 클래스를 반환하며, model_df 클래스 다음의 변수를 포함한다.:\nstep : character. 이진분류 모델의 프로세스에서 진행한 해당 단계\nrun_models()로 생성 된 객체의 경우 변수는 “1.Fitted”라는 값을 갖는다.\n\nmodel_id : model 식별자\ntarget : target 변수의 이름\npositive : target variable의 positive class\nfitted_model : list. model_id 알고리즘으로 적합된 모델 객체\n\n\nresult <- train %>% \n  run_models(target = \"Class\", positive = \"malignant\")\n\nresult\n\n\n# A tibble: 7 x 7\n  step     model_id    target is_factor positive negative fitted_model\n  <chr>    <chr>       <chr>  <lgl>     <chr>    <chr>    <list>      \n1 1.Fitted logistic    Class  TRUE      maligna… benign   <glm>       \n2 1.Fitted rpart       Class  TRUE      maligna… benign   <rpart>     \n3 1.Fitted ctree       Class  TRUE      maligna… benign   <BinaryTr>  \n4 1.Fitted randomFore… Class  TRUE      maligna… benign   <rndmFrs.>  \n5 1.Fitted ranger      Class  TRUE      maligna… benign   <ranger>    \n6 1.Fitted xgboost     Class  TRUE      maligna… benign   <xgb.Bstr>  \n7 1.Fitted lasso       Class  TRUE      maligna… benign   <lognet>    \n\n모델의 평가\n적합된 모델의 예측 성능을 평가한다.\nrun_predict()을 이용하여 적합한 모델로 test set target 변수 예측\nrun_predict()는 run_models()에 의해서 적합된 모델로 model_df 클래스 개체에 포함된 test set의 target 변수를 예측한다.\nrun_predict()는 모델로 예측할 때, 병렬처리로 수행된다. 다만, MS-Windows 운영체제와 RStudio 환경에서는 지원하지 않는다.\nmodel_df 클래스 객체에는 다음 변수가 포함된다.:\nstep : character. 이진분류 모델의 프로세스에서 진행한 해당 단계\nrun_predict()로 생성 된 객체의 경우 변수는 “2.Predicted”라는 값을 갖는다.\n\nmodel_id : model 식별자\ntarget : target 변수의 이름\npositive : target variable의 positive class\nfitted_model : list. model_id 알고리즘으로 적합된 모델 객체\npredicted : 각 모델별 예측 결과\n\n\npred <- result %>%\n  run_predict(test)\n\npred\n\n\n# A tibble: 7 x 8\n  step      model_id   target is_factor positive negative fitted_model\n  <chr>     <chr>      <chr>  <lgl>     <chr>    <chr>    <list>      \n1 2.Predic… logistic   Class  TRUE      maligna… benign   <glm>       \n2 2.Predic… rpart      Class  TRUE      maligna… benign   <rpart>     \n3 2.Predic… ctree      Class  TRUE      maligna… benign   <BinaryTr>  \n4 2.Predic… randomFor… Class  TRUE      maligna… benign   <rndmFrs.>  \n5 2.Predic… ranger     Class  TRUE      maligna… benign   <ranger>    \n6 2.Predic… xgboost    Class  TRUE      maligna… benign   <xgb.Bstr>  \n7 2.Predic… lasso      Class  TRUE      maligna… benign   <lognet>    \n# … with 1 more variable: predicted <list>\n\nrun_performance()을 이용한 성능평가 지표의 계산\nrun_performance()는 run_predict()에 의해 예측된 model_df 클래스 결과로 상능평가 지표를 계산한다.\nrun_performance()는 성능평가 지표를 계산할 때, 병렬처리로 수행된다. 다만, MS-Windows 운영체제와 RStudio 환경에서는 지원하지 않는다.\nmodel_df 클래스 객체에는 다음 변수가 포함된다.:\nstep : character. 이진분류 모델의 프로세스에서 진행한 해당 단계\nrun_performance()로 생성 된 객체의 경우 변수는 “3.Performanced”라는 값을 갖는다.\n\nmodel_id : model 식별자\ntarget : target 변수의 이름\npositive : target variable의 positive class\nfitted_model : list. model_id 알고리즘으로 적합된 모델 객체\npredicted : 각 모델별 예측 결과. Each value has a predict_class class object.\nperformance : list. 개별 모델별로 평가 지표(mertic)을 계산한다. 모델별로 성능 지표의 수치벡터를 갖는다.\n\n\n# Calculate performace metrics.\nperf <- run_performance(pred)\n\nperf\n\n\n# A tibble: 7 x 7\n  step    model_id  target positive fitted_model predicted performance\n  <chr>   <chr>     <chr>  <chr>    <list>       <list>    <list>     \n1 3.Perf… logistic  Class  maligna… <glm>        <fct [21… <dbl [15]> \n2 3.Perf… rpart     Class  maligna… <rpart>      <fct [21… <dbl [15]> \n3 3.Perf… ctree     Class  maligna… <BinaryTr>   <fct [21… <dbl [15]> \n4 3.Perf… randomFo… Class  maligna… <rndmFrs.>   <fct [21… <dbl [15]> \n5 3.Perf… ranger    Class  maligna… <ranger>     <fct [21… <dbl [15]> \n6 3.Perf… xgboost   Class  maligna… <xgb.Bstr>   <fct [21… <dbl [15]> \n7 3.Perf… lasso     Class  maligna… <lognet>     <fct [21… <dbl [15]> \n\nperformance 변수는 다음과 같은 15개의 성능 지표를 표함한 list 개체다.:\nZeroOneLoss : Normalized Zero-One Loss(Classification Error Loss).\nAccuracy : Accuracy.\nPrecision : Precision.\nRecall : Recall.\nSensitivity : Sensitivity.\nSpecificity : Specificity.\nF1_Score : F1 Score.\nFbeta_Score : F-Beta Score.\nLogLoss : Log loss / Cross-Entropy Loss.\nAUC : Area Under the Receiver Operating Characteristic Curve (ROC AUC).\nGini : Gini Coefficient.\nPRAUC : Area Under the Precision-Recall Curve (PR AUC).\nLiftAUC : Area Under the Lift Chart.\nGainAUC : Area Under the Gain Chart.\nKS_Stat : Kolmogorov-Smirnov Statistic.\n\n\n# Performance by analytics models\nperformance <- perf$performance\nnames(performance) <- perf$model_id\nperformance\n\n\n$logistic\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.04761905  0.95238095  0.94444444  0.91891892  0.91891892 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.97058824  0.93150685  0.93150685  1.64471887  0.94455485 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.90798887  0.06533148  1.22195953  0.78790219 88.95071542 \n\n$rpart\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.04285714  0.95714286  0.92207792  0.95945946  0.95945946 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.95588235  0.94039735  0.94039735  0.76045925  0.95791932 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.91931638  0.67851233  1.78136627  0.79655727 91.53418124 \n\n$ctree\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.03333333  0.96666667  0.94666667  0.95945946  0.95945946 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.97058824  0.95302013  0.95302013  0.68735941  0.97267488 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.96184420  0.68037312  1.82779002  0.80611326 94.35612083 \n\n$randomForest\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.01904762  0.98095238  0.97297297  0.97297297  0.97297297 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.98529412  0.97297297  0.97297297  0.09088358  0.99364070 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.98708267  0.81129910  1.92087086  0.81969112 97.17806041 \n\n$ranger\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.01904762  0.98095238  0.97297297  0.97297297  0.97297297 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.98529412  0.97297297  0.97297297  0.07684005  0.99691971 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.99383943  0.95454997  2.05646914  0.82181467 97.17806041 \n\n$xgboost\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.03333333  0.96666667  0.94666667  0.95945946  0.95945946 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.97058824  0.95302013  0.95302013  0.09384642  0.99682035 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.99383943  0.92716301  2.02693559  0.82175032 94.59459459 \n\n$lasso\nZeroOneLoss    Accuracy   Precision      Recall Sensitivity \n 0.02857143  0.97142857  0.98571429  0.93243243  0.93243243 \nSpecificity    F1_Score Fbeta_Score     LogLoss         AUC \n 0.99264706  0.95833333  0.95833333  0.06740949  0.99672099 \n       Gini       PRAUC     LiftAUC     GainAUC     KS_Stat \n 0.99344197  0.98130004  2.02404320  0.82168597 97.17806041 \n\nlist 객체를 다음처럼 깔끔한 형식으로 변경하면 결과를 한눈에 쉽게 볼 수 있다.:\n\n\n# 성능 평가지표 정보를 행렬로 변경하여 출력\nsapply(performance, \"c\")\n\n\n               logistic       rpart       ctree randomForest\nZeroOneLoss  0.04761905  0.04285714  0.03333333   0.01904762\nAccuracy     0.95238095  0.95714286  0.96666667   0.98095238\nPrecision    0.94444444  0.92207792  0.94666667   0.97297297\nRecall       0.91891892  0.95945946  0.95945946   0.97297297\nSensitivity  0.91891892  0.95945946  0.95945946   0.97297297\nSpecificity  0.97058824  0.95588235  0.97058824   0.98529412\nF1_Score     0.93150685  0.94039735  0.95302013   0.97297297\nFbeta_Score  0.93150685  0.94039735  0.95302013   0.97297297\nLogLoss      1.64471887  0.76045925  0.68735941   0.09088358\nAUC          0.94455485  0.95791932  0.97267488   0.99364070\nGini         0.90798887  0.91931638  0.96184420   0.98708267\nPRAUC        0.06533148  0.67851233  0.68037312   0.81129910\nLiftAUC      1.22195953  1.78136627  1.82779002   1.92087086\nGainAUC      0.78790219  0.79655727  0.80611326   0.81969112\nKS_Stat     88.95071542 91.53418124 94.35612083  97.17806041\n                 ranger     xgboost       lasso\nZeroOneLoss  0.01904762  0.03333333  0.02857143\nAccuracy     0.98095238  0.96666667  0.97142857\nPrecision    0.97297297  0.94666667  0.98571429\nRecall       0.97297297  0.95945946  0.93243243\nSensitivity  0.97297297  0.95945946  0.93243243\nSpecificity  0.98529412  0.97058824  0.99264706\nF1_Score     0.97297297  0.95302013  0.95833333\nFbeta_Score  0.97297297  0.95302013  0.95833333\nLogLoss      0.07684005  0.09384642  0.06740949\nAUC          0.99691971  0.99682035  0.99672099\nGini         0.99383943  0.99383943  0.99344197\nPRAUC        0.95454997  0.92716301  0.98130004\nLiftAUC      2.05646914  2.02693559  2.02404320\nGainAUC      0.82181467  0.82175032  0.82168597\nKS_Stat     97.17806041 94.59459459 97.17806041\n\ncompare_performance()는 list 객체(모델 성능 비교 결과)를 반환하며, list 객체는 다음과 같은 성분(component)가 포함되어 있다.\nrecommend_model : character. 적합된 다양한 모델 중에서 가장 권장되는 모델의 이름\ntop_count : numeric. 모델별 최고 성능 지표의 수\nmean_rank : numeric. 모델별 개별 성능 지표 순위의 평균\ntop_metric : list. 모델별 개별 성능 메트릭에서 최상의 성능을 제공하는 성능 메트릭의 이름\n이 예제에서 compare_performance()는 “ranger”모델을 권장한다.\n\n\n# 개별 모델별 성능 평가지표의 비교\ncomp_perf <- compare_performance(pred)\n\ncomp_perf\n\n\n$recommend_model\n[1] \"ranger\"\n\n$top_metric_count\n    logistic        rpart        ctree randomForest       ranger \n           0            0            0            5            9 \n     xgboost        lasso \n           1            5 \n\n$mean_rank\n    logistic        rpart        ctree randomForest       ranger \n    6.769231     6.000000     4.769231     2.769231     1.653846 \n     xgboost        lasso \n    3.500000     2.538462 \n\n$top_metric\n$top_metric$logistic\nNULL\n\n$top_metric$rpart\nNULL\n\n$top_metric$ctree\nNULL\n\n$top_metric$randomForest\n[1] \"ZeroOneLoss\" \"Accuracy\"    \"Recall\"      \"F1_Score\"   \n[5] \"KS_Stat\"    \n\n$top_metric$ranger\n[1] \"ZeroOneLoss\" \"Accuracy\"    \"Recall\"      \"F1_Score\"   \n[5] \"AUC\"         \"Gini\"        \"LiftAUC\"     \"GainAUC\"    \n[9] \"KS_Stat\"    \n\n$top_metric$xgboost\n[1] \"Gini\"\n\n$top_metric$lasso\n[1] \"Precision\"   \"Specificity\" \"LogLoss\"     \"PRAUC\"      \n[5] \"KS_Stat\"    \n\nplot_performance()이용한 ROC 곡선의 시각화\ncompare_performance()은 ROC 곡선을 그린다.\n\n\n# ROC 곡선의 시각화\nplot_performance(pred)\n\n\n\n\ncut-off 튜닝\n일반적으로 예측 확률이 0.5보다 클 경우에는 positive class로 예측한다. 즉, cut-off로 0.5를 사용하는 것이다. 이것은 대부분의 모델 알고리즘에서 적용된다. 그러나 경우에 따라서 cut-off의 값을 변경하여, 성능을 튜닝할 수도 있다.\nplot_cutoff()는 cut-off의 값을 선정할 수 있는 플롯을 시각화하며, cut-off 값도 반환한다.\n\n\npred_best <- pred %>% \n  filter(model_id == comp_perf$recommend_model) %>% \n  select(predicted) %>% \n  pull %>% \n  .[[1]] %>% \n  attr(\"pred_prob\")\n\ncutoff <- plot_cutoff(pred_best, test$Class, \"malignant\", type = \"mcc\")\n\n\n\ncutoff\n\n\n[1] 0.42\n\ncutoff2 <- plot_cutoff(pred_best, test$Class, \"malignant\", type = \"density\")\n\n\n\ncutoff2\n\n\n[1] 0.6477\n\ncutoff3 <- plot_cutoff(pred_best, test$Class, \"malignant\", type = \"prob\")\n\n\n\ncutoff3\n\n\n[1] 0.42\n\nperformance_metric()을 이용한 예측과 조정된 컷오프 예측의 성능 비교\n원래 예측 성능과 조정된 컷오프 예측의 성능을 비교한다. 최상의 성능을 나타낸 모델 comp_perf$recommend_model과 컷오프를 변경한 모델을 비교해 보자.\n\n\ncomp_perf$recommend_model\n\n\n[1] \"ranger\"\n\n# 예측 확률의 추출\nidx <- which(pred$model_id == comp_perf$recommend_model)\npred_prob <- attr(pred$predicted[[idx]], \"pred_prob\")\n\n# 혹은, dplyr을 이용한 예측 확률의 추출\npred_prob <- pred %>% \n  filter(model_id == comp_perf$recommend_model) %>% \n  select(predicted) %>% \n  pull %>% \n  \"[[\"(1) %>% \n  attr(\"pred_prob\")\n\n# 예측된 확률값\npred_prob  \n\n\n  [1] 0.000000000 0.984200000 0.000000000 0.986300000 0.978838095\n  [6] 0.000000000 0.000000000 0.000000000 0.861914286 0.003653968\n [11] 0.972980952 0.695388095 0.000000000 0.802765079 0.904215873\n [16] 0.000000000 0.235516667 0.000000000 0.000000000 0.000000000\n [21] 0.000000000 0.598628571 0.960562698 0.015192857 0.934376984\n [26] 0.067979365 0.000950000 0.999600000 0.000000000 0.214118254\n [31] 0.000000000 0.000000000 0.097299206 0.000000000 0.000000000\n [36] 0.000000000 0.000000000 0.002722222 0.780812698 0.000000000\n [41] 0.000000000 0.000000000 0.000000000 0.985964286 0.000000000\n [46] 0.000000000 0.979764286 0.427121429 0.000000000 0.934456349\n [51] 0.000000000 0.000000000 0.018697619 0.000000000 0.990339683\n [56] 0.997214286 0.000000000 0.961404762 0.000000000 0.994466667\n [61] 0.000000000 0.000000000 0.983619841 0.009397619 0.975446032\n [66] 0.000000000 1.000000000 0.019753968 0.996466667 0.998711111\n [71] 0.283431746 0.000000000 0.202021429 0.000000000 0.995159524\n [76] 0.892195238 0.000000000 0.827315873 0.995550000 0.997761905\n [81] 0.912645238 0.900494444 0.993251587 0.000000000 0.000000000\n [86] 0.000000000 0.778804762 0.000000000 0.926170635 0.278473016\n [91] 0.774467460 0.911610317 1.000000000 0.881719048 0.967116667\n [96] 0.000000000 0.243321429 0.880563492 0.971373016 0.959742857\n[101] 0.950256349 0.909520635 0.000000000 0.950880159 0.000000000\n[106] 0.000000000 0.000000000 0.227507937 0.705684127 0.000000000\n[111] 1.000000000 0.950929365 0.933356349 0.000000000 0.000000000\n[116] 0.000000000 0.000000000 0.000000000 0.208778571 0.958753175\n[121] 0.000000000 0.000000000 0.000000000 0.000000000 0.000000000\n[126] 0.000000000 0.029394444 0.000000000 0.999083333 0.947929365\n[131] 0.101830952 0.992433333 0.000000000 0.000000000 0.911637302\n[136] 0.000000000 0.024001587 0.000000000 0.000000000 0.000000000\n[141] 0.010020635 0.915786508 0.000000000 0.000000000 0.000000000\n[146] 0.000000000 0.990155556 0.000000000 0.000000000 0.000000000\n[151] 0.987831746 0.000000000 0.000000000 0.986268254 0.000000000\n[156] 0.000000000 0.000000000 0.005063492 0.000000000 0.000000000\n[161] 0.009394444 0.000000000 0.000000000 0.000000000 0.000000000\n[166] 0.197366667 0.274388095 0.000000000 0.973695238 0.917184921\n[171] 0.992461905 0.000000000 0.000000000 0.000000000 0.992255556\n[176] 0.000000000 0.000000000 0.000000000 0.958853175 0.000000000\n[181] 0.988611111 0.999800000 0.001685714 0.000000000 0.000000000\n[186] 0.959733333 0.000000000 0.979682540 0.000000000 0.000000000\n[191] 0.000000000 0.000800000 0.000000000 0.000000000 0.000000000\n[196] 0.612531746 0.000000000 0.000000000 0.975539683 0.000200000\n[201] 0.000000000 0.000000000 0.000000000 0.993961111 0.000000000\n[206] 0.902404762 0.000000000 0.003653968 0.979246032 0.949333333\n\n# Accuracy 비교\nperformance_metric(pred_prob, test$Class, \"malignant\", \"Accuracy\")\n\n\n[1] 0.9809524\n\nperformance_metric(pred_prob, test$Class, \"malignant\", \"Accuracy\",\n                   cutoff = cutoff)\n\n\n[1] 0.9857143\n\n# Confusion Matrix 비교\nperformance_metric(pred_prob, test$Class, \"malignant\", \"ConfusionMatrix\")\n\n\n           actual\npredict     benign malignant\n  benign       134         2\n  malignant      2        72\n\nperformance_metric(pred_prob, test$Class, \"malignant\", \"ConfusionMatrix\", \n                   cutoff = cutoff)\n\n\n           actual\npredict     benign malignant\n  benign       134         1\n  malignant      2        73\n\n# F1 Score 비교\nperformance_metric(pred_prob, test$Class, \"malignant\", \"F1_Score\")\n\n\n[1] 0.972973\n\nperformance_metric(pred_prob, test$Class,  \"malignant\", \"F1_Score\", \n                   cutoff = cutoff)\n\n\n[1] 0.9798658\n\nperformance_metric(pred_prob, test$Class,  \"malignant\", \"F1_Score\", \n                   cutoff = cutoff2)\n\n\n[1] 0.9726027\n\n조정된 컷오프(cut-off)의 성능이 양호하면, 컷오프를 적용하여 positives class를 예측한다.\n예측\n여러 모델에서 최상의 모델을 선택한 경우, 해당 모델로 예측을 수행한다.\n예측을 위한 데이터 생성\n이전 샘플링 예제에서 사용된 데이터 세트에서 100개의 샘플을 추출하여 예측할 데이터를 생성한다.\n\n\ndata_pred <- train_under %>% \n  cleanse \n\n\n── Checking unique value ─────────────────────────── unique value is one ──\nNo variables that unique value is one.\n\n── Checking unique rate ─────────────────────────────── high unique rate ──\n• Id = 326(0.976047904191617)\n\n── Checking character variables ─────────────────────── categorical data ──\nNo character variables.\n\nset.seed(1234L)\ndata_pred <- data_pred %>% \n  nrow %>% \n  seq %>% \n  sample(size = 50) %>% \n  data_pred[., ]\n\n\n\nalookr과 dplyr를 이용한 예측\n다음과 같이 alookr과 dplyr 패키지를 사용하여 예측한다. 마지막 factor() 함수는 불필요한 정보를 제거한다.\n\n\npred_actual <- pred %>%\n  filter(model_id == comp_perf$recommend_model) %>% \n  run_predict(data_pred) %>% \n  select(predicted) %>% \n  pull %>% \n  \"[[\"(1) %>% \n  factor()\n\npred_actual\n\n\n [1] benign    malignant malignant benign    malignant benign   \n [7] benign    malignant malignant malignant malignant benign   \n[13] benign    malignant malignant benign    benign    benign   \n[19] malignant benign    malignant malignant malignant benign   \n[25] benign    benign    benign    benign    malignant malignant\n[31] malignant malignant benign    malignant malignant malignant\n[37] benign    benign    benign    benign    malignant benign   \n[43] benign    malignant benign    benign    benign    benign   \n[49] malignant benign   \nLevels: benign malignant\n\n컷오프로 예측하려면 다음과 같이 run_predict() 함수에 컷오프 cutoff 인수를 지정한다.:\n이 예제에서는 컷오프(cut-off) 사용 결과와 사용하지 않는 결과 간에 차이가 없다.\n\n\npred_actual2 <- pred %>%\n  filter(model_id == comp_perf$recommend_model) %>% \n  run_predict(data_pred, cutoff) %>% \n  select(predicted) %>% \n  pull %>% \n  \"[[\"(1) %>% \n  factor()\n\npred_actual2\n\n\n [1] benign    malignant malignant benign    malignant benign   \n [7] benign    malignant malignant malignant malignant benign   \n[13] benign    malignant malignant benign    benign    benign   \n[19] malignant benign    malignant malignant malignant benign   \n[25] benign    benign    benign    benign    malignant malignant\n[31] malignant malignant benign    malignant malignant malignant\n[37] benign    benign    benign    benign    malignant benign   \n[43] benign    malignant benign    benign    benign    benign   \n[49] malignant benign   \nLevels: benign malignant\n\nsum(pred_actual != pred_actual2)\n\n\n[1] 0\n\n\n\n\n",
    "preview": "posts/2020-03-27-r-alookr/2020-03-27-r-alookr_files/figure-html5/split_data7-1.png",
    "last_modified": "2021-10-23T09:17:47+09:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 768
  },
  {
    "path": "posts/2020-03-22-window_function/",
    "title": "Introduce window function",
    "description": "\"데이터 분석의 중요성이 대두되면서 1990년대 말부터 많은 DBMS는 행과 행간의 관계를 정의하거나 행과 행간을 비교하고 연산하는 함수의 기능을 SQL에 추가하기 시작했다. 그리고 이들을 `window function`이라 불렀다.\"",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2020-03-22",
    "categories": [
      "Tidyverse"
    ],
    "contents": "\n\nContents\nwindows function\ndplyr에서 windows function 구현\n데이터 생성\n문제정의\n\nKOSPI 데이터에서의 집계\n거래소별로 데이터의 집계\n\nwindows function\nTidyverse가 조작하는, 특히 dplyr가 조작하는 데이터 객체인 data.frame이나 tbl_df 객체는 DBMS(Data Base Management System)의 table과 유사하다. 이들은 컬럼(R에서는 변수, Variable)과 컬럼간의 연산이나 비교가 비교적 쉽다. 또한 집합에 대한 집계도 쉽게 처리한다. 그러나, 행(R에서는 관측치, Observation)과 행간의 관계를 정의하거나 행과 행간을 비교하고 연산하는 것은 쉽지 않다.\nDBMS에서는 SQL(Structured Query Language)을 이용해서 데이터를 조작한다. 그리고 Tidyverse에서는 dplyr 패키지가 마치 SQL과 유사한 방법으로 데이터를 조작한다.\n데이터 분석의 중요성이 대두되면서 1990년대 말부터 많은 DBMS는 행과 행간의 관계를 정의하거나 행과 행간을 비교하고 연산하는 함수의 기능을 SQL에 추가하기 시작했다. 그리고 이들을 window function이라 불렀다.window function을 활용하면 복잡한 프로그램을 통해 행과 행간의 연산을 수행하던 것을 간단한 SQL 표현으로 쉽게 해결할 수 있다.\ndplyr에서 windows function 구현\ndplyr 패키지를 사용하여 SQL의 windows function에 해당하는 기능을 수행해 보자.\n데이터 생성\n예제를 위한 데이터를 생성하기 위해서 quantmod 패키지로 KOSPI 지수와 KOSDAQ 지수를 가져온다. 2019-12-01부터 2020-03-20의 주가 지수를 수집했다.\n\n\nlibrary(quantmod)\n\nKOSPI <- getSymbols('^KS11', from = '2019-12-01', to = '2020-03-20', auto.assign = FALSE)\nKOSPI <- data.frame(market = \"KOSPI\",\n                    date = as.Date(row.names(as.matrix(KOSPI))), \n                    open = as.vector(KOSPI[, 1]), \n                    high = as.vector(KOSPI[, 2]), \n                    low = as.vector(KOSPI[, 3]), \n                    close = as.vector(KOSPI[, 4]), \n                    volume = as.vector(KOSPI[, 5]), \n                    adjusted = as.vector(KOSPI[, 6]))\n\nKOSDAQ <- getSymbols('^KQ11', from = '2019-12-01', to = '2020-03-20', auto.assign = FALSE)\nKOSDAQ <- data.frame(market = \"KOSDAQ\",\n                     date = as.Date(row.names(as.matrix(KOSDAQ))), \n                     open = as.vector(KOSDAQ[, 1]), \n                     high = as.vector(KOSDAQ[, 2]), \n                     low = as.vector(KOSDAQ[, 3]), \n                     close = as.vector(KOSDAQ[, 4]), \n                     volume = as.vector(KOSDAQ[, 5]), \n                     adjusted = as.vector(KOSDAQ[, 6]))\n\n\n\n문제정의\n다음의 문제는 SQL에서 windows function을 사용해서 구할 수 있는 문제들이다.\nKOSPI 지수 데이터\n2019-12-01부터 2020-03-20에서의 KOSPI 거래량 일일 누계를 구한다.\n5 거래일 기준으로 KOSPI 지수의 이동평균을 구한다.\n2019-12-01부터 2020-03-20에서 KOSPI 지수의 순위를 구한다.\n이전 거래일 대비 주가지수의 차이를 구한다.\n작을수록 상위 랭크\n\n\nKOSPI 시장과 와 KOSDAQ 시장 데이터를 합친 후, 거래소별로\nKOSPI 지수 데이터 사례의 지표들을 구한다.\n\nKOSPI 데이터에서의 집계\nzoo 패키지는 시계열 분석을 지원하는 패키지다. 그리고 windows function은 time windows function을 내포하는 것인만큼 이 패키지가 아주 유용하게 사용된다.\ncumsum는 누적합을 구하는데 이용하는 함수고, rank는 순위를 구하는 함수다. 이미 여러번 사용해 보았을 것이다.\n\n\nvol <- KOSPI$volume[1:5]\nvol\n\n\n[1] 385300 450000 347000 472200 440200\n\n# 누적합\ncumsum(vol)\n\n\n[1]  385300  835300 1182300 1654500 2094700\n\n# 순위\nrank(vol)\n\n\n[1] 2 4 1 5 3\n\ndiff는 차분을 구하는 함수로, 이전 거래일 대비 주가지수의 차이를 쉽게 구할 수는 있지만, 관측치의 개수보다 1개 적은 값을 반환하는 문제가 있다. 그리고 이 함수가 dplyr 내에서 사용되면 1개 모자란 값때문에 에러가 발생한다. 그래서 맨 앞에 NA를 추가하는 것이 필요하다.\n\n\nlibrary(dplyr)\n\nclose <- KOSPI$close[1:5]\nclose\n\n\n[1] 2091.92 2084.07 2068.89 2060.74 2081.85\n\n# 차분구하기\ndiff(close)\n\n\n[1]  -7.849854 -15.180175  -8.149903  21.110108\n\n# dplyr 내에서 사용\nKOSPI %>% \n  head(n = 5) %>% \n  select(date, close) %>% \n  mutate(diff_close = c(NA, diff(close)))\n\n\n        date   close diff_close\n1 2019-12-02 2091.92         NA\n2 2019-12-03 2084.07  -7.849854\n3 2019-12-04 2068.89 -15.180175\n4 2019-12-05 2060.74  -8.149903\n5 2019-12-06 2081.85  21.110108\n\n이동평균은 zoo 패키지의 rollapply 함수를 사용한다. 이 함수는 차수만큰 롤링라면서 특정 함수를 적용하는 기능을 수행한다. 이동평균이므로 mean 함수를 적용했다.\n이상을 정리하여, 첫번제 문제인 KOSPI 지수 데이터의 집계를 다음과 같이 수행했다. 관측치가 많아서 마지막 10개 관측치만 출력해 본다.\n\n\nKOSPI_summary <- KOSPI %>% \n  select(-(open:low), -adjusted) %>% \n  arrange(date) %>% \n  mutate(total_volume = cumsum(volume),\n         rank_close = rank(close),\n         diff_prev = round(c(NA, diff(close)), 2),\n         avg_close_5 = round(zoo::rollapply(close, 5, mean, fill = TRUE, \n                                        align = \"right\", partial = TRUE)))\n\nKOSPI_summary %>% \n  select(-market) %>% \n  tail(n = 10)\n\n\n         date   close  volume total_volume rank_close diff_prev\n66 2020-03-09 1954.77  666900     41200500          9    -85.45\n67 2020-03-10 1962.93  638600     41839100         10      8.16\n68 2020-03-11 1908.27  679700     42518800          8    -54.66\n69 2020-03-12 1834.33  850300     43369100          7    -73.94\n70 2020-03-13 1771.44 1035300     44404400          6    -62.89\n71 2020-03-16 1714.86  682000     45086400          5    -56.58\n72 2020-03-17 1672.44  649800     45736200          4    -42.42\n73 2020-03-18 1591.20  728000     46464200          3    -81.24\n74 2020-03-19 1457.64  977700     47441900          1   -133.56\n75 2020-03-20 1566.15  817000     48258900          2    108.51\n   avg_close_5\n66        2031\n67        2021\n68        1990\n69        1940\n70        1886\n71        1838\n72        1780\n73        1717\n74        1642\n75        1600\n\n거래소별로 데이터의 집계\nKOSPI와 KOSDAQ를 합쳐서 stock라는 객체를 생성한다.\n\n\nstock <- KOSPI %>% \n  union(KOSDAQ)\n\nstock %>% \n  arrange(date, market) %>% \n  tail(n = 10)\n\n\n    market       date    open    high     low   close volume adjusted\n141 KOSDAQ 2020-03-16  538.68  541.87  504.51  504.51   1200   504.51\n142  KOSPI 2020-03-16 1805.43 1805.43 1714.38 1714.86 682000  1714.86\n143 KOSDAQ 2020-03-17  488.02  518.72  483.51  514.73   1400   514.73\n144  KOSPI 2020-03-17 1640.84 1722.97 1637.88 1672.44 649800  1672.44\n145 KOSDAQ 2020-03-18  520.79  526.10  485.14  485.14   1300   485.14\n146  KOSPI 2020-03-18 1686.12 1693.95 1591.12 1591.20 728000  1591.20\n147 KOSDAQ 2020-03-19  501.59  502.12  419.55  428.35   1600   428.35\n148  KOSPI 2020-03-19 1626.09 1626.09 1439.43 1457.64 977700  1457.64\n149 KOSDAQ 2020-03-20  443.88  467.75  435.11  467.75   1200   467.75\n150  KOSPI 2020-03-20 1498.49 1566.82 1466.48 1566.15 817000  1566.15\n\n거래소별로 집계하는 방법은 매우 쉽다. 앞서 구한 스크립트에 그룹핑해서 집계할 변수를 group_by 함수에 적용하면 된다.\n\n\nstock_summary <- stock %>% \n  select(-(open:low), -adjusted) %>% \n  group_by(market) %>% \n  arrange(date) %>% \n  mutate(total_volume = cumsum(volume),\n         rank_close = rank(close),\n         diff_prev = round(c(NA, diff(close)), 2),\n         avg_close_5 = round(zoo::rollapply(close, 5, mean, fill = TRUE, \n                                        align = \"right\", partial = TRUE))) \n\n\n\n집계된 KOSPI 결과를 보면, 앞서 개별적으로 구한 결과와 동일하다.\n\n\nstock_summary %>%   \n  filter(market == \"KOSPI\") %>% \n  tail(n = 10) \n\n\n# A tibble: 10 x 8\n# Groups:   market [1]\n   market date       close  volume total_volume rank_close diff_prev\n   <chr>  <date>     <dbl>   <dbl>        <dbl>      <dbl>     <dbl>\n 1 KOSPI  2020-03-09 1955.  666900     41200500          9    -85.4 \n 2 KOSPI  2020-03-10 1963.  638600     41839100         10      8.16\n 3 KOSPI  2020-03-11 1908.  679700     42518800          8    -54.7 \n 4 KOSPI  2020-03-12 1834.  850300     43369100          7    -73.9 \n 5 KOSPI  2020-03-13 1771. 1035300     44404400          6    -62.9 \n 6 KOSPI  2020-03-16 1715.  682000     45086400          5    -56.6 \n 7 KOSPI  2020-03-17 1672.  649800     45736200          4    -42.4 \n 8 KOSPI  2020-03-18 1591.  728000     46464200          3    -81.2 \n 9 KOSPI  2020-03-19 1458.  977700     47441900          1   -134.  \n10 KOSPI  2020-03-20 1566.  817000     48258900          2    109.  \n# … with 1 more variable: avg_close_5 <dbl>\n\n집계된 KOSDAQ 결과를 보면, KOSPI의 시황과 유사하다.\n\n\nstock_summary %>%   \n  filter(market == \"KOSDAQ\") %>% \n  tail(n = 10) \n\n\n# A tibble: 10 x 8\n# Groups:   market [1]\n   market date       close volume total_volume rank_close diff_prev\n   <chr>  <date>     <dbl>  <dbl>        <dbl>      <dbl>     <dbl>\n 1 KOSDAQ 2020-03-09  615.   1300        62400         10    -28.1 \n 2 KOSDAQ 2020-03-10  620.   1400        63800         12      5.37\n 3 KOSDAQ 2020-03-11  596.   1600        65400          8    -24.4 \n 4 KOSDAQ 2020-03-12  563.   1600        67000          7    -32.1 \n 5 KOSDAQ 2020-03-13  524    1500        68500          6    -39.5 \n 6 KOSDAQ 2020-03-16  505.   1200        69700          4    -19.5 \n 7 KOSDAQ 2020-03-17  515.   1400        71100          5     10.2 \n 8 KOSDAQ 2020-03-18  485.   1300        72400          3    -29.6 \n 9 KOSDAQ 2020-03-19  428.   1600        74000          1    -56.8 \n10 KOSDAQ 2020-03-20  468.   1200        75200          2     39.4 \n# … with 1 more variable: avg_close_5 <dbl>\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-23T09:25:25+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-26-R_web_application/",
    "title": "R 웹 어플리케이션 개발에 대한 연구",
    "description": "\"RSS(R StatServer)란 R의 RServe 라이브러리를 이용해서 Java/JSP 기반의 웹 어플리케이션 구축을 위해서 개발한 서버 환경이다.\"",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2020-02-26",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n들어가기\n2007-6-20\nR Web의 현재 모습\n\n2007-08-31\nRSS 프로젝트\nRSS 소개\nRSS 구조\nR Tag Library 소개\n몇 가지 예제 화면\n\n2007-11-10 추계학술논문 발표회\n2009년 어느 시점의 AIS\nAIS의 개념\nAIS 시스템 구성도\nAIS 주요 내용\nAIS 구현 방법\n\n에필로그\n\n\n\n\n들어가기\n\n그 시절은 국내에서 R을 사용하는 R 유저가 많지 않을 때 였다. S-PLUS로 데이터분석 프로젝트를 수행하던 시절, MathSoft라는 회사에서 Linux/UNIX에서는 S-PLUS Analytic Server, MS-Windows에서는 StatServer라는 이름의 상용 웹 어플리케이션을 판매하던 시절이었다. S-PLUS라는 통계 소프트웨어(R의 Commercial Version)를 연동하여 웹 어플리케이션을 개발하는 기능을 구현한 제품이었다. 지금의 R Shiny Server 정도로 이해하면 된다. 2006년이던가? 야심차게 R과 연동하는 웹 어플리케이션 서버를 개발했었다. 그리고 최근에 네이버 블로그와 몇 개의 백업 디렉토리에서 그 흔적을 발견하였고, 아련한 추억을 되새기는 차원에서 정리해 보았다. 그 당시에 썼던 글을 있는 그대로 인용해 본다.\n\n2007-6-20\nR Web의 현재 모습\n작년말부터 R과 Rserve, Mysql(DBMS), Tomcat(웹서버)를 이용해서 Web에서 R을 이용한 어플리케이션을 개발하기 위한 프로그램을 만드는 작업을 시작했었다.\n그러나 국내의 IT(엄밀히 말하면 프로그램 개발)업에 종사하면서 업무 외 시간을 이용해서 개인적으로 어떤 일을 하는 것은 정말로 힘들다. 모 회사의 프로젝트를 몇개월 진행하면서 휴일및 공휴일 밤시간대를 포기해야했다. 그러니 처음 기획했던 계획은 답보에 그쳤다. 이제 다시 꺼내서 다듬으려 하니 몇 달의 공백이 크게 느껴진다.\n현재까지 만들어진 통계모델 (통계량, 그래프, 집계테이블을 조합한 분석 모델)을 설계하는 관리자 화면이다.\n완성도가 높은 프로그램이 완성되면, 상업적인 목적을 배제한 학교등에는 공개할 생각에 있다. 아님 R을 이용한 사업을 구상해 볼 수도 있겠다.\n오랜만에 실행시켜 보니 몇개의 버그가 눈에 띈다.\n\n\n\nFigure 1: R Web의 스크린샷\n\n\n\n2007-08-31\nRSS 프로젝트\nRSS라고 명명했다. R StatServer의 약자이다.\n사실 처음 시도한 지는 2년도 더 된다. 지금은 기능을 보강하고, 좀더 완성된 모습의 도구를 만들고 있는 중이다.\nR을 이용하여서 웹어플리케이션을 만드는 시도는 이전부터 많은 시도가 있었다. 그러고 이 RSS도 그 중에 하나이다.\n언젠가는 멋진 놈이 탄생하리라 생각한다. ^^\nRSS 소개\nRSS(R StatServer)란 R의 RServe 라이브러리를 이용해서 Java/JSP 기반의 웹 어플리케이션 구축을 위해서 개발한 서버 환경이다.\n통계학이 실사구시(實事求是)의 학문이고 궁극적인 목표가 이용후생(利用厚生)이라 생각한다. 이미 교육 현장에서는 R이 통계 이론의 탐구를 위한 훌륭한 교육 도구로 자리를 잡았다고 생각한다. 하지만 산업 전반에서의 통계적 방법론을 구현할 수 있는 도구로서의 R의 모습은 아직 성숙되지 못한 감이 없지 않다.\n그것은 기업에서 원하는 Desktop 분석 도구로서의 R이 아니라 응용 프로그램을 구축(System Integration)할 수 있는 수단의 부재도 한 몫 한다고 생각한다. 이런 고민에서 출발해서 RSS(R StatServer)를 개발하게 되었다.\n웹 어플리케이션을 기반으로 한 이유는 기업의 응용 프로그램이 웹 기반에서 동작하는 것이 표준이 될 정도로 대중화되었기 때문이다. 그리고 핵심 기술은 JSP에서 동작하는 R custom tag library이다.\nRSS(R StatServer)는 플랫폼 독립적으로 설계되기었기 때문에 MS Windows와 UNIX, LINUX에서도 동작이 된다. 본 시스템은 Windows XP에서 R 2.4.X, R 2.5.X, Tomcat 5.5, Tomcat 6.0, Oracle 9i(MySQL 5.X, MySQL 6.X)에서 개발하고 테스트 되었다. 개발 도구는 JDK(Java Development Kit) 1.4.X, 1.5.X를 이용하였다.\nRSS 구조\nRSS은 기업에서의 어플리케이션을 개발하기 위한 라이센스를 획득하기 위해서 GPL(General Public License)을 따르는 라이브러를 수정없이 사용하였다.\n핵심 엔진은 R System이며, R과 인터페이스하기 위한 RServe와 DBMS와 인터페이스하기 위한 RODBC, 그리고 결과를 HTML의 Table tag로 만들기 위한 XML Packages, Tree 객체를 그리기 위한 tree Packages가 사용되었다. Tree를 적용하기 위한 그래픽 포맷은 SVG (Scalable Vector Graphics )를 사용하였다.\nRSS의 개념적인 구조는 다음의 그림과 같다.\n\n\n\nFigure 2: RSS 구조도\n\n\n\nR Tag Library 소개\nR Tag Library란 R에서 R의 RServe 라이브러리를 이용해서 Java/JSP 기반의 웹 어플리케이션 구축을 위해서 개발한 라이브러리이다.\n이 라이브러리는 플랫폼 독립적으로 운영되기 때문에 MS Windows와 UNIX, LINUX에서도 동작이 된다.\n현재 개발한 Tag는 다음과 같은 6개의 Tag이다.\n\n\n\nFigure 3: Tag 종류\n\n\n\n몇 가지 예제 화면\n몇 가지 예제 화면을 예시한다.\n\n\n\nFigure 4: R:script Tag 예제\n\n\n\n\n\n\nFigure 5: R:graph Tag 예제\n\n\n\n\n\n\nFigure 6: R:tree Tag 예제\n\n\n\n2007-11-10 추계학술논문 발표회\n2007년 한국통계학회에서 진행한 추계학술논문 발표회에서 발표하였다.\n\n\n\nFigure 7: 발표 세션 정보\n\n\n\n논문의 초록이다.\n\n\n\nFigure 8: 발표 논문 초록\n\n\n\n다음은 발표 슬라이드에서의 개요 부분이다.\nR 웹 어플리케이션 서버(R StatServer: RSS)란?\nR 수행 결과를 웹 화면에 출력하여 어플리케이션을 개발하는 서버 환경\nJSP Custom tag를 구현한 Java기반의 웹 어플리케이션 서버\nRserve Packages를 이용하여 R과의 TCP/IP 통신\nMS-Windows, UNIX, LINUX등의 운영체제 지원\n“R을 이용한 통계모델 및 그래프의 손쉬운 웹 퍼블리싱”\n“JSP의 기초만 알면 쉽게 R 웹 어플리케이션 개발 가능”\n“GPL 라이센스를 따르는 소프트웨어를 수정 없이 사용하였기에 관공서,기업 등에서 사용하는데 법적인 문제가 없음”\n\n발표 슬라이드는 여기에 링크되어 있다.\n아쉽게도 논문의 본문을 찾을 수 없었다. 인쇄된 논문집이 있었는데, 몇번 이사를 하면서 분실한 것 같다.\n2009년 어느 시점의 AIS\n어느 순간 이름이 RSS에서 AIS로 바뀌어 있었다.\nAIS의 개념\nAnalytic Intelligence Server(AIS)는 통계 프로그램 언어인 R 기반의 솔루션으로 Predictive Model, Multi-Dimension Analysis, Visualization 등의 통계적 자료분석의 방법론을 웹 어플리케이션에 구현할 수 있는 통계분석 서버입니다.\n\n\n\nFigure 9: AIS의 개념\n\n\n\nAIS 시스템 구성도\nRSS에 R:Lattice 태그가 추가되어서 8개의 Tag를 지원하고 있다.\n\n\n\nFigure 10: AIS의 시스템 구성도\n\n\n\nAIS 주요 내용\n\n\n\nFigure 11: AIS의 주요 내용\n\n\n\nAIS 구현 방법\n\n\n\nFigure 12: AIS의 구현 방법\n\n\n\n에필로그\n그 당시에는 나름 global Product에서 AIS를 따라올 제품이 없었다. 아쉬운 점은 R 패키지로 구성되는 서버임에도 CRAN에 올려 오픈소스화 하지 못한 점과, 지속적으로 업그레이드하지 않고 사장시켰다는 점이다.\n주인을 잘못 만나 빛을 제대로 보지 못한 AIS에게 미안함이 드는 밤이다. 나름 Linux/UNIX에서는 S-PLUS Analytic Server에 견줄만 했는데…\n“아 그렇게 열성적인 적도 있었구나!” 다시한번 발자취를 뒤돌아보는 기회였다.\n\n\n",
    "preview": "posts/2020-02-26-R_web_application/img/rweb.gif",
    "last_modified": "2021-10-23T09:33:35+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-01-28-introduce_dbplyr/",
    "title": "Introduce dbplyr package",
    "description": "`dbplyr` 패키지는 `DBMS`를 위한 `dplyr` 패키지의 `Backend`다. `dplyr` 패키지의 R 환경에서 data.frame  객체에서 상속된 데이터 객체를 조작해서 데이터 분석을 수행하는 패키지라면, `dbplyr` 패키지는 `dplyr` 패키지의 문법으로 DBMS 환경의 `Tables` 자원을 액세스하여 데이터 분석을 수행하게 도와 준다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2020-01-28",
    "categories": [
      "Tidyverse"
    ],
    "contents": "\n\nContents\ndbplyr\n테스트 DBMS의 환경\nDBMS 접속 및 DBMS 정보\n테스트 데이터 확인\n\n데이터 집계하기\nQuery를 이용한 데이터 집계\ndbplyr를 이용한 데이터 집계\n\nExplain Plan\ndisconnection\n\n참고 리소스\n\ndbplyr\ndbplyr 패키지는 DBMS를 위한 dplyr 패키지의 Backend다. dplyr 패키지의 R 환경에서 data.frame 객체에서 상속된 데이터 객체를 조작해서 데이터 분석을 수행하는 패키지라면, dbplyr 패키지는 dplyr 패키지의 문법으로 DBMS 환경의 Tables 자원을 액세스하여 데이터 분석을 수행하게 도와 준다.\n\n\n\nhttps://dbplyr.tidyverse.org/\n테스트 DBMS의 환경\nMac OS X 노트북 환경에 PostgrSQL DBMS 12 버전을= 설치한 환경에 https://github.com/lcheng6/nyc-taxi-data-code-annotated의 정보를 이용해서New York 지역의 Taxi 운행 공공 데이터를 분석한 사례의 일부 데이터를 migration하였다.\nDBMS 접속 및 DBMS 정보\nPostgreSQL DBMS의 정보는 다음과 같다.\n\n\nlibrary(RPostgreSQL)\n\ncon <- dbConnect(dbDriver(\"PostgreSQL\"), dbname = \"nyc-taxi-data\", host = \"localhost\")\n\n# DBMS 정보 \ndbGetInfo(con)\n\n\n$host\n[1] \"localhost\"\n\n$port\n[1] \"5432\"\n\n$user\n[1] \"choonghyunryu\"\n\n$dbname\n[1] \"nyc-taxi-data\"\n\n$serverVersion\n[1] \"12.0.5\"\n\n$protocolVersion\n[1] 3\n\n$backendPId\n[1] 72687\n\n$rsId\nlist()\n\n테스트 데이터 확인\n예제로 사용할 trips 테이블은 New York City에서의 택시 운행 정보를 담은 테이블이다.\n테이블 컬럼 정보\ntrips 테이블이 폴함하고 있는 컬럼 이름은 다음과 같다. 30개의 데이터 컬럼을 가지고 있다.\n\n\n# trips 테이블 컬럼 이름\ndbListFields(con, \"trips\")\n\n\n [1] \"id\"                    \"cab_type_id\"          \n [3] \"vendor_id\"             \"pickup_datetime\"      \n [5] \"dropoff_datetime\"      \"store_and_fwd_flag\"   \n [7] \"rate_code_id\"          \"pickup_longitude\"     \n [9] \"pickup_latitude\"       \"dropoff_longitude\"    \n[11] \"dropoff_latitude\"      \"passenger_count\"      \n[13] \"trip_distance\"         \"fare_amount\"          \n[15] \"extra\"                 \"mta_tax\"              \n[17] \"tip_amount\"            \"tolls_amount\"         \n[19] \"ehail_fee\"             \"improvement_surcharge\"\n[21] \"congestion_surcharge\"  \"total_amount\"         \n[23] \"payment_type\"          \"trip_type\"            \n[25] \"pickup_nyct2010_gid\"   \"dropoff_nyct2010_gid\" \n[27] \"pickup_location_id\"    \"dropoff_location_id\"  \n[29] \"pickup\"                \"dropoff\"              \n\n컬럼 이름으로는 정보가 부족하다. 그래서 PostgreSQL DBMS의 메타 정보를 읽어서 데이터 타입과 컬럼의 description을 조회해 보았다. 간단하게 get_schema라는 이름의 사용자 정의 함수를 만들어서 조회해 본다.\n\n\n# Table 정보 가져오는 함수\nget_schema <- function(con, tab) {\n  sql <- sprintf(\"select column_name\n                        ,data_type\n                        ,col_description('public.%s'::regclass, ordinal_position)\n                    from information_schema.columns\n                  where table_schema = 'public' \n                    and table_name = '%s'\", tab, tab)\n  dbGetQuery(con, sql)\n}\n\n# trips 테이블 컬럼 정보\nget_schema(con, \"trips\")\n\n\n             column_name                   data_type col_description\n1                     id                     integer            <NA>\n2            cab_type_id                     integer            <NA>\n3              vendor_id                        text            <NA>\n4        pickup_datetime timestamp without time zone            <NA>\n5       dropoff_datetime timestamp without time zone            <NA>\n6     store_and_fwd_flag                        text            <NA>\n7           rate_code_id                     integer            <NA>\n8       pickup_longitude                     numeric            <NA>\n9        pickup_latitude                     numeric            <NA>\n10     dropoff_longitude                     numeric            <NA>\n11      dropoff_latitude                     numeric            <NA>\n12       passenger_count                     integer            <NA>\n13         trip_distance                     numeric            <NA>\n14           fare_amount                     numeric            <NA>\n15                 extra                     numeric            <NA>\n16               mta_tax                     numeric            <NA>\n17            tip_amount                     numeric            <NA>\n18          tolls_amount                     numeric            <NA>\n19             ehail_fee                     numeric            <NA>\n20 improvement_surcharge                     numeric            <NA>\n21  congestion_surcharge                     numeric            <NA>\n22          total_amount                     numeric            <NA>\n23          payment_type                        text            <NA>\n24             trip_type                     integer            <NA>\n25   pickup_nyct2010_gid                     integer            <NA>\n26  dropoff_nyct2010_gid                     integer            <NA>\n27    pickup_location_id                     integer            <NA>\n28   dropoff_location_id                     integer            <NA>\n29                pickup                USER-DEFINED            <NA>\n30               dropoff                USER-DEFINED            <NA>\n\n다른 방법으로는 dbColumnInfo() 함수를 이용하는 방법이다. 이 함수는 RPostgreSQL 패키지에서 제공하는 함수다.\n\n\nrs <- dbSendQuery(con, \"SELECT * FROM trips limit 10\")\ndbColumnInfo(rs)\n\n\n                    name    Sclass      type len precision scale\n1                     id   integer   INTEGER   4        -1    -1\n2            cab_type_id   integer   INTEGER   4        -1    -1\n3              vendor_id character      TEXT  -1        -1    -1\n4        pickup_datetime   POSIXct TIMESTAMP   8        -1    -1\n5       dropoff_datetime   POSIXct TIMESTAMP   8        -1    -1\n6     store_and_fwd_flag character      TEXT  -1        -1    -1\n7           rate_code_id   integer   INTEGER   4        -1    -1\n8       pickup_longitude    double   DECIMAL  -1        -1    -1\n9        pickup_latitude    double   DECIMAL  -1        -1    -1\n10     dropoff_longitude    double   DECIMAL  -1        -1    -1\n11      dropoff_latitude    double   DECIMAL  -1        -1    -1\n12       passenger_count   integer   INTEGER   4        -1    -1\n13         trip_distance    double   DECIMAL  -1        -1    -1\n14           fare_amount    double   DECIMAL  -1        -1    -1\n15                 extra    double   DECIMAL  -1        -1    -1\n16               mta_tax    double   DECIMAL  -1        -1    -1\n17            tip_amount    double   DECIMAL  -1        -1    -1\n18          tolls_amount    double   DECIMAL  -1        -1    -1\n19             ehail_fee    double   DECIMAL  -1        -1    -1\n20 improvement_surcharge    double   DECIMAL  -1        -1    -1\n21  congestion_surcharge    double   DECIMAL  -1        -1    -1\n22          total_amount    double   DECIMAL  -1        -1    -1\n23          payment_type character      TEXT  -1        -1    -1\n24             trip_type   integer   INTEGER   4        -1    -1\n25   pickup_nyct2010_gid   integer   INTEGER   4        -1    -1\n26  dropoff_nyct2010_gid   integer   INTEGER   4        -1    -1\n27    pickup_location_id   integer   INTEGER   4        -1    -1\n28   dropoff_location_id   integer   INTEGER   4        -1    -1\n29                pickup character   UNKNOWN  -1   1107460    -1\n30               dropoff character   UNKNOWN  -1   1107460    -1\n   nullOK\n1   FALSE\n2    TRUE\n3    TRUE\n4    TRUE\n5    TRUE\n6    TRUE\n7    TRUE\n8    TRUE\n9    TRUE\n10   TRUE\n11   TRUE\n12   TRUE\n13   TRUE\n14   TRUE\n15   TRUE\n16   TRUE\n17   TRUE\n18   TRUE\n19   TRUE\n20   TRUE\n21   TRUE\n22   TRUE\n23   TRUE\n24   TRUE\n25   TRUE\n26   TRUE\n27   TRUE\n28   TRUE\n29   TRUE\n30   TRUE\n\n데이터 건수\ntrips 테이블의 데이터 건수는 84,345,018건이다. 데이터 건수를 화인하기 위해서 \"SELECT count(*) FROM trips\" 문을 수행하였으며, 집계를 위한 Query 수행 시간이 짧지는 았았다.\n\n\ndbGetQuery(con, \"SELECT count(*) FROM trips\")\n\n\n     count\n1 84345018\n\n데이터 집계하기\n데이터를 집계하는 간단한 예제로 dbplyr의 사용 방법을 살펴보자.\nQuery를 이용한 데이터 집계\n택시의 주행 거리당 운임의 평균을 계산한다. Query로 간단하게 집계할 수 있다.\n\n\nsql <- \"select avg(fare_amount / trip_distance) as fare_per_distance\n          from trips\n         where trip_distance > 0\n           and fare_amount > 0\"\n\nexcute_time <- system.time(fare <- dbGetQuery(con, sql))\nexcute_time\n\n\n   user  system elapsed \n  0.001   0.000 153.817 \n\nfare\n\n\n  fare_per_distance\n1          5.623411\n\n시스템 환경에 따라 차이가 있을 수 있으나, 예제에서는 153.817초 동안 수행되었다. 그리고 킬로미터 당 약 5.6 달러의 평균 요금을 지불하였음을 알 수 있다.\n요금 결제 방법에 따른 평균 총 지불 운임에 대한 통계는 다음과 같다. 그런데 지불 방법에 대한 코드 값이 대소문자가 섞여 있고, 동일한 내용이 몇 개의 코드로 표현되는 것이 있어 정확한 통계를 산출하지 못했다.\n\n\nsql <- \"SELECT payment_type\n              ,AVG(total_amount) mean_fare \n          FROM trips t \n      GROUP BY payment_type\n      ORDER BY mean_fare DESC\"\n\ndbGetQuery(con, sql)\n\n\n  payment_type  mean_fare\n1          Cre 14.3489055\n2          CRE 14.1596819\n3          CRD 13.6265705\n4          NA  12.5874196\n5          Dis 12.2977820\n6          No  10.8333954\n7          CAS 10.1805913\n8          Cas  9.9806661\n9          CSH  0.6285917\n\n그래서 지불 방법의 코드를 대문자로 변환하고 동일한 의미로 분류하여 집계하였다.\n\n\nsql <- \"SELECT CASE WHEN UPPER(payment_type) IN ('CRE', 'CRD') THEN 'Credit Card'\n               WHEN UPPER(payment_type) IN ('CAS', 'CSH') THEN 'Cash' \n               WHEN UPPER(payment_type) = 'DIS' THEN 'Dispute'\n               WHEN UPPER(payment_type) = 'NO' THEN 'No charge'\n               ELSE 'Unknown' END AS payment_type\n              ,COUNT(*) as freq\n              ,AVG(total_amount) mean_fare \n  FROM trips t \n GROUP BY CASE WHEN UPPER(payment_type) IN ('CRE', 'CRD') THEN 'Credit Card'\n               WHEN UPPER(payment_type) IN ('CAS', 'CSH') THEN 'Cash' \n               WHEN UPPER(payment_type) = 'DIS' THEN 'Dispute'\n               WHEN UPPER(payment_type) = 'NO' THEN 'No charge'\n               ELSE 'Unknown' END\n ORDER BY mean_fare DESC\"\n\ndbGetQuery(con, sql)\n\n\n  payment_type     freq mean_fare\n1  Credit Card 29294014 14.232335\n2      Dispute    36768 12.297782\n3      Unknown   208765 11.169581\n4         Cash 54805471  8.851864\n\ndbplyr를 이용한 데이터 집계\ndbplyr를 이용한 데이터 집계의 장점은 R 환경에서 일관된 dplyr 문법으로 데이터를 집계할 수 있다는 점이다. 특히 SQL query에 익숙치 않은 분석가도 익숙한 dplyr 문법으로 데이터 분석을 수행할 수 있다.\n먼저 tbl() 함수로 해당 테이블에 접근할 수 있는 tbl 컨넥션을 생성한다.\n\n\nlibrary(dplyr)\nlibrary(dbplyr)\n\ntrips_db <- tbl(con, \"trips\")\n\nis(trips_db)\n\n\n[1] \"tbl_PostgreSQLConnection\"\n\ntrips_db\n\n\n# Source:   table<trips> [?? x 30]\n# Database: postgres 12.0.5 [@localhost:5432/nyc-taxi-data]\n      id cab_type_id vendor_id pickup_datetime     dropoff_datetime   \n   <int>       <int> <chr>     <dttm>              <dttm>             \n 1     1           1 VTS       2010-01-19 04:22:00 2010-01-19 04:29:00\n 2     2           1 CMT       2010-01-27 00:42:46 2010-01-27 01:00:20\n 3     3           1 VTS       2010-01-30 19:00:00 2010-01-30 19:32:00\n 4     4           1 CMT       2010-01-18 11:16:32 2010-01-18 11:27:57\n 5     5           1 CMT       2010-01-30 19:18:50 2010-01-30 19:23:31\n 6     6           1 CMT       2010-01-01 14:37:46 2010-01-01 14:43:19\n 7     7           1 VTS       2010-01-17 12:13:00 2010-01-17 12:30:00\n 8     8           1 VTS       2010-01-23 22:20:00 2010-01-23 22:24:00\n 9     9           1 VTS       2010-01-26 09:14:00 2010-01-26 09:29:00\n10    10           1 VTS       2010-01-31 04:12:00 2010-01-31 04:18:00\n# … with more rows, and 25 more variables: store_and_fwd_flag <chr>,\n#   rate_code_id <int>, pickup_longitude <dbl>,\n#   pickup_latitude <dbl>, dropoff_longitude <dbl>,\n#   dropoff_latitude <dbl>, passenger_count <int>,\n#   trip_distance <dbl>, fare_amount <dbl>, extra <dbl>,\n#   mta_tax <dbl>, tip_amount <dbl>, tolls_amount <dbl>,\n#   ehail_fee <dbl>, improvement_surcharge <dbl>,\n#   congestion_surcharge <dbl>, total_amount <dbl>,\n#   payment_type <chr>, trip_type <int>, pickup_nyct2010_gid <int>,\n#   dropoff_nyct2010_gid <int>, pickup_location_id <int>,\n#   dropoff_location_id <int>, pickup <chr>, dropoff <chr>\n\ntrips_db 객체를 콘솔에 출력한 결과의 포맷은 dplyr을 사용한 결과와 유사하다. 그러나 데이터의 건수를 위미하는 영역은 정확한 건수를 표시하지 않고, 물음표로 표현된다.\n앞서 구한 택시의 주행 거리당 운임의 평균은 dplyr의 방법처럼 쉽게 구할 수 있다. 단지, 차이점은 데이터 객체 대신에 tbl 컨넥션을 데이터 소스로 지정한다는 점이다.\n그런데 153.817초 동안 수행된 Query와는 달리 바로 수행이 종료된다.\n\n\nsystem.time(fare_db <- trips_db %>% \n              select(fare_amount, trip_distance) %>% \n              filter(fare_amount > 0) %>% \n              filter(trip_distance > 0) %>% \n              summarise(fare_per_distance = mean(fare_amount / trip_distance)))\n\n\n   user  system elapsed \n  0.019   0.002   0.023 \n\nis(fare_db)\n\n\n[1] \"tbl_PostgreSQLConnection\"\n\nfare_db %>% show_query()\n\n\n<SQL>\nSELECT AVG(\"fare_amount\" / \"trip_distance\") AS \"fare_per_distance\"\nFROM (SELECT *\nFROM (SELECT \"fare_amount\", \"trip_distance\"\nFROM \"trips\") \"q01\"\nWHERE (\"fare_amount\" > 0.0)) \"q02\"\nWHERE (\"trip_distance\" > 0.0)\n\n그 이유는 생성된 객체가 결과가 아니라 아직도 tbl 컨넥션임을 알 수 있다. 그리고 show_query() 함수로 기술된 dplyr 문법의 표현식이 연결된 DBMS에서 수행될 Query를 조회할 수도 있다.\n\n\nis(fare_db)\n\n\n[1] \"tbl_PostgreSQLConnection\"\n\nfare_db %>% show_query()\n\n\n<SQL>\nSELECT AVG(\"fare_amount\" / \"trip_distance\") AS \"fare_per_distance\"\nFROM (SELECT *\nFROM (SELECT \"fare_amount\", \"trip_distance\"\nFROM \"trips\") \"q01\"\nWHERE (\"fare_amount\" > 0.0)) \"q02\"\nWHERE (\"trip_distance\" > 0.0)\n\ncollect() 함수가 실제로 연결된 DBMS에서 Query를 실행한 후 결과를 R 영역으로 가져온다.\n\n\nsystem.time(fare2 <- fare_db %>% collect())\n\n\n   user  system elapsed \n  0.042   0.000 116.439 \n\nfare2\n\n\n# A tibble: 1 x 1\n  fare_per_distance\n              <dbl>\n1              5.62\n\n요금 결제 방법에 따른 평균 총 지불 운임에 대한 통계는 다음과 같이 구한다.\n\n\npayment_db <- trips_db %>% \n  select(payment_type, total_amount) %>% \n  mutate(payment_type = toupper(payment_type)) %>%\n  mutate(payment_type = ifelse(payment_type %in% c('CRE', 'CRD'), \n                               'Credit Card', payment_type)) %>% \n  mutate(payment_type = ifelse(payment_type %in% c('CAS', 'CSH'), \n                               'Cash', payment_type)) %>% \n  mutate(payment_type = ifelse(payment_type %in% c('DIS'), \n                               'Dispute', payment_type)) %>%            \n  mutate(payment_type = ifelse(payment_type %in% c('NO'), \n                               'No charge', 'Unknown')) %>%\n  group_by(payment_type) %>% \n  summarise(freq = n(),\n            mean_fare = mean(total_amount)) %>% \n  arrange(desc(mean_fare)) \n\npayment_db %>% show_query()\n\n\n<SQL>\nSELECT \"payment_type\", COUNT(*) AS \"freq\", AVG(\"total_amount\") AS \"mean_fare\"\nFROM (SELECT CASE WHEN (\"payment_type\" IN ('NO')) THEN ('No charge') WHEN NOT(\"payment_type\" IN ('NO')) THEN ('Unknown') END AS \"payment_type\", \"total_amount\"\nFROM (SELECT CASE WHEN (\"payment_type\" IN ('DIS')) THEN ('Dispute') WHEN NOT(\"payment_type\" IN ('DIS')) THEN (\"payment_type\") END AS \"payment_type\", \"total_amount\"\nFROM (SELECT CASE WHEN (\"payment_type\" IN ('CAS', 'CSH')) THEN ('Cash') WHEN NOT(\"payment_type\" IN ('CAS', 'CSH')) THEN (\"payment_type\") END AS \"payment_type\", \"total_amount\"\nFROM (SELECT CASE WHEN (\"payment_type\" IN ('CRE', 'CRD')) THEN ('Credit Card') WHEN NOT(\"payment_type\" IN ('CRE', 'CRD')) THEN (\"payment_type\") END AS \"payment_type\", \"total_amount\"\nFROM (SELECT UPPER(\"payment_type\") AS \"payment_type\", \"total_amount\"\nFROM \"trips\") \"q01\") \"q02\") \"q03\") \"q04\") \"q05\"\nGROUP BY \"payment_type\"\nORDER BY \"mean_fare\" DESC\n\npayment_db %>% collect()\n\n\n# A tibble: 1 x 3\n  payment_type     freq mean_fare\n  <chr>           <dbl>     <dbl>\n1 Unknown      84345018      10.7\n\n그런데, 결과를 보면 원하는 답이 아니고, 수행 속도도 매우 느리다. 그래서 mutate() 함수를 여러 개 독립, 순차적으로 기술한 것을 하나의 함수로 합쳐서 수행해 보았다.\n수행속도도 개선되었고 원하는 결과를 얻을 수 있었다.\n\n\npayment2_db <- trips_db %>% \n  select(payment_type, total_amount) %>% \n  mutate(payment_type = toupper(payment_type)) %>%\n  mutate(payment_type = ifelse(payment_type %in% c('CRE', 'CRD'), \n                               'Credit Card', ifelse(payment_type %in% c('CAS', 'CSH'), \n                               'Cash', ifelse(payment_type %in% c('DIS'), \n                               'Dispute', ifelse(payment_type %in% c('NO'), \n                               'No charge', 'Unknown'))))) %>% \n  group_by(payment_type) %>% \n  summarise(freq = n(),\n            mean_fare = mean(total_amount)) %>% \n  arrange(desc(mean_fare)) \n\npayment2_db %>% show_query()\n\n\n<SQL>\nSELECT \"payment_type\", COUNT(*) AS \"freq\", AVG(\"total_amount\") AS \"mean_fare\"\nFROM (SELECT CASE WHEN (\"payment_type\" IN ('CRE', 'CRD')) THEN ('Credit Card') WHEN NOT(\"payment_type\" IN ('CRE', 'CRD')) THEN (CASE WHEN (\"payment_type\" IN ('CAS', 'CSH')) THEN ('Cash') WHEN NOT(\"payment_type\" IN ('CAS', 'CSH')) THEN (CASE WHEN (\"payment_type\" IN ('DIS')) THEN ('Dispute') WHEN NOT(\"payment_type\" IN ('DIS')) THEN (CASE WHEN (\"payment_type\" IN ('NO')) THEN ('No charge') WHEN NOT(\"payment_type\" IN ('NO')) THEN ('Unknown') END) END) END) END AS \"payment_type\", \"total_amount\"\nFROM (SELECT UPPER(\"payment_type\") AS \"payment_type\", \"total_amount\"\nFROM \"trips\") \"q01\") \"q02\"\nGROUP BY \"payment_type\"\nORDER BY \"mean_fare\" DESC\n\npayment2_db %>% collect()\n\n\n# A tibble: 4 x 3\n  payment_type     freq mean_fare\n  <chr>           <dbl>     <dbl>\n1 Credit Card  29294014     14.2 \n2 Dispute         36768     12.3 \n3 Unknown        208765     11.2 \n4 Cash         54805471      8.85\n\n아직 dbplyr가 복잡한 dplyr 구문을 Query로 파싱할 때에 성능의 이슈나, 올바로 파싱하지 못하는 문제점을 안고 있다. 특히, mutate() 함수를 여러 개 독립, 순차적으로 기술하는 것은 피해야 한다.\nExplain Plan\nshow_query() 함수로 DBMS에서 수행될 Query를 살펴 보는 것이 중요하다. 그런데 dbplyr는 Explain Plan도 지원하므로 수행 속도의 개선을 위한 튜닝에서 적절하게 활용하는 것도 좋다.\n다음은 택시의 주행 거리당 운임의 평균을 계산한 dbplyr 구문의 Explain Plan이다.\n\n\ncat(remote_query_plan(fare_db))\n\n\n                                                                                    QUERY PLAN\n1                            Finalize Aggregate  (cost=2795452.65..2795452.66 rows=1 width=32)\n2                                    ->  Gather  (cost=2795452.43..2795452.64 rows=2 width=32)\n3                                                                           Workers Planned: 2\n4                         ->  Partial Aggregate  (cost=2794452.43..2794452.44 rows=1 width=32)\n5               ->  Parallel Seq Scan on trips  (cost=0.00..2619740.10 rows=34942465 width=21)\n6                                      Filter: ((fare_amount > 0.0) AND (trip_distance > 0.0))\n\n다음은 택시의 주행 거리당 운임의 평균을 계산한 Query의 Explain Plan이다. dbplyr 구문의 것도 거의 유사하다.\n\n\nsql2 <- \"EXPLAIN ANALYZE \n          SELECT payment_type\n                ,AVG(total_amount) mean_fare \n           FROM trips t \n       GROUP BY payment_type\n       ORDER BY mean_fare DESC\"\n\nplan <- dbGetQuery(con, sql2)\nplan\n\n\n                                                                                                                                                        QUERY PLAN\n1                                                            Sort  (cost=2620742.87..2620742.89 rows=9 width=36) (actual time=79915.407..79917.957 rows=9 loops=1)\n2                                                                                                                               Sort Key: (avg(total_amount)) DESC\n3                                                                                                                             Sort Method: quicksort  Memory: 25kB\n4                                     ->  Finalize GroupAggregate  (cost=2620740.38..2620742.73 rows=9 width=36) (actual time=79915.133..79917.932 rows=9 loops=1)\n5                                                                                                                                          Group Key: payment_type\n6                                              ->  Gather Merge  (cost=2620740.38..2620742.48 rows=18 width=36) (actual time=79915.071..79917.752 rows=27 loops=1)\n7                                                                                                                                               Workers Planned: 2\n8                                                                                                                                              Workers Launched: 2\n9                                                        ->  Sort  (cost=2619740.36..2619740.38 rows=9 width=36) (actual time=79891.627..79891.631 rows=9 loops=3)\n10                                                                                                                                          Sort Key: payment_type\n11                                                                                                                            Sort Method: quicksort  Memory: 26kB\n12                                                                                                                 Worker 0:  Sort Method: quicksort  Memory: 26kB\n13                                                                                                                 Worker 1:  Sort Method: quicksort  Memory: 26kB\n14                                      ->  Partial HashAggregate  (cost=2619740.10..2619740.21 rows=9 width=36) (actual time=79891.487..79891.508 rows=9 loops=3)\n15                                                                                                                                         Group Key: payment_type\n16                           ->  Parallel Seq Scan on trips t  (cost=0.00..2443989.07 rows=35150207 width=14) (actual time=0.094..46938.267 rows=28115006 loops=3)\n17                                                                                                                                         Planning Time: 0.160 ms\n18                                                                                                                                    Execution Time: 79918.048 ms\n\n요금 결제 방법에 따른 평균 총 지불 운임에 대한 통계를 계산하기 위한 dbplyr 구문의 Explain Plan이다.\n\n\ncat(remote_query_plan(payment_db))\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         QUERY PLAN\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Sort  (cost=12199179.18..12199179.21 rows=9 width=72)\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Sort Key: (avg(trips.total_amount)) DESC\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ->  Finalize GroupAggregate  (cost=12199174.22..12199179.04 rows=9 width=72)\n4                          Group Key: (CASE WHEN (CASE WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END = 'DIS'::text) THEN 'Dispute'::text WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END <> 'DIS'::text) THEN CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END = 'NO'::text) THEN 'No charge'::text WHEN (CASE WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END = 'DIS'::text) THEN 'Dispute'::text WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END <> 'DIS'::text) THEN CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END <> 'NO'::text) THEN 'Unknown'::text ELSE NULL::text END)\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ->  Gather Merge  (cost=12199174.22..12199176.32 rows=18 width=72)\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Workers Planned: 2\n7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ->  Sort  (cost=12198174.19..12198174.22 rows=9 width=72)\n8                           Sort Key: (CASE WHEN (CASE WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END = 'DIS'::text) THEN 'Dispute'::text WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END <> 'DIS'::text) THEN CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END = 'NO'::text) THEN 'No charge'::text WHEN (CASE WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END = 'DIS'::text) THEN 'Dispute'::text WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END <> 'DIS'::text) THEN CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END <> 'NO'::text) THEN 'Unknown'::text ELSE NULL::text END)\n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ->  Partial HashAggregate  (cost=12198171.51..12198174.05 rows=9 width=72)\n10                           Group Key: CASE WHEN (CASE WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END = 'DIS'::text) THEN 'Dispute'::text WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END <> 'DIS'::text) THEN CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END = 'NO'::text) THEN 'No charge'::text WHEN (CASE WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END = 'DIS'::text) THEN 'Dispute'::text WHEN (CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END <> 'DIS'::text) THEN CASE WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN upper(trips.payment_type) ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END <> 'NO'::text) THEN 'Unknown'::text ELSE NULL::text END\n11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ->  Parallel Seq Scan on trips  (cost=0.00..11934544.96 rows=35150207 width=42)\n\ncat(remote_query_plan(payment2_db))\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              QUERY PLAN\n1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Sort  (cost=4114627.43..4114627.46 rows=9 width=72)\n2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Sort Key: (avg(trips.total_amount)) DESC\n3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ->  Finalize GroupAggregate  (cost=4114624.54..4114627.29 rows=9 width=72)\n4                          Group Key: (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (upper(trips.payment_type) <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = 'DIS'::text) THEN 'Dispute'::text WHEN (upper(trips.payment_type) <> 'DIS'::text) THEN CASE WHEN (upper(trips.payment_type) = 'NO'::text) THEN 'No charge'::text WHEN (upper(trips.payment_type) <> 'NO'::text) THEN 'Unknown'::text ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END)\n5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       ->  Gather Merge  (cost=4114624.54..4114626.64 rows=18 width=72)\n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Workers Planned: 2\n7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                ->  Sort  (cost=4113624.51..4113624.54 rows=9 width=72)\n8                           Sort Key: (CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (upper(trips.payment_type) <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = 'DIS'::text) THEN 'Dispute'::text WHEN (upper(trips.payment_type) <> 'DIS'::text) THEN CASE WHEN (upper(trips.payment_type) = 'NO'::text) THEN 'No charge'::text WHEN (upper(trips.payment_type) <> 'NO'::text) THEN 'Unknown'::text ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END)\n9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               ->  Partial HashAggregate  (cost=4113623.90..4113624.37 rows=9 width=72)\n10                           Group Key: CASE WHEN (upper(trips.payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(trips.payment_type) <> ALL ('{CRE,CRD}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (upper(trips.payment_type) <> ALL ('{CAS,CSH}'::text[])) THEN CASE WHEN (upper(trips.payment_type) = 'DIS'::text) THEN 'Dispute'::text WHEN (upper(trips.payment_type) <> 'DIS'::text) THEN CASE WHEN (upper(trips.payment_type) = 'NO'::text) THEN 'No charge'::text WHEN (upper(trips.payment_type) <> 'NO'::text) THEN 'Unknown'::text ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END ELSE NULL::text END\n11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ->  Parallel Seq Scan on trips  (cost=0.00..3849997.35 rows=35150207 width=42)\n\n요금 결제 방법에 따른 평균 총 지불 운임에 대한 통계를 계산하기 위한 Query의 Explain Plan이다.\n\n\nsql3 <- \"EXPLAIN ANALYZE \n          SELECT CASE WHEN UPPER(payment_type) IN ('CRE', 'CRD') THEN 'Credit Card'\n                 WHEN UPPER(payment_type) IN ('CAS', 'CSH') THEN 'Cash' \n                 WHEN UPPER(payment_type) = 'DIS' THEN 'Dispute'\n                 WHEN UPPER(payment_type) = 'NO' THEN 'No charge'\n                 ELSE 'Unknown' END AS payment_type\n                ,COUNT(*) as freq\n                ,AVG(total_amount) mean_fare \n     FROM trips t \n  GROUP BY CASE WHEN UPPER(payment_type) IN ('CRE', 'CRD') THEN 'Credit Card'\n                WHEN UPPER(payment_type) IN ('CAS', 'CSH') THEN 'Cash' \n                WHEN UPPER(payment_type) = 'DIS' THEN 'Dispute'\n                WHEN UPPER(payment_type) = 'NO' THEN 'No charge'\n                ELSE 'Unknown' END\n  ORDER BY mean_fare DESC\"\n\nplan <- dbGetQuery(con, sql3)\nplan\n\n\n                                                                                                                                                                                                                                                                                                                                                 QUERY PLAN\n1                                                                                                                                                                                                                                                   Sort  (cost=3411622.93..3411622.96 rows=9 width=72) (actual time=108677.198..108679.859 rows=4 loops=1)\n2                                                                                                                                                                                                                                                                                                                        Sort Key: (avg(total_amount)) DESC\n3                                                                                                                                                                                                                                                                                                                      Sort Method: quicksort  Memory: 25kB\n4                                                                                                                                                                                                                            ->  Finalize GroupAggregate  (cost=3411620.22..3411622.79 rows=9 width=72) (actual time=108677.047..108679.841 rows=4 loops=1)\n5                          Group Key: (CASE WHEN (upper(payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(payment_type) = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (upper(payment_type) = 'DIS'::text) THEN 'Dispute'::text WHEN (upper(payment_type) = 'NO'::text) THEN 'No charge'::text ELSE 'Unknown'::text END)\n6                                                                                                                                                                                                                                     ->  Gather Merge  (cost=3411620.22..3411622.32 rows=18 width=72) (actual time=108676.924..108679.708 rows=12 loops=1)\n7                                                                                                                                                                                                                                                                                                                                        Workers Planned: 2\n8                                                                                                                                                                                                                                                                                                                                       Workers Launched: 2\n9                                                                                                                                                                                                                                               ->  Sort  (cost=3410620.19..3410620.22 rows=9 width=72) (actual time=108652.747..108652.749 rows=4 loops=3)\n10                          Sort Key: (CASE WHEN (upper(payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(payment_type) = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (upper(payment_type) = 'DIS'::text) THEN 'Dispute'::text WHEN (upper(payment_type) = 'NO'::text) THEN 'No charge'::text ELSE 'Unknown'::text END)\n11                                                                                                                                                                                                                                                                                                                     Sort Method: quicksort  Memory: 25kB\n12                                                                                                                                                                                                                                                                                                          Worker 0:  Sort Method: quicksort  Memory: 25kB\n13                                                                                                                                                                                                                                                                                                          Worker 1:  Sort Method: quicksort  Memory: 25kB\n14                                                                                                                                                                                                                             ->  Partial HashAggregate  (cost=3410619.76..3410620.05 rows=9 width=72) (actual time=108652.662..108652.670 rows=4 loops=3)\n15                           Group Key: CASE WHEN (upper(payment_type) = ANY ('{CRE,CRD}'::text[])) THEN 'Credit Card'::text WHEN (upper(payment_type) = ANY ('{CAS,CSH}'::text[])) THEN 'Cash'::text WHEN (upper(payment_type) = 'DIS'::text) THEN 'Dispute'::text WHEN (upper(payment_type) = 'NO'::text) THEN 'No charge'::text ELSE 'Unknown'::text END\n16                                                                                                                                                                                                                    ->  Parallel Seq Scan on trips t  (cost=0.00..3146993.21 rows=35150207 width=42) (actual time=0.108..84754.703 rows=28115006 loops=3)\n17                                                                                                                                                                                                                                                                                                                                  Planning Time: 0.322 ms\n18                                                                                                                                                                                                                                                                                                                            Execution Time: 108679.976 ms\n\nExplain Plan의 결과를 보면 SQL Query > payment2_db > payment_db의 순으로 성능이 좋은 것을 알 수 있다. 복잡한 연산에 대해서는 dbplyr보다는 직접 DB에 접속하여 SQL Query를 수행하는 것이 성능상 이롭다. 그러나 dbplyr는 버전 업을 통해서 성능이 개선될 것이라는 믿음을 가지고 있다.\n선택은 가용자의 몫이지만 간단한 집계의 경우는 dbplyr를 사용해 보기를 권한다.\ndisconnection\n모든 작업이 마무리되면 연결된 DB 컨넥션을 종료한다.\n\n\ndbDisconnect(con)\n\n\n[1] TRUE\n\n참고 리소스\n데이터 관련 : https://github.com/toddwschneider/nyc-taxi-data\ndbplyr 관련 : https://cran.r-project.org/web/packages/dbplyr/vignettes/dbplyr.html\n\n\n\n",
    "preview": "posts/2020-01-28-introduce_dbplyr/img/logo.png",
    "last_modified": "2021-10-23T10:29:22+09:00",
    "input_file": {},
    "preview_width": 240,
    "preview_height": 277
  },
  {
    "path": "posts/2020-01-08-complex_function/",
    "title": "draw the complex function",
    "description": "`복소수 함수(complex function)`를 이용한 시각화의 대표적인 것에 `망델브로 세트`가 있다. 이번에 소개할 이미지도 몇 개의 복소수 함수를 좌표에 출력한 이미지다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2020-01-08",
    "categories": [
      "Gallery"
    ],
    "contents": "\n\nContents\ncomplex function\n참고 리소스\n\ncomplex function\n복소수 함수(complex function)를 이용한 시각화의 대표적인 것에 망델브로 세트가 있다. 이번에 소개할 이미지도 몇 개의 복소수 함수를 좌표에 출력한 이미지다.\n\\(R(z) = z^3 + \\frac{-0.2 + 0.11i}{z^3}\\)\n\\(R(z) = z^3 + \\frac{-0.2 + 0.11i}{z^3}\\)를 이용한 색상이 없는 이미지다.\n\n\n\nFigure 1: blank image\n\n\n\n상기 이미지에 heat.colors 함수의 팔레트 색상을 입힌 이미지다.\n\n\n\nFigure 2: heat.colors 팔레트\n\n\n\n\\(R(z) = z^2 - \\frac{0.6}{z}\\)\n\\(R(z) = z^2 - \\frac{0.6}{z}\\)를 이용한 색상이 없는 이미지다.\n\n\n\nFigure 3: blank image\n\n\n\n상기 이미지에 topo.colors 함수의 팔레트 색상을 입힌 이미지다.\n\n\n\nFigure 4: topo.colors 팔레트\n\n\n\n상기 이미지에 terrain.colors 함수의 팔레트 색상을 입힌 이미지다.\n\n\n\nFigure 5: terrain.colors 팔레트\n\n\n\n참고 리소스\n아티클 : https://www.maa.org/sites/default/files/pdf/Mathhorizons/pdfs/ColoringPage_MH_Nov17.pdf\n소스 : https://a-blog-from-sydney.blogspot.com/2020/01/complex-coloring-and-contour-levels.html\n\n\n\n\n",
    "preview": "posts/2020-01-08-complex_function/img/complex_01.png",
    "last_modified": "2021-10-11T09:38:36+09:00",
    "input_file": {},
    "preview_width": 800,
    "preview_height": 800
  },
  {
    "path": "posts/2020-01-06-finance_analytics/",
    "title": "finance analytics",
    "description": "`finance` 분석을 위한 여러 R 패키지의 설치 및 `finance data`를 수집하기 위한 몇 가지의 방법에 대해서 살펴 본다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2020-01-06",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n들어가기\nfinance analytics를 위한 R 패키지\nQuandl finance 데이터\n주가 데이터 수집하기\n데이터 연산 및 조작에 대한 슬라이드의 공유\n차례\n\n들어가기\n\nfinance 분석을 위한 여러 R 패키지의 설치 및 finance data를 수집하기 위한 몇 가지의 방법에 대해서 살펴 본다.\n\nfinance analytics를 위한 R 패키지\nfinance analytics를 위한 R 패키지 및 그 설치 방법을 소개한다.\nQuandl finance 데이터\nQuandl은 finance 데이터의 공유 서비스를 수행하는 사이트다. 이 사이트에서 무료 public 데이터를 수집하는 방법을 알아본다.\n주가 데이터 수집하기\n국내외 주가 데이터를 수집하는 방법과 주가 데이터를 분석하는 방법에 대해서 알아본다.\n데이터 연산 및 조작에 대한 슬라이드의 공유\n다음에 링크를 걸어 둔 슬라이드 파일은 금융 데이터를 수집하고 분석하는 방법에 관한 자료로 2016년도에 정리되었다. Quandl finance 데이터의 수집에 대한 내용은 현재의 버전과 상이함을 밝혀둔다.\n00_Finance_Analytics.pdf\n슬라이드 발췌 이미지를 예시해 본다.\n\n\n\nFigure 1: 슬라이드 발췌 이미지\n\n\n\n차례\n이 문서는 다음과 같은 아젠다를 이야기 한다.\n패키지 설치하기\nQuandl finance 데이터 수집하기\n주가 데이터 수집하기\n\n\n\n",
    "preview": "posts/2020-01-06-finance_analytics/img/quandl.png",
    "last_modified": "2021-10-23T10:33:40+09:00",
    "input_file": {},
    "preview_width": 1454,
    "preview_height": 642
  },
  {
    "path": "posts/2020-01-05- manipulation_extract/",
    "title": "데이터 연산 및 조작",
    "description": "`벡터`의 조작과 `행렬`의 조작은 기본적인 데이터 조작의 방법이다. 일차원으로서의 벡터와 이차원으로서의 행렬의 조작에 익숙하면 데이터 프레임이나 여기서 상속된 `tibble` 객체의 데이터 조작을 쉽게 수행할 수 있을 것이다. 물론 `tibble`이나 여기서 파생된 `data_df`는 `dplyr` 패키지로 또 다른 방법에 의한 데이터 조작을 지원하기도 한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2020-01-05",
    "categories": [
      "Tidyverse"
    ],
    "contents": "\n\nContents\n들어가기\n벡터\n행렬\n데이터 연산 및 조작에 대한 슬라이드의 공유\n차례\n\n들어가기\n\n벡터의 조작과 행렬의 조작은 기본적인 데이터 조작의 방법이다. 일차원으로서의 벡터와 이차원으로서의 행렬의 조작에 익숙하면 데이터 프레임이나 여기서 상속된 tibble 객체의 데이터 조작을 쉽게 수행할 수 있을 것이다. 물론 tibble이나 여기서 파생된 data_df는 dplyr 패키지로 또 다른 방법에 의한 데이터 조작을 지원하기도 한다.\n\n벡터\n순서가 있는 일차원 데이터인 벡터의 원소 조작에 대한 방법을 소개한다.\n행렬\n동일한 모드로 구성된 이차원 행렬의 행과 열을 기준으로 원소를 조작하는 방법을 소개한다.\n데이터 연산 및 조작에 대한 슬라이드의 공유\n다음에 링크를 걸어 둔 슬라이드 파일은 Extract과 Replace를 중심으로 작성된 벡터와 행렬의 데이터 조작에 관한 자료다. 2017년도에 정리된 자료를 공유한다.\n01_DataManipulation_Extract.pdf\n슬라이드 발췌 이미지를 예시해 본다.\n\n\n\nFigure 1: 슬라이드 발췌 이미지\n\n\n\n차례\n이 문서는 다음과 같은 아젠다를 이야기 한다.\n벡터의 조작\n행렬의 조작\n\n\n\n",
    "preview": "posts/2020-01-05- manipulation_extract/img/logical.png",
    "last_modified": "2021-10-23T09:54:53+09:00",
    "input_file": {},
    "preview_width": 1406,
    "preview_height": 1040
  },
  {
    "path": "posts/2019-12-31-documents_taxonomy/",
    "title": "Documents Taxonomy",
    "description": "`Documents Taxonomy`는 `문서(텍스트)들을 분류체계 기준으로 분류`하는 것으로, 대표적인 것에 콜센터의 상담 내용을 상담 분류 체계로 분류하는 것이 있다. 엄밀하게 구분하면 Taxonomy와 Classification은 다른 개념이지만, 여기서는 `Classification Model로 Documents Taxonomy의 가능성을 진단`해 본다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2019-12-31",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n들어가기\n학습목표\n학습방법\nDocuments Taxonomy에 대한 슬라이드의 공유\n차례\n\n들어가기\n\nTaxonomy는 사전적으로 “사물이나 생명체 등을 분류하기 위해서 사용되는 분류체계”로 해석되며, 분류체계는 트리형의 위계적 (Hirerachy) 구조로 표현한다. Documents Taxonomy는 문서(텍스트)들을 분류체계 기준으로 분류하는 것으로, 대표적인 것에 콜센터의 상담 내용을 상담 분류 체계로 분류하는 것이 있다. 엄밀하게 구분하면 Taxonomy와 Classification은 다른 개념이지만, 여기서는 Classification Model로 Documents Taxonomy의 가능성을 진단해 본다.\n\n학습목표\n다음을 네 가지 섹션의 이해를 목표로 학습을 수행한다.\n데이터 구조의 이해:\nVectorization\nDTM(Document Term Matrix)\n\n데이터 처리의 이해:\nTF-IDF\nn-gram\nFeature hashing\n\n모델의 이해:\nLASSO regression\n\n패키지의 이해:\ntext2vec\nglmnet\n\n학습방법\n\n“사람들의 대화를 들어보면 개인별로 즐겨 사용하는 언어적 특징이 있는 것처럼, 대통령의 연설문에도 개인적 특징이 담겨있지 않을까?”\n\n연설문만으로 어떤 대통령이 연설했는가를 분류\n데이터:\n대통령 연설문 전문\n\n분류 방법:\n연설문 내용으로 어느 대통령이 연설했는지 분류\n김대중, 노무현, 이명박\n\n성능 비교:\n몇 가지 방법에 대해서 분류의 성능 비교\n\nDocuments Taxonomy에 대한 슬라이드의 공유\n다음에 링크를 걸어 둔 슬라이드 파일은 2017년도에 모 미트업에서 발표한 슬라이드 pdf 파일이다. 그 이후에 많은 시간이 흘렀기 때문에 일부 개선된 것들이 있을 수 있다. 감안해서 참고하기 바란다.\nDocuments_Texonomy.pdf\n슬라이드 발췌 이미지를 예시해 본다.\n\n\n\nFigure 1: 슬라이드 발췌 이미지\n\n\n\n차례\n이 문서는 다음과 같은 아젠다를 이야기 한다.\n들어가기\n데이터 전처리\n모델 생성\n모델 성능 비교\n\n\n\n",
    "preview": "posts/2019-12-31-documents_taxonomy/img/lasso.png",
    "last_modified": "2021-10-23T09:46:47+09:00",
    "input_file": {},
    "preview_width": 1402,
    "preview_height": 1034
  },
  {
    "path": "posts/2019-12-29-president_speech/",
    "title": "대통령 연설문 분석하기",
    "description": "비정형 텍스트 데이터를 조작할 경우에 종종 발생하는 요건으로, `character 벡터의 개별 원소`를 특정 구분자로 나누어서 `여러 개의 원소로 만들거나`, 데이터 프레임에서 상속된 객체에서 특정 `character 변수`를 동일한 방법으로 나누어서 `여러 관측치로 만들 필요성` 있다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2019-12-29",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n들어가기\nCrawling\nPreprocessing\nText Analytics\n데이터 조작에 대한 슬라이드의 공유\n차례\n\n들어가기\n\nR에서 데이터를 scraping하는 기술은 데이터 조작 함수를 구사하는 능력에 비례한다. 그 이유는 원하는 부분의 데이터만 취하기 위해서는 데이터 조작이 필수적이기 때문이다. 물론 원천 데이터가 웹 채널에 있기 때문에 HTML 문법과 이를 긁어올 수 있는 rvest 패키지 등의 사용법도 중요하다. Text Analytics는 NLP(Natural language processing)를 위한 전문적인 도구와 기술이 필요하다. 그러나 다행이도 일반적인 데이터 분석 기법을 적용할 수도 있고, 최근에는 Deep Learning 기법이 응용되는 분야이기도 하다. 일반적인 데이터 분석 기법을 적용하여 Text Analytics을 이해해 보자.\n\nCrawling\n대통령기록연구실 홈페이지에서 특정 대통령의 재임 기간의 연설문을 수집한다.\nPreprocessing\n텍스트 분석을 위해서 수집한 텍스트 데이터를 전처리한다. 몇 가지의 형태소 분석기를 분석해 본다.\nText Analytics\nDocument Term Matrix를 생성한 다음, 여러 기법의 텍스트 데이터 분석을 수행한다.\n데이터 조작에 대한 슬라이드의 공유\n다음에 링크를 걸어 둔 슬라이드 파일은 2017년도에 모 미트업에서 발표한 슬라이드 pdf 파일이다. 그 이후에 많은 시간이 흘렀기 때문에 일부 개선된 것들이 있을 수 있다. 감안해서 참고하기 바란다.\n01_President_Speech_Slide.pdf\n슬라이드 발췌 이미지를 예시해 본다.\n\n\n\nFigure 1: 슬라이드 발췌 이미지\n\n\n\n차례\n이 문서는 다음과 같은 아젠다를 이야기 한다.\nLearning Outline\nScraping Data\n수집 데이터 전처리\nText Analytics 개요\nFrequency Analysis\nAssociation Rules\nClustering/Topic Analytics\n\n\n\n",
    "preview": "posts/2019-12-29-president_speech/img/TA.png",
    "last_modified": "2021-10-23T09:42:49+09:00",
    "input_file": {},
    "preview_width": 1398,
    "preview_height": 1024
  },
  {
    "path": "posts/2019-12-28-multi_derived/",
    "title": "하나에서 여러 개를 파생하기",
    "description": "비정형 텍스트 데이터를 조작할 경우에 종종 발생하는 요건으로, `character 벡터의 개별 원소`를 특정 구분자로 나누어서 `여러 개의 원소로 만들거나`, 데이터 프레임에서 상속된 객체에서 특정 `character 변수`를 동일한 방법으로 나누어서 `여러 관측치로 만들 필요성` 있다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2019-12-28",
    "categories": [
      "Tidyverse"
    ],
    "contents": "\n\nContents\n들어가기\n데이터의 준비\n벡터의 한 개 원소에서 여러 개 원소 추출하기\n식품의 식자재 추출\n벡터를 리스트로, 그리고 다시 벡터로 변경하기\n벡터를 tbl_df로 변경하기\n부수적인 텍스트 전처리\n대표적인 식재료의 wordcloud\n\n하나의 레코드에서 여러 레코드 파생하기\n국가명 보정 함수 정의\n국가명 보정하고 추출하기\n하나에서 여러 레코드 파생하기\n\n국가별 식품 정보 분석하기\n국가별 설탕에 대한 기호\n나머지 관심 지표의 확인\n\n\n들어가기\n\n비정형 텍스트 데이터를 조작할 경우에 종종 발생하는 요건으로, character 벡터의 개별 원소를 특정 구분자로 나누어서 여러 개의 원소로 만들거나, 데이터 프레임에서 상속된 객체에서 특정 character 변수를 동일한 방법으로 나누어서 여러 관측치로 만들 필요성 있다. 텍스트 조작처럼 성가신 것이 없는데, 이들 두 요건에 대한 솔루션을 제시해 본다. 결국은 dplyr 패키지와 tidyr 패키지의 사용법에 대한 소개다.\n\n데이터의 준비\nhttps://www.kaggle.com/openfoodfacts/world-food-facts에서 world-food-facts.zip를 다운로드한 후 압축을 풀면 en.openfoodfacts.org.products.tsv라는 이름의 텍스트 파일을 얻을 수 있다. 이 파일은 컬럼의 구분자를 탭문자로 정의한 데이터 파일이다.\n예제에서는 en.openfoodfacts.org.products.tsv를 FoodFacts.tsv라는 이름으로 변경 후 ./data 경로에 저장한 것을 전제로 수행한다.\n이 데이터는 전 세계 식품의 영양 정보를 수집한 데이터로 Open Food Facts라는 단체에서 공개한 public 데이터다.\n\n\ndata_repo <- \"./data\" \nfname <- paste(data_repo, \"FoodFacts.tsv\", sep =\"/\")\n\n# 파일의 용량\nfile.size(fname) / 1024^2\n\n\n[1] 963.456\n\n# 데이터를 R로 가져온다.\nfoodfact <- readr::read_tsv(fname, progress = FALSE)\n\n# 메모리에 적재된 R 데이터의 용량\nformat(object.size(foodfact), units = \"Mb\")\n\n\n[1] \"942.2 Mb\"\n\n# 차원\ndim(foodfact)\n\n\n[1] 356001    163\n\n# 변수의 이름\nnames(foodfact)\n\n\n  [1] \"code\"                                      \n  [2] \"url\"                                       \n  [3] \"creator\"                                   \n  [4] \"created_t\"                                 \n  [5] \"created_datetime\"                          \n  [6] \"last_modified_t\"                           \n  [7] \"last_modified_datetime\"                    \n  [8] \"product_name\"                              \n  [9] \"generic_name\"                              \n [10] \"quantity\"                                  \n [11] \"packaging\"                                 \n [12] \"packaging_tags\"                            \n [13] \"brands\"                                    \n [14] \"brands_tags\"                               \n [15] \"categories\"                                \n [16] \"categories_tags\"                           \n [17] \"categories_en\"                             \n [18] \"origins\"                                   \n [19] \"origins_tags\"                              \n [20] \"manufacturing_places\"                      \n [21] \"manufacturing_places_tags\"                 \n [22] \"labels\"                                    \n [23] \"labels_tags\"                               \n [24] \"labels_en\"                                 \n [25] \"emb_codes\"                                 \n [26] \"emb_codes_tags\"                            \n [27] \"first_packaging_code_geo\"                  \n [28] \"cities\"                                    \n [29] \"cities_tags\"                               \n [30] \"purchase_places\"                           \n [31] \"stores\"                                    \n [32] \"countries\"                                 \n [33] \"countries_tags\"                            \n [34] \"countries_en\"                              \n [35] \"ingredients_text\"                          \n [36] \"allergens\"                                 \n [37] \"allergens_en\"                              \n [38] \"traces\"                                    \n [39] \"traces_tags\"                               \n [40] \"traces_en\"                                 \n [41] \"serving_size\"                              \n [42] \"no_nutriments\"                             \n [43] \"additives_n\"                               \n [44] \"additives\"                                 \n [45] \"additives_tags\"                            \n [46] \"additives_en\"                              \n [47] \"ingredients_from_palm_oil_n\"               \n [48] \"ingredients_from_palm_oil\"                 \n [49] \"ingredients_from_palm_oil_tags\"            \n [50] \"ingredients_that_may_be_from_palm_oil_n\"   \n [51] \"ingredients_that_may_be_from_palm_oil\"     \n [52] \"ingredients_that_may_be_from_palm_oil_tags\"\n [53] \"nutrition_grade_uk\"                        \n [54] \"nutrition_grade_fr\"                        \n [55] \"pnns_groups_1\"                             \n [56] \"pnns_groups_2\"                             \n [57] \"states\"                                    \n [58] \"states_tags\"                               \n [59] \"states_en\"                                 \n [60] \"main_category\"                             \n [61] \"main_category_en\"                          \n [62] \"image_url\"                                 \n [63] \"image_small_url\"                           \n [64] \"energy_100g\"                               \n [65] \"energy-from-fat_100g\"                      \n [66] \"fat_100g\"                                  \n [67] \"saturated-fat_100g\"                        \n [68] \"-butyric-acid_100g\"                        \n [69] \"-caproic-acid_100g\"                        \n [70] \"-caprylic-acid_100g\"                       \n [71] \"-capric-acid_100g\"                         \n [72] \"-lauric-acid_100g\"                         \n [73] \"-myristic-acid_100g\"                       \n [74] \"-palmitic-acid_100g\"                       \n [75] \"-stearic-acid_100g\"                        \n [76] \"-arachidic-acid_100g\"                      \n [77] \"-behenic-acid_100g\"                        \n [78] \"-lignoceric-acid_100g\"                     \n [79] \"-cerotic-acid_100g\"                        \n [80] \"-montanic-acid_100g\"                       \n [81] \"-melissic-acid_100g\"                       \n [82] \"monounsaturated-fat_100g\"                  \n [83] \"polyunsaturated-fat_100g\"                  \n [84] \"omega-3-fat_100g\"                          \n [85] \"-alpha-linolenic-acid_100g\"                \n [86] \"-eicosapentaenoic-acid_100g\"               \n [87] \"-docosahexaenoic-acid_100g\"                \n [88] \"omega-6-fat_100g\"                          \n [89] \"-linoleic-acid_100g\"                       \n [90] \"-arachidonic-acid_100g\"                    \n [91] \"-gamma-linolenic-acid_100g\"                \n [92] \"-dihomo-gamma-linolenic-acid_100g\"         \n [93] \"omega-9-fat_100g\"                          \n [94] \"-oleic-acid_100g\"                          \n [95] \"-elaidic-acid_100g\"                        \n [96] \"-gondoic-acid_100g\"                        \n [97] \"-mead-acid_100g\"                           \n [98] \"-erucic-acid_100g\"                         \n [99] \"-nervonic-acid_100g\"                       \n[100] \"trans-fat_100g\"                            \n[101] \"cholesterol_100g\"                          \n[102] \"carbohydrates_100g\"                        \n[103] \"sugars_100g\"                               \n[104] \"-sucrose_100g\"                             \n[105] \"-glucose_100g\"                             \n[106] \"-fructose_100g\"                            \n[107] \"-lactose_100g\"                             \n[108] \"-maltose_100g\"                             \n[109] \"-maltodextrins_100g\"                       \n[110] \"starch_100g\"                               \n[111] \"polyols_100g\"                              \n[112] \"fiber_100g\"                                \n[113] \"proteins_100g\"                             \n[114] \"casein_100g\"                               \n[115] \"serum-proteins_100g\"                       \n[116] \"nucleotides_100g\"                          \n[117] \"salt_100g\"                                 \n[118] \"sodium_100g\"                               \n[119] \"alcohol_100g\"                              \n[120] \"vitamin-a_100g\"                            \n[121] \"beta-carotene_100g\"                        \n[122] \"vitamin-d_100g\"                            \n[123] \"vitamin-e_100g\"                            \n[124] \"vitamin-k_100g\"                            \n[125] \"vitamin-c_100g\"                            \n[126] \"vitamin-b1_100g\"                           \n[127] \"vitamin-b2_100g\"                           \n[128] \"vitamin-pp_100g\"                           \n[129] \"vitamin-b6_100g\"                           \n[130] \"vitamin-b9_100g\"                           \n[131] \"folates_100g\"                              \n[132] \"vitamin-b12_100g\"                          \n[133] \"biotin_100g\"                               \n[134] \"pantothenic-acid_100g\"                     \n[135] \"silica_100g\"                               \n[136] \"bicarbonate_100g\"                          \n[137] \"potassium_100g\"                            \n[138] \"chloride_100g\"                             \n[139] \"calcium_100g\"                              \n[140] \"phosphorus_100g\"                           \n[141] \"iron_100g\"                                 \n[142] \"magnesium_100g\"                            \n[143] \"zinc_100g\"                                 \n[144] \"copper_100g\"                               \n[145] \"manganese_100g\"                            \n[146] \"fluoride_100g\"                             \n[147] \"selenium_100g\"                             \n[148] \"chromium_100g\"                             \n[149] \"molybdenum_100g\"                           \n[150] \"iodine_100g\"                               \n[151] \"caffeine_100g\"                             \n[152] \"taurine_100g\"                              \n[153] \"ph_100g\"                                   \n[154] \"fruits-vegetables-nuts_100g\"               \n[155] \"fruits-vegetables-nuts-estimate_100g\"      \n[156] \"collagen-meat-protein-ratio_100g\"          \n[157] \"cocoa_100g\"                                \n[158] \"chlorophyl_100g\"                           \n[159] \"carbon-footprint_100g\"                     \n[160] \"nutrition-score-fr_100g\"                   \n[161] \"nutrition-score-uk_100g\"                   \n[162] \"glycemic-index_100g\"                       \n[163] \"water-hardness_100g\"                       \n\n벡터의 한 개 원소에서 여러 개 원소 추출하기\n식품의 식자재 추출\n식품의 식자재 종류는 ingredients_text 변수에 텍스트로 들어 있다. 그래서 텍스트의 조작을 통해서 식자재를 분리해야 한다. 그러나 다음 예시처럼 일부 몇 건을 살펴보자. 하나의 ingredients_text에는 여러 개의 식자재들이 기술되어 있다.\n\n\nexam <- foodfact$ingredients_text[c(2:3, 13)]\nexam\n\n\n[1] \"Bananas, vegetable oil (coconut oil, corn oil and/or palm oil) sugar, natural banana flavor.\"                                                                                                                                                                                                                                                                                     \n[2] \"Peanuts, wheat flour, sugar, rice flour, tapioca starch, salt, leavening (ammonium bicarbonate, baking soda), soy sauce (water, soybeans, wheat, salt), potato starch.\"                                                                                                                                                                                                           \n[3] \"Roasted peanuts (peanuts, peanut or canola oil, salt), sesame sticks (unbleached wheat flour, sesame seeds, sunflower oil, sa;t, beet powder, turmeric), chili crackers (rice, corn starch, soy sauce[water, soybeans, wheat, salt], brown rice syrup, paprika, onion powder, garlic powder), tamari roasted almonds (almonds, tamari shoyu [water, wheat, soybeans, salt]), salt\"\n\n벡터를 리스트로, 그리고 다시 벡터로 변경하기\n하나의 벡터에는 임의의 갯수의 식자재가 포함되어 있다. 그런데 자세히 보면 다음과 같은 몇 가지의 대표적인 구분자 패턴이 존재한다.\n콤마(,)로 분리\n괄호로 분리\n중괄호로 분리\n먼저 앞에서 찾은 식자재들의 분리 패턴을 이용해서, 여러 개의 식자재를 추출해 보자. 구분자로 분리된 수 많큼 원소의 개수가 늘어난다. 벡터의 길이가 3개에서 60으로 늘어났다. 중요한 것은 str_split() 함수는 분리된 개별 문자열의 결과를 리스트로 반환하기 때문에 unlist() 함수로 리스트를 벡터로 변경해 주어야 한는 것이다.\n\n\nlibrary(dplyr)\nlibrary(stringr)\n\nfoodfact %>%\n  .[c(2:3, 13), ] %>% \n  transmute(ingredients = str_split(ingredients_text, pattern = \",|\\\\(|\\\\)|\\\\[|\\\\]\")) %>%\n  unlist()\n\n\n               ingredients1                ingredients2 \n                  \"Bananas\"           \" vegetable oil \" \n               ingredients3                ingredients4 \n              \"coconut oil\" \" corn oil and/or palm oil\" \n               ingredients5                ingredients6 \n                   \" sugar\"   \" natural banana flavor.\" \n               ingredients7                ingredients8 \n                  \"Peanuts\"              \" wheat flour\" \n               ingredients9               ingredients10 \n                   \" sugar\"               \" rice flour\" \n              ingredients11               ingredients12 \n          \" tapioca starch\"                     \" salt\" \n              ingredients13               ingredients14 \n              \" leavening \"      \"ammonium bicarbonate\" \n              ingredients15               ingredients16 \n             \" baking soda\"                          \"\" \n              ingredients17               ingredients18 \n              \" soy sauce \"                     \"water\" \n              ingredients19               ingredients20 \n                \" soybeans\"                    \" wheat\" \n              ingredients21               ingredients22 \n                    \" salt\"                          \"\" \n              ingredients23               ingredients24 \n          \" potato starch.\"          \"Roasted peanuts \" \n              ingredients25               ingredients26 \n                  \"peanuts\"     \" peanut or canola oil\" \n              ingredients27               ingredients28 \n                    \" salt\"                          \"\" \n              ingredients29               ingredients30 \n          \" sesame sticks \"    \"unbleached wheat flour\" \n              ingredients31               ingredients32 \n            \" sesame seeds\"            \" sunflower oil\" \n              ingredients33               ingredients34 \n                    \" sa;t\"              \" beet powder\" \n              ingredients35               ingredients36 \n                \" turmeric\"                          \"\" \n              ingredients37               ingredients38 \n         \" chili crackers \"                      \"rice\" \n              ingredients39               ingredients40 \n             \" corn starch\"                \" soy sauce\" \n              ingredients41               ingredients42 \n                    \"water\"                 \" soybeans\" \n              ingredients43               ingredients44 \n                   \" wheat\"                     \" salt\" \n              ingredients45               ingredients46 \n                         \"\"         \" brown rice syrup\" \n              ingredients47               ingredients48 \n                 \" paprika\"             \" onion powder\" \n              ingredients49               ingredients50 \n           \" garlic powder\"                          \"\" \n              ingredients51               ingredients52 \n \" tamari roasted almonds \"                   \"almonds\" \n              ingredients53               ingredients54 \n           \" tamari shoyu \"                     \"water\" \n              ingredients55               ingredients56 \n                   \" wheat\"                 \" soybeans\" \n              ingredients57               ingredients58 \n                    \" salt\"                          \"\" \n              ingredients59               ingredients60 \n                         \"\"                     \" salt\" \n\n벡터를 tbl_df로 변경하기\n앞의 결과는 길이가 60인 벡터를 반환한다. 그러나, 우리는 dplyr 패키지를 사용하기 위해서 tibble 패키지의 enframe() 함수를 사용해서 벡터를 tbl_df 객체로 변경할 수 있다.\n\n\nfoodfact %>%\n  .[c(2:3, 13), ] %>% \n  transmute(ingredients = str_split(ingredients_text, pattern = \",|\\\\(|\\\\)|\\\\[|\\\\]\")) %>%\n  unlist() %>% \n  tibble::enframe() %>% \n  knitr::kable()\n\n\nname\nvalue\ningredients1\nBananas\ningredients2\nvegetable oil\ningredients3\ncoconut oil\ningredients4\ncorn oil and/or palm oil\ningredients5\nsugar\ningredients6\nnatural banana flavor.\ningredients7\nPeanuts\ningredients8\nwheat flour\ningredients9\nsugar\ningredients10\nrice flour\ningredients11\ntapioca starch\ningredients12\nsalt\ningredients13\nleavening\ningredients14\nammonium bicarbonate\ningredients15\nbaking soda\ningredients16\n\ningredients17\nsoy sauce\ningredients18\nwater\ningredients19\nsoybeans\ningredients20\nwheat\ningredients21\nsalt\ningredients22\n\ningredients23\npotato starch.\ningredients24\nRoasted peanuts\ningredients25\npeanuts\ningredients26\npeanut or canola oil\ningredients27\nsalt\ningredients28\n\ningredients29\nsesame sticks\ningredients30\nunbleached wheat flour\ningredients31\nsesame seeds\ningredients32\nsunflower oil\ningredients33\nsa;t\ningredients34\nbeet powder\ningredients35\nturmeric\ningredients36\n\ningredients37\nchili crackers\ningredients38\nrice\ningredients39\ncorn starch\ningredients40\nsoy sauce\ningredients41\nwater\ningredients42\nsoybeans\ningredients43\nwheat\ningredients44\nsalt\ningredients45\n\ningredients46\nbrown rice syrup\ningredients47\npaprika\ningredients48\nonion powder\ningredients49\ngarlic powder\ningredients50\n\ningredients51\ntamari roasted almonds\ningredients52\nalmonds\ningredients53\ntamari shoyu\ningredients54\nwater\ningredients55\nwheat\ningredients56\nsoybeans\ningredients57\nsalt\ningredients58\n\ningredients59\n\ningredients60\nsalt\n\n부수적인 텍스트 전처리\n전체 데이터로 부수적인 텍스트 전처리를 수행한다. 식자재를 의미하지 않는 몇몇 텍스트의 패턴을 정규표현식을 이용해서 제거한다. 모든 의미 없는 텍스트를 제거하거나 보정하기에는 많은 시간과 노력이 필요하므로, 적당한 선에서 타협을 하였다.\n다음 코드는 간단한 전처리 후 도출된 식자재의 돗수(frequency)를 구한다. 그리고 마지막으로 돗수가 큰 상위 100개의 식자재를 추출하였다. 상위에 랭크된 것들은 제대로 추출된 식자재의 이름이다. 아마도 상위 100개의 식자재 이름은 무리없이 사용할 수 있을 것이다.\n\n\n# dplyr, tibble을 이용한 식자재의 추출\ningredients <- foodfact %>%\n  transmute(ingredients = str_split(ingredients_text, pattern = \",|\\\\(|\\\\)|\\\\[|\\\\]\")) %>%\n  unlist() %>%\n  tibble::enframe() %>%\n  mutate(value = str_replace(value, \"org|organic\", \"\"),\n         value = str_replace(value, \"and/or\", \"\"),\n         value = str_replace(value, \"-> en:\\\\w+\", \"\"),\n         value = str_replace(value, \"-> exists  -- ok\", \"\"),\n         value = str_replace(value, \"-\", \" \"),\n         value = str_replace_all(value, \"[[:punct:]]\", \" \"),\n         value = str_replace_all(value, \"  \", \" \"),\n         value = str_trim(value)) %>%\n  filter(value != \"\", !str_detect(value, \"completed|\\\\d\")) %>%\n  count(value) %>%\n  transmute(ingredient = value, freq = n) %>% \n  arrange(desc(freq))\n\n# 도수가 큰 상위 100건 추출\ntop100 <- head(ingredients, 100)\ntop100 %>% \n  knitr::kable(format.args = list(big.mark = ','))\n\n\ningredient\nfreq\nsalt\n99,466\nsugar\n65,631\nwater\n61,163\nsel\n48,847\nsucre\n35,901\neau\n33,960\ncitric acid\n32,105\nriboflavin\n21,773\nfolic acid\n21,237\nniacin\n21,237\ndextrose\n19,493\nnatural flavor\n17,395\ncorn syrup\n17,388\nwheat flour\n17,142\nsoy lecithin\n16,009\nspices\n15,539\nsea salt\n14,724\nenzymes\n13,850\nreduced iron\n13,261\nthiamine mononitrate\n12,486\nSugar\n12,343\nsoybean oil\n11,894\nnatural flavors\n11,775\nWater\n10,751\nhigh fructose corn syrup\n10,717\nmaltodextrin\n10,555\nmilk\n10,471\ngarlic\n10,470\nxanthan gum\n10,337\nvinegar\n9,180\ncream\n9,060\nwhey\n8,829\ncocoa butter\n8,817\nthiamin mononitrate\n8,762\nfarine de blé\n8,702\nhuile de tournesol\n8,560\narômes\n8,489\nascorbic acid\n8,343\narôme\n8,076\npalm oil\n7,862\npreservative\n7,692\nvegetable oil\n7,673\ncaramel color\n7,535\nyeast\n7,341\nhuile de colza\n7,249\nguar gum\n7,119\ncorn starch\n7,115\nsirop de glucose\n7,039\nlait\n7,008\niron\n6,895\nbeurre de cacao\n6,883\ngarlic powder\n6,764\ncolor\n6,740\nonion powder\n6,319\nmodified corn starch\n6,193\ncanola oil\n6,144\népices\n6,066\ncarrageenan\n5,991\nmodified food starch\n5,991\nonion\n5,948\nail\n5,923\nmalted barley flour\n5,875\npâte de cacao\n5,740\npaprika\n5,722\ncheese culture\n5,643\nbutter\n5,496\nlactose\n5,451\nnatural and artificial flavors\n5,317\nvitamin c\n5,317\nmono and diglycerides\n5,295\nsodium citrate\n5,291\nlactic acid\n5,241\nartificial flavor\n5,221\nhoney\n5,138\npotassium sorbate\n5,099\ntomato paste\n5,065\ndistilled vinegar\n4,991\nlevure\n4,952\narôme naturel\n4,846\nbrown sugar\n4,844\ncane sugar\n4,725\nsunflower oil\n4,641\nbaking soda\n4,591\nsodium bicarbonate\n4,538\nsodium phosphate\n4,483\ncheese cultures\n4,467\nspice\n4,460\nacidifiant acide citrique\n4,402\nyeast extract\n4,383\npoivre\n4,345\ncalcium chloride\n4,329\nvitamin a palmitate\n4,315\nchocolate liquor\n4,287\npectin\n4,240\nmolasses\n4,178\ncocoa\n4,045\npasteurized milk\n4,001\nwheat\n3,994\noignons\n3,981\nonions\n3,940\n\n하위 10건을 살펴보니, 한자와 일본어가 포함되어 있다. 달걀과 식용색소인 황색5호도 눈에 띈다.\n\n\n# 하위 10건 추출\ntail(ingredients, 10) %>% \n  knitr::kable()\n\n\ningredient\nfreq\n食用黃梔子色素\n1\n香料 カラメル色素\n1\n高分岐環狀糊精\n1\n高果糖糖漿\n1\n高梁\n1\n鶏卵\n1\n鹿角菜膠\n1\n麦芽エキス\n1\n黃色五號\n1\n黑醋粉\n1\n\n대표적인 식재료의 wordcloud\n앞서 구한 상위 100개의 식재료로 wordcloud 플롯을 그려본다. 식품에 소금, 설탕, 물이 가장 많이 사용되는 식재료임을 쉽게 알 수 있는 플롯이다.\n\n\npar(mar = c(0, 0, 0, 0))\n\n# 팔레트 정의\npal <- RColorBrewer::brewer.pal(6, \"Dark2\")\n\n# 워드클라우드 생성\nwordcloud::wordcloud(top100$ingredient, top100$freq, colors = pal)\n\n\n\n\n하나의 레코드에서 여러 레코드 파생하기\n앞서 다룬 식자재 데이터는 일차원 데이터다. 그런데 이차원 이상의 데이터는 결국 하나의 레코드(관측치, Observations)를 여러 레코드로 분리하는 방법으로 데이터를 조작해야 한다.\n다음의 시나리오를 적용해 본다.\n국가의 영문명인 countries_en 변수는 식품의 국가명을 영문으로 표현한 변수다.\n복수의 나라 명을 포함하며 컴마(,)로 분리하고 있다.\n개별 국가로 분리해야 하며, 관심있는 나머지 변수로 추출한다.\n관심있는 나머지 변수는 개별 국가의 관측치에 복제된다.\n\n영문 외의 여러 나라 문자를 포함한다.\n영문으로 변경해야 한다.\n\n\n식품에 포함된 여러 영양소의 규모를 제공한다.\n접미어 _100g로 끝나는 변수들은 100g 당 해당 영양소의 포함 중량이다.\n관심있는 몇 개의 영양소를 국가별로 살펴보자.\n\n식품의 카테고리와 라벨에도 몇몇 정보를 얻을 수 있다.\n카테고리 변수인 categories_en에서 고기가 포함된 식품을 추출해 본다.\n라벨 변수인 labels에서 채식 식품을 추출해 본다.\n\n국가명 보정 함수 정의\n국가명은 여러 국가의 언어로 표현되어 있다. 그래서 이들을 영문으로 변경하는 clean_country라는 이름의 함수를 정의하였다. 이 함수는 국가의 이름을 영문 대문자로 변경해 준다.\n\n\n# 국가명 보정 사용자 정의 함수\nclean_country <- function(x) {\n  library(magrittr)\n  \n  x %>% \n    stringr::str_to_upper() %>% \n    stringr::str_replace(\"UNITED STATES|EN:US\", \"US\") %>%\n    stringr::str_replace(\"UNITED KINGDOM|EN:GB|FR:GRANDE-BRETAGNE\", \"UK\") %>% \n    stringr::str_replace(\"FR:REINO-UNIDO|FR:ANGLETERRE|المملكة-المتحدة\", \"UK\") %>% \n    stringr::str_replace(\"FR:SCOTLAND\", \"SCOTLAND\") %>%\n    stringr::str_replace(\"FR:DEUTSCHLAND|FR:ALEMANIA|NL:DEUTSCHLAND\", \"GERMANY\") %>%\n    stringr::str_replace(\"EN:FR|FR:ФРАНЦИЯ|FR:FRANKRIJK|FR:FRANKREICH\", \"FRANCE\") %>% \n    stringr::str_replace(\"FR:MARSEILLE-5|FR:PUYRICARD|NL:FRANKREICH\", \"FRANCE\") %>% \n    stringr::str_replace(\"FR:BOURGOGNE-AUBE-NOGENT-SUR-SEINE|RÉUNION\", \"FRANCE\") %>% \n    stringr::str_replace(\"HOLLAND|FR:NEDERLAND\", \"NETHERLANDS\") %>% \n    stringr::str_replace(\"FR:GRIEKENLAND\", \"GREECE\") %>% \n    stringr::str_replace(\"FR:IRLAND\", \"IRELAND\") %>% \n    stringr::str_replace(\"EN:CN|FR:QUEBEC\", \"CANADA\") %>% \n    stringr::str_replace(\"EN:AU\", \"AUSTRALIA\") %>% \n    stringr::str_replace(\"EN:ES|ESPAÑA|FR:SPANJE|FR:SPANIEN\", \"SPAIN\") %>% \n    stringr::str_replace(\"CÔTE D'IVOIRE\", \"IVORY COAST\") %>% \n    stringr::str_replace(\"FR:MAURICIO\", \"MAURITIUS\") %>% \n    stringr::str_replace(\"FR:SCHWEIZ|NL:SCHWEIZ\", \"SWITZERLAND\") %>% \n    stringr::str_replace(\"FR:BELGIQU|NL:BELGIEN\", \"BELGIUM\") %>% \n    stringr::str_replace(\"XX:DANEMARK\", \"DENMARK\") %>%\n    stringr::str_replace(\"OTHER-TURQUIE\", \"TURKEY\") %>%\n    stringr::str_replace(\"OTHER-JAPON|OTHER-日本\", \"JAPAN\") %>% \n    stringr::str_replace(\"日本\", \"JAPAN\") %>% \n    stringr::str_replace(\"السعودية\", \"SAUDI ARABIA\") %>% \n    stringr::str_replace(\"سلطنة-عمان\", \"OMAN\") %>%\n    stringr::str_replace(\"العراق\", \"IRAQ\") %>%\n    stringr::str_replace(\"العراق\", \"IRAQ\") %>%\n    stringr::str_replace(\"中华人民共和国|香港\", \"CHINA\") %>%\n    stringr::str_replace(\"ព្រះរាជាណាចក្រកម្ពុជា\", \"CAMBODIA\")\n}\n\n\n\n국가명 보정하고 추출하기\n벡터를 컴마 구분자로 분리하여 리스트로 만든 후 벡터로 변경한다.\n\n\n# 원래의 데이터 건수\nnrow(foodfact)\n\n\n[1] 356001\n\n# 국가명 추출\ncountries <- foodfact %>% \n  mutate(countries_clean = clean_country(countries_en)) %>% \n  select(countries_clean) %>% \n  pull() %>% \n  stringr::str_split(\",\") %>% \n  unlist\n\n# 추출된 국가의 건수\nlength(countries)\n\n\n[1] 363850\n\n# 상위 10개의 국가\ncountries %>% \n  table() %>% \n  sort(decreasing = TRUE) %>% \n  head(n = 10) %>% \n  knitr::kable(format.args = list(big.mark = ','))\n\n\n.\nFreq\nUS\n173,708\nFRANCE\n129,624\nSWITZERLAND\n17,210\nGERMANY\n9,454\nSPAIN\n6,064\nUK\n5,991\nBELGIUM\n4,092\nAUSTRALIA\n2,319\nRUSSIA\n1,641\nITALY\n1,632\n\n하나에서 여러 레코드 파생하기\n예시로 데이터 프레임을 생성한다. 컴마로 여러 단어를 묶은 city 변수를 주목하자.\n\n\n# 예제 데이터프레임\nexam <- data.frame(city = c(\"서울\", \"부산,인천,광주\", \"강원,경기\"), \n                   falg = c(\"특별시\", \"광역시\", \"시도\"),\n                   num = c(234, 123, 53)) \nexam %>% \n  knitr::kable()\n\n\ncity\nfalg\nnum\n서울\n특별시\n234\n부산,인천,광주\n광역시\n123\n강원,경기\n시도\n53\n\ncity 변수를 컴마를 기준으로 분리해서 new_city 변수를 생성한다. 이 변수는 stringr::str_split의 결과인 리스트 객체를 포함하는 변수다.\n\n\n# 컴마로 분리 후 new_city 변수 생성\nexam %>% \n  mutate(new_city = stringr::str_split(city, \",\")) %>% \n  select(-city) %>% \n  knitr::kable()\n\n\nfalg\nnum\nnew_city\n특별시\n234\n서울\n광역시\n123\n부산, 인천, 광주\n시도\n53\n강원, 경기\n\nnew_city 변수의 개별 리스트의 성분별로 레코드를 만들어 주기 위해서 tidyr 패키지의 unnest() 함수를 사용한다. 이제 비로소 도시별로 관측치가 만들어졌다.\n\n\n# 최종 결과\nexam %>% \n  mutate(new_city = stringr::str_split(city, \",\")) %>% \n  select(-city) %>% \n  tidyr::unnest(new_city) %>% \n  knitr::kable()\n\n\nfalg\nnum\nnew_city\n특별시\n234\n서울\n광역시\n123\n부산\n광역시\n123\n인천\n광역시\n123\n광주\n시도\n53\n강원\n시도\n53\n경기\n\nfoodfact를 개별 국가별로 레코드를 분리하면서 몇 개의 관심있는 변수를 추출하고, 파생한다.\n\n\n# 국가별 관심 지표의 추출\ncountries_additives <- foodfact %>% \n  mutate(meat_flag = stringr::str_detect(categories_en, \"meat|Meat\"),\n         vegan_flag = stringr::str_detect(labels, \"vegan|Vegan\")) %>% \n  mutate(countries_clean = clean_country(countries_en)) %>% \n  select(countries_clean, energy_100g, sugars_100g, salt_100g,\n         alcohol_100g, sodium_100g, cholesterol_100g,\n         meat_flag, vegan_flag) %>% \n  mutate(countries = stringr::str_split(countries_clean, \",\")) %>% \n  select(-countries_clean) %>% \n  tidyr::unnest(countries)\n\n\n\n\n\n\n\n\nhead(countries_additives) %>% \n  as.data.frame() %>% \n  knitr::kable(format.args = list(big.mark = ','))\n\n\nenergy_100g\nsugars_100g\nsalt_100g\nalcohol_100g\nsodium_100g\ncholesterol_100g\nmeat_flag\nvegan_flag\ncountries\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nFRANCE\n2,243\n14.29\n0.00000\nNA\n0.000\n0.018\nNA\nNA\nUS\n1,941\n17.86\n0.63500\nNA\n0.250\n0.000\nNA\nNA\nUS\n2,540\n3.57\n1.22428\nNA\n0.482\nNA\nNA\nNA\nUS\n1,552\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nUS\n1,933\n11.54\nNA\nNA\nNA\nNA\nNA\nNA\nUS\n\n국가별 식품 정보 분석하기\n우여곡절로 만든 데이터 셋을 이용해서 몇 가지 분석을 수행해 보자.\n국가별 설탕에 대한 기호\n국가별 식품에 설탕 함량이 높은 국가는 다음과 같다. 건수가 적은 국가의 경우에는 Outlier에 영향을 받는 등 대표성을 띠지 못하기 때문에 5개 이상 설탕이 함유된 식품이 있는 국가만 대상으로 삼았다.\n\n\n# 국가별 설탕 관련 지표의 생성\nsugar <- countries_additives %>% \n  group_by(countries) %>% \n  summarise(sugar_mean = mean(sugars_100g, na.rm = TRUE),\n            freq_food = n(),\n            freq_sugar = sum(!is.na(sugars_100g)),\n            sugar_percn = round(freq_sugar / freq_food * 100, 2)) %>% \n  filter(freq_sugar > 5) %>% \n  arrange(desc(sugar_mean))\n\nsugar %>% \n  knitr::kable(format.args = list(big.mark = ','))\n\n\ncountries\nsugar_mean\nfreq_food\nfreq_sugar\nsugar_percn\nMADAGASCAR\n40.916667\n114\n6\n5.26\nUNITED ARAB EMIRATES\n31.077778\n84\n9\n10.71\nARGENTINA\n29.938333\n66\n6\n9.09\nJAPAN\n27.373846\n365\n13\n3.56\nTUNISIA\n27.092414\n159\n29\n18.24\nMOROCCO\n26.799167\n208\n36\n17.31\nSAUDI ARABIA\n26.486957\n117\n23\n19.66\nMONACO\n25.933333\n14\n9\n64.29\nALGERIA\n25.338889\n188\n18\n9.57\nLITHUANIA\n25.100000\n20\n8\n40.00\nBULGARIA\n25.005714\n66\n14\n21.21\nMONTENEGRO\n24.622353\n19\n17\n89.47\nCOLOMBIA\n24.451875\n44\n16\n36.36\nFINLAND\n23.461111\n49\n18\n36.73\nCHINA\n23.037235\n199\n17\n8.54\nCZECH REPUBLIC\n22.945652\n441\n46\n10.43\nBRAZIL\n22.404615\n363\n26\n7.16\nTURKEY\n19.653158\n49\n19\n38.78\nRUSSIA\n19.523095\n1,637\n90\n5.50\nNEW ZEALAND\n19.241333\n116\n75\n64.66\nFRENCH POLYNESIA\n19.018077\n86\n26\n30.23\nNEDERLAND\n18.807143\n7\n7\n100.00\nNETHERLANDS\n18.796199\n804\n392\n48.76\nDENMARK\n18.124776\n435\n201\n46.21\nCANADA\n17.701837\n1,029\n280\n27.21\nBOSNIA AND HERZEGOVINA\n17.335714\n17\n14\n82.35\nNEW CALEDONIA\n17.329167\n108\n24\n22.22\nMARTINIQUE\n17.258139\n120\n43\n35.83\nSLOVENIA\n17.209091\n46\n22\n47.83\nPOLAND\n17.199130\n227\n69\n30.40\nUS\n17.109738\n132,765\n122,367\n92.17\nNA\n16.765593\n267\n59\n22.10\nMALI\n16.533333\n7\n6\n85.71\nSINGAPORE\n16.430000\n57\n10\n17.54\nSENEGAL\n16.363000\n52\n10\n19.23\nBELGIUM\n15.734168\n4,082\n1,633\n40.00\nSLOVAKIA\n15.720000\n34\n10\n29.41\nSWITZERLAND\n15.718281\n17,191\n11,896\n69.20\nAUSTRALIA\n15.301966\n2,309\n610\n26.42\nUK\n15.102567\n5,863\n2,192\n37.39\nIRELAND\n15.102544\n221\n68\n30.77\nROMANIA\n15.093158\n266\n95\n35.71\nGERMANY\n14.885447\n9,439\n5,777\n61.20\nSERBIA\n14.664797\n350\n296\n84.57\nLUXEMBOURG\n14.616463\n232\n82\n35.34\nPORTUGAL\n14.407170\n919\n516\n56.15\nNORWAY\n14.382727\n133\n55\n41.35\nREPUBLIC OF MACEDONIA\n14.062500\n9\n8\n88.89\nGUADELOUPE\n13.917464\n240\n138\n57.50\nITALY\n13.883119\n1,630\n545\n33.44\nANDORRA\n13.817500\n17\n8\n47.06\nFRANCE\n13.418091\n129,221\n94,819\n73.38\nSAINT PIERRE AND MIQUELON\n13.390000\n40\n22\n55.00\nSWEDEN\n13.303786\n414\n317\n76.57\nHUNGARY\n13.023129\n327\n179\n54.74\nSOUTH AFRICA\n12.914667\n42\n15\n35.71\nTAIWAN\n12.705714\n170\n28\n16.47\nSPAIN\n12.680907\n6,043\n3,128\n51.76\nGREECE\n12.430750\n106\n40\n37.74\nEUROPEAN UNION\n12.356111\n48\n18\n37.50\nAUSTRIA\n12.314074\n471\n135\n28.66\nMEXICO\n12.012467\n172\n92\n53.49\nHONG KONG\n11.557895\n157\n38\n24.20\nSCOTLAND\n11.190909\n20\n11\n55.00\nCROATIA\n11.058889\n29\n18\n62.07\nTHAILAND\n10.873667\n254\n30\n11.81\nINDIA\n10.483333\n92\n12\n13.04\nALBANIA\n10.392857\n13\n7\n53.85\nFRENCH GUIANA\n10.106609\n104\n64\n61.54\nCHILE\n9.997826\n119\n23\n19.33\nSOUTH KOREA\n8.995000\n41\n6\n14.63\nBELGIE\n8.525000\n27\n26\n96.30\nCUBA\n7.546667\n11\n6\n54.55\nCAMBODIA\n5.751111\n44\n9\n20.45\nLEBANON\n5.537500\n42\n8\n19.05\nMALTA\n3.781818\n21\n11\n52.38\nAZERBAIJAN\n0.000000\n19\n12\n63.16\n\n시각화를 통해서 국가별 식품에 설탕 함량이 높은 국가에 대한 이해를 더 쉽게 할 수 있다.\n마다가스카르, 아랍 에미레이트, 아르헨티나의 식품에는 평균적으로 많은 설탕이 함유됨을 알 수 있다. 마다가스카르는 독보적이다.\n\n\nlibrary(ggplot2)\n\n# 테마와 한글 폰트 설정\ntheme_set(theme_classic(base_family='NanumSquare'))\n\nsugar %>% \n  arrange(desc(sugar_mean)) %>% \n  head(10) %>% \n  mutate(order_seq = 10:1) %>% \n  ggplot(aes(x = reorder(countries, order_seq), y = sugar_mean ))  +\n  geom_bar(fill = \"orange\", colour = \"orange\", alpha = .5, stat = 'identity') + \n  scale_y_continuous(name = \"100g 중 식품의 평균 설탕 함유량\") +\n  xlab(\"국가\") + \n  ggtitle(\"식품의 평균 설탕 함유량이 높은 상위 10개 국가\") +\n  coord_flip()\n\n\n\n\n식품 중에 설탕 함량이 낮은 국가는 아제르바이잔, 몰타, 레바논, 캄보디아 등이다.\n\n\nsugar %>% \n  arrange(sugar_mean) %>% \n  head(10) %>% \n  knitr::kable(format.args = list(big.mark = ','))\n\n\ncountries\nsugar_mean\nfreq_food\nfreq_sugar\nsugar_percn\nAZERBAIJAN\n0.000000\n19\n12\n63.16\nMALTA\n3.781818\n21\n11\n52.38\nLEBANON\n5.537500\n42\n8\n19.05\nCAMBODIA\n5.751111\n44\n9\n20.45\nCUBA\n7.546667\n11\n6\n54.55\nBELGIE\n8.525000\n27\n26\n96.30\nSOUTH KOREA\n8.995000\n41\n6\n14.63\nCHILE\n9.997826\n119\n23\n19.33\nFRENCH GUIANA\n10.106609\n104\n64\n61.54\nALBANIA\n10.392857\n13\n7\n53.85\n\n\n\nsugar %>% \n  arrange(sugar_mean) %>% \n  head(10) %>% \n  mutate(order_seq = 10:1) %>% \n  ggplot(aes(x = reorder(countries, order_seq), y = sugar_mean ))  +\n  geom_bar(fill = \"blue\", colour = \"blue\", alpha = .5, stat = 'identity') + \n  scale_y_continuous(name = \"100g 중 식품의 평균 설탕 함유량\") +\n  xlab(\"국가\") +\n  ggtitle(\"식품의 평균 설탕 함유량이 낮은 상위 10개 국가\") +  \n  coord_flip()\n\n\n\n\n식품 중에 설탕이 포함된 비율이 높은 국가는 네덜란드, 벨기에, 미국 등이다. 네덜란드는 모든 식품에 설탕이 함유되어 있음을 알 수 있다.\n\n\nsugar %>% \n  arrange(desc(sugar_percn)) %>% \n  head(10) %>% \n  knitr::kable(format.args = list(big.mark = ','))\n\n\ncountries\nsugar_mean\nfreq_food\nfreq_sugar\nsugar_percn\nNEDERLAND\n18.80714\n7\n7\n100.00\nBELGIE\n8.52500\n27\n26\n96.30\nUS\n17.10974\n132,765\n122,367\n92.17\nMONTENEGRO\n24.62235\n19\n17\n89.47\nREPUBLIC OF MACEDONIA\n14.06250\n9\n8\n88.89\nMALI\n16.53333\n7\n6\n85.71\nSERBIA\n14.66480\n350\n296\n84.57\nBOSNIA AND HERZEGOVINA\n17.33571\n17\n14\n82.35\nSWEDEN\n13.30379\n414\n317\n76.57\nFRANCE\n13.41809\n129,221\n94,819\n73.38\n\n\n\nsugar %>% \n  arrange(desc(sugar_percn)) %>% \n  head(10) %>% \n  mutate(order_seq = 10:1) %>% \n  ggplot(aes(x = reorder(countries, order_seq), y = sugar_percn ))  +\n  geom_bar(fill = \"orange\", colour = \"orange\", alpha = .5, stat = 'identity') + \n  scale_y_continuous(name = \"식품 중 설탕 포함 비율\") +\n  xlab(\"국가\") +\n  ggtitle(\"식품 중에 설탕의 포함 비율이 높은 상위 10개 국가\")  +\n  coord_flip()\n\n\n\n\n식품 중에 설탕이 포함된 비율이 낮은 국가는 일본, 마다가스카르, 러시아 등이다. 특히 마다가스카르는 앞에서 식품 중에 설탕의 함량이 높은 국가였다. 식품에 설탕이 들어가는 음식은 적지만, 설탕이 들어가는 식품의 설탕 함유량은 높은 국가다.\n\n\nsugar %>% \n  arrange(sugar_percn) %>% \n  head(10) %>% \n  knitr::kable(format.args = list(big.mark = ','))\n\n\ncountries\nsugar_mean\nfreq_food\nfreq_sugar\nsugar_percn\nJAPAN\n27.37385\n365\n13\n3.56\nMADAGASCAR\n40.91667\n114\n6\n5.26\nRUSSIA\n19.52310\n1,637\n90\n5.50\nBRAZIL\n22.40462\n363\n26\n7.16\nCHINA\n23.03724\n199\n17\n8.54\nARGENTINA\n29.93833\n66\n6\n9.09\nALGERIA\n25.33889\n188\n18\n9.57\nCZECH REPUBLIC\n22.94565\n441\n46\n10.43\nUNITED ARAB EMIRATES\n31.07778\n84\n9\n10.71\nTHAILAND\n10.87367\n254\n30\n11.81\n\n\n\nsugar %>% \n  arrange(desc(sugar_percn)) %>% \n  tail(10) %>% \n  mutate(order_seq = 1:10) %>% \n  ggplot(aes(x = reorder(countries, order_seq), y = sugar_percn ))  +\n  geom_bar(fill = \"blue\", colour = \"blue\", alpha = .5, stat = 'identity') + \n  scale_y_continuous(name = \"식품 중 설탕 포함 비율\") +\n  xlab(\"국가\") + \n  ggtitle(\"식품 중에 설탕의 포함 비율이 낮은 상위 10개 국가\") +\n  coord_flip()\n\n\n\n\n나머지 관심 지표의 확인\n설탕의 함량에 대한 변수로 국가별 몇 가지 분석을 수행하였다. 집계한 데이터에는 설탕의 함량 이외에도 소금, 알콜, 나트륨, 콜레스테롤의 함량과 고기의 포함 여부와 채식식품 여부의 변수가 있으니 유사 분석을 수행할 수 있을 것이다.\n\n\nnames(countries_additives)\n\n\n[1] \"energy_100g\"      \"sugars_100g\"      \"salt_100g\"       \n[4] \"alcohol_100g\"     \"sodium_100g\"      \"cholesterol_100g\"\n[7] \"meat_flag\"        \"vegan_flag\"       \"countries\"       \n\n\n\n\n",
    "preview": "posts/2019-12-28-multi_derived/2019-12-28-multi_derived_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-10-11T09:14:08+09:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 960
  },
  {
    "path": "posts/2019-12-26-manipulation_data/",
    "title": "데이터 조작하기",
    "description": "`외부 데이터`를 읽어들여 `데이터 프레임`을 만들고, 이를 조작하여 데이터 분석을 위한 데이터 셋을 생성하는 것은 데이터 분석 과정에서 대단히 중요한 과정이다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2019-12-26",
    "categories": [
      "Tidyverse"
    ],
    "contents": "\n\nContents\n들어가기\ndplyr\nparallel processing\n데이터 조작에 대한 슬라이드의 공유\n차례\n\n들어가기\n\n외부 데이터를 읽어들여 데이터 프레임을 만들고, 이를 조작하여 데이터 분석을 위한 데이터 셋을 생성하는 것은 데이터 분석 과정에서 대단히 중요한 과정이다. R에서 데이터를 조작하는 기술은 데이터 조작 함수를 구사하는 능력에 비례한다. base, stats 패키지에 포함된 전통적인 R(S-PLUS) 데이터 조작 함수의 대체제는 sqldf 패키지를 거쳐, dplyr 패키지로 꽃을 피웠다 할 수 있다.\n\ndplyr\n본 포스트는 데이터 조작의 전반적인 것을 다루는데 특히 Data Wrangling을 위한 dplyr 패키지 소객에 방점을 둔다.\nparallel processing\n또한 대용량의 데이터를 조작하는 방법으로서 멀티 코어를 사용하는 방법에 대해서도 소개한다.\n데이터 조작에 대한 슬라이드의 공유\n다음에 링크를 걸어 둔 슬라이드 파일은 2017년도에 모 미트업에서 발표한 자료를 현행화 시킨 pdf 파일이다.\nManipulation_Data.pdf\n슬라이드 발췌 이미지를 예시해 본다.\n\n\n\nFigure 1: 슬라이드 발췌 이미지\n\n\n\n차례\n이 문서는 다음과 같은 아젠다를 이야기 한다.\nGeneral Information\nRead Large Data\nData Wrangling\nSQL Based Data Wrangling 5. dplyr Data Wrangling\nParallel Processing\n\n\n\n",
    "preview": "posts/2019-12-26-manipulation_data/img/HA.png",
    "last_modified": "2021-10-23T09:50:57+09:00",
    "input_file": {},
    "preview_width": 1404,
    "preview_height": 1020
  },
  {
    "path": "posts/2019-12-19-func_operator/",
    "title": "모든 연산자는 함수다",
    "description": "UNIX나 Linux의 쉘 스크립트에서의 파이프의 기능은 필수 불가결하다. 마치 파이프라인을 엮어서 물을 원하는 방향으로 흘려보내는 듯한 자유로운 파이프의 구사는 스크립트의 성능을 배가시켜 준다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2019-12-19",
    "categories": [
      "Tidyverse"
    ],
    "contents": "\n\nContents\n들어가기\n연산자란?\n단항 연산자\n다항 연산자\n\n함수란?\n함수의 구조\n함수의 정의\n함수의 호출\n\n모든 연산자는 함수다\n사용자 정의 연산자 만들기\n연산자의 함수형 호출\n산술연산\n조건 추출 연산\n\n연산자의 함수형 호출 응용\n파이프에서의 연산자의 함수형 호출 응용\n파이프의 구현\n\n\n\n들어가기\n\nUNIX나 Linux의 쉘 스크립트에서의 파이프의 기능은 필수 불가결하다. 마치 파이프라인을 엮어서 물을 원하는 방향으로 흘려보내는 듯한 자유로운 파이프의 구사는 스크립트의 성능을 배가시켜 준다. tidyverse 패키지군을 사용할 때에도 파이프 기능을 이용해서 표현식을 엮어 주는 것이 효율적이다. 그리고 파이프 연산을 기술할 때, “모든 연산자는 함수다”라는 특징을 이용하면 유용하다.\n이 토픽은 연산자를 함수처럼 사용할 수 있는 방법을 제시한다. 아주 가끔 응용할 수 있는 유용한 팁이다.\n\n연산자란?\n연산자(operator)는 논리식이나 산술연산 등 다양한 연산처리를 수행하며 표현식(expression)에서 미리 정의된 기호로 표현된다.\n통상적으로 연산자는 하나 이상의 피연산자(operend)와 결합하여 연산을 수행한다. 변수(variable)나 상수(constant)가 피연산자로 사용되며, 피연산자의 수에 따라 분류도 가능하다.\n단항 연산자\n하나의 피연산자를 갖는 연산자를 단항 연산자라 한다. 논리부정(indicates logical negation, NOT)을 수행하는 연산자가 대표적인 단항 연산자다.\n\n\n# 상수를 피연산자로 갖는 단항 연산자\n!TRUE\n\n\n[1] FALSE\n\n# 변수를 피연산자로 갖는 단항 연산자\nflag <- TRUE\n!flag\n\n\n[1] FALSE\n\n다항 연산자\n두개 이상의 피연산자를 갖는 연산자를 다항 연산자라 한다. 두개의 피연산자를 갖는 이항 연산자가 대부분이다. C언어의 경우에는 3개의 피연산자를 갖는 연산자가 존재하지만, R에서는 다항 연산자를 이항 연산자로 갈음해도 무방하다.\n우리가 즐겨 사용하는 산술 연산자(arithmetic operators)도 이항 연산자다.\n\n\n# 상수를 피연산자로 갖는 이항 연산자 - 산술 연산자\npi + 5\n\n\n[1] 8.141593\n\n# 변수를 피연산자로 갖는 이항 연산자 - 관계 연산자 (relational operators)\nidx <- 2\nidx > 0\n\n\n[1] TRUE\n\n앞의 예제에서 <- 도 이항 연산자로서의 할당 연산자(assignment operators)다. 이 연산자는 우측의 피연사의 값을 좌측의 피연산자이 이름에 할당한다.\n함수란?\n함수(function)는 반복적으로 사용되는 유용한 루틴이나, 큰 프로그램 로직을 부분적으로 쪼개서 구조화 시킨 프로그램 코드의 집합이다. 대부분의 프로그래밍 언어에서 지원하는 기능이다.\n붕어빵을 만드는 반복적인 작업을 생각해 보자. 우리는 붕어빵을 만들어 주는 틀에 재료를 넣어 동일한 크기의 붕어빵을 쉽게 찍어내는 광경을 떠올릴 것이다. 붕어빵을 만들는 작업에 사용하는 틀이 함수와 유사하다.\n함수의 구조\n함수는 일반적으로 그림(함수의 구조)처럼 입력값을 처리하여 출력값 f(x)를 반환하는 구조를 갖는다. R의 함수에서는 입력값을 인수 목록(argument list)라 하고 값을 처리하는 로직 부분을 함수의 몸체(body)라 한다. 즉, R의 함수는 인수 목록과 함수의 몸체로 구성된다.\n앞서 언급한 붕어빵을 만드는 틀을 R 함수에 비유한다면 인수 리스트로 반죽과 단팥을 붓고, 넣어서 빵을 굽는 몸체를 통해서 붕어빵을 반환(return)하는 것이다.\n\n\n\nFigure 1: 함수의 구조\n\n\n\n함수의 정의\n함수는 다음과 같이 정의한다.\n\nfunction( arglist ) expr\n\n예약어(keyword) function의 괄호 안에 인수 목록인 arglist을 기술한다. 그리고 함수 몸체는 표현식(expression)을 의미하는 expr 부분에 기술한다.\n그리고 함수를 정의하는 목적이, 정의된 함수를 필요할 때마다 사용하기 위함이므로 정의된 함수를 저장해야 한다. 그러기 위해서는 다음처럼 할당 연산자를 통해서 함수의 기능을 유추하기 쉬운 이름에 저장해야 한다.\n\nfunction name <- function( arglist ) expr\n\n예제로 간단하게 홀수와 짝수를 구별하는 함수를 정의한 후 class_odd_even라는 이름에 저장한다.\n\n\nclass_odd_even <- function(x) {\n  ifelse(x %% 2 == 0, \"ODD\", \"EVEN\")\n}\n\n\n\n함수의 호출\n함수는 다음과 같이 호출한다.\n\n함수 이름(arglist)\n\nclass_odd_even라는 이름의 함수를 호출하는 몇 가지 사례다.\n일반적인 함수의 이름만으로 호출하는 방법\n우리에게 아주 익숙한 함수의 호출 방법이다.\n\n\nclass_odd_even(3)\n\n\n[1] \"EVEN\"\n\n인용문자(quotation charactor)로 표현한 함수의 이름으로 호출하는 방법\n작은 따옴표 : single quotate(’)\n큰 따옴표 : double quotate(\")\n역인용문자 : backquote(`)\n\n\n\n# single quotate(')\n'class_odd_even'(1:5)\n\n\n[1] \"EVEN\" \"ODD\"  \"EVEN\" \"ODD\"  \"EVEN\"\n\n\n\n# double quotate(\")\n\"class_odd_even\"(c(1, 3, 2, 5, 4, 7, 8))\n\n\n[1] \"EVEN\" \"EVEN\" \"ODD\"  \"EVEN\" \"ODD\"  \"EVEN\" \"ODD\" \n\n\n\n# backquote(`)\n`class_odd_even`(8)\n\n\n[1] \"ODD\"\n\n모든 연산자는 함수다\nR에서 연산자는 함수라고 할 수 있다. 다음과 같은 2개의 특징을 가지고 있기 때문이다.\n함수의 정의 방법으로 사용자 정의 연산자를 생성한다.\n연산자를 함수의 호츨 방법으로 사용할 수도 있다.\n사용자 정의 연산자 만들기\n사용자 정의 이항 연산자는 다음과 같은 표현식으로 만들 수 있다.\n\n%operator name% <- function( arglist ) expr\n\n사용자 정의 연산자는 함수를 정의하는 방법과 동일하게 생성한다. 차이점은 이항 연산자이기 때문에 인수 목록인 arglist이 두 개의 인수이어야 하고, 연산자의 이름은 %로 둘러 쌓여야 한다는 점이다.\n행렬의 곱(matrix multiplication)을 구하는 연산자인 %*%와 행렬을 포함한 배열(array)의 외적(outer product of arrays)을 구하는 연산자가 %o%이므로 사용자 정의 연산자의 이름은 %*%와 %o%을 피해야 한다.\n\n\na <- matrix(1:4, ncol = 2)\na\n\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nb <- matrix(c(2, 3, 5, 7), ncol = 2)\nb\n\n\n     [,1] [,2]\n[1,]    2    5\n[2,]    3    7\n\n\n\n# 행렬의 곱셈\na * b\n\n\n     [,1] [,2]\n[1,]    2   15\n[2,]    6   28\n\n\n\n# 행렬의 곱 - 행렬의 곱셈과는 다르다.\na %*% b\n\n\n     [,1] [,2]\n[1,]   11   26\n[2,]   16   38\n\n\n\nx <- 1:3\ny <- 3:1\n\n# 벡터의 외적\nx %o% x\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    2    4    6\n[3,]    3    6    9\n\nx %o% y\n\n\n     [,1] [,2] [,3]\n[1,]    3    2    1\n[2,]    6    4    2\n[3,]    9    6    3\n\n다음의 사용자 연산자 %c%는 좌측의 피연산자와 우측의 피연산의 크기를 비교하여,\n좌측의 피연산자의 크기가 크면 1을\n우측의 피연산자의 크기가 크면 -1을\n좌측의 피연산자와 우측의 피연산자의 크기가 같으면 0을 반환한다.\n\n\n\"%c%\" <- function(x, y) {\n  ifelse(x > y, 1, ifelse(x < y, -1, 0))\n}\n\n34 %c% 23\n\n\n[1] 1\n\n23 + 11 %c% 35\n\n\n[1] 22\n\n23 * 11 %c% 35\n\n\n[1] -23\n\n34 %c% 34\n\n\n[1] 0\n\n\"%c%\"(34, 23)\n\n\n[1] 1\n\n연산 결과를 보면 사용자 정의 연산자인 %c%의 연산자 우선 순위가 덧셈과 곱셈보다 앞선다. 그러므로 덧셈과 곱셈의 연산을 먼저 처리하려면 괄호를 사용해서 덧셈과 곱셈의 연산을 우선적으로 처리해야 한다.\n\n\n(23 + 11) %c% 35\n\n\n[1] -1\n\n(23 * 11) %c% 35\n\n\n[1] 1\n\n연산자의 함수형 호출\n인용문자(quotation charactor)로 표현한 함수의 이름으로 호출하는 방법을 준용하여 연산자를 호출할 수 있다.\n산술연산\n\n\n# 3 + 6\n\"+\"(3, 6)\n\n\n[1] 9\n\n# 거듭 제곱 연산자\n# 3 ^ 2\n\"^\"(3, 2)\n\n\n[1] 9\n\n# 거듭 제곱 연산자\n# 3 ** 2\n3 ** 2\n\n\n[1] 9\n\n# 그러나 \"**\"(3, 2)는 지원하지 않는다.\n# \"**\"(3, 2)\n\n\n\n조건 추출 연산\n\n\n# letters[1:5]\n# 벡터의 특정 원소를 추출하는 [ 연산자를 함수처럼 호출\n\"[\"(letters, 1:5)\n\n\n[1] \"a\" \"b\" \"c\" \"d\" \"e\"\n\n\n\n# iris[, 4]\n# 4번째 변수를 추출하는 [ 연산자를 함수처럼 호출\nmax(\"[\"(iris, , 4))\n\n\n[1] 2.5\n\n\n\n# iris[2, ]\n# 2번째 관측치를 추출하는 [ 연산자를 함수처럼 호출\n\"[\"(iris, 2, )\n\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n2          4.9           3          1.4         0.2  setosa\n\n\n\n# iris$Species\n# 변수를 추출하는 $ 연산자를 함수처럼 호출\ntable(\"$\"(iris, \"Species\"))\n\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n연산자의 함수형 호출 응용\n파이프에서의 연산자의 함수형 호출 응용\n파이프의 구현\ntidyverse 프로젝트의 magrittr 패키지는 R에서 파이프를 지원한다. 이 패키지의 기본 파이프 연산자인 %>%는 좌측 표현식의 결과를 우측 함수의 첫번째 인수값으로 사용한다.\n다음은 dplyr 패키지로 파이프를 구현한 예제와 연산자를 함수처럼 사용하는 방법에 파이프를 응용한 예제다. dplyr 패키지가 magrittr 패키지를 사용하기 때문에 dplyr 패키지만 로드하면 굳이 magrittr 패키지를 로드할 필요는 없다.\ndplyr 패키지의 사례가 깔끔하고 직관적이다. 개인적으로도 추천하는 방법이다. 하지만 파이프에서의 연산자의 함수형 호출의 사례를 위해서 몇 개의 예제를 만들어 보았다.\n\n\nlibrary(dplyr)\n\n# 6개의 실린더를 가지고 있는 자동차의 개수 구하기\n# dplyr 패키지의 이용\nmtcars %>% \n  filter(cyl == 6) %>% \n  tally() %>% \n  pull()\n\n\n[1] 7\n\n\n\n# subset 연산자를 사용하여, 원소의 이름이 \"6\"인 것을 취한다.\n# .은 이전 표현식의 결과를 의미한다.\nmtcars$cyl %>% \n  table %>% \n  .[\"6\"] %>% \n  as.integer()\n\n\n[1] 7\n\n\n\n# subset 연산자인 [를 함수처럼 호출\nmtcars$cyl %>% \n  table %>% \n  \"[\"(\"6\") %>% \n  as.integer()\n\n\n[1] 7\n\n파이프를 이용하여 R 스크립트를 작성할 때, 다음의 방법처럼 작업 단위별로 개행을 하여 기술하는 것을 권장한다. 그래야 마치 물이 흐르는 것처럼 행간을 이동하면서 로직의 흐름을 쉽게 파악할 수 있게 된다. 한 줄에 여러 작업 단위를 기술하면 코드의 길이를 짧게 할 수도 있지만 코드를 해석하기 어려울 수도 있다.\n이 예제에서는 두번 연산자의 함수형 호출을 시도한다.\n페이지의 개수를 구하기 위해서 전체 컨텐츠의 개수를 한 페이지에 게시할 수 있는 15로 나눈 몫을 얻을 때, %/% 연산자를 함수의 방식으로 호출한다. 그리고 마지막으로 계산된 페이지의 수에 1을 더하는 + 연산자를 함수의 방식으로 호출한다.\n\n\nlibrary(magrittr)\nlibrary(rvest)\nlibrary(stringr)\n\n# Target URL - https://en.mogi.vn/buy-property\n\n# 컨텐츠의 개수를 유추할 수 있는 텍스트 추출 \nxml2::read_html('https://en.mogi.vn/buy-property') %>%\n  html_nodes('div.property-result-summary') %>%\n  html_text\n\n\n[1] \"Result 1 - 15 in 31,068\"\n\n# Target URL에서 페이지 개수 구하기\nxml2::read_html('https://en.mogi.vn/buy-property') %>%\n  html_nodes('div.property-result-summary') %>%\n  html_text %>%\n  strsplit(\"in \") %>%\n  sapply(\"[\", 2) %>%\n  str_remove(\",\") %>%\n  as.integer() %>%\n  \"%/%\"(15) %>% \n  \"+\"(1)\n\n\n[1] 2072\n\n파이프로 연결되는 표현식에는 독립적인 하나의 로직을 구사한다. 상기 스크립트에서는 전체 컨텐츠의 개수를 한 페이지에 게시할 수 있는 15로 나누는 것을 하나의 독립된 로직으로 본 것이다.\n마지막에 1을 더한 것도 독립된 로직이다. 만약에 15로 나눈 값의 나머지가 있다면 하나의 페이지를 더 필요로 하는 것을 의미한다. 만약에 나머지가 0이라면 1개의 페이지가 덤으로 구해질 수는 있다는 점에 유의해야 하는 로직이다.\n물론 아래의 방법을 사용할 수도 있으나, 사용자의 성향에 따라 선택할 여지가 있어 보인다.\n\n\n# Target URL에서 페이지 개수 구하기\nxml2::read_html('https://en.mogi.vn/buy-property') %>%\n  html_nodes('div.property-result-summary') %>%\n  html_text %>%\n  strsplit(\"in \") %>%\n  sapply(\"[\", 2) %>%\n  str_remove(\",\") %>%\n  as.integer() %/% 15 + 1\n\n\n[1] 2072\n\n\n\n\n",
    "preview": "posts/2019-12-19-func_operator/img/function.png",
    "last_modified": "2021-10-11T08:40:52+09:00",
    "input_file": {},
    "preview_width": 1610,
    "preview_height": 598
  },
  {
    "path": "posts/2019-04-13-tridokus/",
    "title": "tridokus",
    "description": "스도쿠에서 각 셀에 할당된 숫자에 색상을 적용하면 제법 보기 좋은 그림이 그려진다. 그리고 각 셀에 중복으로 숫자를 할당하면 더 멋진 그림을 그릴 수 있다. 각 셀에 세개의 숫자를 할당한 `트라이도쿠스(tridokus)`를 중심으로 스도쿠의 그림에 빠져 보자.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2019-04-13",
    "categories": [
      "Gallery"
    ],
    "contents": "\n\nContents\ntridokus\nn-sudokus\ncolourlovers Palettes로 그린 3-sudokus\nColorBrewer Palettes로 그린 3-sudokus\ngrDevices Palettes로 그린 3-sudokus\n복합 n-sudokus\n\n참고 리소스\n\ntridokus\n스도쿠(sudokus)라는 보드 게임이 있다. 9 \\(\\times\\) 9 칸에서 진행되는 숫자 퍼즐 게임이다.\n스도쿠를 구성하는 셀(cell)은 총 81셀, 3 \\(\\times\\) 3셀 9개로 세분화되며, 지켜야 할 룰은 다음과 같다.\n각각의 가로줄(row)과 세로줄(column)에 1~9가 중복 없이 하나씩 들어간다.\n3 \\(\\times\\) 3칸(box) 안에는 1~9가 중복 없이 하나씩 들어간다.\n스도쿠에서 각 셀에 할당된 숫자에 색상을 적용하면 제법 보기 좋은 그림이 그려진다. 그리고 각 셀에 중복으로 숫자를 할당하면 더 멋진 그림을 그릴 수 있다. 각 셀에 세개의 숫자를 할당한 트라이도쿠스(tridokus)를 중심으로 스도쿠의 그림에 빠져 보자.\nR을 이용해서 몇 개의 작품을 만들어 본다.\nn-sudokus\n하나의 셀 안에 3, 5, 7, 9개의 색상을 할당한 3-sudokus, 5-sudokus, 7-sudokus, 9-sudokus의 플롯을 차례로 그려 본다.\n\n\n\nFigure 1: 3-sudokus\n\n\n\n\n\n\nFigure 2: 5-sudokus\n\n\n\n\n\n\nFigure 3: 7-sudokus\n\n\n\n\n\n\nFigure 4: 9-sudokus\n\n\n\ncolourlovers Palettes로 그린 3-sudokus\ncolourlovers 패키지의 팔레트를 이용해서 몇 개의 플롯을 그려본다. 113451, 292482, 482774, 694737, 953498번 팔레트의 색상으로 출력해 본다. 앞서 그린 n-sudokus들은 1930번 팔레트 색상이다.\n\n\n\nFigure 5: 113451 팔레트\n\n\n\n\n\n\nFigure 6: 292482 팔레트\n\n\n\n\n\n\nFigure 7: 482774 팔레트\n\n\n\n\n\n\nFigure 8: 694737 팔레트\n\n\n\n\n\n\nFigure 9: 953498 팔레트\n\n\n\nColorBrewer Palettes로 그린 3-sudokus\nColorBrewer 패키지의 팔레트를 이용해서 몇 개의 플롯을 그려본다. Accent, Blues, BrBG 팔레트의 색상으로 출력해 본다. 앞서 그린 n-sudokus들은 1930번 팔레트 색상이다.\n\n\n\nFigure 10: Accent 팔레트\n\n\n\n\n\n\nFigure 11: Blues 팔레트\n\n\n\n\n\n\nFigure 12: BrBG 팔레트\n\n\n\ngrDevices Palettes로 그린 3-sudokus\ngrDevices 패키지에서 지원하는 대표적인 팔레트에 rainbow, heat.colors, terrain.colors, topo.colors, cm.colors 팔레트가 있다. 이 중에서 cm.colors 팔레트와 gray 팔레트로 n-sudokus를 그려 본다.\n\n\n\nFigure 13: cm.colors 팔레트\n\n\n\n\n\n\nFigure 14: gray 팔레트\n\n\n\n복합 n-sudokus\n1~9-sudokus을 랜덤하게 그려서 3 \\(\\times\\) 3 n-sudokus로 구성된 그림을 그려 본다. colourlovers의 292482번 팔레트와 몇 가지의 팔레트를 랜덤하게 섞어 그려 보았다.\n\n\n\nFigure 15: 292482 팔레트로 그린 복합 n-sudokus\n\n\n\n\n\n\nFigure 16: 몇 개의 팔레트로 그린 복합 n-sudokus\n\n\n\n참고 리소스\n아티클 : https://fronkonstin.com/2018/06/01/coloring-sudokus/\n소스 : https://github.com/aschinchon/sudokus-colored\n\n\n\n\n",
    "preview": "posts/2019-04-13-tridokus/img/tridoku_1930_3.png",
    "last_modified": "2021-10-11T00:35:51+09:00",
    "input_file": {},
    "preview_width": 2400,
    "preview_height": 2400
  },
  {
    "path": "posts/2019-04-09-neural_style_transfer/",
    "title": "neural style transfer",
    "description": "`Neural Network` 알고리즘으로 사진이나 그림을 변형시키는 기법에 대한 `A Neural Algorithm of Artistic Style`이라는 논문이 히트친 적이 있다. 고호의 화풍으로 사진의 변형하여 예술적인 이미지를 만들어내는 것이다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2019-04-09",
    "categories": [
      "Gallery"
    ],
    "contents": "\n\nContents\nneural style transfer\n고흐의 기법을 이용한 시각화\n피카소의 기법을 이용한 시각화\n뭉크의 기법을 이용한 시각화\n가쓰시카 호쿠사이의 기법을 이용한 시각화\n\n참고 리소스\n\n\n\n\nneural style transfer\nNeural Network 알고리즘으로 사진이나 그림을 변형시키는 기법에 대한 A Neural Algorithm of Artistic Style이라는 논문이 히트친 적이 있다. 고호의 화풍으로 사진의 변형하여 예술적인 이미지를 만들어내는 것이다.\nR을 이용해서 몇 개의 작품을 만들어 본다.\n고흐의 기법을 이용한 시각화\n나의 집 베란다에서는 북한산을 조망할 수 있다. 언제인지는 모르지만 사진으로 남긴 북한산의 모습을 고흐의 화풍으로 변형해 보았다.\n\n\n\n본가 화단에 심어 놓은 붉은색의 튤립을 고흐는 어떻게 해석하였을까?\n\n\n\n피카소의 기법을 이용한 시각화\n피카소의 그림은 언제나 난해하다. 아파트의 모습이 희미하게 나타난다.\n\n\n\n피카소의 튤립도 난해하다. 몇 군에에서 몇 개의 꽃망울을 발견할 수 있다.\n\n\n\n뭉크의 기법을 이용한 시각화\n뭉크의 화풍으로는 북한산에 노을이 진 것 같다. 뭉크의 절규를 모티브로 하였다.\n\n\n\n뭉크의 튤립은 제법 붉은 기운이 많이 보인다.\n\n\n\n가쓰시카 호쿠사이의 기법을 이용한 시각화\n가쓰시카 호쿠사이의 화풍으로는 북한산에 노을이 진 것 같다. 가쓰시카 호쿠사이의 가나가와 해변의 높은 파도 아래를 모티브로 하였다.\n\n\n\n가쓰시카 호쿠사이의 튤립 사이에 파도의 포말이 느껴진다.\n\n\n\n참고 리소스\n소스 : https://github.com/rstudio/keras/blob/master/vignettes/examples/neural_style_transfer.R\n\n\n\n",
    "preview": "posts/2019-04-09-neural_style_transfer/img/neural_style_transfer.png",
    "last_modified": "2021-10-11T00:32:28+09:00",
    "input_file": {},
    "preview_width": 2000,
    "preview_height": 700
  },
  {
    "path": "posts/2019-04-04-mandelbrot/",
    "title": "망델브로 세트",
    "description": "고교 시절에 일본의 과학 잡지인 `Newton`을 번역해서 만든 `월간과학`이라는 잡지를 열독한 적이 있다. 그 잡지에서 본  망델브로 세트라는 Fractal 이미지에 매료되었었다. 마치 미술 작품처럼 아름다운 색채의 기하학적 무늬는 아직도 잊지 못하고 있다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2019-04-04",
    "categories": [
      "Gallery"
    ],
    "contents": "\n\nContents\n망델브로 세트\nRe(c) = -0.833, Im(c) = 0.2055\nRe(c) = -0.833222, Im(c) = 0.20690\nRe(c) = -0.7436499, Im(c) = 0.131882\nRe(c) = -0.550, Im(c) = 0.480\nRe(c) = -0.675, Im(c) = 0.032\nRe(c) = 0.339, Im(c) = 0.573\nRe(c) = -0.743030, Im(c) = 0.126433\nRe(c) = -0.3615, Im(c) = -0.634125\n참고 리소스\n\n\n\n\n\n망델브로 세트\n고교 시절에 일본의 과학 잡지인 Newton을 번역해서 만든 월간과학이라는 잡지를 열독한 적이 있다. 그 잡지에서 본 망델브로 세트라는 Fractal 이미지에 매료되었었다. 마치 미술 작품처럼 아름다운 색채의 기하학적 무늬는 아직도 잊지 못하고 있다.\nR을 이용해서 몇 개의 작품을 만들어 본다.\nRe(c) = -0.833, Im(c) = 0.2055\nCoordinates of the center: Re(c) = -0.833, Im(c) = 0.2055\nColor palette: Vaccine\n\n\n\nRe(c) = -0.833222, Im(c) = 0.20690\nCoordinates of the center: Re(c) = -0.833222, Im(c) = 0.206907\nColor palette: Spectral\n\n\n\nRe(c) = -0.7436499, Im(c) = 0.131882\nCoordinates of the center: Re(c) = -0.7436499, Im(c) = 0.131882\nColor palette: Heat\n\n\n\nRe(c) = -0.550, Im(c) = 0.480\nCoordinates of the center: Re(c) = -0.550, Im(c) = 0.480\nColor palette: Lava\n\n\n\nRe(c) = -0.675, Im(c) = 0.032\nCoordinates of the center: Re(c) = -0.675, Im(c) = 0.032\nColor palette: Greyscale\n\n\n\nRe(c) = 0.339, Im(c) = 0.573\nCoordinates of the center: Re(c) = 0.339, Im(c) = 0.573\nColor palette: Ice\n\n\n\nRe(c) = -0.743030, Im(c) = 0.126433\nCoordinates of the center: Re(c) = -0.743030, Im(c) = 0.126433\nColor palette: Spectral\n\n\n\nRe(c) = -0.3615, Im(c) = -0.634125\nCoordinates of the center: Re(c) = -0.3615, Im(c) = -0.634125\nColor palette: BlueOrange\n\n\n\n참고 리소스\n응용 소스 코드 : https://github.com/blmoore/mandelbrot\n\n\n\n",
    "preview": "posts/2019-04-04-mandelbrot/img/mandelbrot.png",
    "last_modified": "2021-10-11T00:25:15+09:00",
    "input_file": {},
    "preview_width": 2000,
    "preview_height": 700
  },
  {
    "path": "posts/2018-09-23-hongchun/",
    "title": "고향집 풍경, 텃밭과 담장 밑의 숨소리",
    "description": "강원도 홍천의 본가는 변두리 시골 마을에 있다. 아무 생각없이 아침 마실길에 카메라에 식물들을 담아 보았다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2018-09-23",
    "categories": [
      "Gallery"
    ],
    "contents": "\n\nContents\n고향집의 식물들\n수줍은 식물의 모습들\n무궁화\n김장배추\n들깨\n대파\n가지\n호박\n더덕\n대추\n채송화\n나팔꽃\n낑깡\n그외의 친구들\n\n\n\n\n\n고향집의 식물들\n강원도 홍천의 본가는 변두리 시골 마을에 있다. 아무 생각없이 아침 마실길에 카메라에 식물들을 담아 보았다.\n수줍은 식물의 모습들\n몇 개의 사진을 공유해 본다.\n무궁화\n흰색의 무궁화가 나랏꽃이라 한다. 강원도 홍천이 무궁화 고장이라지만, 내 어릴적에도 동네에는 몇 그루 없었던 기억이 난다.\n\n\n\n김장배추\n아직은 축위가 오지 않은 초가을이라 포기가 작고, 묶어주지 않아 포기들이 넓게 벌어져 있다.\n\n\n\n들깨\n열매는 열었으나, 익지 않은 들깨. 깻잎 향이 콧등을 스치는 것 같다.\n\n\n\n대파\n김장을 준비하는 텃밭에는 대파 한 고랑이 든실하게 자라 있었다.\n\n\n\n가지\n어릴적에는 가지는 심지는 않았다. 딱히 가지로 만든 반찬을 먹고 자란 기억도 없다. 앞마당에 연결된 텃밭에 가지가 몇 개 열려 있었다.\n\n\n\n호박\n철이 지나서 호박은 열리지 않았거, 때늦은 꽃망울이 귀여워서 앵글에 담아 보았다.\n\n\n\n더덕\n호롱같은 더덕꽃. 더덕 덩굴 말아올린 기둥 가까이서 꽃망울을 터뜨렸다.\n\n\n\n대추\n어릴적 집 울타리 안에 맛난 과실나무가 별로 없었다. 그래도 언제나 가을에 열매를 안겨주었던 대추나무는 친근한 과실수였다. 해가 지나면서 죽고, 나고 또 다른 나무 그루로 반가히 맞아주는 대추나무에 열매가 제법 열려 있었다.\n\n\n\n채송화\n역시 철지난 채송화가 수줍제 꽃망을을 터뜨리고 있었다.\n\n\n\n나팔꽃\n보라색 나팔꽃, 당장을 기어올라 꽃망울을 수줍게 내밀고 있다.\n\n\n\n낑깡\n십여년 전에 어머니가 분양받아온 낑깡이 홍천에서도 자라고 있었다. 방에 들여 놓은 화분을 마당에 내놓았는데, 지나가는 길에 앵글에 잡히고 말았다.\n\n\n\n그외의 친구들\n이리 저리 거닐다 몇몇 친구를 앵글에 담았다.\n철지난 다알리아 꽃이 피었다.\n\n\n\n이름 모를 하얀 꽃을 담고,\n\n\n\n이름 모를 관엽식물도 눈에 들어 온다.\n\n\n\n\n\n\n",
    "preview": "posts/2018-09-23-hongchun/img/hongchun.png",
    "last_modified": "2021-10-11T00:10:16+09:00",
    "input_file": {},
    "preview_width": 2000,
    "preview_height": 700
  },
  {
    "path": "posts/2018-07-22-dlookr/",
    "title": "dlookr 0.3.2 - DBMS 테이블의 품질진단",
    "description": "이번에 개선된 0.3.2 버전에서는 DBMS의 테이블로 존재하는 데이터의 진단과 EDA를 지원한다. 본 포스트는 DBMS의 테이블에 대한 품질진단과 EDA를 수행하는 방법을 제시한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2018-07-22",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n개요\ndlookr 주요 기능\n0.3.2 버전의 추가 기능\n\ndlookr 설치\ndlookr 사용 방법\nDBMS의 테이블을 지원하는 함수들\n\nDBMS의 테이블 품질진단\n데이터 준비\nDBMS의 테이블 생성\ndiagnose()을 이용한 DBMS에서 테이블의 데이터 품질진단\ndiagnose_numeric()을 이용한 테이블의 수치형 컬럼의 상세 진단\ndiagnose_category()을 이용한 테이블의 문자형 컬럼의 상세 진단\ndiagnose_outlier()를 이용한 테이블의 수치형 컬럼의 이상치 진단\nplot_outlier()를 이용한 이상치 진단 시각화\ndiagnose_report()를 이용한 진단 보고서 작성\n데이터 진단 리포트 내용\n\n\n\n개요\ndlookr은 데이터 분석과정에서 데이터 품질진단, EDA 및 변수변환을 지원하는 패키지다. 이 패키지는 dplyr 패키지와 협업하여 데이터를 탐색하고 조작할 수있는 유연한 기능을 제공한다. 특히 자동화된 3종의 보고서는 데이터 품질진단, 탐색적 데이터분석(EDA), 데이터 변환을 수행하는데 훌륭한 가이드를 제공한다.\n이번에 개선된 0.3.2 버전에서는 DBMS의 테이블로 존재하는 데이터의 진단과 EDA를 지원한다. 본 포스트는 DBMS의 테이블에 대한 품질진단과 EDA를 수행하는 방법을 제시한다.\ndlookr 주요 기능\n데이터 프레임이나 tbl_df의 품질을 진단\n데이터 프레임이나 tbl_df의 EDA\n데이터 프레임이나 tbl_df 변수변환을 수행하거나 파생변수를 생성\n위 세가지 작업을 지원하는 자동화된 보고서 생성\n0.3.2 버전의 추가 기능\nDBMS 테이블의 품질을 진단\nDBMS 테이블의 EDA\n위 두가지 작업을 지원하는 자동화된 보고서 생성\ndlookr 설치\n현재 CRAN에는 dlookr 0.3.2 버전의 소스만 등록되어 있다. MS-Windows와 OS X의 binary는 며칠이 걸릴 것이다. 그러므로 github을 통해 설치하기 바란다.\nGitHub에 등록된 vignettes이 없는 개발버전은 다음처럼 설치한다.:\n\n\ndevtools::install_github(\"choonghyunryu/dlookr\")\n\n\n\n혹은 GitHub에 등록된 vignettes을 포함한 개발버전은 다음처럼 설치한다.\n\n\ninstall.packages(c(\"nycflights13\", \"ISLR\", \"DBI\", \"RSQLite\"))\ndevtools::install_github(\"choonghyunryu/dlookr\", build_vignettes = TRUE)\n\n\n\ndlookr 사용 방법\ndlookr에는 몇 가지 vignette 파일을 포함하고 있는데, 이 포스트는 이를 기초로 작성하였다.\n제공되는 vignette는 다음과 같다.\nData quality diagnosis\nExploratory Data Analysis\nData Transformation\n\n\nbrowseVignettes(package = \"dlookr\")\n\n\n\n여기서는 DBMS 테이블의 품질진단에 대한 내용만 다룬다. 좀 더 많은 기능의 dlookr 사용 방법은 vignette이나 https://choonghyunryu.github.io/ko/2018/05/dlookr-데이터진단-eda-데이터변환을-위한-패키지/를 참고하기 바란다.\nDBMS의 테이블을 지원하는 함수들\nDBMS 테이블 진단 및 EDA 기능은 DBMS side(DBMS 자원 사용)에서 SQL을 수행하는 In-Database 모드를 지원한다. 데이터의 크기가 큰 경우 In-Database 모드를 사용하는 것이 더 빠르다.\nDBMS의 SQL은 이상치를 구하거나 샘플링 기반 알고리즘을 구현하기가 어렵다. 따라서 일부 기능은 In-Database 모드를 아직 지원하지 않는다. 이 경우 테이블 데이터를 R로 가져와 계산하는 In-Memory 모드로 수행됩니다. 이 경우 데이터 크기가 클 경우 실행 속도가 느려질 수 있다. collect_size 인수를 지원하는데, 이 인수를 사용하면 지정된 수의 샘플 데이터를 R로 가져올 수 있다.\nDBMS의 테이블 품질진단\n데이터 준비\ndlookr 패키지로 EDA를 수행하는 기초적인 사용 방법을 설명하기 위해서 Carseats를 사용한다. ISLR 패키지의 Carseats는 400개의 매장에서 아동용 카시트를 판매하는 시뮬레이션 데이터다. 이 데이터는 판매량을 예측하는 목적으로 생성한 데이터 프레임이다.\n\n\nlibrary(ISLR)\nstr(Carseats)\n\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n\n개별 변수들의 의미는 다음과 같다. (ISLR::Carseats Man page 참고)\nSales\n지역의 단위 판매량 (단위: 천개)\n\nCompPrice\n지역의 경쟁 업체가 부과하는 가격\n\nIncome\n지역 공동체 수입 수준 (단위: 천달러)\n\nAdvertising\n회사의 지역에 대한 광고 예산 (단위: 천달러)\n\nPopulation\n지역의 인구 규모 (단위: 천명)\n\nPrice\n지역의 자동차 좌석 요금\n\nShelveLoc\n각 사이트에서 자동차 좌석의 선반 위치의 품질을 나타내는 수준. Bad, Good, Medium.\n\nAge\n각 지역의 평균 연령\n\nEducation\n각 지역의 교육 수준\n\nUrban\n점포의 도시 또는 농촌 소재 여부. Yes는 도시, No는 농촌.\n\nUS\n점포의 미국 소재 여부. Yes는 미국 소재, No는 미국 외 소재.\n\n데이터 분석을 수행할 때, 결측치가 포함된 데이터를 자주 접한다. 그러나 Carseats는 결측치가 없은 완전한 데이터다. 그래서 다음과 같이 결측치를 생성하였다. 그리고 carseats라는 이름의 데이터 프레임 객체를 생성한다.\n\n\ncarseats <- ISLR::Carseats\n\nset.seed(123)\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\n\nset.seed(456)\ncarseats[sample(seq(NROW(carseats)), 10), \"Urban\"] <- NA\n\n\n\nDBMS의 테이블 생성\ncarseats 데이터 프레임을 SQLite DBMS에 복사하고 TB_CARSEATS라는 테이블을 만든다. Mysql / MariaDB, PostgreSQL, Oracle DBMS 등의 사용자 환경에 적용할 수 있다.\n\n\nif (!require(DBI)) install.packages('DBI')\nif (!require(RSQLite)) install.packages('RSQLite')\n\nlibrary(dplyr)\n\n# connect DBMS\ncon_sqlite <- DBI::dbConnect(RSQLite::SQLite(), \":memory:\")\n\n# copy carseats to the DBMS with a table named TB_CARSEATS\ncopy_to(con_sqlite, carseats, name = \"TB_CARSEATS\", overwrite = TRUE)\n\n\n\ndiagnose()을 이용한 DBMS에서 테이블의 데이터 품질진단\n다음의 diagnose()는 SQLite DBMS에 존재하는 TB_CARSEATS 테이블의 컬럼을 진단한다.\n\n\nlibrary(dlookr)\n\n# Diagnosis of all columns, and In-Database mode\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose()\n\n\n# A tibble: 11 x 6\n   variables   types     missing_count missing_percent unique_count\n   <chr>       <chr>             <dbl>           <dbl>        <int>\n 1 Sales       double                0             0            336\n 2 CompPrice   double                0             0             73\n 3 Income      double               20             5             99\n 4 Advertising double                0             0             28\n 5 Population  double                0             0            275\n 6 Price       double                0             0            101\n 7 ShelveLoc   character             0             0              3\n 8 Age         double                0             0             56\n 9 Education   double                0             0              9\n10 Urban       character            10             2.5            3\n11 US          character             0             0              2\n# … with 1 more variable: unique_rate <dbl>\n\n# Select columns, and In-memory mode\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose(Sales, Income, in_database = FALSE)\n\n\n# A tibble: 2 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 Sales     numeric             0               0          336\n2 Income    numeric            20               5           99\n# … with 1 more variable: unique_rate <dbl>\n\n# Positions values select columns, and In-memory mode\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose(1, 3, 8, in_database = FALSE)\n\n\n# A tibble: 3 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 Sales     numeric             0               0          336\n2 Income    numeric            20               5           99\n3 Age       numeric             0               0           56\n# … with 1 more variable: unique_rate <dbl>\n\n# Positions values select columns, and In-memory mode and collect size is 200\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose(-8, -9, -10, in_database = FALSE, collect_size = 200)\n\n\n# A tibble: 8 x 6\n  variables   types     missing_count missing_percent unique_count\n  <chr>       <chr>             <int>           <dbl>        <int>\n1 Sales       numeric               0             0            182\n2 CompPrice   numeric               0             0             65\n3 Income      numeric               9             4.5           83\n4 Advertising numeric               0             0             23\n5 Population  numeric               0             0            162\n6 Price       numeric               0             0             82\n7 ShelveLoc   character             0             0              3\n8 US          character             0             0              2\n# … with 1 more variable: unique_rate <dbl>\n\ndiagnose()가 반환하는 tbl_df 객체의 변수는 다음과 같다.\nvariables : 변수명\ntypes : 변수의 데이터 유형\nmissing_count : 결측치 수\nmissing_percent : 결측치의 백분율\nunique_count : 유일값의 수\nunique_rate : 유일값의 비율. unique_count / 관측치의 수\ndplyr을 이용해서 결측치를 포함한 변수를 결측치의 비중별로 정렬할 수 있다.:\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose() %>%\n  select(-unique_count, -unique_rate) %>% \n  filter(missing_count > 0) %>% \n  arrange(desc(missing_count))\n\n\n# A tibble: 2 x 4\n  variables types     missing_count missing_percent\n  <chr>     <chr>             <dbl>           <dbl>\n1 Income    double               20             5  \n2 Urban     character            10             2.5\n\ndiagnose_numeric()을 이용한 테이블의 수치형 컬럼의 상세 진단\ndiagnose_numeric()은 데이터 프레임의 수치형(연속형과 이산형) 변수를 진단한다. 사용 방법은 diagnose()와 동일하나 더 많은 진단 정보를 반환한다. 그런데 두 번째 및 후속 인수 목록에 수치형이 아닌 변수를 지정하면 해당 변수는 자동적으로 무시한다.\ndiagnose_numeric()이 반환하는 tbl_df 객체의 변수는 다음과 같다.\nmin : 최소값\nQ1 : 1/4분위수, 25백분위수\nmean : 산술평균\nmedian : 중위수, 50백분위수\nQ3 : 3/4분위수, 75백분위수\nmax : 최대값\nzero : 0의 값을 갖는 관측치의 개수\nminus : 음수를 갖는 관측치의 개수\noutlier : 이상치의 개수\nzero, minus, outlier는 데이터의 무결성을 진단하는데 유용한 측도다. 예를 들어 어떤 경우의 수치 데이터는 0이나 음수를 가질 수 없는 경우가 있기 때문이다. ’직원의 급여’라는 가상의 수치형 변수는 음수나 0을 가질 수 없기 때문에 데이터 진단 과정에서 0이나 음수의 포함 여부를 살펴보아야 한다.\n다음처럼 diagnose_numeric()는 TB_CARSEATS 테이블의 모든 수치형 컬럼을 진단할 수 있다.:\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_numeric\n\n\n# A tibble: 8 x 10\n  variables     min     Q1   mean median     Q3   max  zero minus\n  <chr>       <dbl>  <dbl>  <dbl>  <dbl>  <dbl> <dbl> <int> <int>\n1 Sales           0   5.39   7.50   7.49   9.32  16.3     1     0\n2 CompPrice      77 115    125.   125    135    175       0     0\n3 Income         21  44     69.3   69     92    120       0     0\n4 Advertising     0   0      6.64   5     12     29     144     0\n5 Population     10 139    265.   272    398.   509       0     0\n6 Price          24 100    116.   117    131    191       0     0\n7 Age            25  39.8   53.3   54.5   66     80       0     0\n8 Education      10  12     13.9   14     16     18       0     0\n# … with 1 more variable: outlier <int>\n\n수치형 변수가 논리적으로 음수나 0의 값을 가질 수 없을 경우에, filter()로 논리적으로 부합하지 않은 변수를 쉽게 찾아낸다.:\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_numeric %>% \n  filter(minus > 0 | zero > 0) \n\n\n# A tibble: 2 x 10\n  variables     min    Q1  mean median    Q3   max  zero minus outlier\n  <chr>       <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <int> <int>   <int>\n1 Sales           0  5.39  7.50   7.49  9.32  16.3     1     0       2\n2 Advertising     0  0     6.64   5    12     29     144     0       0\n\ndiagnose_category()을 이용한 테이블의 문자형 컬럼의 상세 진단\ndiagnose_category()은 데이터 프레임의 범주형(factor, ordered, character) 변수를 진단한다. 사용 방법은 diagnose()와 유사하나 더 많은 진단 정보를 반환한다. 그런데 두 번째 및 후속 인수 목록에 범주형이 아닌 변수를 지정하면 해당 변수는 자동적으로 무시한다. top 인수는 변수별로 반환할 수준(levels)의 개수를 지정한다. 기본값은 10으로 상위 top 10의 수준을 반환한다. 물론 수준의 개수가 10개 미만일 경우에는 모든 수준을 반환한다.\ndiagnose_category()이 반환하는 tbl_df 객체의 변수는 다음과 같다.\nvariables : 변수의 이름\nlevels: 수준의 이름\nN : 관측치의 수\nfreq : 수준별 도수(frequency)\nratio : 수준별 상대도수(백분율 표현)\nrank : 레벨별 도수 크기의 순위\n다음처럼 diagnose_category()는 flights의 모든 범주형 변수를 진단할 수 있다.:\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_category\n\n\n# A tibble: 8 x 6\n  variables levels     N  freq ratio  rank\n  <chr>     <chr>  <int> <int> <dbl> <int>\n1 ShelveLoc Medium   400   219  54.8     1\n2 ShelveLoc Bad      400    96  24       2\n3 ShelveLoc Good     400    85  21.2     3\n4 Urban     Yes      400   275  68.8     1\n5 Urban     No       400   115  28.7     2\n6 Urban     <NA>     400    10   2.5     3\n7 US        Yes      400   258  64.5     1\n8 US        No       400   142  35.5     2\n\ndplyr 패키지의 filter()와 협업하여 결측치가 top 10에 포함된 사례를 조회한 결과에서 Urban 변수가 10건의 결측치로 top 3에 랭크된 것을 알 수 있다.:\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_category %>% \n  filter(is.na(levels))\n\n\n# A tibble: 1 x 6\n  variables levels     N  freq ratio  rank\n  <chr>     <chr>  <int> <int> <dbl> <int>\n1 Urban     <NA>     400    10   2.5     3\n\n다음은 수준이 차지하는 비중이 5% 이하인 목록을 반환한다.:\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_category  %>%\n  filter(ratio <= 5)\n\n\n# A tibble: 1 x 6\n  variables levels     N  freq ratio  rank\n  <chr>     <chr>  <int> <int> <dbl> <int>\n1 Urban     <NA>     400    10   2.5     3\n\ndiagnose_outlier()를 이용한 테이블의 수치형 컬럼의 이상치 진단\ndiagnose_outlier()은 데이터 프레임의 수치형(연속형과 이산형) 변수의 이상치(outliers)를 진단한다. 사용 방법은 diagnose()와 동일하다.\ndiagnose_outlier()이 반환하는 tbl_df 객체의 변수는 다음과 같다.\noutliers_cnt : 이상치의 개수\noutliers_ratio : 이상치의 비율(백분율)\noutliers_mean : 이상치들의 산술평균\nwith_mean : 이상치를 포함한 전체 관측치의 평균\nwithout_mean : 이상치를 제거한 관측치의 산술평균\n다음처럼 diagnose_outlier()는 flights의 모든 수치형 변수의 이상치를 진단할 수 있다.:\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_outlier\n\n\n# A tibble: 8 x 6\n  variables   outliers_cnt outliers_ratio outliers_mean with_mean\n  <chr>              <int>          <dbl>         <dbl>     <dbl>\n1 Sales                  2           0.5           16.0      7.50\n2 CompPrice              2           0.5          126      125.  \n3 Income                 0           0            NaN       69.3 \n4 Advertising            0           0            NaN        6.64\n5 Population             0           0            NaN      265.  \n6 Price                  5           1.25         100.     116.  \n7 Age                    0           0            NaN       53.3 \n8 Education              0           0            NaN       13.9 \n# … with 1 more variable: without_mean <dbl>\n\nplot_outlier()를 이용한 이상치 진단 시각화\nplot_outlier()은 테이블의 수치형(연속형과 이산형) 컬럼의 이상치(outliers)를 시각화한다. 사용 방법은 diagnose()와 동일하다.\nplot_outlier()이 시각화하는 플롯은 다음을 포함한다.\n이상치를 포함한 박스플롯\n이상치를 제거한 박스플롯\n이상치를 포함한 히스토그램\n이상치를 제거한 히스토그램\n다음은 diagnose_outlier()와 plot_outlier(), dplyr 패키지의 함수를 사용하여 이상치의 비율이 0.5% 이상인 모든 수치형 변수의 이상치를 시각화 한다.\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  plot_outlier(con_sqlite %>% \n                 tbl(\"TB_CARSEATS\") %>% \n                 diagnose_outlier() %>%\n                 filter(outliers_ratio >= 0.5) %>% \n                 select(variables) %>% \n                 pull())\n\n\n\n\ndiagnose_report()를 이용한 진단 보고서 작성\ndiagnose_report()는 데이터 프레임이나 데이터 프레임을 상속받은 객체(tbl_df, tbl 등)의 모든 변수들에 대해서 데이터 진단을 수행한다. 그리고 dlookr 0.3.2 버전에서는 DBMS의 테이블에 대한 데이터 진단도 수행한다.\ndiagnose_report()는 진단 보고서를 다음과 같은 두 개의 형태로 작성한다.\nLatex에 기반한 pdf 파일\nhtml 파일\n보고서의 목차는 다음과 같다.\n데이터 진단\n데이터 품질 총괄\n전체변수 품질현황 목록\n결측치 진단\n유일값 진단(문자형과 범주형)\n유일값 진단(수치형)\n\n데이터 품질 상세\n범주형 변수 품질 현황\n수치형 변수 품질 현황\n수치변수 품질현황 (zero)\n수치변수 품질현황 (minus)\n\n\n이상치 진단\n데이터 품질 총괄\n수치변수의 이상치 진단\n이상치 상세 진단\n\n\n다음은 tbl_dbi 클래스 객체로 매핑된 TB_CARSEATS 데이블의 품질진단 리포트를 작성한다. 파일 형식은 pdf이며, 파일이름은 DataDiagnosis_Report.pdf다.\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_report()\n\n\n\n다음은 DataDiagnosis_Report.html라는 이름의 html 형식의 보고서를 생성한다.\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_report(output_format = \"html\")\n\n\n\n다음은 Diagn.html라는 이름의 html 형식의 보고서를 생성한다.\n\n\ncon_sqlite %>% \n  tbl(\"TB_CARSEATS\") %>% \n  diagnose_report(output_format = \"html\", output_file = \"Diagn.html\")\n\n\n\n데이터 진단 보고서는 데이터 진단 과정에 도움을 주기 위한 자동화 보고서다. 보고서 결과를 참고하여 데이터 보완이나 재획득을 판단한다.\n데이터 진단 리포트 내용\npdf 파일의 내용\n보고서의 표지는 다음 그림 1과 같다.\n\n\n\nFigure 1: 데이터진단 보고서 표지\n\n\n\n보고서의 차례는 다음 그림 2와 같다.\n\n\n\nFigure 2: 데이터진단 보고서 차례\n\n\n\n대부분의 정보는 보고서에서 표로 표현된다. 표의 예시는 다음 그림 3과 같다.\n\n\n\nFigure 3: 데이터진단 보고서 도표 예시\n\n\n\n데이터진단 보고서에서 이상치 진단 내용은 시각화 결과를 포함한다. 그 결과는 다음 그림 4와 같다.\n\n\n\nFigure 4: 데이터진단 보고서 이상치 진단 내용\n\n\n\nhtml 파일의 내용\n보고서의 타이틀과 목차는 다음 그림 5와 같다.\n\n\n\nFigure 5: 데이터진단 보고서 타이틀과 목차\n\n\n\n대부분의 정보는 보고서에서 표로 표현된다. html 파일에서 표의 예시는 다음 그림 6과 같다.\n\n\n\nFigure 6: 데이터진단 보고서 도표 예시 (웹)\n\n\n\n데이터진단 보고서에서 이상치 진단 내용은 시각화 결과를 포함한다. html 파일의 결과는 다음 그림 7과 같다.\n\n\n\nFigure 7: 데이터진단 보고서 이상치 진단 내용 (웹)\n\n\n\n\n\n\n",
    "preview": "posts/2018-07-22-dlookr/2018-07-22-dlookr_files/figure-html5/plot_outlier_pipe-1.png",
    "last_modified": "2021-10-23T09:12:20+09:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 768
  },
  {
    "path": "posts/2018-05-13-Asynchronous/",
    "title": "Asynchronous programming in R",
    "description": "오늘 소개할 포스트는 수십분에서 수 시간의 run-time을 요구하는 작업을 대기 상태없이 수행하여, 바로 다른 작업을 수행할 수 있는 방법을 제시한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2018-05-13",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n들어가기\n배경\n응용의 실마리\n\n동기화 vs 비동기화\nR에서의 동기화 vs 비동기화\n\nfuture 패키지\n동기화\n비동기화\n명시적 호출\n암시적 호출\n백그라운드 프로세스\nPlan\n\n\npromises 패키지\n\n들어가기\n배경\n\n무거운 모델을 수행할 경우나 heavy한 데이터를 DBMS에서 가져올 경우에, 수십분에서 수 시간의 run-time이 요구될 수 있다. 이런 경우에는 꼼짝없이 해당 작업이 끝나기를 기다린 후 다른 작업을 수행할 수 밖에 없다. 오늘 소개할 포스트는 수십분에서 수 시간의 run-time을 요구하는 작업을 대기 상태없이 수행하여, 바로 다른 작업을 수행할 수 있는 방법을 제시한다.\n\n응용의 실마리\nrstudio::conf 2018의 컨퍼런스 세션에서 Joe Cheng의 “Scaling Shiny apps with async programming”에서 Synchronous(비동기화)를 지원하는 future 패키지에 대한 소개를 들으면서 응용의 가능성을 확인하였다.\n동기화 R programming을 비동기화 R programming으로 변형한다면, 불필요한 대기 시간없이 원활한 R 분석 작업을 수행하여, 생산성을 높일 수 있게 되는 것이다.\n동기화 vs 비동기화\n동기화(Synchronous) 프로그래밍이란 프로그램 코드의 작업을 끝마쳐야 다음 프로그램 코드에 제어권을 넘겨주는 방식이다. Blocking 방식으로 현 작업이 완료될 때 까지 대기 상태가 된다.\n비동기화(Asynchronous) 프로그래밍이란 프로그램 코드의 작업이 끝마치지 않은 상태에서도 제어권을 넘겨주는 방식이다. Non-blocking 방식으로 현 작업이 완료되지 않아도 대기 상태가 된다.\nR에서의 동기화 vs 비동기화\n다음 그림은 R에서의 동기화와 비동기화를 설명해 준다. 그런데 R은 기본적으로 동기화 방식으로 프로그램을 수행한다.\n\n\n\nFigure 1: Synchronous vs Asynchronous in R\n\n\n\nfuture 패키지\nfuture 패키지는 R에서 비동기화 프로그래밍을 구현해주는 패키지다. 또한 이 패키지는 Parallel 및 Distributed Processing을 지원한다.\n동기화\n동기화 프로그램의 매커니즘을 확인하기 위해 다음과 같은 사용자 정의함수를 만들었다. 함수가 호출된 후 5초 동안 대기했다가 인자값 x를 반환하는 함수다. 즉 이 함수는 5초 가량의 run-time을 갖는다.\n\n\ngetValue <- function(x) {\n  Sys.sleep(15)\n  \n  return(x)\n}\n\n\n\nR은 기본적으로 동기화 매커니즘을 가지고 있으므로 5초 가량 getValue() 함수가 수행된 후에 x의 값 3.141593가 출력된다.\n\n\nsystem.time(x <- getValue(pi))\n\n\n   user  system elapsed \n  0.000   0.001  15.003 \n\n\n\nx\n\n\n[1] 3.141593\n\n비동기화\n비동기(Asynchronous) 처리를 하기 위해서는 tuture 패키지를 사용한다.\n이 패키지는 앞서 Parallel 및 Distributed Processing 기능도 수행한다고 언급했었는데 plan() 함수로 이를 지정할 수있다. 예제에서는 multiprocess를 지정하였다.\n\n\nlibrary(future)\n\nplan(multiprocess)\n\nsystem.time(x <- future(getValue(pi)))\n\n\n   user  system elapsed \n  0.012   0.000   0.014 \n\nfuture() 함수가 반환한 x 객체는 MulticoreFuture class 객체다.\n\n\nis(x)\n\n\n[1] \"MultisessionFuture\"\n\nx\n\n\nMultisessionFuture:\nLabel: '<none>'\nExpression:\ngetValue(pi)\nLazy evaluation: FALSE\nAsynchronous evaluation: TRUE\nLocal evaluation: TRUE\nEnvironment: <environment: R_GlobalEnv>\nCapture standard output: TRUE\nCapture condition classes: 'condition'\nGlobals: 1 objects totaling 5.48 KiB (function 'getValue' of 5.48 KiB)\nPackages: <none>\nL'Ecuyer-CMRG RNG seed: <none> (seed = FALSE)\nResolved: FALSE\nValue: <not collected>\nConditions captured: <none>\nEarly signaling: FALSE\nOwner process: 560ddf9a-fa0f-46e5-3e34-b57dba672453\nClass: 'MultisessionFuture', 'ClusterFuture', 'MultiprocessFuture', 'Future', 'environment'\n\n실제 계산된 값을 사용하기 위해서는 value() 함수로 MulticoreFuture 객체를 원래 반환값의 객체로 변환해 주어야 한다.\n즉, future() 함수로 던지고, value() 함수로 받아야 한다.\n\n\nc\n\n\nfunction (...)  .Primitive(\"c\")\n\n명시적 호출\n명시적(Explicit)으로 호출하는 방법은 앞에서 다룬 future() 함수로 던지고, value() 함수로 받는 방법이다. 즉, futures를 사용할 때, f <- future({ expr }) 와 v <- value(f) 방법을 사용한다.\n그런데 future() 함수로 호출한 expr이 종료되지 않은 상태에서 value(f)를 실행하면 어떤 결과가 발생할까? 이 경우에는 호출한 value(f)는 expr이 완전히 종료한 후에 값을 반환한다. 그러므로 확실하게 expr가 종료된 경우에만 value() 함수를 사용해야 한다.\nexpr의 종료여부는 resolved() 함수를 이용하여 확인한다. 만약 future() 함수가 반환한 MulticoreFuture 객체 f를 생성할 때 수행한 expr이 백그라운드에서 종료하면 TRUE를 반환하고, 아직도 백그라운드에서 수행중이면 FALSE를 반환한다. 그러므로 resolved() 함수로 expr의 종료 여부를 확인 후 value()함수를 사용한다.\n\n\nsystem.time(x <- future(getValue(pi)))\n\n\n   user  system elapsed \n  0.006   0.000   0.007 \n\nresolved(x)\n\n\n[1] FALSE\n\nSys.sleep(6)\nresolved(x)\n\n\n[1] FALSE\n\n암시적 호출\n암시적(Implicit)으로 호출하는 방법은 v %<-% { expr }처럼 %<-% 연산자를 사용하는 방법이다. 앞에서 다룬 future() 함수로 던지고, value() 함수로 받는 방법을 암시적인 호출로 변경하면 다음과 같다.\n\n\nsystem.time(x %<-% getValue(pi))\n\n\n   user  system elapsed \n  0.008   0.000   0.009 \n\n%<-% 연산자를 이용한 암시적인 호출의 결과는 MulticoreFuture 객체가 아닌 expr의 수행 결과의 객체다.\n\n\nis(x)\n\n\n[1] \"numeric\" \"vector\" \n\nx\n\n\n[1] 3.141593\n\n백그라운드 프로세스\nfuture 패키지의 future() 함수나 %<-% 연산자를 호출하면 expr은 백그라운드(Background)에서 수행된다.\n\n\ngetValue2 <- function() {\n  Sys.sleep(5)\n  \n  pid <- Sys.getpid()\n  cat(\"Resolving ...\\n\")\n  return(pid)\n}\n\n\n\n\n\nSys.getpid()\n\n\n[1] 90916\n\nsystem.time(pid <- getValue2())\n\n\nResolving ...\n   user  system elapsed \n  0.001   0.000   5.002 \n\npid\n\n\n[1] 90916\n\n%<-% 연산자 expr가 백그라운드에서 수행되기 때문에, 호출된 getValue2() 함수의 cat() 함수의 결과가 출력되지 않는다. 왜냐하면 cat() 함수는 포그라운드(Foreground) 콘솔에 문자열을 출력하는 함수이기 때문이다. 또한 %<-% 연산자가 호출되지 이전의 프로세스 아이디와 expr이 수행되는 환경의 프로세스 아이디가 다름을 알 수 있다.\n\n\nSys.getpid()\n\n\n[1] 90916\n\nsystem.time(pid %<-% getValue2())\n\n\n   user  system elapsed \n  0.008   0.000   0.008 \n\npid\n\n\nResolving ...\n[1] 91045\n\nPlan\nfuture의 실행 계획(Plan)은 plan() 함수로 지정할 수 있다. 그리고, plan() 함수로 지정할 수 있는 수행 계획은 다음과 같다.\n동기화\nsequential : 현재의 R 프로세스에서 순차적으로 수행된다.\ntransparent : 현재의 R 프로세스에서 순차적으로 수행되고, 호출 환경에 할당이 수행된다.\n\n비동기화\nmultisession : 동일한 컴퓨터의 백그라운드에서 실행되는 별도의 R 세션에서 expr을 비동기 적으로(병렬로) 수행한다.\nmulticore : 동일한 컴퓨터에서 백그라운드로 실행되는 별도의 forked R 프로세스에서 expr을 비동기적으로(병렬로) 수행한다. Windows에서는 지원되지 않는다.\ncluster : multicore 수행을 지원되면 사용되며, 그렇지 않으면 multisession 수행을 사용된다.\nremote : 일반적으로 다른 네트워크에있는 별도의 컴퓨터에서 실행되는 별도의 R 세션에서 expr을 비동기적으로 수행한다.\n\n다음은 동기화 호출인 sequential Plan의 결과로 future() 함수의 수행 시간이 5초 가량 걸렸으며, 반환한 객체 x도 SequentialFuture 객체임을 알 수 있다.\n\n\nplan(sequential)\n\nsystem.time(x <- future(getValue(pi)))\n\n\n   user  system elapsed \n  0.008   0.000  15.008 \n\nis(x)\n\n\n[1] \"SequentialFuture\"\n\nvalue(x)\n\n\n[1] 3.141593\n\npromises 패키지\npromises 패키지는 비동기 작업의 결과에 엑세스할 수 있는 기능을 지원한다.\n\n\nlibrary(dplyr)\n\nggplot2::diamonds %>%\n  filter(cut %in% c(\"Good\", \"Very Good\")) %>%\n  head(10) \n\n\n# A tibble: 10 x 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 2  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 3  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 4  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 5  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 6  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n 7  0.3  Good      J     SI1      64      55   339  4.25  4.28  2.73\n 8  0.3  Good      J     SI1      63.4    54   351  4.23  4.29  2.7 \n 9  0.3  Good      J     SI1      63.8    56   351  4.23  4.26  2.71\n10  0.3  Very Good J     SI1      62.7    59   351  4.21  4.27  2.66\n\npromises 패키지는 다음처럼 tidyverse의 dplyr의 %>% 연산자에 대응하는 %…>%를 이용하여 promise(future) 객체의 데이터를 액세스할 수 있다.\n\n\nlibrary(promises)\n\nfuture(ggplot2::diamonds) %...>%\n  filter(cut %in% c(\"Good\", \"Very Good\")) %...>%\n  head(10) %...>%\n  View()\n\n\n\n\n\n\n",
    "preview": "posts/2018-05-13-Asynchronous/img/R_Async.png",
    "last_modified": "2021-10-11T00:02:20+09:00",
    "input_file": {},
    "preview_width": 931,
    "preview_height": 529
  },
  {
    "path": "posts/2018-05-12-r-dlookr/",
    "title": "dlookr - 데이터진단, EDA, 데이터변환을 위한 패키지",
    "description": "`dlookr`은 데이터 분석과정에서 데이터 품질진단, EDA 및 변수변환을 지원하는 신규 패키지다. 이 패키지는 `dplyr` 패키지와 협업하여 데이터를 탐색하고 조작할 수있는 유연한 기능을 제공한다. 특히 자동화된 3종의 보고서는 `데이터 품질진단`, `탐색적 데이터분석(EDA)`, `데이터 변환`을 수행하는데 훌륭한 가이드를 제공한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2018-05-12",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n개요\ndlookr 설치\n사용 방법\n데이터 품질진단\n데이터: nycflights13\ndiagnose()을 이용한 변수의 개괄적 진단\ndiagnose_numeric()을 이용한 수치형 변수의 상세 진단\ndiagnose_category()을 이용한 범주형 변수의 상세 진단\ndiagnose_outlier()를 이용한 이상치 진단\nplot_outlier()를 이용한 이상치의 시각화\n\n탐색적 데이터 분석\ndatasets\n단변량 데이터 EDA\n이변량 데이터 EDA\nTarget 변수에 기반한 EDA\n\n데이터 변환\n결측치의 대체\n이상치의 대체\n표준화와 치우친 데이터의 보정\nBinning\n\n\n보고서 생성\ndiagnose_report()를 이용한 진단 보고서 작성\neda_report()를 이용한 EDA 보고서 작성\ntransformation_report()를 이용한 데이터변환 보고서 작성\n데이터변환 리포트 내용\npdf 파일의 내용\nhtml 파일의 내용\n\n\n\n개요\ndlookr은 데이터 분석과정에서 데이터 품질진단, EDA 및 변수변환을 지원하는 신규 패키지다. 이 패키지는 dplyr 패키지와 협업하여 데이터를 탐색하고 조작할 수있는 유연한 기능을 제공한다. 특히 자동화된 3종의 보고서는 데이터 품질진단, 탐색적 데이터분석(EDA), 데이터 변환을 수행하는데 훌륭한 가이드를 제공한다.\n주요 기능:\n데이터의 품질을 진단할 수 있다.\n데이터의 탐색과 이해를 통해서 데이터 분석을 수행하기 위한 적절한 시나리오를 찾을 수 있다.\n변수변환을 수행하거나 파생변수를 생성할 수 있다.\n위 세가지 작업을 지원하는 자동화된 보고서를 생성한다.\ndlookr이라는 이름은 데이터 분석 과정에서 looking at the data에서 유래하여 작명하였다.\ndlookr 설치\nCRAN에 등록된, 릴리즈된 패키지는 다음과 같이 설치한다.:\n\n\ninstall.packages(\"dlookr\")\n\n\n\n혹은 GitHub에 등록된 vignettes이 없는 개발버전은 다음처럼 설치한다.:\n\n\ndevtools::install_github(\"choonghyunryu/dlookr\")\n\n\n\n혹은 GitHub에 등록된 vignettes을 포함한 개발버전은 다음처럼 설치한다.\n\n\ninstall.packages(c(\"nycflights13\", \"ISLR\"))\ndevtools::install_github(\"choonghyunryu/dlookr\", build_vignettes = TRUE)\n\n\n\n사용 방법\ndlookr에는 몇 가지 vignette 파일을 포함하고 있는데, 이 포스트는 이를 기초로 작성하였다.\n제공되는 vignette는 다음과 같다.\nData quality diagnosis\nExploratory Data Analysis\nData Transformation\n\n\nbrowseVignettes(package = \"dlookr\")\n\n\n\n데이터 품질진단\n데이터: nycflights13\ndlookr 패키지의 기초적인 사용 방법을 설명하기 위해서 nycflights13 패키지의 flights 데이터를 사용한다. flights 데이터 프레임은 2013년 NYC를 출발한 모든 항공편에 출발과 도착에 대한 정보를 담은 데이터다.\n\n\nlibrary(nycflights13)\ndim(flights)\n\n\n[1] 336776     19\n\nflights\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\ndiagnose()을 이용한 변수의 개괄적 진단\ndiagnose()은 데이터 프레임의 변수를 진단한다. dplyr의 함수처럼 첫 번째 인수는 tibble(또는 데이터 프레임)이다. 두 번째 및 후속 인수는 해당 데이터 프레임 내의 변수를 나타낸다.\ndiagnose()가 반환하는 tbl_df 객체의 변수는 다음과 같다.\nvariables : 변수명\ntypes : 변수의 데이터 유형\nmissing_count : 결측치 수\nmissing_percent : 결측치의 백분율\nunique_count : 유일값의 수\nunique_rate : 유일값의 비율. unique_count / 관측치의 수\n다음처럼 diagnose()는 flights의 모든 변수를 진단할 수 있다.:\n\n\nlibrary(dlookr)\nlibrary(dplyr)\n\ndiagnose(flights)\n\n\n# A tibble: 19 x 6\n   variables      types     missing_count missing_percent unique_count\n   <chr>          <chr>             <int>           <dbl>        <int>\n 1 year           integer               0           0                1\n 2 month          integer               0           0               12\n 3 day            integer               0           0               31\n 4 dep_time       integer            8255           2.45          1319\n 5 sched_dep_time integer               0           0             1021\n 6 dep_delay      numeric            8255           2.45           528\n 7 arr_time       integer            8713           2.59          1412\n 8 sched_arr_time integer               0           0             1163\n 9 arr_delay      numeric            9430           2.80           578\n10 carrier        character             0           0               16\n11 flight         integer               0           0             3844\n12 tailnum        character          2512           0.746         4044\n13 origin         character             0           0                3\n14 dest           character             0           0              105\n15 air_time       numeric            9430           2.80           510\n16 distance       numeric               0           0              214\n17 hour           numeric               0           0               20\n18 minute         numeric               0           0               60\n19 time_hour      POSIXct               0           0             6936\n# … with 1 more variable: unique_rate <dbl>\n\n결측치 : 결측치가 아주 많은 변수, 즉 missing_percent가 100에 가까운 변수는 분석에서 제외하는 것을 고려해야 한다.\n유일값 : 유일값이 하나인(unique_count = 1) 변수는 데이터 분석에서 제외하는 것을 고려한다. 그리고 데이터 유형이 수치형(integer, numeric)이 아니면서 유일값의 개수가 관측치의 개수와 같은(unique_rate = 1) 변수는 식별자일 확률이 크다. 그러므로 이 변수도 분석 모델에 적합치 않은 변수다.\nyear는 unique_count가 1이므로 분석 모델에 사용하지 않는 것을 고려할 수 있다. 다만 year, month, day의 조합으로 년월일을 구성하는 경우에는 굳이 제거하지 않아도 될 것이다.\n다음은 선택된 몇 개의 변수에 대해서만 진단을 수행한다.:\n\n\n# Select columns by name\ndiagnose(flights, year, month, day)\n\n\n# A tibble: 3 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 year      integer             0               0            1\n2 month     integer             0               0           12\n3 day       integer             0               0           31\n# … with 1 more variable: unique_rate <dbl>\n\n# Select all columns between year and day (inclusive)\ndiagnose(flights, year:day)\n\n\n# A tibble: 3 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 year      integer             0               0            1\n2 month     integer             0               0           12\n3 day       integer             0               0           31\n# … with 1 more variable: unique_rate <dbl>\n\n# Select all columns except those from year to day (inclusive)\ndiagnose(flights, -(year:day))\n\n\n# A tibble: 16 x 6\n   variables      types     missing_count missing_percent unique_count\n   <chr>          <chr>             <int>           <dbl>        <int>\n 1 dep_time       integer            8255           2.45          1319\n 2 sched_dep_time integer               0           0             1021\n 3 dep_delay      numeric            8255           2.45           528\n 4 arr_time       integer            8713           2.59          1412\n 5 sched_arr_time integer               0           0             1163\n 6 arr_delay      numeric            9430           2.80           578\n 7 carrier        character             0           0               16\n 8 flight         integer               0           0             3844\n 9 tailnum        character          2512           0.746         4044\n10 origin         character             0           0                3\n11 dest           character             0           0              105\n12 air_time       numeric            9430           2.80           510\n13 distance       numeric               0           0              214\n14 hour           numeric               0           0               20\n15 minute         numeric               0           0               60\n16 time_hour      POSIXct               0           0             6936\n# … with 1 more variable: unique_rate <dbl>\n\ndplyr을 이용해서 결측치를 포함한 변수를 결측치의 비중별로 정렬할 수 있다.:\n\n\nflights %>%\n  diagnose() %>%\n  select(-unique_count, -unique_rate) %>% \n  filter(missing_count > 0) %>% \n  arrange(desc(missing_count))\n\n\n# A tibble: 6 x 4\n  variables types     missing_count missing_percent\n  <chr>     <chr>             <int>           <dbl>\n1 arr_delay numeric            9430           2.80 \n2 air_time  numeric            9430           2.80 \n3 arr_time  integer            8713           2.59 \n4 dep_time  integer            8255           2.45 \n5 dep_delay numeric            8255           2.45 \n6 tailnum   character          2512           0.746\n\ndiagnose_numeric()을 이용한 수치형 변수의 상세 진단\ndiagnose_numeric()은 데이터 프레임의 수치형(연속형과 이산형) 변수를 진단한다. 사용 방법은 diagnose()와 동일하나 더 많은 진단 정보를 반환한다. 그런데 두 번째 및 후속 인수 목록에 수치형이 아닌 변수를 지정하면 해당 변수는 자동적으로 무시한다.\ndiagnose_numeric()이 반환하는 tbl_df 객체의 변수는 다음과 같다.\nmin : 최소값\nQ1 : 1/4분위수, 25백분위수\nmean : 산술평균\nmedian : 중위수, 50백분위수\nQ3 : 3/4분위수, 75백분위수\nmax : 최대값\nzero : 0의 값을 갖는 관측치의 개수\nminus : 음수를 갖는 관측치의 개수\noutlier : 이상치의 개수\n데이터 프레임에 summary() 함수를 적용하면 수치형 변수의 min, Q1, mean, median, Q3 , max를 콘솔에 출력하여 데이터의 분포를 파악할 수 있도록 도와준다. 그러나 그 결과는 분석가가 눈으로만 살펴볼 수 밖에 없는 단점이 있다. 그런데 이런 정보들을 tbl_df와 같은 데이터 프레임 구조로 반환하면 활용의 범위가 넓어진다.\nzero, minus, outlier는 데이터의 무결성을 진단하는데 유용한 측도다. 예를 들어 어떤 경우의 수치 데이터는 0이나 음수를 가질 수 없는 경우가 있기 때문이다. ’직원의 급여’라는 가상의 수치형 변수는 음수나 0을 가질 수 없기 때문에 데이터 진단 과정에서 0이나 음수의 포함 여부를 살펴보아야 한다.\n다음처럼 diagnose_numeric()는 flights의 모든 수치형 변수를 진단할 수 있다.:\n\n\ndiagnose_numeric(flights)\n\n\n# A tibble: 14 x 10\n   variables        min    Q1    mean median    Q3   max  zero  minus\n   <chr>          <dbl> <dbl>   <dbl>  <dbl> <dbl> <dbl> <int>  <int>\n 1 year            2013  2013 2013      2013  2013  2013     0      0\n 2 month              1     4    6.55      7    10    12     0      0\n 3 day                1     8   15.7      16    23    31     0      0\n 4 dep_time           1   907 1349.     1401  1744  2400     0      0\n 5 sched_dep_time   106   906 1344.     1359  1729  2359     0      0\n 6 dep_delay        -43    -5   12.6      -2    11  1301 16514 183575\n 7 arr_time           1  1104 1502.     1535  1940  2400     0      0\n 8 sched_arr_time     1  1124 1536.     1556  1945  2359     0      0\n 9 arr_delay        -86   -17    6.90     -5    14  1272  5409 188933\n10 flight             1   553 1972.     1496  3465  8500     0      0\n11 air_time          20    82  151.      129   192   695     0      0\n12 distance          17   502 1040.      872  1389  4983     0      0\n13 hour               1     9   13.2      13    17    23     0      0\n14 minute             0     8   26.2      29    44    59 60696      0\n# … with 1 more variable: outlier <int>\n\n수치형 변수가 논리적으로 음수나 0의 값을 가질 수 없을 경우에, filter()로 논리적으로 부합하지 않은 변수를 쉽게 찾아낸다.:\n\n\ndiagnose_numeric(flights) %>% \n  filter(minus > 0 | zero > 0) \n\n\n# A tibble: 3 x 10\n  variables   min    Q1  mean median    Q3   max  zero  minus outlier\n  <chr>     <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <int>  <int>   <int>\n1 dep_delay   -43    -5 12.6      -2    11  1301 16514 183575   43216\n2 arr_delay   -86   -17  6.90     -5    14  1272  5409 188933   27880\n3 minute        0     8 26.2      29    44    59 60696      0       0\n\ndiagnose_category()을 이용한 범주형 변수의 상세 진단\ndiagnose_category()은 데이터 프레임의 범주형(factor, ordered, character) 변수를 진단한다. 사용 방법은 diagnose()와 유사하나 더 많은 진단 정보를 반환한다. 그런데 두 번째 및 후속 인수 목록에 범주형이 아닌 변수를 지정하면 해당 변수는 자동적으로 무시한다. top 인수는 변수별로 반환할 수준(levels)의 개수를 지정한다. 기본값은 10으로 상위 top 10의 수준을 반환한다. 물론 수준의 개수가 10개 미만일 경우에는 모든 수준을 반환한다.\ndiagnose_category()이 반환하는 tbl_df 객체의 변수는 다음과 같다.\nvariables : 변수의 이름\nlevels: 수준의 이름\nN : 관측치의 수\nfreq : 수준별 도수(frequency)\nratio : 수준별 상대도수(백분율 표현)\nrank : 레벨별 도수 크기의 순위\n다음처럼 diagnose_category()는 flights의 모든 범주형 변수를 진단할 수 있다.:\n\n\ndiagnose_category(flights)\n\n\n# A tibble: 43 x 6\n   variables levels      N  freq ratio  rank\n   <chr>     <chr>   <int> <int> <dbl> <int>\n 1 carrier   UA     336776 58665 17.4      1\n 2 carrier   B6     336776 54635 16.2      2\n 3 carrier   EV     336776 54173 16.1      3\n 4 carrier   DL     336776 48110 14.3      4\n 5 carrier   AA     336776 32729  9.72     5\n 6 carrier   MQ     336776 26397  7.84     6\n 7 carrier   US     336776 20536  6.10     7\n 8 carrier   9E     336776 18460  5.48     8\n 9 carrier   WN     336776 12275  3.64     9\n10 carrier   VX     336776  5162  1.53    10\n# … with 33 more rows\n\ndplyr 패키지의 filter()와 협업하여 결측치가 top 10에 포함된 사례를 조회한 결과에서 tailnum 변수가 2,512건의 결측치로 top 1에 랭크된 것을 알 수 있다.:\n\n\ndiagnose_category(flights) %>% \n  filter(is.na(levels))\n\n\n# A tibble: 1 x 6\n  variables levels      N  freq ratio  rank\n  <chr>     <chr>   <int> <int> <dbl> <int>\n1 tailnum   <NA>   336776  2512 0.746     1\n\n다음은 수준이 차지하는 비중이 0.01% 이하인 목록을 반환한다. top 인수값을 500으로 넉넉하게 지정한 것에 주목해야 한다. 만약 기본값인 10을 사용하였다면 0.01% 이하의 값은 목록에 포함되지 못했을 것이다.:\n\n\nflights %>%\n  diagnose_category(top = 500)  %>%\n  filter(ratio <= 0.01)\n\n\n# A tibble: 10 x 6\n   variables levels      N  freq    ratio  rank\n   <chr>     <chr>   <int> <int>    <dbl> <int>\n 1 carrier   OO     336776    32 0.00950     16\n 2 dest      JAC    336776    25 0.00742     97\n 3 dest      PSP    336776    19 0.00564     98\n 4 dest      EYW    336776    17 0.00505     99\n 5 dest      HDN    336776    15 0.00445    100\n 6 dest      MTJ    336776    15 0.00445    100\n 7 dest      SBN    336776    10 0.00297    102\n 8 dest      ANC    336776     8 0.00238    103\n 9 dest      LEX    336776     1 0.000297   104\n10 dest      LGA    336776     1 0.000297   104\n\n분석 모델에서 관측치에서 차지하는 비중이 미미한 수준들은 제거하거나 하나로 합치는 것도 고려해볼 수 있다.\ndiagnose_outlier()를 이용한 이상치 진단\ndiagnose_outlier()은 데이터 프레임의 수치형(연속형과 이산형) 변수의 이상치(outliers)를 진단한다. 사용 방법은 diagnose()와 동일하다.\ndiagnose_outlier()이 반환하는 tbl_df 객체의 변수는 다음과 같다.\noutliers_cnt : 이상치의 개수\noutliers_ratio : 이상치의 비율(백분율)\noutliers_mean : 이상치들의 산술평균\nwith_mean : 이상치를 포함한 전체 관측치의 평균\nwithout_mean : 이상치를 제거한 관측치의 산술평균\n다음처럼 diagnose_outlier()는 flights의 모든 수치형 변수의 이상치를 진단할 수 있다.:\n\n\ndiagnose_outlier(flights)\n\n\n# A tibble: 14 x 6\n   variables      outliers_cnt outliers_ratio outliers_mean with_mean\n   <chr>                 <int>          <dbl>         <dbl>     <dbl>\n 1 year                      0       0                NaN     2013   \n 2 month                     0       0                NaN        6.55\n 3 day                       0       0                NaN       15.7 \n 4 dep_time                  0       0                NaN     1349.  \n 5 sched_dep_time            0       0                NaN     1344.  \n 6 dep_delay             43216      12.8               93.1     12.6 \n 7 arr_time                  0       0                NaN     1502.  \n 8 sched_arr_time            0       0                NaN     1536.  \n 9 arr_delay             27880       8.28             121.       6.90\n10 flight                    1       0.000297        8500     1972.  \n11 air_time               5448       1.62             400.     151.  \n12 distance                715       0.212           4955.    1040.  \n13 hour                      0       0                NaN       13.2 \n14 minute                    0       0                NaN       26.2 \n# … with 1 more variable: without_mean <dbl>\n\n다음은 이상치를 5% 이상 포함한 수치형 변수중에서 이상치의 평균이 전체 평균대비 규모가 큰 순으로 정렬하여 반환한다.:\n\n\ndiagnose_outlier(flights) %>% \n  filter(outliers_ratio > 5) %>% \n  mutate(rate = outliers_mean / with_mean) %>% \n  arrange(desc(rate)) %>% \n  select(-outliers_cnt)\n\n\n# A tibble: 2 x 6\n  variables outliers_ratio outliers_mean with_mean without_mean  rate\n  <chr>              <dbl>         <dbl>     <dbl>        <dbl> <dbl>\n1 arr_delay           8.28         121.       6.90       -3.69  17.5 \n2 dep_delay          12.8           93.1     12.6         0.444  7.37\n\n데이터 분석 과정에서 이상치의 평균이 전체 평균대비 규모가 클 경우에는 이상치를 대체하거나 제거하는 것이 바람직할 수 있다.\nplot_outlier()를 이용한 이상치의 시각화\nplot_outlier()은 데이터 프레임의 수치형(연속형과 이산형) 변수의 이상치(outliers)를 시각화한다. 사용 방법은 diagnose()와 동일하다.\nplot_outlier()이 시각화하는 플롯은 다음을 포함한다.\n이상치를 포함한 박스플롯\n이상치를 제거한 박스플롯\n이상치를 포함한 히스토그램\n이상치를 제거한 히스토그램\n다음은 diagnose_outlier()와 plot_outlier(), dplyr 패키지의 함수를 사용하여 이상치의 비율이 0.5% 이상인 모든 수치형 변수의 이상치를 시각화 한다.\n\n\nflights %>%\n  plot_outlier(diagnose_outlier(flights) %>% \n                 filter(outliers_ratio >= 0.5) %>% \n                 select(variables) %>% \n                 unlist())\n\n\n\n\n시각화 결과를 보고 이상치의 제거 및 대체 여부를 결정해야 한다. 경우에 따라서는 이상치가 포함된 변수를 데이터 분석 모델에서 제거하는 것도 고려해야 한다.\n시각화 결과를 보면 arr_delay는 이상치를 제거한 관측치들은 정규분포와 유사한 분포를 보이고 있다. 선형 모형의 경우에는 이상치를 제거하거나 대체하는 것도 검토해볼 수 있겠다. 그리고 air_time은 이상치를 제거하기 전후의 분포가 대략 비슷한 모양을 보인다.\n탐색적 데이터 분석\ndatasets\ndlookr 패키지로 EDA를 수행하는 기초적인 사용 방법을 설명하기 위해서 Carseats를 사용한다. ISLR 패키지의 Carseats는 400개의 매장에서 아동용 카시트를 판매하는 시뮬레이션 데이터다. 이 데이터는 판매량을 예측하는 목적으로 생성한 데이터 프레임이다.\n\n\nlibrary(ISLR)\nstr(Carseats)\n\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n\n개별 변수들의 의미는 다음과 같다. (ISLR::Carseats Man page 참고)\nSales\n지역의 단위 판매량 (단위: 천개)\n\nCompPrice\n지역의 경쟁 업체가 부과하는 가격\n\nIncome\n지역 공동체 수입 수준 (단위: 천달러)\n\nAdvertising\n회사의 지역에 대한 광고 예산 (단위: 천달러)\n\nPopulation\n지역의 인구 규모 (단위: 천명)\n\nPrice\n지역의 자동차 좌석 요금\n\nShelveLoc\n각 사이트에서 자동차 좌석의 선반 위치의 품질을 나타내는 수준. “Bad”, “Good”, “Medium”.\n\nAge\n각 지역의 평균 연령\n\nEducation\n각 지역의 교육 수준\n\nUrban\n점포의 도시 또는 농촌 소재 여부. Yes는 도시, No는 농촌.\n\nUS\n점포의 미국 소재 여부. Yes는 미국 소재, No는 미국 외 소재.\n\n데이터 분석을 수행할 때, 결측치가 포함된 데이터를 자주 접한다. 그러나 Carseats는 결측치가 없은 완전한 데이터다. 그래서 다음과 같이 결측치를 생성하였다. 그리고 carseats라는 이름의 데이터 프레임 객체를 생성한다.\n\n\ncarseats <- ISLR::Carseats\n\nset.seed(123)\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\n\nset.seed(456)\ncarseats[sample(seq(NROW(carseats)), 10), \"Urban\"] <- NA\n\n\n\n단변량 데이터 EDA\ndescribe()을 이용한 기술통계량 계산\ndescribe()는 수치 데이터의 기술통계량을 계산해 준다. 기술통계량은 수치 변수의 분포를 판단하는 것을 도와준다.\ndescribe()가 반환하는 tbl_df 객체의 변수는 다음과 같다.\nn : 결측치를 제외한 데이터 건수\nna : 결측치 건수\nmean : 산술평균\nsd : 표준편차\nse_mean : 표준오차. sd/sqrt(n)\nIQR : 사분위 범위(Interquartile range) (Q3-Q1)\nskewness : 왜도\nkurtosis : 첨도\np25 : Q1. 25% 백분위수\np50 : 중위수. 50% 백분위수\np75 : Q3. 75% 백분위수\np01, p05,p10,p20,p30` : 1%, 5%, 20%, 30% 백분위수\np40, p60,p70,p80` : 40%, 60%, 70%, 80% 백분위수\np90, p95,p99,p100` : 90%, 95%, 99%, 100% 백분위수\n다음처럼 describe()는 carseats의 모든 수치 변수의 통계량을 계산한다.:\n\n\ndescribe(carseats)\n\n\n# A tibble: 8 x 26\n  variable      n    na   mean     sd se_mean    IQR skewness kurtosis\n  <chr>     <int> <int>  <dbl>  <dbl>   <dbl>  <dbl>    <dbl>    <dbl>\n1 Sales       400     0   7.50   2.82   0.141   3.93   0.186   -0.0809\n2 CompPrice   400     0 125.    15.3    0.767  20     -0.0428   0.0417\n3 Income      380    20  69.3   28.1    1.44   48      0.0360  -1.10  \n4 Advertis…   400     0   6.64   6.65   0.333  12      0.640   -0.545 \n5 Populati…   400     0 265.   147.     7.37  260.    -0.0512  -1.20  \n6 Price       400     0 116.    23.7    1.18   31     -0.125    0.452 \n7 Age         400     0  53.3   16.2    0.810  26.2   -0.0772  -1.13  \n8 Education   400     0  13.9    2.62   0.131   4      0.0440  -1.30  \n# … with 17 more variables: p00 <dbl>, p01 <dbl>, p05 <dbl>,\n#   p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>,\n#   p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>,\n#   p99 <dbl>, p100 <dbl>\n\n왜도 : 왼쪽으로 치우친 분포의 데이터, 즉 skewness가 제법 큰 양수를 갖는 변수는 정규분포를 따르도록 log, sqrt 변환 등을 고려해야 한다. Advertising 변수는 변수변환을 고려해야할 것 같다.\n산술평균, 표준편차, 표준오차 : 표준오차(se_mean)가 7.3688218로 상당히 큰 Population는 대표치인 산술평균(mean)의 대표성이 낮다. 산술평균에 비해서 표준편차(sd)의 크기도 상당히 큰 편이다.\ndplyr을 이용해서 왼쪽이나 오른쪽으로 치우친 정도(왜도)의 크기별로 정렬할 수 있다.:\n\n\ncarseats %>%\n  describe() %>%\n  select(variable, skewness, mean, p25, p50, p75) %>% \n  filter(!is.na(skewness)) %>% \n  arrange(desc(abs(skewness)))\n\n\n# A tibble: 8 x 6\n  variable    skewness   mean    p25    p50    p75\n  <chr>          <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 Advertising   0.640    6.64   0      5     12   \n2 Sales         0.186    7.50   5.39   7.49   9.32\n3 Price        -0.125  116.   100    117    131   \n4 Age          -0.0772  53.3   39.8   54.5   66   \n5 Population   -0.0512 265.   139    272    398.  \n6 Education     0.0440  13.9   12     14     16   \n7 CompPrice    -0.0428 125.   115    125    135   \n8 Income        0.0360  69.3   44     69     92   \n\ndescribe() 함수는 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  group_by(US, Urban) %>% \n  describe(Sales, Income) \n\n\n# A tibble: 12 x 28\n   variable US    Urban     n    na  mean    sd se_mean   IQR skewness\n   <chr>    <fct> <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n 1 Income   No    No       40     4 62.1  29.8    4.72  51.8    0.367 \n 2 Income   No    Yes      91     3 67.5  27.4    2.87  48      0.0518\n 3 Income   No    <NA>      3     1 59.7  37.0   21.4   37     -0.162 \n 4 Income   Yes   No       68     3 70.2  30.7    3.72  53      0.0414\n 5 Income   Yes   Yes     172     9 71.4  26.5    2.02  45     -0.0201\n 6 Income   Yes   <NA>      6     0 79.8  34.9   14.3   53.5   -0.506 \n 7 Sales    No    No       44     0  6.42  2.78   0.418  3.29   0.132 \n 8 Sales    No    Yes      94     0  7.00  2.56   0.264  3.49   0.491 \n 9 Sales    No    <NA>      4     0  7.04  1.04   0.518  1.26  -0.887 \n10 Sales    Yes   No       71     0  8.21  2.61   0.310  4.08  -0.0453\n11 Sales    Yes   Yes     181     0  7.77  2.95   0.219  3.99   0.134 \n12 Sales    Yes   <NA>      6     0  6.84  3.70   1.51   4.87   0.599 \n# … with 18 more variables: kurtosis <dbl>, p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\nnormality()을 이용한 수치형 변수의 정규성 검정\nnormality()는 수치 데이터의 정규성 검정을 수행한다. Shapiro-Wilk 정규성 검정을 수행하며, 관측치의 개수가 5000보다 클 경우에는 5000개의 단순 임의 추출을 수행한 후 검정한다.\nnormality()가 반환하는 tbl_df 객체의 변수는 다음과 같다.\nstatistic : Shapiro-Wilk 검정의 통계량\np_value : Shapiro-Wilk 검정의 p-value\nsample : Shapiro-Wilk 검정을 수행한 샘플 관측치의 개수\n다음처럼 normality()는 carseats의 모든 수치 변수의 정규성 검정을 수행한다.:\n\n\nnormality(carseats)\n\n\n# A tibble: 8 x 4\n  vars        statistic  p_value sample\n  <chr>           <dbl>    <dbl>  <dbl>\n1 Sales           0.995 2.54e- 1    400\n2 CompPrice       0.998 9.77e- 1    400\n3 Income          0.961 1.55e- 8    400\n4 Advertising     0.874 1.49e-17    400\n5 Population      0.952 4.08e-10    400\n6 Price           0.996 3.90e- 1    400\n7 Age             0.957 1.86e- 9    400\n8 Education       0.924 2.43e-13    400\n\ndplyr을 이용해서 정규분포를 따르지 않는 변수를 p_value 순으로 정렬할 수 있다.:\n\n\ncarseats %>%\n  normality() %>%\n  filter(p_value <= 0.01) %>% \n  arrange(abs(p_value))\n\n\n# A tibble: 5 x 4\n  vars        statistic  p_value sample\n  <chr>           <dbl>    <dbl>  <dbl>\n1 Advertising     0.874 1.49e-17    400\n2 Education       0.924 2.43e-13    400\n3 Population      0.952 4.08e-10    400\n4 Age             0.957 1.86e- 9    400\n5 Income          0.961 1.55e- 8    400\n\n특히 Advertising 변수는 정규분포에서 가장 벗어난 것으로 파악된다.\nnormality() 함수는 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  group_by(ShelveLoc, US) %>%\n  normality(Income) %>% \n  arrange(desc(p_value))\n\n\n# A tibble: 6 x 6\n  variable ShelveLoc US    statistic p_value sample\n  <chr>    <fct>     <fct>     <dbl>   <dbl>  <dbl>\n1 Income   Bad       No        0.965 0.350       34\n2 Income   Good      Yes       0.958 0.0359      61\n3 Income   Bad       Yes       0.952 0.0236      62\n4 Income   Good      No        0.879 0.0140      24\n5 Income   Medium    Yes       0.964 0.00190    135\n6 Income   Medium    No        0.944 0.00161     84\n\nIncome 변수는 정규분포를 따르지 않지만, 유의수준 0.01 기준으로 US가 No이면서 ShelveLoc가 Good, Bad인 경우는 정규분포를 따르는 것으로 볼 수 있다.\n다음은 범주형 변수인 ShelveLoc, US 변수의 조합별로 log(Income)의 정규성 검정을 수행하여, 정규분포를 따르는 변수를 조회한다.\n\n\ncarseats %>%\n  mutate(log_income = log(Income)) %>%\n  group_by(ShelveLoc, US) %>%\n  normality(log_income) %>%\n  filter(p_value > 0.01)\n\n\n# A tibble: 1 x 6\n  variable   ShelveLoc US    statistic p_value sample\n  <chr>      <fct>     <fct>     <dbl>   <dbl>  <dbl>\n1 log_income Bad       No        0.946   0.100     34\n\nplot_normality()를 이용한 수치변수의 정규성 시각화\nplot_normality()는 수치 데이터의 정규성을 시각화한다.\nplot_normality()가 시각화하는 정보는 다음과 같다.\n원 데이터의 히스토그램\n원 데이터의 Q-Q plot\nlog 변환 데이터의 히스토그램\nsqrt 변환 데이터의 히스토그램\n데이터 분석 과정에서 멱분포(power-law distribution)를 따르는 수치 데이터를 접하는 경우가 많다. 멱분포를 따르는 수치 데이터는 log, sqrt 변환을 수행하여 정규분포로 변화라기 때문에 log, sqrt 변환데 데이터의 히스토그램을 그린다.\nplot_normality()도 normality() 함수처럼 여러 개의 변수를 지정할 수 있다.\n\n\n# Select columns by name\nplot_normality(carseats, Sales, CompPrice)\n\n\n\n\nplot_normality() 함수도 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  filter(ShelveLoc == \"Good\") %>%\n  group_by(US) %>%\n  plot_normality(Income)\n\n\n\n\n이변량 데이터 EDA\ncorrelate()을 이용한 상관계수 계산\n다음처럼 correlate()는 carseats의 모든 수치 변수의 조합의 상관계수를 구한다.:\n\n\ncorrelate(carseats)\n\n\n# A tibble: 56 x 3\n   var1        var2      coef_corr\n   <fct>       <fct>         <dbl>\n 1 CompPrice   Sales        0.0641\n 2 Income      Sales        0.153 \n 3 Advertising Sales        0.270 \n 4 Population  Sales        0.0505\n 5 Price       Sales       -0.445 \n 6 Age         Sales       -0.232 \n 7 Education   Sales       -0.0520\n 8 Sales       CompPrice    0.0641\n 9 Income      CompPrice   -0.0918\n10 Advertising CompPrice   -0.0242\n# … with 46 more rows\n\n다음은 선택된 몇 개의 변수를 포함한 조합에 대해서만 정규성 검정을 수행한다.\n\n\n# Select columns by name\ncorrelate(carseats, Sales, CompPrice, Income)\n\n\n# A tibble: 21 x 3\n   var1      var2        coef_corr\n   <fct>     <fct>           <dbl>\n 1 CompPrice Sales          0.0641\n 2 Income    Sales          0.153 \n 3 Sales     CompPrice      0.0641\n 4 Income    CompPrice     -0.0918\n 5 Sales     Income         0.153 \n 6 CompPrice Income        -0.0918\n 7 Sales     Advertising    0.270 \n 8 CompPrice Advertising   -0.0242\n 9 Income    Advertising    0.0674\n10 Sales     Population     0.0505\n# … with 11 more rows\n\ncorrelate()는 두벌의 변수 조합을 만든다. 그래서 다음과 같은 filter() 함수를 사용해서 한 벌의 조합에 대한 상관계수를 구할 수 있다.:\n\n\ncarseats %>%\n  correlate(Sales:Income) %>%\n  filter(as.integer(var1) > as.integer(var2))\n\n\n# A tibble: 3 x 3\n  var1      var2      coef_corr\n  <fct>     <fct>         <dbl>\n1 CompPrice Sales        0.0641\n2 Income    Sales        0.153 \n3 Income    CompPrice   -0.0918\n\ncorrelate() 함수도 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  filter(ShelveLoc == \"Good\") %>%\n  group_by(Urban, US) %>%\n  correlate(Sales) %>%\n  filter(abs(coef_corr) > 0.5)\n\n\n# A tibble: 6 x 5\n  Urban US    var1  var2       coef_corr\n  <fct> <fct> <fct> <fct>          <dbl>\n1 No    No    Sales Population    -0.530\n2 No    No    Sales Price         -0.838\n3 No    Yes   Sales Price         -0.655\n4 Yes   No    Sales Price         -0.837\n5 Yes   No    Sales Age           -0.644\n6 Yes   Yes   Sales Price         -0.604\n\nplot_correlate()를 이용한 상관행렬의 시각화\nplot_correlate()는 상관행렬을 시각화한다.\n\n\nplot_correlate(carseats)\n\n\n\n\nplot_correlate()도 correlate() 함수처럼 여러 개의 변수를 지정할 수 있다.\n다음은 선택된 몇 개의 변수를 포함한 상관행렬의 시각화를 수행한다.\n\n\n# Select columns by name\nplot_correlate(carseats, Sales, Price)\n\n\n\n\nplot_correlate() 함수도 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  filter(ShelveLoc == \"Good\") %>%\n  group_by(Urban, US) %>%\n  plot_correlate(Sales)\n\n\n\n\nTarget 변수에 기반한 EDA\nTarget 변수 정의\nTarget 변수 기반으로 EDA를 수행하려면 target_by 클래스 객체를 생성해야 한다. target_by()는 data.frame 또는 data.frame을 상속받은 객체로 target_by 클래스를 생성한다. target_by()는 grouped_df를 생성하는 dplyr의 group_by()와 유사하다. 차이점은 하나의 변수만 지정한다는 것이다.\n다음은 carseats data.frame에서 US를 target 변수로 지정하는 예제다.:\n\n\ncateg <- target_by(carseats, US)\n\n\n\nTarget 변수가 범주형 변수인 경우의 EDA\nTarget 변수가 범주형일 때 EDA를 수행해 보자. 범주형 변수 US가 target 변수일 때, target 변수와 예측 변수(Predictor) 사이의 관계를 살펴본다.\n예측변수가 수치형 변수인 경우\nrelate()는 taregt 변수와 예측변수 사이의 관계를 보여준다. 다음 예제는 예측변수 Sales와 target 변수 US 사이의 관계를 보여준다. 예측변수 Sales는 수치형 변수다. 이 경우, target 변수의 각 레벨에 대한 기술통계(descriptive statistics)가 표현된다.\n\n\n# If the variable of interest is a numarical variable\ncat_num <- relate(categ, Sales)\ncat_num\n\n\n# A tibble: 3 x 27\n  variable US        n    na  mean    sd se_mean   IQR skewness\n  <chr>    <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n1 Sales    No      142     0  6.82  2.60   0.218  3.44   0.323 \n2 Sales    Yes     258     0  7.87  2.88   0.179  4.23   0.0760\n3 Sales    total   400     0  7.50  2.82   0.141  3.93   0.186 \n# … with 18 more variables: kurtosis <dbl>, p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\nsummary(cat_num)\n\n\n   variable             US          n               na   \n Length:3           No   :1   Min.   :142.0   Min.   :0  \n Class :character   Yes  :1   1st Qu.:200.0   1st Qu.:0  \n Mode  :character   total:1   Median :258.0   Median :0  \n                              Mean   :266.7   Mean   :0  \n                              3rd Qu.:329.0   3rd Qu.:0  \n                              Max.   :400.0   Max.   :0  \n      mean             sd           se_mean            IQR       \n Min.   :6.823   Min.   :2.603   Min.   :0.1412   Min.   :3.442  \n 1st Qu.:7.160   1st Qu.:2.713   1st Qu.:0.1602   1st Qu.:3.686  \n Median :7.496   Median :2.824   Median :0.1791   Median :3.930  \n Mean   :7.395   Mean   :2.768   Mean   :0.1796   Mean   :3.866  \n 3rd Qu.:7.682   3rd Qu.:2.851   3rd Qu.:0.1988   3rd Qu.:4.077  \n Max.   :7.867   Max.   :2.877   Max.   :0.2184   Max.   :4.225  \n    skewness          kurtosis             p00        \n Min.   :0.07603   Min.   :-0.32638   Min.   :0.0000  \n 1st Qu.:0.13080   1st Qu.:-0.20363   1st Qu.:0.0000  \n Median :0.18556   Median :-0.08088   Median :0.0000  \n Mean   :0.19489   Mean   : 0.13350   Mean   :0.1233  \n 3rd Qu.:0.25432   3rd Qu.: 0.36344   3rd Qu.:0.1850  \n Max.   :0.32308   Max.   : 0.80776   Max.   :0.3700  \n      p01              p05             p10             p20       \n Min.   :0.4675   Min.   :3.147   Min.   :3.917   Min.   :4.754  \n 1st Qu.:0.6868   1st Qu.:3.148   1st Qu.:4.018   1st Qu.:4.910  \n Median :0.9062   Median :3.149   Median :4.119   Median :5.066  \n Mean   :1.0072   Mean   :3.183   Mean   :4.073   Mean   :5.051  \n 3rd Qu.:1.2771   3rd Qu.:3.200   3rd Qu.:4.152   3rd Qu.:5.199  \n Max.   :1.6480   Max.   :3.252   Max.   :4.184   Max.   :5.332  \n      p25             p30             p40             p50       \n Min.   :5.080   Min.   :5.306   Min.   :5.994   Min.   :6.660  \n 1st Qu.:5.235   1st Qu.:5.587   1st Qu.:6.301   1st Qu.:7.075  \n Median :5.390   Median :5.867   Median :6.608   Median :7.490  \n Mean   :5.411   Mean   :5.775   Mean   :6.506   Mean   :7.313  \n 3rd Qu.:5.576   3rd Qu.:6.010   3rd Qu.:6.762   3rd Qu.:7.640  \n Max.   :5.763   Max.   :6.153   Max.   :6.916   Max.   :7.790  \n      p60             p70             p75             p80        \n Min.   :7.496   Min.   :7.957   Min.   :8.523   Min.   : 8.772  \n 1st Qu.:7.787   1st Qu.:8.386   1st Qu.:8.921   1st Qu.: 9.265  \n Median :8.078   Median :8.815   Median :9.320   Median : 9.758  \n Mean   :8.076   Mean   :8.740   Mean   :9.277   Mean   : 9.665  \n 3rd Qu.:8.366   3rd Qu.:9.132   3rd Qu.:9.654   3rd Qu.:10.111  \n Max.   :8.654   Max.   :9.449   Max.   :9.988   Max.   :10.464  \n      p90              p95             p99             p100      \n Min.   : 9.349   Min.   :11.28   Min.   :13.64   Min.   :14.90  \n 1st Qu.:10.325   1st Qu.:11.86   1st Qu.:13.78   1st Qu.:15.59  \n Median :11.300   Median :12.44   Median :13.91   Median :16.27  \n Mean   :10.795   Mean   :12.08   Mean   :13.86   Mean   :15.81  \n 3rd Qu.:11.518   3rd Qu.:12.49   3rd Qu.:13.97   3rd Qu.:16.27  \n Max.   :11.736   Max.   :12.54   Max.   :14.03   Max.   :16.27  \n\nrelate()로 생성된 relate 클래스 객체를, plot ()으로 target 변수와 예측변수 사이의 관계를 시각화한다. US와 Sales 간의 관계는 밀도 플롯(density plot)으로 표현된다.\n\n\nplot(cat_num)\n\n\n\n\n예측변수가 범주형 변수인 경우\n다음 예제는 ShelveLoc과 target 변수 US 사이의 관계를 보여준다. 예측변수인 ShelveLoc는 범주형 변수다. 이 경우는 두 변수의 분할표(contentency table)를 보여준다. summary() 함수는 분할표에 대해 독립성 검정을 수행한다.\n\n\n# If the variable of interest is a categorical variable\ncat_cat <- relate(categ, ShelveLoc)\ncat_cat\n\n\n     ShelveLoc\nUS    Bad Good Medium\n  No   34   24     84\n  Yes  62   61    135\n\nsummary(cat_cat)\n\n\nCall: xtabs(formula = formula_str, data = data, addNA = TRUE)\nNumber of cases in table: 400 \nNumber of factors: 2 \nTest for independence of all factors:\n    Chisq = 2.7397, df = 2, p-value = 0.2541\n\nplot()은 target 변수와 예측변수 사이의 관계를 시각화한다. US와 ShelveLoc 사이의 관계는 모자이크 플롯(mosaics plot)으로 표현된다.\n\n\nplot(cat_cat)\n\n\n\n\nTarget 변수가 수치형 변수일 때의 EDA\nTarget 변수가 수치형일 때 EDA를 수행해 보자. 수치형 변수 `Sales가 target 변수일 때, target 변수와 예측 변수(Predictor) 사이의 관계를 살펴본다.\n\n\n# If the variable of interest is a numarical variable\nnum <- target_by(carseats, Sales)\n\n\n\n예측변수가 수치형 변수인 경우\n다음 예제는 Price와 target 변수 Sales 사이의 관계를 보여준다. 예측변수인 Price는 수치형 변수다. 이 경우, target ~ predictor 관계의 단순 회귀 모델(simple linear model)의 결과를 보여준다. summary() 함수는 모델의 세부 사항을 표현한다.\n\n\n# If the variable of interest is a numarical variable\nnum_num <- relate(num, Price)\nnum_num\n\n\n\nCall:\nlm(formula = formula_str, data = data)\n\nCoefficients:\n(Intercept)        Price  \n   13.64192     -0.05307  \n\nsummary(num_num)\n\n\n\nCall:\nlm(formula = formula_str, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5224 -1.8442 -0.1459  1.6503  7.5108 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.641915   0.632812  21.558   <2e-16 ***\nPrice       -0.053073   0.005354  -9.912   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.532 on 398 degrees of freedom\nMultiple R-squared:  0.198, Adjusted R-squared:  0.196 \nF-statistic: 98.25 on 1 and 398 DF,  p-value: < 2.2e-16\n\nplot()은 target 변수와 예측변수 사이의 관계를 시각화한다. Sales와 Price 간의 관계는 산점도(scatter plot)로 시각화된다. 왼쪽 그림은 Sales와 Price의 산포도와 회귀선 및 회귀선의 신뢰구간을 나타낸다. 오른쪽 그림은 원 데이터와 선형모델의 예측값 사이의 관계를 산점도로 나타낸 것이다. 두 변수 사이에 선형 관계가 있는 경우 관측치의 산점도는 빨간색 대각선에 수렴한다.\n\n\nplot(num_num)\n\n\n\n\n예측변수가 범주형 변수인 경우\n다음 예제는 ShelveLoc과 target 변수 Sales 사이의 관계를 보여준다. 예측변수인 ShelveLoc은 범주형 변수다. target-predictor 관계의 one-way ANOVA를 수행한 결과를 보여준다. 결과는 분산분석의 관점에서 표현된다. summary() 함수는 예측변수의 각 레벨에 대한 회귀 계수를 보여준다. 다시말해 target ~ predictor 관계의 단순 회귀분석에 대한 상세 정보를 보여준다.\n\n\n# If the variable of interest is a categorical variable\nnum_cat <- relate(num, ShelveLoc)\nnum_cat\n\n\nAnalysis of Variance Table\n\nResponse: Sales\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nShelveLoc   2 1009.5  504.77   92.23 < 2.2e-16 ***\nResiduals 397 2172.7    5.47                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(num_cat)\n\n\n\nCall:\nlm(formula = formula(formula_str), data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3066 -1.6282 -0.0416  1.5666  6.1471 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       5.5229     0.2388  23.131  < 2e-16 ***\nShelveLocGood     4.6911     0.3484  13.464  < 2e-16 ***\nShelveLocMedium   1.7837     0.2864   6.229  1.2e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.339 on 397 degrees of freedom\nMultiple R-squared:  0.3172,    Adjusted R-squared:  0.3138 \nF-statistic: 92.23 on 2 and 397 DF,  p-value: < 2.2e-16\n\nplot ()은 target 변수와 예측변수 사이의 관계를 시각화한다. Sales와 ShelveLoc의 관계는 박스플롯(box plot)으로 표현된다.\n\n\nplot(num_cat)\n\n\n\n\n데이터 변환\ndlookr은 결측치와 이상치의 대체, 치우친 데이터를 보정해준다. 또한 연속형 변수를 범주형 변수로 비닝하는 것을 도와준다.\n다음은 dlookr이 제공하는 데이터 변환 함수와 함수의 기능 목록이다.:\nfind_na()는 결측치가 포함된 변수를 찾아주고, imputate_na()는 결측치를 대체한다.\nfind_outliers()는 이상치가 포함된 변수를 찾아주고, imputate_outlier()는 이상치를 대체한다.\nsummary.imputation()와 plot.imputation()는 대체된 변수의 정보를 보혀주고 시각화를 제공한다.\nfind_skewness()는 치우친 데이터의 변수를 찾아주고, transform()는 치우친 데이터의 보정을 수행한다.\n또한 transform()는 수치형 변수의 표준화를 수행한다.\nsummary.transform()와 plot.transform()는 변환된 변수의 정보를 보혀주고 시각화를 제공한다.\nbinning()와 binning_by()는 수치 데이터를 비닝하여 범주형 데이터로 변환한다.\nprint.bins()와 summary.bins()는 비닝 결과를 보여주고 요약해 준다.\nplot.bins()와 plot.optimal_bins()는 비닝 결과의 시각화를 제공한다.\ntransformation_report()는 데이터 변환을 수행한 후 그 결과를 보고서로 만들어 준다.\n결측치의 대체\nimputate_na()을 이용한 결측치의 대체\nimputate_na()는 변수에 포함된 결측치를 대체한다. 결측치가 포함된 예측변수(predictor)는 수치형 변수와 범주형 변수 모두 지원하며, 다음과 같은 method를 지원한다.\npredictor가 수치형 변수일 경우\n“mean” : 산술평균으로 대체\n“median” : 중위수로 대체\n“mode” : 최빈수로 대체\n“knn” : K-nearest neighbors를 이용한 대체\ntarget 변수를 지정해야 함\n\n“rpart” : Recursive Partitioning and Regression Trees를 이용한 대체\ntarget 변수를 지정해야 함\n\n“mice” : Multivariate Imputation by Chained Equations를 이용한 대체\ntarget 변수를 지정해야 함\nrandom seed를 지정해야 함\n\n\npredictor가 범주형 변수일 경우\n“mode” : 최빈수로 대체\n“rpart” : Recursive Partitioning and Regression Trees를 이용한 대체\ntarget 변수를 지정해야 함\n\n“mice” : Multivariate Imputation by Chained Equations를 이용한 대체\ntarget 변수를 지정해야 함\nrandom seed를 지정해야 함\n\n\n다음처럼 imputate_na()는 carseats의 수치형 변수인 Income를 “rpart” 방법으로 결측치를 대체한다. summary()는 결측치 대체 정보를 요약하고, plot()은 결측정보를 시각화한다.\n\n\nincome <- imputate_na(carseats, Income, US, method = \"rpart\")\n\n# result of imputate\nincome\n\n\n  [1]  73.00000  48.00000  35.00000 100.00000  64.00000 113.00000\n  [7] 105.00000  81.00000 110.00000 113.00000  78.00000  94.00000\n [13]  35.00000  58.63636 117.00000  95.00000  32.00000  74.00000\n [19] 110.00000  76.00000  90.00000  29.00000  46.00000  31.00000\n [25] 119.00000  32.00000 115.00000 118.00000  74.00000  99.00000\n [31]  94.00000  58.00000  32.00000  38.00000  54.00000  84.00000\n [37]  76.00000  41.00000  73.00000  60.00000  98.00000  53.00000\n [43]  69.00000  42.00000  79.00000  63.00000  90.00000  98.00000\n [49]  52.00000  93.00000  32.00000  90.00000  40.00000  64.00000\n [55] 103.00000  81.00000  82.00000  91.00000  93.00000  71.00000\n [61] 102.00000  32.00000  45.00000  88.00000  67.00000  26.00000\n [67]  92.00000  61.00000  69.00000  59.00000  81.00000  51.00000\n [73]  45.00000  90.00000  68.00000 111.00000  87.00000  71.00000\n [79]  48.00000  67.00000 100.00000  72.00000  83.00000  36.00000\n [85]  25.00000 103.00000  84.00000  67.00000  42.00000  56.07143\n [91]  67.14286  46.00000 113.00000  30.00000  97.00000  25.00000\n [97]  42.00000  82.00000  77.00000  47.00000  69.00000  93.00000\n[103]  22.00000  91.00000  96.00000 100.00000  33.00000 107.00000\n[109]  79.00000  65.00000  62.00000 118.00000  99.00000  29.00000\n[115]  87.00000  35.00000  75.00000  75.34722  88.00000  94.00000\n[121] 105.00000  89.00000 100.00000 103.00000 113.00000  78.00000\n[127]  68.00000  48.00000 100.00000 120.00000  84.00000  69.00000\n[133]  87.00000  98.00000  31.00000  94.00000  68.81481  42.00000\n[139] 103.00000  62.00000  60.00000  42.00000  84.00000  88.00000\n[145]  68.00000  63.00000  83.00000  54.00000 119.00000 120.00000\n[151]  84.00000  58.00000  67.77778  36.00000  69.00000  72.00000\n[157]  34.00000  58.00000  90.00000  60.00000  28.00000  21.00000\n[163]  74.00000  64.00000  64.00000  58.00000  67.00000  73.00000\n[169]  89.00000  41.00000  39.00000 106.00000 102.00000  91.00000\n[175]  24.00000  89.00000 107.00000  72.00000  89.86364  25.00000\n[181] 112.00000  83.00000  60.00000  74.00000  33.00000 100.00000\n[187]  51.00000  32.00000  37.00000 117.00000  37.00000  42.00000\n[193]  26.00000  70.00000  56.07143  93.00000  65.50000  61.00000\n[199]  80.00000  88.00000  92.00000  83.00000  78.00000  82.00000\n[205]  80.00000  22.00000  67.00000 105.00000  54.00000  21.00000\n[211]  41.00000 118.00000  69.00000  84.00000 115.00000  83.00000\n[217]  33.00000  44.00000  61.00000  79.00000 120.00000  44.00000\n[223] 119.00000  45.00000  82.00000  25.00000  33.00000  64.00000\n[229]  67.50000 104.00000  60.00000  69.00000  80.00000  76.00000\n[235]  62.00000  32.00000  34.00000  28.00000  24.00000 105.00000\n[241]  80.00000  63.00000  46.00000  68.81481  30.00000  43.00000\n[247]  56.00000 114.00000  52.00000  67.00000 105.00000 111.00000\n[253]  97.00000  24.00000 104.00000  55.55556  40.00000  62.00000\n[259]  38.00000  36.00000 117.00000  42.00000  77.00000  26.00000\n[265]  29.00000  35.00000  93.00000  82.00000  57.00000  69.00000\n[271]  26.00000  56.00000  33.00000 106.00000  93.00000 119.00000\n[277]  69.00000  48.00000 113.00000  57.00000  86.00000  69.00000\n[283]  96.00000 110.00000  46.00000  26.00000 118.00000  44.00000\n[289]  40.00000  77.00000 111.00000  70.00000  66.00000  84.00000\n[295]  76.00000  35.00000  44.00000  83.00000  75.34722  40.00000\n[301]  78.00000  93.00000  77.00000  52.00000  98.00000  67.77778\n[307]  32.00000  92.00000  80.00000 111.00000  65.00000  68.00000\n[313] 117.00000  81.00000  33.00000  21.00000  36.00000  30.00000\n[319]  72.00000  45.00000  70.00000  39.00000  50.00000 105.00000\n[325]  65.00000  69.00000  30.00000  41.72727  66.00000  54.00000\n[331]  59.00000  63.00000  33.00000  60.00000 117.00000  70.00000\n[337]  35.00000  38.00000  24.00000  44.00000  29.00000 120.00000\n[343] 102.00000  42.00000  80.00000  68.00000 107.00000  31.85714\n[349] 102.00000  27.00000 101.00000 115.00000 103.00000  67.00000\n[355]  68.81481 100.00000 109.00000  73.00000  96.00000  62.00000\n[361]  86.00000  25.00000  55.00000  75.00000  21.00000  30.00000\n[367]  56.00000 106.00000  22.00000 100.00000  41.00000  81.00000\n[373]  50.00000  83.77778  47.00000  46.00000  60.00000  61.00000\n[379]  88.00000 111.00000  64.00000  65.00000  28.00000 117.00000\n[385]  37.00000  73.00000 116.00000  75.34722  89.00000  42.00000\n[391]  75.00000  63.00000  42.00000  51.00000  58.00000 108.00000\n[397]  23.00000  26.00000  47.50000  37.00000\nattr(,\"var_type\")\n[1] \"numerical\"\nattr(,\"method\")\n[1] \"rpart\"\nattr(,\"na_pos\")\n [1]  14  90  91 118 137 153 179 195 197 229 244 256 299 306 328 348\n[17] 355 374 388 399\nattr(,\"type\")\n[1] \"missing values\"\nattr(,\"message\")\n[1] \"complete imputation\"\nattr(,\"success\")\n[1] TRUE\nattr(,\"class\")\n[1] \"imputation\" \"numeric\"   \n\n# summary of imputate\nsummary(income)\n\n\n* Impute missing values based on Recursive Partitioning and Regression Trees\n - method : rpart\n\n* Information of Imputation (before vs after)\n             Original   Imputation\nn        380.00000000 400.00000000\nna        20.00000000   0.00000000\nmean      69.32105263  69.07811282\nsd        28.06686473  27.53886441\nse_mean    1.43979978   1.37694322\nIQR       48.00000000  45.50000000\nskewness   0.03601821   0.05313579\nkurtosis  -1.10286001  -1.04030028\np00       21.00000000  21.00000000\np01       21.79000000  21.99000000\np05       26.00000000  26.00000000\np10       31.90000000  32.00000000\np20       40.00000000  41.00000000\np25       44.00000000  44.75000000\np30       50.00000000  51.00000000\np40       62.00000000  62.00000000\np50       69.00000000  69.00000000\np60       78.00000000  77.00000000\np70       87.30000000  84.60000000\np75       92.00000000  90.25000000\np80       98.00000000  96.00000000\np90      108.10000000 107.00000000\np95      115.05000000 115.00000000\np99      119.21000000 119.01000000\np100     120.00000000 120.00000000\n\n# viz of imputate\nplot(income)\n\n\n\n\n다음은 범주형 변수인 urban을 “mice” 방법으로 결측치를 대체한다. summary()는 결측치 대체 정보를 요약하고, plot()은 결측정보를 시각화한다.\n\n\nlibrary(mice)\n\nurban <- imputate_na(carseats, Urban, US, method = \"mice\")\n\n\n\n iter imp variable\n  1   1  Income  Urban\n  1   2  Income  Urban\n  1   3  Income  Urban\n  1   4  Income  Urban\n  1   5  Income  Urban\n  2   1  Income  Urban\n  2   2  Income  Urban\n  2   3  Income  Urban\n  2   4  Income  Urban\n  2   5  Income  Urban\n  3   1  Income  Urban\n  3   2  Income  Urban\n  3   3  Income  Urban\n  3   4  Income  Urban\n  3   5  Income  Urban\n  4   1  Income  Urban\n  4   2  Income  Urban\n  4   3  Income  Urban\n  4   4  Income  Urban\n  4   5  Income  Urban\n  5   1  Income  Urban\n  5   2  Income  Urban\n  5   3  Income  Urban\n  5   4  Income  Urban\n  5   5  Income  Urban\n\n# result of imputate\nurban\n\n\n  [1] Yes Yes Yes Yes Yes No  Yes Yes No  No  No  Yes Yes Yes Yes No \n [17] Yes Yes No  Yes Yes No  Yes Yes Yes No  No  Yes Yes Yes Yes Yes\n [33] No  Yes Yes No  No  Yes Yes No  No  Yes Yes Yes Yes Yes No  Yes\n [49] Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes No  Yes Yes\n [65] No  No  Yes Yes Yes Yes Yes No  Yes No  No  No  Yes No  Yes Yes\n [81] Yes Yes Yes Yes No  No  Yes No  Yes Yes No  Yes Yes Yes Yes Yes\n [97] No  Yes No  No  No  Yes No  Yes Yes Yes No  Yes Yes No  Yes Yes\n[113] Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes No  Yes No  Yes Yes\n[129] Yes No  Yes Yes Yes Yes Yes No  No  Yes Yes No  Yes Yes Yes Yes\n[145] No  Yes Yes No  No  Yes No  No  No  No  No  Yes Yes No  Yes No \n[161] No  No  Yes No  No  Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes\n[177] No  Yes No  Yes Yes Yes Yes Yes No  Yes No  Yes Yes No  No  Yes\n[193] No  Yes Yes Yes Yes Yes Yes Yes No  Yes No  Yes Yes Yes Yes No \n[209] Yes No  No  Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes\n[225] No  Yes Yes Yes No  No  No  No  Yes No  No  Yes Yes Yes Yes Yes\n[241] Yes Yes No  Yes Yes No  Yes Yes Yes Yes Yes No  Yes No  Yes Yes\n[257] Yes Yes No  No  Yes Yes Yes Yes Yes Yes No  No  Yes Yes Yes Yes\n[273] Yes Yes Yes Yes Yes Yes No  Yes No  No  No  No  No  Yes No  Yes\n[289] No  Yes No  Yes Yes Yes Yes No  Yes Yes Yes No  Yes Yes Yes Yes\n[305] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  No  No \n[321] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes No  Yes\n[337] Yes Yes Yes Yes Yes No  No  Yes No  Yes No  No  Yes No  No  No \n[353] Yes No  Yes Yes Yes Yes Yes Yes No  No  Yes Yes Yes No  No  Yes\n[369] No  Yes Yes Yes No  Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes\n[385] Yes Yes Yes No  Yes Yes Yes Yes Yes No  Yes Yes No  Yes Yes Yes\nattr(,\"var_type\")\n[1] categorical\nattr(,\"method\")\n[1] mice\nattr(,\"na_pos\")\n [1]  38  90 159 206 237 252 281 283 335 378\nattr(,\"seed\")\n[1] 89689\nattr(,\"type\")\n[1] missing values\nattr(,\"message\")\n[1] complete imputation\nattr(,\"success\")\n[1] TRUE\nLevels: No Yes\n\n# summary of imputate\nsummary(urban)\n\n\n* Impute missing values based on Multivariate Imputation by Chained Equations\n - method : mice\n - random seed : 89689\n\n* Information of Imputation (before vs after)\n     original imputation original_percent imputation_percent\nNo        115        119            28.75              29.75\nYes       275        281            68.75              70.25\n<NA>       10          0             2.50               0.00\n\n# viz of imputate\nplot(urban)\n\n\n\n\ndplyr과의 협업\n다음은 dplyr를 이용해서 이상치를 대체한 Income 변수를 US의 수준별로 산술평균을 구하는 예제다.\n\n\n# The mean before and after the imputation of the Income variable\ncarseats %>%\n  mutate(Income_imp = imputate_na(carseats, Income, US, method = \"knn\")) %>%\n  group_by(US) %>%\n  summarise(orig = mean(Income, na.rm = TRUE),\n    imputation = mean(Income_imp))\n\n\n# A tibble: 2 x 3\n  US     orig imputation\n  <fct> <dbl>      <dbl>\n1 No     65.7       65.5\n2 Yes    71.3       71.5\n\n이상치의 대체\nimputate_outlier()을 이용한 이상치의 대체\nimputate_outlier()는 변수에 포함된 이상치를 대체한다. 이상치가 포함된 예측변수(predictor)는 수치형 변수만 지원하며, 다음과 같은 method를 지원한다.\npredictor가 수치형 변수일 경우\n“mean” : 산술평균으로 대체\n“median” : 중위수로 대체\n“mode” : 최빈수로 대체\n“capping” : 상위 이상치를 95/백분위수로 대체하고 하위 이상치를 5/백분위수로 대체\n\n다음처럼 imputate_outlier()는 carseats의 수치형 변수인 Price를 “capping” 방법으로 이상치를 대체한다. summary()는 이상치 대체 정보를 요약하고, plot()은 결측정보를 시각화한다.\n\n\nprice <- imputate_outlier(carseats, Price, method = \"capping\")\n\n# result of imputate\nprice\n\n\n  [1] 120.00  83.00  80.00  97.00 128.00  72.00 108.00 120.00 124.00\n [10] 124.00 100.00  94.00 136.00  86.00 118.00 144.00 110.00 131.00\n [19]  68.00 121.00 131.00 109.00 138.00 109.00 113.00  82.00 131.00\n [28] 107.00  97.00 102.00  89.00 131.00 137.00 128.00 128.00  96.00\n [37] 100.00 110.00 102.00 138.00 126.00 124.00  77.00 134.00  95.00\n [46] 135.00  70.00 108.00  98.00 149.00 108.00 108.00 129.00 119.00\n [55] 144.00 154.00  84.00 117.00 103.00 114.00 123.00 107.00 133.00\n [64] 101.00 104.00 128.00  91.00 115.00 134.00  99.00  99.00 150.00\n [73] 116.00 104.00 136.00  92.00  70.00  89.00 145.00  90.00  79.00\n [82] 128.00 139.00  94.00 121.00 112.00 134.00 126.00 111.00 119.00\n [91] 103.00 107.00 125.00 104.00  84.00 148.00 132.00 129.00 127.00\n[100] 107.00 106.00 118.00  97.00  96.00 138.00  97.00 139.00 108.00\n[109] 103.00  90.00 116.00 151.00 125.00 127.00 106.00 129.00 128.00\n[118] 119.00  99.00 128.00 131.00  87.00 108.00 155.00 120.00  77.00\n[127] 133.00 116.00 126.00 147.00  77.00  94.00 136.00  97.00 131.00\n[136] 120.00 120.00 118.00 109.00  94.00 129.00 131.00 104.00 159.00\n[145] 123.00 117.00 131.00 119.00  97.00  87.00 114.00 103.00 128.00\n[154] 150.00 110.00  69.00 157.00  90.00 112.00  70.00 111.00 160.00\n[163] 149.00 106.00 141.00 155.05 137.00  93.00 117.00  77.00 118.00\n[172]  55.00 110.00 128.00 155.05 122.00 154.00  94.00  81.00 116.00\n[181] 149.00  91.00 140.00 102.00  97.00 107.00  86.00  96.00  90.00\n[190] 104.00 101.00 173.00  93.00  96.00 128.00 112.00 133.00 138.00\n[199] 128.00 126.00 146.00 134.00 130.00 157.00 124.00 132.00 160.00\n[208]  97.00  64.00  90.00 123.00 120.00 105.00 139.00 107.00 144.00\n[217] 144.00 111.00 120.00 116.00 124.00 107.00 145.00 125.00 141.00\n[226]  82.00 122.00 101.00 163.00  72.00 114.00 122.00 105.00 120.00\n[235] 129.00 132.00 108.00 135.00 133.00 118.00 121.00  94.00 135.00\n[244] 110.00 100.00  88.00  90.00 151.00 101.00 117.00 156.00 132.00\n[253] 117.00 122.00 129.00  81.00 144.00 112.00  81.00 100.00 101.00\n[262] 118.00 132.00 115.00 159.00 129.00 112.00 112.00 105.00 166.00\n[271]  89.00 110.00  63.00  86.00 119.00 132.00 130.00 125.00 151.00\n[280] 158.00 145.00 105.00 154.00 117.00  96.00 131.00 113.00  72.00\n[289]  97.00 156.00 103.00  89.00  74.00  89.00  99.00 137.00 123.00\n[298] 104.00 130.00  96.00  99.00  87.00 110.00  99.00 134.00 132.00\n[307] 133.00 120.00 126.00  80.00 166.00 132.00 135.00  54.00 129.00\n[316] 171.00  72.00 136.00 130.00 129.00 152.00  98.00 139.00 103.00\n[325] 150.00 104.00 122.00 104.00 111.00  89.00 112.00 134.00 104.00\n[334] 147.00  83.00 110.00 143.00 102.00 101.00 126.00  91.00  93.00\n[343] 118.00 121.00 126.00 149.00 125.00 112.00 107.00  96.00  91.00\n[352] 105.00 122.00  92.00 145.00 146.00 164.00  72.00 118.00 130.00\n[361] 114.00 104.00 110.00 108.00 131.00 162.00 134.00  77.00  79.00\n[370] 122.00 119.00 126.00  98.00 116.00 118.00 124.00  92.00 125.00\n[379] 119.00 107.00  89.00 151.00 121.00  68.00 112.00 132.00 160.00\n[388] 115.00  78.00 107.00 111.00 124.00 130.00 120.00 139.00 128.00\n[397] 120.00 159.00  95.00 120.00\nattr(,\"method\")\n[1] \"capping\"\nattr(,\"var_type\")\n[1] \"numerical\"\nattr(,\"outlier_pos\")\n[1]  43 126 166 175 368\nattr(,\"outliers\")\n[1]  24  49 191 185  53\nattr(,\"type\")\n[1] \"outliers\"\nattr(,\"message\")\n[1] \"complete imputation\"\nattr(,\"success\")\n[1] TRUE\nattr(,\"class\")\n[1] \"imputation\" \"numeric\"   \n\n# summary of imputate\nsummary(price)\n\n\nImpute outliers with capping\n\n* Information of Imputation (before vs after)\n            Original  Imputation\nn        400.0000000 400.0000000\nna         0.0000000   0.0000000\nmean     115.7950000 115.8927500\nsd        23.6766644  22.6109187\nse_mean    1.1838332   1.1305459\nIQR       31.0000000  31.0000000\nskewness  -0.1252862  -0.0461621\nkurtosis   0.4518850  -0.3030578\np00       24.0000000  54.0000000\np01       54.9900000  67.9600000\np05       77.0000000  77.0000000\np10       87.0000000  87.0000000\np20       96.8000000  96.8000000\np25      100.0000000 100.0000000\np30      104.0000000 104.0000000\np40      110.0000000 110.0000000\np50      117.0000000 117.0000000\np60      122.0000000 122.0000000\np70      128.3000000 128.3000000\np75      131.0000000 131.0000000\np80      134.0000000 134.0000000\np90      146.0000000 146.0000000\np95      155.0500000 155.0025000\np99      166.0500000 164.0200000\np100     191.0000000 173.0000000\n\n# viz of imputate\nplot(price)\n\n\n\n\ndplyr과의 협업\n다음은 dplyr를 이용해서 이상치를 대체한 Price 변수를 US의 수준별로 산술평균을 구하는 예제다.\n\n\n# The mean before and after the imputation of the Price variable\ncarseats %>%\n  mutate(Price_imp = imputate_outlier(carseats, Price, method = \"capping\")) %>%\n  group_by(US) %>%\n  summarise(orig = mean(Price, na.rm = TRUE),\n    imputation = mean(Price_imp, na.rm = TRUE))\n\n\n# A tibble: 2 x 3\n  US     orig imputation\n  <fct> <dbl>      <dbl>\n1 No     114.       114.\n2 Yes    117.       117.\n\n표준화와 치우친 데이터의 보정\ntransform()의 기능\ntransform()는 변수를 변환한다. 수치형 변수만 지원하며, 다음과 같은 method를 제공한다.\n표준화\n“zscore” : z-score 변환. (x - mu) / sigma\n“minmax” : minmax 변환. (x - min) / (max - min)\n\n치우침 보정\n“log” : log 변환. log(x)\n“log+1” : log 변환. log(x + 1). 0을 포함한 값들이 많을 때 유용함.\n“sqrt” : 제곱근 변환\n“1/x” : 1 / x 변환\n“x^2” : 제곱 변환\n“x^3” : 세제곱 변환\n\ntransform()을 이용한 표준화\n표준화를 수행하는 method “zscore”와 “minmax”를 이용한다.\n\n\ncarseats %>% \n  mutate(Income_minmax = transform(carseats$Income, method = \"minmax\"),\n    Sales_minmax = transform(carseats$Sales, method = \"minmax\")) %>% \n  select(Income_minmax, Sales_minmax) %>% \n  boxplot()\n\n\n\n\ntransform()을 이용한 치우친 데이터의 보정\nfind_skewness()는 치우친 데이터를 찾기 위해서 왜도를 구한다.\n\n\n# find index of skewed variables\nfind_skewness(carseats)\n\n\n[1] 4\n\n# find names of skewed variables\nfind_skewness(carseats, index = FALSE)\n\n\n[1] \"Advertising\"\n\n# compute the skewness\nfind_skewness(carseats, value = TRUE)\n\n\n      Sales   CompPrice      Income Advertising  Population \n      0.185      -0.043       0.036       0.637      -0.051 \n      Price         Age   Education \n     -0.125      -0.077       0.044 \n\n# compute the skewness & filtering with threshold\nfind_skewness(carseats, value = TRUE, thres = 0.1)\n\n\n      Sales Advertising       Price \n      0.185       0.637      -0.125 \n\nAdvertising의 왜도가 0.637로 좌측으로 어느정도 기울어져 있어서 다음처럼 transformation()를 이용해서 “log” 방법으로 변환한다. summary()는 변환정보를 요약하고, plot()은 변환정보를 시각화한다.\n\n\nAdvertising_log = transform(carseats$Advertising, method = \"log\")\n\n# result of transformation\nhead(Advertising_log)\n\n\n[1] 2.397895 2.772589 2.302585 1.386294 1.098612 2.564949\n\n# summary of transformation\nsummary(Advertising_log)\n\n\n* Resolving Skewness with log\n\n* Information of Transformation (before vs after)\n            Original Transformation\nn        400.0000000    400.0000000\nna         0.0000000      0.0000000\nmean       6.6350000           -Inf\nsd         6.6503642            NaN\nse_mean    0.3325182            NaN\nIQR       12.0000000            Inf\nskewness   0.6395858            NaN\nkurtosis  -0.5451178            NaN\np00        0.0000000           -Inf\np01        0.0000000           -Inf\np05        0.0000000           -Inf\np10        0.0000000           -Inf\np20        0.0000000           -Inf\np25        0.0000000           -Inf\np30        0.0000000           -Inf\np40        2.0000000      0.6931472\np50        5.0000000      1.6094379\np60        8.4000000      2.1265548\np70       11.0000000      2.3978953\np75       12.0000000      2.4849066\np80       13.0000000      2.5649494\np90       16.0000000      2.7725887\np95       19.0000000      2.9444390\np99       23.0100000      3.1359198\np100      29.0000000      3.3672958\n\n# viz of transformation\nplot(Advertising_log)\n\n\n\n\nlog 변환된 값에 -Inf가 포함되어 있는 것으로 보아 원 데이터에 0이 포함되어 있는 듯하다. 그래서 이번에는 “log+1” 방법으로 변환한다.\n\n\nAdvertising_log <- transform(carseats$Advertising, method = \"log+1\")\n\n# result of transformation\nhead(Advertising_log)\n\n\n[1] 2.484907 2.833213 2.397895 1.609438 1.386294 2.639057\n\n# summary of transformation\nsummary(Advertising_log)\n\n\n* Resolving Skewness with log+1\n\n* Information of Transformation (before vs after)\n            Original Transformation\nn        400.0000000   400.00000000\nna         0.0000000     0.00000000\nmean       6.6350000     1.46247709\nsd         6.6503642     1.19436323\nse_mean    0.3325182     0.05971816\nIQR       12.0000000     2.56494936\nskewness   0.6395858    -0.19852549\nkurtosis  -0.5451178    -1.66342876\np00        0.0000000     0.00000000\np01        0.0000000     0.00000000\np05        0.0000000     0.00000000\np10        0.0000000     0.00000000\np20        0.0000000     0.00000000\np25        0.0000000     0.00000000\np30        0.0000000     0.00000000\np40        2.0000000     1.09861229\np50        5.0000000     1.79175947\np60        8.4000000     2.23936878\np70       11.0000000     2.48490665\np75       12.0000000     2.56494936\np80       13.0000000     2.63905733\np90       16.0000000     2.83321334\np95       19.0000000     2.99573227\np99       23.0100000     3.17846205\np100      29.0000000     3.40119738\n\n# viz of transformation\nplot(Advertising_log)\n\n\n\n\nBinning\nbinning()을 이용한 개별 변수의 Binning\nbinning()는 수치형 변수를 비닝하여 범주형 변수로 변환한다. 다음과 같은 type의 비닝을 지원한다.\n“quantile” : 동일한 돗수가 포함되도록 quantile을 이용하여 범주화\n“equal” : 동일한 길이의 구간을 갖도록 범주화\n“pretty” : 적당히 보기 좋은 구간으로 범주화\n“kmeans” : K-means clustering 기법을 이용한 범주화\n“bclust” : Bagged clustering 기법을 이용한 범주화\nbinning()을 이용하여 Income을 비닝하는 몇 가지의 방법을 예시한다.:\n\n\n# Binning the carat variable. default type argument is \"quantile\"\nbin <- binning(carseats$Income)\n# Print bins class object\nbin\n\n\nbinned type: quantile\nnumber of bins: 10\nx\n [21,31.36667]  (31.36667,40]        (40,50]        (50,62] \n            38             40             37             40 \n       (62,69]        (69,78]  (78,87.56667]  (87.56667,98] \n            40             34             37             41 \n (98,108.6333] (108.6333,120]           <NA> \n            35             38             20 \n\n# Summarise bins class object\nsummary(bin)\n\n\n           levels freq   rate\n1   [21,31.36667]   38 0.0950\n2   (31.36667,40]   40 0.1000\n3         (40,50]   37 0.0925\n4         (50,62]   40 0.1000\n5         (62,69]   40 0.1000\n6         (69,78]   34 0.0850\n7   (78,87.56667]   37 0.0925\n8   (87.56667,98]   41 0.1025\n9   (98,108.6333]   35 0.0875\n10 (108.6333,120]   38 0.0950\n11           <NA>   20 0.0500\n\n# Plot bins class object\nplot(bin)\n\n\n\n# Using labels argument\nbin <- binning(carseats$Income, nbins = 4,\n              labels = c(\"LQ1\", \"UQ1\", \"LQ3\", \"UQ3\"))\nbin\n\n\nbinned type: quantile\nnumber of bins: 4\nx\n LQ1  UQ1  LQ3  UQ3 <NA> \n  98   97   91   94   20 \n\n# Using another type argument\nbinning(carseats$Income, nbins = 5, type = \"equal\")\n\n\nbinned type: equal\nnumber of bins: 5\nx\n   [21,40.8]  (40.8,60.6]  (60.6,80.4] (80.4,100.2]  (100.2,120] \n          78           68           92           79           63 \n        <NA> \n          20 \n\nbinning(carseats$Income, nbins = 5, type = \"pretty\")\n\n\nbinned type: pretty\nnumber of bins: 5\nx\n  [20,40]   (40,60]   (60,80]  (80,100] (100,120]      <NA> \n       78        68        92        79        63        20 \n\nbinning(carseats$Income, nbins = 5, type = \"kmeans\")\n\n\nbinned type: kmeans\nnumber of bins: 5\nx\n     [21,49]    (49,70.5]  (70.5,87.5] (87.5,104.5]  (104.5,120] \n         113           86           67           63           51 \n        <NA> \n          20 \n\nbinning(carseats$Income, nbins = 5, type = \"bclust\")\n\n\nbinned type: bclust\nnumber of bins: 5\nx\n  [21,50.5] (50.5,65.5]   (65.5,85]  (85,108.5] (108.5,120] \n        115          55          91          81          38 \n       <NA> \n         20 \n\n# -------------------------\n# Using pipes & dplyr\n# -------------------------\nlibrary(dplyr)\n\ncarseats %>%\n mutate(Income_bin = binning(carseats$Income)) %>%\n group_by(ShelveLoc, Income_bin) %>%\n summarise(freq = n()) %>%\n arrange(desc(freq)) %>%\n head(10)\n\n\n# A tibble: 10 x 3\n# Groups:   ShelveLoc [1]\n   ShelveLoc Income_bin      freq\n   <fct>     <ord>          <int>\n 1 Medium    [21,31.36667]     25\n 2 Medium    (62,69]           23\n 3 Medium    (50,62]           22\n 4 Medium    (31.36667,40]     21\n 5 Medium    (40,50]           20\n 6 Medium    (69,78]           20\n 7 Medium    (108.6333,120]    20\n 8 Medium    (78,87.56667]     19\n 9 Medium    (87.56667,98]     19\n10 Medium    (98,108.6333]     19\n\nbinning_by()을 이용한 Optimal Binning\nbinning_by()는 수치형 변수를 Optimal Binning하여 범주형 변수로 변환한다. 이 방법은 스코어카드 모형을 개발할때 자주 사용하는 방법이다.\n다음의 binning_by() 예제는 US가 binary class를 갖는 target 변수일 경우에 Advertising를 Optimal Binning하는 방법의 예시다.\n\n\n# optimal binning\nbin <- binning_by(carseats, \"US\", \"Advertising\")\n\n\n` US ` ~ ` Advertising `\n<environment: 0x7fd84661f238>\n\nbin\n\n\nbinned type: optimal\nnumber of bins: 3\nx\n[-1,0]  (0,6] (6,29] \n   144     69    187 \n\n# summary optimal_bins class\nsummary(bin)\n\n\n── Binning Table ──────────────────────── Several Metrics ── \n     Bin CntRec CntPos CntNeg RatePos RateNeg    Odds      WoE\n1 [-1,0]    144     19    125 0.07364 0.88028  0.1520 -2.48101\n2  (0,6]     69     54     15 0.20930 0.10563  3.6000  0.68380\n3 (6,29]    187    185      2 0.71705 0.01408 92.5000  3.93008\n4  Total    400    258    142 1.00000 1.00000  1.8169       NA\n       IV     JSD     AUC\n1 2.00128 0.20093 0.03241\n2 0.07089 0.00869 0.01883\n3 2.76272 0.21861 0.00903\n4 4.83489 0.42823 0.06028\n\n── General Metrics ───────────────────────────────────────── \n• Gini index                       :  -0.87944\n• IV (Jeffrey)                     :  4.83489\n• JS (Jensen-Shannon) Divergence   :  0.42823\n• Kolmogorov-Smirnov Statistics    :  0.80664\n• HHI (Herfindahl-Hirschman Index) :  0.37791\n• HHI (normalized)                 :  0.06687\n• Cramer's V                       :  0.81863 \n\n── Significance Tests ──────────────────── Chisquare Test ── \n   Bin A  Bin B statistics      p_value\n1 [-1,0]  (0,6]   87.67064 7.731349e-21\n2  (0,6] (6,29]   34.73349 3.780706e-09\n\n# information value \nattr(bin, \"iv\")\n\n\nNULL\n\n# information value table\nattr(bin, \"ivtable\")\n\n\nNULL\n\n# visualize optimal_bins class\nplot(bin, sub = \"bins of Advertising variable\")\n\n\n\n\n보고서 생성\ndiagnose_report()를 이용한 진단 보고서 작성\ndiagnose_report()는 데이터 프레임이나 데이터 프레임을 상속받은 객체(tbl_df, tbl 등)의 모든 변수들에 대해서 데이터 진단을 수행한다.\ndiagnose_report()는 진단 보고서를 다음과 같은 두 개의 형태로 작성한다.\nLatex에 기반한 pdf 파일\nhtml 파일\n보고서의 목차는 다음과 같다.\n데이터 진단\n데이터 품질 총괄\n전체변수 품질현황 목록\n결측치 진단\n유일값 진단(문자형과 범주형)\n유일값 진단(수치형)\n\n데이터 품질 상세\n범주형 변수 품질 현황\n수치형 변수 품질 현황\n수치변수 품질현황 (zero)\n수치변수 품질현황 (minus)\n\n\n이상치 진단\n데이터 품질 총괄\n수치변수의 이상치 진단\n이상치 상세 진단\n\n\n다음은 tbl_df 클래스 객체인 flights의 품질진단 리포트를 작성한다. 파일 형식은 pdf이며, 파일이름은 DataDiagnosis_Report.pdf다.\n\n\nflights %>%\n  diagnose_report()\n\n\n\n다음은 DataDiagnosis_Report.html라는 이름의 html 형식의 보고서를 생성한다.\n\n\nflights %>%\n  diagnose_report(output_format = \"html\")\n\n\n\n다음은 Diagn.html라는 이름의 html 형식의 보고서를 생성한다.\n\n\nflights %>%\n  diagnose_report(output_format = \"html\", output_file = \"Diagn.html\")\n\n\n\n데이터 진단 보고서는 데이터 진단 과정에 도움을 주기 위한 자동화 보고서다. 보고서 결과를 참고하여 데이터 보완이나 재획득을 판단한다.\neda_report()를 이용한 EDA 보고서 작성\neda_report()는 데이터 프레임이나 데이터 프레임을 상속받은 객체(tbl_df, tbl 등)의 모든 변수들에 대해서 EDA를 수행한다.\neda_report()는 EDA 보고서를 다음과 같은 두 개의 형태로 작성한다.\nLatex에 기반한 pdf 파일\nhtml 파일\n보고서의 목차는 다음과 같다.\n개요\n데이터셋 정보\n변수 정보\n수치변수\n\n일변량 변수 EDA\n기술통계\n수치변수의 정규성 검정\n통계량과 시각화 정보\n\n\n변수들간의 관계\n상관계수\n변수 조합의 상관계수\n수치변수의 상관행렬 플롯\n\n\nTarget 변수 기반의 EDA\n그룹화된 기술통계\n그룹화된 Target과 수치형 변수 관계\n그룹화된 Target과 범주형 변수 관계\n\n그룹화된 Target과 변수들간의 관계\n그룹화된 Target과 수치형 변수의 상관계수\n그룹화된 Target과 수치형 변수의 상관행렬 플롯\n\n\n다음은 carseats의 품질진단 리포트를 작성한다. 파일 형식은 pdf이며, 파일이름은 EDA_Report.pdf다.\n\n\ncarseats %>%\n  eda_report(target = Sales)\n\n\n\n다음은 EDA.html라는 이름의 html 형식의 보고서를 생성한다.\n\n\ncarseats %>%\n  eda_report(target = Sales, output_format = \"html\", output_file = \"EDA.html\")\n\n\n\nEDA 보고서는 EDA 과정에 도움을 주기 위한 자동화 보고서다. 보고서 결과를 참고하여 데이터 분석 시나리오를 설계한다.\ntransformation_report()를 이용한 데이터변환 보고서 작성\ntransformation_report()는 데이터 프레임이나 데이터 프레임을 상속받은 객체(tbl_df, tbl 등)의 모든 변수들에 대해서 데이터변환 보고서를 작성한다.\ntransformation_report()는 데이터변환 보고서를 다음과 같은 두 개의 형태로 작성한다.\nLatex에 기반한 pdf 파일\nhtml 파일\n보고서의 목차는 다음과 같다.:\n값의 대체\n결측치\n결측치의 대체 정보\n(해당 변수들)\n\n이상치\n이상치의 대체 정보\n(해당 변수들)\n\n\n치우침의 해결\n치우친 변수들의 정보\n(해당 변수들)\n\n\n비닝\n비닝을 위한 수치형 변수들\n비닝\n(해당 변수들)\n\nOptimal 비닝\n(해당 변수들)\n\n\n다음은 carseats의 데이터변환 보고서를 작성한다. 파일 형식은 pdf이며, 파일이름은 Transformation_Report.pdf다.\n\n\ncarseats %>%\n  transformation_report(target = US)\n\n\n\n다음은 transformation.html라는 이름의 html 형식의 보고서를 생성한다.\n\n\ncarseats %>%\n  transformation_report(target = US, output_format = \"html\", \n    output_file = \"transformation.html\")\n\n\n\n데이터변환 보고서는 데이터 변환 과정에 도움을 주기 위한 자동화 보고서다. 보고서 결과를 참고하여 데이터 변환 시나리오를 설계한다.\n데이터변환 리포트 내용\npdf 파일의 내용\n보고서의 표지\n보고서의 표지는 다음 그림과 같다.:\n\n\n\n(#fig:eda_title_pdf)EDA report cover\n\n\n\n보고서 차례\n보고서의 차례는 다음 그림과 같다.:\n\n\n\nFigure 1: EDA Report Contents\n\n\n\n표의 표현\n대부분의 정보는 보고서에 표로 표시된다. 다음 예제는 표의 예를 보여준다.:\n\n\n\nFigure 2: Sample data diagnostic report table\n\n\n\n시각화의 표현\nEDA 보고서에서는 선형 관계에 대한 정보를 표와 시각화 결과로 표현한다. 결과는 다음 그림과 같다.:\n\n\n\nFigure 3: Linear relationship information in EDA reports\n\n\n\nhtml 파일의 내용\n보고서의 표지 및 차례\n보고서의 표지와 차례는 다음 그림과 같다.::\n\n\n\nFigure 4: EDA report titles and table of contents\n\n\n\n표의 표현\n많은 정보가 보고서에서 표로 표현된다. 다음 그림은 html 파일에서의 표의 예시다.:\n\n\n\nFigure 5: EDA report table example (Web)\n\n\n\n시각화의 표현\nEDA 보고서에서 정규성 검정 정보는 시각화 결과를 포함한다. html 파일에서의 결과는 다음 그림과 같다.:\n\n\n\nFigure 6: EDA Report Normality Test Information (Web)\n\n\n\n\n\n\n",
    "preview": "posts/2018-05-12-r-dlookr/2018-05-12-r-dlookr_files/figure-html5/plot_outlier_pipe-1.png",
    "last_modified": "2021-10-23T09:14:34+09:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 768
  },
  {
    "path": "posts/2018-05-07-r-vignette01.ko/",
    "title": "dlookr - 데이터 품질 진단",
    "description": "이 문서는 dlookr 기능 중에서 **데이터 품질 진단** 기능을 소개한다. 여러분은 dlookr에서 제공하는 함수로 데이터 프레임과 데이터 프레임을 상속한 tbl_df 데이터의 품질을 진단하는 방법을 일힐 수 있을 것이다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2018-05-07",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n일러두기\nData: nycflights13\n데이터 진단\ndiagnose()을 이용한 변수의 개괄적 진단\ndiagnose_numeric()을 이용한 수치형 변수의 상세 진단\ndiagnose_category()을 이용한 범주형 변수의 상세 진단\ndiagnose_outlier()를 이용한 이상치 진단\nplot_outlier()를 이용한 이상치의 시각화\ndiagnose_report()를 이용한 진단 보고서 작성\n진단 리포트 내용\n\n\n\n\n\n일러두기\n“본 vignett는 dplyr 패키지와의 상호 운용성을 강조하기 위해서 dplyr 패키지의 vignett인 ’Introduction to dplyr’을 참고하였다.”\n분석을 위한 데이터를 획득한 후에는 다음을 수행해야 한다.:\n데이터의 품질을 진단한다.\n만약 데이터 품질의 문제를 발견한다면,\n문제의 데이터를 보완하거나 경우에 따라서는 재획득을 수행햐야 한다.\n\n데이터를 이해하기 위한 탐색을 수행하여, 분석의 전개 방향에 대한 시나리오를 수립한다.\n분석에 효과적인 변수를 파생하거나 변수의 변환을 수행한다.\ndlookr 패키지는 다음의 과정을 빠르고 쉽게 수행하도록 도움을 준다.\n데이터의 진단을 수행하거나 데이터 품질 진단 리포트를 자동으로 생성한다.\n다양한 방법으로 데이터를 탐색하고 EDA(탐색적 데이터 분석) 보고서를 생성한다.\n연속형 변수를 비닝(binning)하여 범주형 변수로 만들고, 비닝의 의사결정에 도움을 주는 리포트를 생성한다.\n이 문서는 dlookr 기능 중에서 데이터 품질 진단 기능을 소개한다. 여러분은 dlookr에서 제공하는 함수로 데이터 프레임과 데이터 프레임을 상속한 tbl_df 데이터의 품질을 진단하는 방법을 일힐 수 있을 것이다.\ndlookr 패키지는 dplyr 패키지와 함께 사용하면 시너지가 증가된다. 특히 데이터 탐색 및 데이터 조작에서 tidyverse 패키지 그룹의 효율성을 높여준다.\nData: nycflights13\ndlookr 패키지의 기초적인 사용 방법을 설명하기 위해서 nycflights13 패키지의 flights 데이터를 사용한다. flights 데이터 프레임은 2013년 NYC를 출발한 모든 항공편에 출발과 도착에 대한 정보를 담은 데이터다.\n\n\nlibrary(nycflights13)\ndim(flights)\n\n\n[1] 336776     19\n\nflights\n\n\n# A tibble: 336,776 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n# … with 336,772 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\n데이터 진단\ndlookr은 데이터를 진단하여 데이터 분석에 사용할 수 없는 변수를 선별하거나, 보정이 필요한 변수를 찾아낸는 것을 목표로 한다.:\ndiagnose()는 변수들의 기본적인 진단 정보를 제공한다.\ndiagnose_category()는 범주형 변수들의 상세 진단 정보를 제공한다.\ndiagnose_numeric()는 수치형 변수들의 상세 진단 정보를 제공한다.\ndiagnose_outlier()와 plot_outlier()는 이상치에 대한 정보와 시각화를 제공한다.\ndiagnose_report()는 데이터 품질 진단을 수행한 후 그 결과를 보고서로 만들어 준다.\ndiagnose()을 이용한 변수의 개괄적 진단\ndiagnose()은 데이터 프레임의 변수를 진단한다. dplyr의 함수처럼 첫 번째 인수는 tibble(또는 데이터 프레임)이다. 두 번째 및 후속 인수는 해당 데이터 프레임 내의 변수를 나타낸다.\ndiagnose()가 반환하는 tbl_df 객체의 변수는 다음과 같다.\nvariables : 변수명\ntypes : 변수의 데이터 유형\nmissing_count : 결측치 수\nmissing_percent : 결측치의 백분율\nunique_count : 유일값의 수\nunique_rate : 유일값의 비율. unique_count / 관측치의 수\n다음처럼 diagnose()는 flights의 모든 변수를 진단할 수 있다.:\n\n\ndiagnose(flights)\n\n\n# A tibble: 19 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 year      integer             0            0               1\n2 month     integer             0            0              12\n3 day       integer             0            0              31\n4 dep_time  integer          8255            2.45         1319\n# … with 15 more rows, and 1 more variable: unique_rate <dbl>\n\n결측치 : 결측치가 아주 많은 변수, 즉 missing_percent가 100에 가까운 변수는 분석에서 제외하는 것을 고려해야 한다.\n유일값 : 유일값이 하나인(unique_count = 1) 변수는 데이터 분석에서 제외하는 것을 고려한다. 그리고 데이터 유형이 수치형(integer, numeric)이 아니면서 유일값의 개수가 관측치의 개수와 같은(unique_rate = 1) 변수는 식별자일 확률이 크다. 그러므로 이 변수도 분석 모델에 적합치 않은 변수다.\nyear는 unique_count가 1이므로 분석 모델에 사용하지 않는 것을 고려할 수 있다. 다만 year, month, day의 조합으로 년월일을 구성하는 경우에는 굳이 제거하지 않아도 될 것이다.\n다음은 선택된 몇 개의 변수에 대해서만 진단을 수행한다.\n\n\n# Select columns by name\ndiagnose(flights, year, month, day)\n\n\n# A tibble: 3 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 year      integer             0               0            1\n2 month     integer             0               0           12\n3 day       integer             0               0           31\n# … with 1 more variable: unique_rate <dbl>\n\n# Select all columns between year and day (inclusive)\ndiagnose(flights, year:day)\n\n\n# A tibble: 3 x 6\n  variables types   missing_count missing_percent unique_count\n  <chr>     <chr>           <int>           <dbl>        <int>\n1 year      integer             0               0            1\n2 month     integer             0               0           12\n3 day       integer             0               0           31\n# … with 1 more variable: unique_rate <dbl>\n\n# Select all columns except those from year to day (inclusive)\ndiagnose(flights, -(year:day))\n\n\n# A tibble: 16 x 6\n  variables      types   missing_count missing_percent unique_count\n  <chr>          <chr>           <int>           <dbl>        <int>\n1 dep_time       integer          8255            2.45         1319\n2 sched_dep_time integer             0            0            1021\n3 dep_delay      numeric          8255            2.45          528\n4 arr_time       integer          8713            2.59         1412\n# … with 12 more rows, and 1 more variable: unique_rate <dbl>\n\ndplyr을 이용해서 결측치를 포함한 변수를 결측치의 비중별로 정렬할 수 있다.:\n\n\nflights %>%\n  diagnose() %>%\n  select(-unique_count, -unique_rate) %>% \n  filter(missing_count > 0) %>% \n  arrange(desc(missing_count))\n\n\n# A tibble: 6 x 4\n  variables types   missing_count missing_percent\n  <chr>     <chr>           <int>           <dbl>\n1 arr_delay numeric          9430            2.80\n2 air_time  numeric          9430            2.80\n3 arr_time  integer          8713            2.59\n4 dep_time  integer          8255            2.45\n# … with 2 more rows\n\ndiagnose_numeric()을 이용한 수치형 변수의 상세 진단\ndiagnose_numeric()은 데이터 프레임의 수치형(연속형과 이산형) 변수를 진단한다. 사용 방법은 diagnose()와 동일하나 더 많은 진단 정보를 반환한다. 그런데 두 번째 및 후속 인수 목록에 수치형이 아닌 변수를 지정하면 해당 변수는 자동적으로 무시한다.\ndiagnose_numeric()이 반환하는 tbl_df 객체의 변수는 다음과 같다.\nmin : 최소값\nQ1 : 1/4분위수, 25백분위수\nmean : 산술평균\nmedian : 중위수, 50백분위수\nQ3 : 3/4분위수, 75백분위수\nmax : 최대값\nzero : 0의 값을 갖는 관측치의 개수\nminus : 음수를 갖는 관측치의 개수\noutlier : 이상치의 개수\n데이터 프레임에 summary() 함수를 적용하면 수치형 변수의 min, Q1, mean, median, Q3 , max를 콘솔에 출력하여 데이터의 분포를 파악할 수 있도록 도와준다. 그러나 그 결과는 분석가가 눈으로만 살펴볼 수 밖에 없는 단점이 있다. 그런데 이런 정보들을 tbl_df와 같은 데이터 프레임 구조로 반환하면 활용의 범위가 넓어진다.\nzero, minus, outlier는 데이터의 무결성을 진단하는데 유용한 측도다. 예를 들어 어떤 경우의 수치 데이터는 0이나 음수를 가질 수 없는 경우가 있기 때문이다. ’직원의 급여’라는 가상의 수치형 변수는 음수나 0을 가질 수 없기 때문에 데이터 진단 과정에서 0이나 음수의 포함 여부를 살펴보아야 한다.\n다음처럼 diagnose_numeric()는 flights의 모든 수치형 변수를 진단할 수 있다.:\n\n\ndiagnose_numeric(flights)\n\n\n# A tibble: 14 x 10\n  variables   min    Q1    mean median    Q3   max  zero minus outlier\n  <chr>     <dbl> <dbl>   <dbl>  <dbl> <dbl> <dbl> <int> <int>   <int>\n1 year       2013  2013 2013      2013  2013  2013     0     0       0\n2 month         1     4    6.55      7    10    12     0     0       0\n3 day           1     8   15.7      16    23    31     0     0       0\n4 dep_time      1   907 1349.     1401  1744  2400     0     0       0\n# … with 10 more rows\n\n수치형 변수가 논리적으로 음수나 0의 값을 가질 수 없을 경우에, filter()로 논리적으로 부합하지 않은 변수를 쉽게 찾아낸다.:\n\n\ndiagnose_numeric(flights) %>% \n  filter(minus > 0 | zero > 0) \n\n\n# A tibble: 3 x 10\n  variables   min    Q1  mean median    Q3   max  zero  minus outlier\n  <chr>     <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <int>  <int>   <int>\n1 dep_delay   -43    -5 12.6      -2    11  1301 16514 183575   43216\n2 arr_delay   -86   -17  6.90     -5    14  1272  5409 188933   27880\n3 minute        0     8 26.2      29    44    59 60696      0       0\n\ndiagnose_category()을 이용한 범주형 변수의 상세 진단\ndiagnose_category()은 데이터 프레임의 범주형(factor, ordered, character) 변수를 진단한다. 사용 방법은 diagnose()와 유사하나 더 많은 진단 정보를 반환한다. 그런데 두 번째 및 후속 인수 목록에 범주형이 아닌 변수를 지정하면 해당 변수는 자동적으로 무시한다. top 인수는 변수별로 반환할 수준(levels)의 개수를 지정한다. 기본값은 10으로 상위 top 10의 수준을 반환한다. 물론 수준의 개수가 10개 미만일 경우에는 모든 수준을 반환한다.\ndiagnose_category()이 반환하는 tbl_df 객체의 변수는 다음과 같다.\nvariables : 변수의 이름\nlevels: 수준의 이름\nN : 관측치의 수\nfreq : 수준별 도수(frequency)\nratio : 수준별 상대도수(백분율 표현)\nrank : 레벨별 도수 크기의 순위\n다음처럼 diagnose_category()는 flights의 모든 범주형 변수를 진단할 수 있다.:\n\n\ndiagnose_category(flights)\n\n\n# A tibble: 43 x 6\n  variables levels      N  freq ratio  rank\n  <chr>     <chr>   <int> <int> <dbl> <int>\n1 carrier   UA     336776 58665  17.4     1\n2 carrier   B6     336776 54635  16.2     2\n3 carrier   EV     336776 54173  16.1     3\n4 carrier   DL     336776 48110  14.3     4\n# … with 39 more rows\n\ndplyr 패키지의 filter()와 협업하여 결측치가 top 10에 포함된 사례를 조회한 결과에서 tailnum 변수가 2,512건의 결측치로 top 1에 랭크된 것을 알 수 있다.:\n\n\ndiagnose_category(flights) %>% \n  filter(is.na(levels))\n\n\n# A tibble: 1 x 6\n  variables levels      N  freq ratio  rank\n  <chr>     <chr>   <int> <int> <dbl> <int>\n1 tailnum   <NA>   336776  2512 0.746     1\n\n다음은 수준이 차지하는 비중이 0.01% 이하인 목록을 반환한다. top 인수값을 500으로 넉넉하게 지정한 것에 주목해야 한다. 만약 기본값인 10을 사용하였다면 0.01% 이하의 값은 목록에 포함되지 못했을 것이다.:\n\n\nflights %>%\n  diagnose_category(top = 500)  %>%\n  filter(ratio <= 0.01)\n\n\n# A tibble: 10 x 6\n  variables levels      N  freq   ratio  rank\n  <chr>     <chr>   <int> <int>   <dbl> <int>\n1 carrier   OO     336776    32 0.00950    16\n2 dest      JAC    336776    25 0.00742    97\n3 dest      PSP    336776    19 0.00564    98\n4 dest      EYW    336776    17 0.00505    99\n# … with 6 more rows\n\n분석 모델에서 관측치에서 차지하는 비중이 미미한 수준들은 제거하거나 하나로 합치는 것도 고려해볼 수 있다.\ndiagnose_outlier()를 이용한 이상치 진단\ndiagnose_outlier()은 데이터 프레임의 수치형(연속형과 이산형) 변수의 이상치(outliers)를 진단한다. 사용 방법은 diagnose()와 동일하다.\ndiagnose_outlier()이 반환하는 tbl_df 객체의 변수는 다음과 같다.\noutliers_cnt : 이상치의 개수\noutliers_ratio : 이상치의 비율(백분율)\noutliers_mean : 이상치들의 산술평균\nwith_mean : 이상치를 포함한 전체 관측치의 평균\nwithout_mean : 이상치를 제거한 관측치의 산술평균\n다음처럼 diagnose_outlier()는 flights의 모든 수치형 변수의 이상치를 진단할 수 있다.:\n\n\ndiagnose_outlier(flights)\n\n\n# A tibble: 14 x 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean\n  <chr>            <int>          <dbl>         <dbl>     <dbl>\n1 year                 0              0           NaN   2013   \n2 month                0              0           NaN      6.55\n3 day                  0              0           NaN     15.7 \n4 dep_time             0              0           NaN   1349.  \n# … with 10 more rows, and 1 more variable: without_mean <dbl>\n\n이상치를 포함하는 수치형 변수는 filter()로 쉽게 찾아낸다.:\n\n\ndiagnose_outlier(flights) %>% \n  filter(outliers_cnt > 0) \n\n\n# A tibble: 5 x 6\n  variables outliers_cnt outliers_ratio outliers_mean with_mean\n  <chr>            <int>          <dbl>         <dbl>     <dbl>\n1 dep_delay        43216      12.8               93.1     12.6 \n2 arr_delay        27880       8.28             121.       6.90\n3 flight               1       0.000297        8500     1972.  \n4 air_time          5448       1.62             400.     151.  \n# … with 1 more row, and 1 more variable: without_mean <dbl>\n\n다음은 이상치를 5% 이상 포함한 수치형 변수중에서 이상치의 평균이 전체 평균대비 규모가 큰 순으로 정렬하여 반환한다.:\n\n\ndiagnose_outlier(flights) %>% \n  filter(outliers_ratio > 5) %>% \n  mutate(rate = outliers_mean / with_mean) %>% \n  arrange(desc(rate)) %>% \n  select(-outliers_cnt)\n\n\n# A tibble: 2 x 6\n  variables outliers_ratio outliers_mean with_mean without_mean  rate\n  <chr>              <dbl>         <dbl>     <dbl>        <dbl> <dbl>\n1 arr_delay           8.28         121.       6.90       -3.69  17.5 \n2 dep_delay          12.8           93.1     12.6         0.444  7.37\n\n데이터 분석 과정에서 이상치의 평균이 전체 평균대비 규모가 클 경우에는 이상치를 대체하거나 제거하는 것이 바람직할 수 있다.\nplot_outlier()를 이용한 이상치의 시각화\nplot_outlier()은 데이터 프레임의 수치형(연속형과 이산형) 변수의 이상치(outliers)를 시각화한다. 사용 방법은 diagnose()와 동일하다.\nplot_outlier()이 시각화하는 플롯은 다음을 포함한다.\n이상치를 포함한 박스플롯\n이상치를 제거한 박스플롯\n이상치를 포함한 히스토그램\n이상치를 제거한 히스토그램\n다음처럼 plot_outlier()는 flights의 arr_delay 변수의 이상치를 시각화할 수 있다.:\n\n\nflights %>%\n  plot_outlier(arr_delay) \n\n\n\n\n다음은 diagnose_outlier()와 plot_outlier(), dplyr 패키지의 함수를 사용하여 이상치의 비율이 0.5% 이상인 모든 수치형 변수의 이상치를 시각화 한다.\n\n\nflights %>%\n  plot_outlier(diagnose_outlier(flights) %>% \n                 filter(outliers_ratio >= 0.5) %>% \n                 select(variables) %>% \n                 pull())\n\n\n\n\n시각화 결과를 보고 이상치의 제거 및 대체 여부를 결정해야 한다. 경우에 따라서는 이상치가 포함된 변수를 데이터 분석 모델에서 제거하는 것도 고려해야 한다.\n시각화 결과를 보면 arr_delay는 이상치를 제거한 관측치들은 정규분포와 유사한 분포를 보이고 있다. 선형 모형의 경우에는 이상치를 제거하거나 대체하는 것도 검토해볼 수 있겠다. 그리고 air_time은 이상치를 제거하기 전후의 분포가 대략 비슷한 모양을 보인다.\ndiagnose_report()를 이용한 진단 보고서 작성\ndiagnose_report()는 데이터 프레임이나 데이터 프레임을 상속받은 객체(tbl_df, tbl 등)의 모든 변수들에 대해서 데이터 진단을 수행한다.\ndiagnose_report()는 진단 보고서를 다음과 같은 두 개의 형태로 작성한다.\nLatex에 기반한 pdf 파일\nhtml 파일\n보고서의 목차는 다음과 같다.\n데이터 진단\n데이터 품질 총괄\n전체변수 품질현황 목록\n결측치 진단\n유일값 진단(문자형과 범주형)\n유일값 진단(수치형)\n\n데이터 품질 상세\n범주형 변수 품질 현황\n수치형 변수 품질 현황\n수치변수 품질현황 (zero)\n수치변수 품질현황 (minus)\n\n\n이상치 진단\n데이터 품질 총괄\n수치변수의 이상치 진단\n이상치 상세 진단\n\n\n다음은 tbl_df 클래스 객체인 flights의 품질진단 리포트를 작성한다. 파일 형식은 pdf이며, 파일이름은 DataDiagnosis_Report.pdf다.\n\n\nflights %>%\n  diagnose_report()\n\n\n\n다음은 Diagn.html라는 이름의 html 형식의 보고서를 생성한다.\n\n\nflights %>%\n  diagnose_report(output_format = \"html\", output_file = \"Diagn.html\")\n\n\n\n데이터 진단 보고서는 데이터 진단 과정에 도움을 주기 위한 자동화 보고서다. 보고서 결과를 참고하여 데이터 보완이나 재획득을 판단한다.\n진단 리포트 내용\npdf 파일의 내용\n보고서의 표지는 다음 그림과 같다.\n\n\n\nFigure 1: 데이터진단 보고서 표지\n\n\n\n보고서의 차례는 다음 그림과 같다.\n\n\n\nFigure 2: 데이터진단 보고서 차례\n\n\n\n대부분의 정보는 보고서에서 표로 표현된다. 표의 예시는 다음 그림과 같다.\n\n\n\nFigure 3: 데이터진단 보고서 도표 예시\n\n\n\n데이터진단 보고서에서 이상치 진단 내용은 시각화 결과를 포함한다. 그 결과는 다음 그림과 같다.\n\n\n\nFigure 4: 데이터진단 보고서 이상치 진단 내용\n\n\n\nhtml 파일의 내용\n보고서의 타이틀과 목차는 다음 그림과 같다.\n\n\n\nFigure 5: 데이터진단 보고서 타이틀과 목차\n\n\n\n대부분의 정보는 보고서에서 표로 표현된다. html 파일에서 표의 예시는 다음 그림과 같다.\n\n\n\nFigure 6: 데이터진단 보고서 도표 예시 (웹)\n\n\n\n데이터진단 보고서에서 이상치 진단 내용은 시각화 결과를 포함한다. html 파일의 결과는 다음 그림과 같다.\n\n\n\nFigure 7: 데이터진단 보고서 이상치 진단 내용 (웹)\n\n\n\n\n\n\n",
    "preview": "posts/2018-05-07-r-vignette01.ko/2018-05-07-r-vignette01.ko_files/figure-html5/plot_outlier-1.png",
    "last_modified": "2021-10-10T23:40:46+09:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 768
  },
  {
    "path": "posts/2018-05-07-r-vignette02.ko/",
    "title": "dlookr - 탐색적 데이터 분석",
    "description": "이 문서는 dlookr 기능 중에서 **탐색적 데이터 분석** 기능을 소개한다. 여러분은 dlookr에서 제공하는 함수로 데이터 프레임과 데이터 프레임을 상속한 tbl_df 데이터의 탐색적 데이터 분석을 수행하는 방법을 일힐 수 있을 것이다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2018-05-07",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n일러두기\n데이터\n탐색적 데이터 분석\n단변량 데이터 EDA\ndescribe()을 이용한 기술통계량 계산\nnormality()을 이용한 수치형 변수의 정규성 검정\nplot_normality()를 이용한 수치변수의 정규성 시각화\n\n이변량 데이터 EDA\ncorrelate()을 이용한 상관계수 계산\nplot_correlate()를 이용한 상관행렬의 시각화\n\nTarget 변수에 기반한 EDA\nTarget 변수 정의\nTarget 변수가 범주형 변수인 경우의 EDA\nTarget 변수가 수치형 변수일 때의 EDA\n\neda_report()를 이용한 EDA 보고서 작성\nEDA 리포트 내용\n\n\n\n\n\n일러두기\n“본 vignett는 dplyr 패키지와의 상호 운용성을 강조하기 위해서 dplyr 패키지의 vignett인 ’Introduction to dplyr’을 참고하였다.”\n분석을 위한 데이터를 획득한 후에는 다음을 수행해야 한다.:\n데이터의 품질을 진단한다.\n만약 데이터 품질의 문제를 발견한다면,\n문제의 데이터를 보완하거나 경우에 따라서는 재획득을 수행햐야 한다.\n\n데이터를 이해하기 위한 탐색을 수행하여, 분석의 전개 방향에 대한 시나리오를 수립한다.\n분석에 효과적인 변수를 파생하거나 변수의 변환을 수행한다.\ndlookr 패키지는 다음의 과정을 빠르고 쉽게 수행하도록 도움을 준다.\n데이터의 진단을 수행하거나 데이터 품질 진단 리포트를 자동으로 생성한다.\n다양한 방법으로 데이터를 탐색하고 EDA(탐색적 데이터 분석) 보고서를 생성한다.\n연속형 변수를 비닝(binning)하여 범주형 변수로 만들고, 비닝의 의사결정에 도움을 주는 리포트를 생성한다.\n이 문서는 dlookr 기능 중에서 탐색적 데이터 분석 기능을 소개한다. 여러분은 dlookr에서 제공하는 함수로 데이터 프레임과 데이터 프레임을 상속한 tbl_df 데이터의 탐색적 데이터 분석을 수행하는 방법을 일힐 수 있을 것이다.\ndlookr 패키지는 dplyr 패키지와 함께 사용하면 시너지가 증가된다. 특히 데이터 탐색 및 데이터 조작에서 tidyverse 패키지 그룹의 효율성을 높여준다.\n데이터\ndlookr 패키지로 EDA를 수행하는 기초적인 사용 방법을 설명하기 위해서 Carseats를 사용한다. ISLR 패키지의 Carseats는 400개의 매장에서 아동용 카시트를 판매하는 시뮬레이션 데이터다. 이 데이터는 판매량을 예측하는 목적으로 생성한 데이터 프레임이다.\n\n\nlibrary(ISLR)\nstr(Carseats)\n\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n\n개별 변수들의 의미는 다음과 같다. (ISLR::Carseats Man page 참고)\nSales\n지역의 단위 판매량 (단위: 천개)\n\nCompPrice\n지역의 경쟁 업체가 부과하는 가격\n\nIncome\n지역 공동체 수입 수준 (단위: 천달러)\n\nAdvertising\n회사의 지역에 대한 광고 예산 (단위: 천달러)\n\nPopulation\n지역의 인구 규모 (단위: 천명)\n\nPrice\n지역의 자동차 좌석 요금\n\nShelveLoc\n각 사이트에서 자동차 좌석의 선반 위치의 품질을 나타내는 수준. “Bad”, “Good”, “Medium”.\n\nAge\n각 지역의 평균 연령\n\nEducation\n각 지역의 교육 수준\n\nUrban\n점포의 도시 또는 농촌 소재 여부. Yes는 도시, No는 농촌.\n\nUS\n점포의 미국 소재 여부. Yes는 미국 소재, No는 미국 외 소재.\n\n데이터 분석을 수행할 때, 결측치가 포함된 데이터를 자주 접한다. 그러나 Carseats는 결측치가 없은 완전한 데이터다. 그래서 다음과 같이 결측치를 생성하였다. 그리고 carseats라는 이름의 데이터 프레임 객체를 생성한다.\n\n\ncarseats <- ISLR::Carseats\n\nset.seed(123)\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\n\nset.seed(456)\ncarseats[sample(seq(NROW(carseats)), 10), \"Urban\"] <- NA\n\n\n\n탐색적 데이터 분석\ndlookr은 수치 데이터의 기술통계량을 계산하여 데이터의 분포를 이해할 수 있도록 도와준다. 또한 변수들 사이의 상관관계를 규명하고 정규성 검정을 수행한다. 그리고 목적변수(Target Variable)와 독립변수들간의 관계를 규명해준다.\n다음은 dlookr이 제공하는 EDA 함수와 함수의 기능 목록이다.:\ndescribe()는 수치 데이터의 기술통계량을 제공한다.\nnormality()와 plot_normality()는 수치 데이터의 정규성 검정과 시각화를 수행한다.\ncorrelate()와 plot_correlate()는 두 수치 데이터 간의 상관계수를 계산하고 시각화를 제공한다.\ntarget_by()는 목적변수(Target Variable)을 정의하고 relate()는 목적변수에 대응하는 관심있는 변수와의 관계를 규명한다.\nplot.relate()는 목적변수에 대응하는 관심있는 변수와의 관계를 시각화한다.\neda_report()는 탐색적 데이터 분석을 수행한 후 그 결과를 보고서로 만들어 준다.\n단변량 데이터 EDA\ndescribe()을 이용한 기술통계량 계산\ndescribe()는 수치 데이터의 기술통계량을 계산해 준다. 기술통계량은 수치 변수의 분포를 판단하는 것을 도와준다.\ndescribe()가 반환하는 tbl_df 객체의 변수는 다음과 같다.\nn : 결측치를 제외한 데이터 건수\nna : 결측치 건수\nmean : 산술평균\nsd : 표준편차\nse_mean : 표준오차. sd/sqrt(n)\nIQR : 사분위 범위(Interquartile range) (Q3-Q1)\nskewness : 왜도\nkurtosis : 첨도\np25 : Q1. 25% 백분위수\np50 : 중위수. 50% 백분위수\np75 : Q3. 75% 백분위수\np01, p05,p10,p20,p30` : 1%, 5%, 20%, 30% 백분위수\np40, p60,p70,p80` : 40%, 60%, 70%, 80% 백분위수\np90, p95,p99,p100` : 90%, 95%, 99%, 100% 백분위수\n다음처럼 describe()는 carseats의 모든 수치 변수의 통계량을 계산한다.:\n\n\ndescribe(carseats)\n\n\n# A tibble: 8 x 26\n  variable        n    na   mean    sd se_mean   IQR skewness kurtosis\n  <chr>       <int> <int>  <dbl> <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Sales         400     0   7.50  2.82   0.141  3.93   0.186   -0.0809\n2 CompPrice     400     0 125.   15.3    0.767 20     -0.0428   0.0417\n3 Income        380    20  69.3  28.1    1.44  48      0.0360  -1.10  \n4 Advertising   400     0   6.64  6.65   0.333 12      0.640   -0.545 \n# … with 4 more rows, and 17 more variables: p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\n왜도 : 왼쪽으로 치우친 분포의 데이터, 즉 skewness가 제법 큰 양수를 갖는 변수는 정규분포를 따르도록 log, sqrt 변환 등을 고려해야 한다. Advertising 변수는 변수변환을 고려해야할 것 같다.\n산술평균, 표준편차, 표준오차 : 표준오차(se_mean)가 7.3688218로 상당히 큰 Population는 대표치인 산술평균(mean)의 대표성이 낮다. 산술평균에 비해서 표준편차(sd)의 크기도 상당히 큰 편이다.\n다음은 선택된 몇 개의 변수에 대해서만 기술통계량을 계산한다.\n\n\n# Select columns by name\ndescribe(carseats, Sales, CompPrice, Income)\n\n\n# A tibble: 3 x 26\n  variable      n    na   mean    sd se_mean   IQR skewness kurtosis\n  <chr>     <int> <int>  <dbl> <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Sales       400     0   7.50  2.82   0.141  3.93   0.186   -0.0809\n2 CompPrice   400     0 125.   15.3    0.767 20     -0.0428   0.0417\n3 Income      380    20  69.3  28.1    1.44  48      0.0360  -1.10  \n# … with 17 more variables: p00 <dbl>, p01 <dbl>, p05 <dbl>,\n#   p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>,\n#   p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>,\n#   p99 <dbl>, p100 <dbl>\n\n# Select all columns between year and day (inclusive)\ndescribe(carseats, Sales:Income)\n\n\n# A tibble: 3 x 26\n  variable      n    na   mean    sd se_mean   IQR skewness kurtosis\n  <chr>     <int> <int>  <dbl> <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Sales       400     0   7.50  2.82   0.141  3.93   0.186   -0.0809\n2 CompPrice   400     0 125.   15.3    0.767 20     -0.0428   0.0417\n3 Income      380    20  69.3  28.1    1.44  48      0.0360  -1.10  \n# … with 17 more variables: p00 <dbl>, p01 <dbl>, p05 <dbl>,\n#   p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>, p50 <dbl>,\n#   p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>, p95 <dbl>,\n#   p99 <dbl>, p100 <dbl>\n\n# Select all columns except those from year to day (inclusive)\ndescribe(carseats, -(Sales:Income))\n\n\n# A tibble: 5 x 26\n  variable       n    na   mean     sd se_mean   IQR skewness kurtosis\n  <chr>      <int> <int>  <dbl>  <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 Advertisi…   400     0   6.64   6.65   0.333  12     0.640    -0.545\n2 Population   400     0 265.   147.     7.37  260.   -0.0512   -1.20 \n3 Price        400     0 116.    23.7    1.18   31    -0.125     0.452\n4 Age          400     0  53.3   16.2    0.810  26.2  -0.0772   -1.13 \n# … with 1 more row, and 17 more variables: p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\ndplyr을 이용해서 왼쪽이나 오른쪽으로 치우친 정도(왜도)의 크기별로 정렬할 수 있다.:\n\n\nlibrary(dplyr)\n\ncarseats %>%\n  describe() %>%\n  select(variable, skewness, mean, p25, p50, p75) %>% \n  filter(!is.na(skewness)) %>% \n  arrange(desc(abs(skewness)))\n\n\n# A tibble: 8 x 6\n  variable    skewness   mean    p25    p50    p75\n  <chr>          <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 Advertising   0.640    6.64   0      5     12   \n2 Sales         0.186    7.50   5.39   7.49   9.32\n3 Price        -0.125  116.   100    117    131   \n4 Age          -0.0772  53.3   39.8   54.5   66   \n# … with 4 more rows\n\ndescribe() 함수는 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  group_by(US) %>% \n  describe(Sales, Income) \n\n\n# A tibble: 4 x 27\n  variable US        n    na  mean    sd se_mean   IQR skewness\n  <chr>    <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n1 Income   No      134     8 65.7  28.2    2.44  50.8    0.130 \n2 Income   Yes     246    12 71.3  27.9    1.78  46     -0.0102\n3 Sales    No      142     0  6.82  2.60   0.218  3.44   0.323 \n4 Sales    Yes     258     0  7.87  2.88   0.179  4.23   0.0760\n# … with 18 more variables: kurtosis <dbl>, p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\n\n\ncarseats %>%\n  group_by(US, Urban) %>% \n  describe(Sales, Income) \n\n\n# A tibble: 12 x 28\n  variable US    Urban     n    na  mean    sd se_mean   IQR skewness\n  <chr>    <fct> <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n1 Income   No    No       40     4  62.1  29.8    4.72  51.8   0.367 \n2 Income   No    Yes      91     3  67.5  27.4    2.87  48     0.0518\n3 Income   No    <NA>      3     1  59.7  37.0   21.4   37    -0.162 \n4 Income   Yes   No       68     3  70.2  30.7    3.72  53     0.0414\n# … with 8 more rows, and 18 more variables: kurtosis <dbl>,\n#   p00 <dbl>, p01 <dbl>, p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>,\n#   p30 <dbl>, p40 <dbl>, p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>,\n#   p80 <dbl>, p90 <dbl>, p95 <dbl>, p99 <dbl>, p100 <dbl>\n\nnormality()을 이용한 수치형 변수의 정규성 검정\nnormality()는 수치 데이터의 정규성 검정을 수행한다. Shapiro-Wilk 정규성 검정을 수행하며, 관측치의 개수가 5000보다 클 경우에는 5000개의 단순 임의 추출을 수행한 후 검정한다.\nnormality()가 반환하는 tbl_df 객체의 변수는 다음과 같다.\nstatistic : Shapiro-Wilk 검정의 통계량\np_value : Shapiro-Wilk 검정의 p-value\nsample : Shapiro-Wilk 검정을 수행한 샘플 관측치의 개수\n다음처럼 normality()는 carseats의 모든 수치 변수의 정규성 검정을 수행한다.:\n\n\nnormality(carseats)\n\n\n# A tibble: 8 x 4\n  vars        statistic  p_value sample\n  <chr>           <dbl>    <dbl>  <dbl>\n1 Sales           0.995 2.54e- 1    400\n2 CompPrice       0.998 9.77e- 1    400\n3 Income          0.961 1.55e- 8    400\n4 Advertising     0.874 1.49e-17    400\n# … with 4 more rows\n\n다음은 선택된 몇 개의 변수에 대해서만 정규성 검정을 수행한다.\n\n\n# Select columns by name\nnormality(carseats, Sales, CompPrice, Income)\n\n\n# A tibble: 3 x 4\n  vars      statistic      p_value sample\n  <chr>         <dbl>        <dbl>  <dbl>\n1 Sales         0.995 0.254           400\n2 CompPrice     0.998 0.977           400\n3 Income        0.961 0.0000000155    400\n\n# Select all columns between year and day (inclusive)\nnormality(carseats, Sales:Income)\n\n\n# A tibble: 3 x 4\n  vars      statistic      p_value sample\n  <chr>         <dbl>        <dbl>  <dbl>\n1 Sales         0.995 0.254           400\n2 CompPrice     0.998 0.977           400\n3 Income        0.961 0.0000000155    400\n\n# Select all columns except those from year to day (inclusive)\nnormality(carseats, -(Sales:Income))\n\n\n# A tibble: 5 x 4\n  vars        statistic  p_value sample\n  <chr>           <dbl>    <dbl>  <dbl>\n1 Advertising     0.874 1.49e-17    400\n2 Population      0.952 4.08e-10    400\n3 Price           0.996 3.90e- 1    400\n4 Age             0.957 1.86e- 9    400\n# … with 1 more row\n\ndplyr을 이용해서 정규분포를 따르지 않는 변수를 p_value 순으로 정렬할 수 있다.:\n\n\nlibrary(dplyr)\n\ncarseats %>%\n  normality() %>%\n  filter(p_value <= 0.01) %>% \n  arrange(abs(p_value))\n\n\n# A tibble: 5 x 4\n  vars        statistic  p_value sample\n  <chr>           <dbl>    <dbl>  <dbl>\n1 Advertising     0.874 1.49e-17    400\n2 Education       0.924 2.43e-13    400\n3 Population      0.952 4.08e-10    400\n4 Age             0.957 1.86e- 9    400\n# … with 1 more row\n\n특히 Advertising 변수는 정규분포에서 가장 벗어난 것으로 파악된다.\nnormality() 함수는 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  group_by(ShelveLoc, US) %>%\n  normality(Income) %>% \n  arrange(desc(p_value))\n\n\n# A tibble: 6 x 6\n  variable ShelveLoc US    statistic p_value sample\n  <chr>    <fct>     <fct>     <dbl>   <dbl>  <dbl>\n1 Income   Bad       No        0.965  0.350      34\n2 Income   Good      Yes       0.958  0.0359     61\n3 Income   Bad       Yes       0.952  0.0236     62\n4 Income   Good      No        0.879  0.0140     24\n# … with 2 more rows\n\nIncome 변수는 정규분포를 따르지 않지만, 유의수준 0.01 기준으로 US가 No이면서 ShelveLoc가 Good, Bad인 경우는 정규분포를 따르는 것으로 볼 수 있다.\n다음은 범주형 변수인 ShelveLoc, US 변수의 조합별로 log(Income)의 정규성 검정을 수행하여, 정규분포를 따르는 변수를 조회한다.\n\n\ncarseats %>%\n  mutate(log_income = log(Income)) %>%\n  group_by(ShelveLoc, US) %>%\n  normality(log_income) %>%\n  filter(p_value > 0.01)\n\n\n# A tibble: 1 x 6\n  variable   ShelveLoc US    statistic p_value sample\n  <chr>      <fct>     <fct>     <dbl>   <dbl>  <dbl>\n1 log_income Bad       No        0.946   0.100     34\n\nplot_normality()를 이용한 수치변수의 정규성 시각화\nplot_normality()는 수치 데이터의 정규성을 시각화한다.\nplot_normality()가 시각화하는 정보는 다음과 같다.\n원 데이터의 히스토그램\n원 데이터의 Q-Q plot\nlog 변환 데이터의 히스토그램\nsqrt 변환 데이터의 히스토그램\n데이터 분석 과정에서 멱분포(power-law distribution)를 따르는 수치 데이터를 접하는 경우가 많다. 멱분포를 따르는 수치 데이터는 log, sqrt 변환을 수행하여 정규분포로 변화라기 때문에 log, sqrt 변환데 데이터의 히스토그램을 그린다.\nplot_normality()도 normality() 함수처럼 여러 개의 변수를 지정할 수 있다.\n\n\n# Select columns by name\nplot_normality(carseats, Sales, CompPrice)\n\n\n\n\nplot_normality() 함수도 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  filter(ShelveLoc == \"Good\") %>%\n  group_by(US) %>%\n  plot_normality(Income)\n\n\n\n\n이변량 데이터 EDA\ncorrelate()을 이용한 상관계수 계산\n다음처럼 correlate()는 carseats의 모든 수치 변수의 조합의 상관계수를 구한다.:\n\n\ncorrelate(carseats)\n\n\n# A tibble: 56 x 3\n  var1        var2  coef_corr\n  <fct>       <fct>     <dbl>\n1 CompPrice   Sales    0.0641\n2 Income      Sales    0.153 \n3 Advertising Sales    0.270 \n4 Population  Sales    0.0505\n# … with 52 more rows\n\n다음은 선택된 몇 개의 변수를 포함한 조합에 대해서만 정규성 검정을 수행한다.\n\n\n# Select columns by name\ncorrelate(carseats, Sales, CompPrice, Income)\n\n\n# A tibble: 21 x 3\n  var1      var2      coef_corr\n  <fct>     <fct>         <dbl>\n1 CompPrice Sales        0.0641\n2 Income    Sales        0.153 \n3 Sales     CompPrice    0.0641\n4 Income    CompPrice   -0.0918\n# … with 17 more rows\n\n# Select all columns between year and day (inclusive)\ncorrelate(carseats, Sales:Income)\n\n\n# A tibble: 21 x 3\n  var1      var2      coef_corr\n  <fct>     <fct>         <dbl>\n1 CompPrice Sales        0.0641\n2 Income    Sales        0.153 \n3 Sales     CompPrice    0.0641\n4 Income    CompPrice   -0.0918\n# … with 17 more rows\n\n# Select all columns except those from year to day (inclusive)\ncorrelate(carseats, -(Sales:Income))\n\n\n# A tibble: 35 x 3\n  var1        var2  coef_corr\n  <fct>       <fct>     <dbl>\n1 Advertising Sales    0.270 \n2 Population  Sales    0.0505\n3 Price       Sales   -0.445 \n4 Age         Sales   -0.232 \n# … with 31 more rows\n\ncorrelate()는 두벌의 변수 조합을 만든다. 그래서 다음과 같은 filter() 함수를 사용해서 한 벌의 조합에 대한 상관계수를 구할 수 있다.:\n\n\ncarseats %>%\n  correlate(Sales:Income) %>%\n  filter(as.integer(var1) > as.integer(var2))\n\n\n# A tibble: 3 x 3\n  var1      var2      coef_corr\n  <fct>     <fct>         <dbl>\n1 CompPrice Sales        0.0641\n2 Income    Sales        0.153 \n3 Income    CompPrice   -0.0918\n\ncorrelate() 함수도 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  filter(ShelveLoc == \"Good\") %>%\n  group_by(Urban, US) %>%\n  correlate(Sales) %>%\n  filter(abs(coef_corr) > 0.5)\n\n\n# A tibble: 6 x 5\n  Urban US    var1  var2       coef_corr\n  <fct> <fct> <fct> <fct>          <dbl>\n1 No    No    Sales Population    -0.530\n2 No    No    Sales Price         -0.838\n3 No    Yes   Sales Price         -0.655\n4 Yes   No    Sales Price         -0.837\n# … with 2 more rows\n\nplot_correlate()를 이용한 상관행렬의 시각화\nplot_correlate()는 상관행렬을 시각화한다.\n\n\nplot_correlate(carseats)\n\n\n\n\nplot_correlate()도 correlate() 함수처럼 여러 개의 변수를 지정할 수 있다. 다음은 선택된 몇 개의 변수를 포함한 상관행렬의 시각화를 수행한다.\n\n\n# Select columns by name\nplot_correlate(carseats, Sales, Price)\n\n\n\n\nplot_correlate() 함수도 dplyr 패키지의 group_by() 함수 구문을 지원한다.\n\n\ncarseats %>%\n  filter(ShelveLoc == \"Good\") %>%\n  group_by(Urban, US) %>%\n  plot_correlate(Sales)\n\n\n\n\nTarget 변수에 기반한 EDA\nTarget 변수 정의\nTarget 변수 기반으로 EDA를 수행하려면 target_by 클래스 객체를 생성해야 한다. target_by()는 data.frame 또는 data.frame을 상속받은 객체로 target_by 클래스를 생성한다. target_by()는 grouped_df를 생성하는 dplyr의 group_by()와 유사하다. 차이점은 하나의 변수만 지정한다는 것이다.\n다음은 carseats data.frame에서 US를 target 변수로 지정하는 예제다.:\n\n\ncateg <- target_by(carseats, US)\n\n\n\nTarget 변수가 범주형 변수인 경우의 EDA\nTarget 변수가 범주형일 때 EDA를 수행해 보자. 범주형 변수 US가 target 변수일 때, target 변수와 예측 변수(Predictor) 사이의 관계를 살펴본다.\n예측변수가 수치형 변수인 경우\nrelate()는 taregt 변수와 예측변수 사이의 관계를 보여준다. 다음 예제는 예측변수 Sales와 target 변수 US 사이의 관계를 보여준다. 예측변수 Sales는 수치형 변수다. 이 경우, target 변수의 각 레벨에 대한 기술통계(descriptive statistics)가 표현된다.\n\n\n# If the variable of interest is a numarical variable\ncat_num <- relate(categ, Sales)\ncat_num\n\n\n# A tibble: 3 x 27\n  variable US        n    na  mean    sd se_mean   IQR skewness\n  <chr>    <fct> <int> <int> <dbl> <dbl>   <dbl> <dbl>    <dbl>\n1 Sales    No      142     0  6.82  2.60   0.218  3.44   0.323 \n2 Sales    Yes     258     0  7.87  2.88   0.179  4.23   0.0760\n3 Sales    total   400     0  7.50  2.82   0.141  3.93   0.186 \n# … with 18 more variables: kurtosis <dbl>, p00 <dbl>, p01 <dbl>,\n#   p05 <dbl>, p10 <dbl>, p20 <dbl>, p25 <dbl>, p30 <dbl>, p40 <dbl>,\n#   p50 <dbl>, p60 <dbl>, p70 <dbl>, p75 <dbl>, p80 <dbl>, p90 <dbl>,\n#   p95 <dbl>, p99 <dbl>, p100 <dbl>\n\nsummary(cat_num)\n\n\n   variable             US          n               na   \n Length:3           No   :1   Min.   :142.0   Min.   :0  \n Class :character   Yes  :1   1st Qu.:200.0   1st Qu.:0  \n Mode  :character   total:1   Median :258.0   Median :0  \n                              Mean   :266.7   Mean   :0  \n                              3rd Qu.:329.0   3rd Qu.:0  \n                              Max.   :400.0   Max.   :0  \n      mean             sd           se_mean            IQR       \n Min.   :6.823   Min.   :2.603   Min.   :0.1412   Min.   :3.442  \n 1st Qu.:7.160   1st Qu.:2.713   1st Qu.:0.1602   1st Qu.:3.686  \n Median :7.496   Median :2.824   Median :0.1791   Median :3.930  \n Mean   :7.395   Mean   :2.768   Mean   :0.1796   Mean   :3.866  \n 3rd Qu.:7.682   3rd Qu.:2.851   3rd Qu.:0.1988   3rd Qu.:4.077  \n Max.   :7.867   Max.   :2.877   Max.   :0.2184   Max.   :4.225  \n    skewness          kurtosis             p00        \n Min.   :0.07603   Min.   :-0.32638   Min.   :0.0000  \n 1st Qu.:0.13080   1st Qu.:-0.20363   1st Qu.:0.0000  \n Median :0.18556   Median :-0.08088   Median :0.0000  \n Mean   :0.19489   Mean   : 0.13350   Mean   :0.1233  \n 3rd Qu.:0.25432   3rd Qu.: 0.36344   3rd Qu.:0.1850  \n Max.   :0.32308   Max.   : 0.80776   Max.   :0.3700  \n      p01              p05             p10             p20       \n Min.   :0.4675   Min.   :3.147   Min.   :3.917   Min.   :4.754  \n 1st Qu.:0.6868   1st Qu.:3.148   1st Qu.:4.018   1st Qu.:4.910  \n Median :0.9062   Median :3.149   Median :4.119   Median :5.066  \n Mean   :1.0072   Mean   :3.183   Mean   :4.073   Mean   :5.051  \n 3rd Qu.:1.2771   3rd Qu.:3.200   3rd Qu.:4.152   3rd Qu.:5.199  \n Max.   :1.6480   Max.   :3.252   Max.   :4.184   Max.   :5.332  \n      p25             p30             p40             p50       \n Min.   :5.080   Min.   :5.306   Min.   :5.994   Min.   :6.660  \n 1st Qu.:5.235   1st Qu.:5.587   1st Qu.:6.301   1st Qu.:7.075  \n Median :5.390   Median :5.867   Median :6.608   Median :7.490  \n Mean   :5.411   Mean   :5.775   Mean   :6.506   Mean   :7.313  \n 3rd Qu.:5.576   3rd Qu.:6.010   3rd Qu.:6.762   3rd Qu.:7.640  \n Max.   :5.763   Max.   :6.153   Max.   :6.916   Max.   :7.790  \n      p60             p70             p75             p80        \n Min.   :7.496   Min.   :7.957   Min.   :8.523   Min.   : 8.772  \n 1st Qu.:7.787   1st Qu.:8.386   1st Qu.:8.921   1st Qu.: 9.265  \n Median :8.078   Median :8.815   Median :9.320   Median : 9.758  \n Mean   :8.076   Mean   :8.740   Mean   :9.277   Mean   : 9.665  \n 3rd Qu.:8.366   3rd Qu.:9.132   3rd Qu.:9.654   3rd Qu.:10.111  \n Max.   :8.654   Max.   :9.449   Max.   :9.988   Max.   :10.464  \n      p90              p95             p99             p100      \n Min.   : 9.349   Min.   :11.28   Min.   :13.64   Min.   :14.90  \n 1st Qu.:10.325   1st Qu.:11.86   1st Qu.:13.78   1st Qu.:15.59  \n Median :11.300   Median :12.44   Median :13.91   Median :16.27  \n Mean   :10.795   Mean   :12.08   Mean   :13.86   Mean   :15.81  \n 3rd Qu.:11.518   3rd Qu.:12.49   3rd Qu.:13.97   3rd Qu.:16.27  \n Max.   :11.736   Max.   :12.54   Max.   :14.03   Max.   :16.27  \n\nrelate()로 생성된 relate 클래스 객체를, plot ()으로 target 변수와 예측변수 사이의 관계를 시각화한다. US와 Sales 간의 관계는 밀도 플롯(density plot)으로 표현된다.\n\n\nplot(cat_num)\n\n\n\n\n예측변수가 범주형 변수인 경우\n다음 예제는 ShelveLoc과 target 변수 US 사이의 관계를 보여준다. 예측변수인 ShelveLoc는 범주형 변수다. 이 경우는 두 변수의 분할표(contentency table)를 보여준다. summary() 함수는 분할표에 대해 독립성 검정을 수행한다.\n\n\n# If the variable of interest is a categorical variable\ncat_cat <- relate(categ, ShelveLoc)\ncat_cat\n\n\n     ShelveLoc\nUS    Bad Good Medium\n  No   34   24     84\n  Yes  62   61    135\n\nsummary(cat_cat)\n\n\nCall: xtabs(formula = formula_str, data = data, addNA = TRUE)\nNumber of cases in table: 400 \nNumber of factors: 2 \nTest for independence of all factors:\n    Chisq = 2.7397, df = 2, p-value = 0.2541\n\nplot()은 target 변수와 예측변수 사이의 관계를 시각화한다. US와 ShelveLoc 사이의 관계는 모자이크 플롯(mosaics plot)으로 표현된다.\n\n\nplot(cat_cat)\n\n\n\n\nTarget 변수가 수치형 변수일 때의 EDA\nTarget 변수가 수치형일 때 EDA를 수행해 보자. 수치형 변수 `Sales가 target 변수일 때, target 변수와 예측 변수(Predictor) 사이의 관계를 살펴본다.\n\n\n# If the variable of interest is a numarical variable\nnum <- target_by(carseats, Sales)\n\n\n\n예측변수가 수치형 변수인 경우\n다음 예제는 Price와 target 변수 Sales 사이의 관계를 보여준다. 예측변수인 Price는 수치형 변수다. 이 경우, target ~ predictor 관계의 단순 회귀 모델(simple linear model)의 결과를 보여준다. summary() 함수는 모델의 세부 사항을 표현한다.\n\n\n# If the variable of interest is a numarical variable\nnum_num <- relate(num, Price)\nnum_num\n\n\n\nCall:\nlm(formula = formula_str, data = data)\n\nCoefficients:\n(Intercept)        Price  \n   13.64192     -0.05307  \n\nsummary(num_num)\n\n\n\nCall:\nlm(formula = formula_str, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5224 -1.8442 -0.1459  1.6503  7.5108 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 13.641915   0.632812  21.558   <2e-16 ***\nPrice       -0.053073   0.005354  -9.912   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.532 on 398 degrees of freedom\nMultiple R-squared:  0.198, Adjusted R-squared:  0.196 \nF-statistic: 98.25 on 1 and 398 DF,  p-value: < 2.2e-16\n\nplot()은 target 변수와 예측변수 사이의 관계를 시각화한다. Sales와 Price 간의 관계는 산점도(scatter plot)로 시각화된다. 왼쪽 그림은 Sales와 Price의 산포도와 회귀선 및 회귀선의 신뢰구간을 나타낸다. 오른쪽 그림은 원 데이터와 선형모델의 예측값 사이의 관계를 산점도로 나타낸 것이다. 두 변수 사이에 선형 관계가 있는 경우 관측치의 산점도는 빨간색 대각선에 수렴한다.\n\n\nplot(num_num)\n\n\n\n\n예측변수가 범주형 변수인 경우\n다음 예제는 ShelveLoc과 target 변수 Sales 사이의 관계를 보여준다. 예측변수인 ShelveLoc은 범주형 변수다. target ~ predictor 관계의 one-way ANOVA를 수행한 결과를 보여준다. 결과는 분산분석의 관점에서 표현된다. summary() 함수는 예측변수의 각 레벨에 대한 회귀 계수를 보여준다. 다시말해 target ~ predictor 관계의 단순 회귀분석에 대한 상세 정보를 보여준다.\n\n\n# If the variable of interest is a categorical variable\nnum_cat <- relate(num, ShelveLoc)\nnum_cat\n\n\nAnalysis of Variance Table\n\nResponse: Sales\n           Df Sum Sq Mean Sq F value    Pr(>F)    \nShelveLoc   2 1009.5  504.77   92.23 < 2.2e-16 ***\nResiduals 397 2172.7    5.47                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsummary(num_cat)\n\n\n\nCall:\nlm(formula = formula(formula_str), data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3066 -1.6282 -0.0416  1.5666  6.1471 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)       5.5229     0.2388  23.131  < 2e-16 ***\nShelveLocGood     4.6911     0.3484  13.464  < 2e-16 ***\nShelveLocMedium   1.7837     0.2864   6.229  1.2e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.339 on 397 degrees of freedom\nMultiple R-squared:  0.3172,    Adjusted R-squared:  0.3138 \nF-statistic: 92.23 on 2 and 397 DF,  p-value: < 2.2e-16\n\nplot ()은 target 변수와 예측변수 사이의 관계를 시각화한다. Sales와 ShelveLoc의 관계는 박스플롯(box plot)으로 표현된다.\n\n\nplot(num_cat)\n\n\n\n\neda_report()를 이용한 EDA 보고서 작성\neda_report()는 데이터 프레임이나 데이터 프레임을 상속받은 객체(tbl_df, tbl 등)의 모든 변수들에 대해서 EDA를 수행한다.\neda_report()는 EDA 보고서를 다음과 같은 두 개의 형태로 작성한다.\nLatex에 기반한 pdf 파일\nhtml 파일\n보고서의 목차는 다음과 같다.\n개요\n데이터셋 정보\n변수 정보\n수치변수\n\n일변량 변수 EDA\n기술통계\n수치변수의 정규성 검정\n통계량과 시각화 정보\n\n\n변수들간의 관계\n상관계수\n변수 조합의 상관계수\n수치변수의 상관행렬 플롯\n\n\nTarget 변수 기반의 EDA\n그룹화된 기술통계\n그룹화된 Target과 수치형 변수 관계\n그룹화된 Target과 범주형 변수 관계\n\n그룹화된 Target과 변수들간의 관계\n그룹화된 Target과 수치형 변수의 상관계수\n그룹화된 Target과 수치형 변수의 상관행렬 플롯\n\n\n다음은 carseats의 품질진단 리포트를 작성한다. 파일 형식은 pdf이며, 파일이름은 EDA_Report.pdf다.\n\n\ncarseats %>%\n  eda_report(target = Sales)\n\n\n\n다음은 EDA.html라는 이름의 html 형식의 보고서를 생성한다.\n\n\ncarseats %>%\n  eda_report(target = Sales, output_format = \"html\", output_file = \"EDA.html\")\n\n\n\nEDA 보고서는 EDA 과정에 도움을 주기 위한 자동화 보고서다. 보고서 결과를 참고하여 데이터 분석 시나리오를 설계한다.\nEDA 리포트 내용\npdf 파일의 내용\n보고서의 표지는 다음 그림과 같다.\n\n\n\nFigure 1: EDA 보고서 표지\n\n\n\n보고서의 차례는 다음 그림과 같다.\n\n\n\nFigure 2: EDA 보고서 차례\n\n\n\n많은 정보는 보고서에서 표로 표현된다. 표의 예시는 다음 그림과 같다.\n\n\n\nFigure 3: EDA 보고서 도표 예시\n\n\n\nEDA 보고서에서 정규성 검정 내용은 시각화 결과를 포함한다. 그 결과는 다음 그림과 같다.\n\n\n\nFigure 4: EDA 보고서 정규성 검정 내용\n\n\n\nEDA 보고서에서 상관관계 정보는 시각화 결과를 포함한다. 그 결과는 다음 그림과 같다.\n\n\n\nFigure 5: EDA 보고서 상관관계 내용\n\n\n\nEDA 보고서에서 선형관계에 대한 정보는 표와 시각화 결과를 포함한다. 그 결과는 다음 그림과 같다.\n\n\n\nFigure 6: EDA 보고서 선형관계 정보\n\n\n\nEDA 보고서에서 ANOVA 정보는 표와 시각화 결과를 포함한다. 그 결과는 다음 그림과 같다.\n\n\n\nFigure 7: EDA 보고서 ANOVA 정보\n\n\n\nhtml 파일의 내용\n보고서의 타이틀과 목차는 다음 그림과 같다.\n\n\n\nFigure 8: EDA 보고서 타이틀과 목차\n\n\n\n많은 정보는 보고서에서 표로 표현된다. html 파일에서 표의 예시는 다음 그림과 같다.\n\n\n\nFigure 9: EDA 보고서 도표 예시 (웹)\n\n\n\nEDA 보고서에서 정규성 검정 정보는 시각화 결과를 포함한다. html 파일의 결과는 다음 그림과 같다.\n\n\n\nFigure 10: EDA 보고서 정규성 검정 정보 (웹)\n\n\n\n\n\n\n",
    "preview": "posts/2018-05-07-r-vignette02.ko/2018-05-07-r-vignette02.ko_files/figure-html5/plot_normality-1.png",
    "last_modified": "2021-10-10T23:47:14+09:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 768
  },
  {
    "path": "posts/2018-05-07-r-vignette03.ko/",
    "title": "dlookr -  데이터 변환",
    "description": "이 문서는 dlookr 기능 중에서 **데이터 변환** 기능을 소개한다. 여러분은 dlookr에서 제공하는 함수로 데이터 프레임과 데이터 프레임을 상속한 tbl_df 데이터의 데이터 변환을 수행하는 방법을 일힐 수 있을 것이다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2018-05-07",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n일러두기\n데이터\n데이터 변환\n결측치의 대체\nimputate_na()을 이용한 결측치의 대체\ndplyr과의 협업\n\n이상치의 대체\nimputate_outlier()을 이용한 이상치의 대체\ndplyr과의 협업\n\n표준화와 치우친 데이터의 보정\ntransform()의 기능\ntransform()을 이용한 표준화\ntransform()을 이용한 치우친 데이터의 보정\n\nBinning\nbinning()을 이용한 개별 변수의 Binning\nbinning_by()을 이용한 Optimal Binning\n\ntransformation_report()를 이용한 데이터변환 보고서 작성\n데이터변환 리포트 내용\n\n\n\n\n\n일러두기\n분석을 위한 데이터를 획득한 후에는 다음을 수행해야 한다.:\n데이터의 품질을 진단한다.\n만약 데이터 품질의 문제를 발견한다면,\n문제의 데이터를 보완하거나 경우에 따라서는 재획득을 수행햐야 한다.\n\n데이터를 이해하기 위한 탐색을 수행하여, 분석의 전개 방향에 대한 시나리오를 수립한다.\n분석에 효과적인 변수를 파생하거나 변수의 변환을 수행한다.\ndlookr 패키지는 다음의 과정을 빠르고 쉽게 수행하도록 도움을 준다.\n데이터의 진단을 수행하거나 데이터 품질 진단 리포트를 자동으로 생성한다.\n다양한 방법으로 데이터를 탐색하고 EDA(탐색적 데이터 분석) 보고서를 생성한다.\n결측치와 이상치를 대체하고, 치우친 데이터를 보정하며, 연속형 변수를 비닝(binning)하여 범주형 변수로 만든다. 그리고 이를 지원하는 자동화된 리포트를 생성한다.\n이 문서는 dlookr 기능 중에서 데이터 변환 기능을 소개한다. 여러분은 dlookr에서 제공하는 함수로 데이터 프레임과 데이터 프레임을 상속한 tbl_df 데이터의 데이터 변환을 수행하는 방법을 일힐 수 있을 것이다.\ndlookr 패키지는 dplyr 패키지와 함께 사용하면 시너지가 증가된다. 특히 데이터 변환에서 tidyverse 패키지 그룹의 효율성을 높여준다.\n데이터\ndlookr 패키지로 EDA를 수행하는 기초적인 사용 방법을 설명하기 위해서 Carseats를 사용한다. ISLR 패키지의 Carseats는 400개의 매장에서 아동용 카시트를 판매하는 시뮬레이션 데이터다. 이 데이터는 판매량을 예측하는 목적으로 생성한 데이터 프레임이다.\n\n\nlibrary(ISLR)\nstr(Carseats)\n\n\n'data.frame':   400 obs. of  11 variables:\n $ Sales      : num  9.5 11.22 10.06 7.4 4.15 ...\n $ CompPrice  : num  138 111 113 117 141 124 115 136 132 132 ...\n $ Income     : num  73 48 35 100 64 113 105 81 110 113 ...\n $ Advertising: num  11 16 10 4 3 13 0 15 0 0 ...\n $ Population : num  276 260 269 466 340 501 45 425 108 131 ...\n $ Price      : num  120 83 80 97 128 72 108 120 124 124 ...\n $ ShelveLoc  : Factor w/ 3 levels \"Bad\",\"Good\",\"Medium\": 1 2 3 3 1 1 3 2 3 3 ...\n $ Age        : num  42 65 59 55 38 78 71 67 76 76 ...\n $ Education  : num  17 10 12 14 13 16 15 10 10 17 ...\n $ Urban      : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 2 1 2 2 1 1 ...\n $ US         : Factor w/ 2 levels \"No\",\"Yes\": 2 2 2 2 1 2 1 2 1 2 ...\n\n개별 변수들의 의미는 다음과 같다. (ISLR::Carseats Man page 참고)\nSales\n지역의 단위 판매량 (단위: 천개)\n\nCompPrice\n지역의 경쟁 업체가 부과하는 가격\n\nIncome\n지역 공동체 수입 수준 (단위: 천달러)\n\nAdvertising\n회사의 지역에 대한 광고 예산 (단위: 천달러)\n\nPopulation\n지역의 인구 규모 (단위: 천명)\n\nPrice\n지역의 자동차 좌석 요금\n\nShelveLoc\n각 사이트에서 자동차 좌석의 선반 위치의 품질을 나타내는 수준. “Bad”, “Good”, “Medium”.\n\nAge\n각 지역의 평균 연령\n\nEducation\n각 지역의 교육 수준\n\nUrban\n점포의 도시 또는 농촌 소재 여부. Yes는 도시, No는 농촌.\n\nUS\n점포의 미국 소재 여부. Yes는 미국 소재, No는 미국 외 소재.\n\n데이터 분석을 수행할 때, 결측치가 포함된 데이터를 자주 접한다. 그러나 Carseats는 결측치가 없은 완전한 데이터다. 그래서 다음과 같이 결측치를 생성하였다. 그리고 carseats라는 이름의 데이터 프레임 객체를 생성한다.\n\n\ncarseats <- ISLR::Carseats\n\nset.seed(123)\ncarseats[sample(seq(NROW(carseats)), 20), \"Income\"] <- NA\n\nset.seed(456)\ncarseats[sample(seq(NROW(carseats)), 10), \"Urban\"] <- NA\n\n\n\n데이터 변환\ndlookr은 결측치와 이상치의 대체, 치우친 데이터를 보정해준다. 또한 연속형 변수를 범주형 변수로 비닝하는 것을 도와준다.\n다음은 dlookr이 제공하는 데이터 변환 함수와 함수의 기능 목록이다.:\nfind_na()는 결측치가 포함된 변수를 찾아주고, imputate_na()는 결측치를 대체한다.\nfind_outliers()는 이상치가 포함된 변수를 찾아주고, imputate_outlier()는 이상치를 대체한다.\nsummary.imputation()와 plot.imputation()는 대체된 변수의 정보를 보혀주고 시각화를 제공한다.\nfind_skewness()는 치우친 데이터의 변수를 찾아주고, transform()는 치우친 데이터의 보정을 수행한다.\n또한 transform()는 수치형 변수의 표준화를 수행한다.\nsummary.transform()와 plot.transform()는 변환된 변수의 정보를 보혀주고 시각화를 제공한다.\nbinning()와 binning_by()는 수치 데이터를 비닝하여 범주형 데이터로 변환한다.\nprint.bins()와 summary.bins()는 비닝 결과를 보여주고 요약해 준다.\nplot.bins()와 plot.optimal_bins()는 비닝 결과의 시각화를 제공한다.\ntransformation_report()는 데이터 변환을 수행한 후 그 결과를 보고서로 만들어 준다.\n결측치의 대체\nimputate_na()을 이용한 결측치의 대체\nimputate_na()는 변수에 포함된 결측치를 대체한다. 결측치가 포함된 예측변수(predictor)는 수치형 변수와 범주형 변수 모두 지원하며, 다음과 같은 method를 지원한다.\npredictor가 수치형 변수일 경우\n“mean” : 산술평균으로 대체\n“median” : 중위수로 대체\n“mode” : 최빈수로 대체\n“knn” : K-nearest neighbors를 이용한 대체\ntarget 변수를 지정해야 함\n\n“rpart” : Recursive Partitioning and Regression Trees를 이용한 대체\ntarget 변수를 지정해야 함\n\n“mice” : Multivariate Imputation by Chained Equations를 이용한 대체\ntarget 변수를 지정해야 함\nrandom seed를 지정해야 함\n\n\npredictor가 범주형 변수일 경우\n“mode” : 최빈수로 대체\n“rpart” : Recursive Partitioning and Regression Trees를 이용한 대체\ntarget 변수를 지정해야 함\n\n“mice” : Multivariate Imputation by Chained Equations를 이용한 대체\ntarget 변수를 지정해야 함\nrandom seed를 지정해야 함\n\n\n다음처럼 imputate_na()는 carseats의 수치형 변수인 Income를 “rpart” 방법으로 결측치를 대체한다. summary()는 결측치 대체 정보를 요약하고, plot()은 결측정보를 시각화한다.\n\n\nincome <- imputate_na(carseats, Income, US, method = \"rpart\")\n\n# result of imputate\nincome\n\n\n  [1]  73.00000  48.00000  35.00000 100.00000  64.00000 113.00000\n  [7] 105.00000  81.00000 110.00000 113.00000  78.00000  94.00000\n [13]  35.00000  58.63636 117.00000  95.00000  32.00000  74.00000\n [19] 110.00000  76.00000  90.00000  29.00000  46.00000  31.00000\n [25] 119.00000  32.00000 115.00000 118.00000  74.00000  99.00000\n [31]  94.00000  58.00000  32.00000  38.00000  54.00000  84.00000\n [37]  76.00000  41.00000  73.00000  60.00000  98.00000  53.00000\n [43]  69.00000  42.00000  79.00000  63.00000  90.00000  98.00000\n [49]  52.00000  93.00000  32.00000  90.00000  40.00000  64.00000\n [55] 103.00000  81.00000  82.00000  91.00000  93.00000  71.00000\n [61] 102.00000  32.00000  45.00000  88.00000  67.00000  26.00000\n [67]  92.00000  61.00000  69.00000  59.00000  81.00000  51.00000\n [73]  45.00000  90.00000  68.00000 111.00000  87.00000  71.00000\n [79]  48.00000  67.00000 100.00000  72.00000  83.00000  36.00000\n [85]  25.00000 103.00000  84.00000  67.00000  42.00000  56.07143\n [91]  67.14286  46.00000 113.00000  30.00000  97.00000  25.00000\n [97]  42.00000  82.00000  77.00000  47.00000  69.00000  93.00000\n[103]  22.00000  91.00000  96.00000 100.00000  33.00000 107.00000\n[109]  79.00000  65.00000  62.00000 118.00000  99.00000  29.00000\n[115]  87.00000  35.00000  75.00000  75.34722  88.00000  94.00000\n[121] 105.00000  89.00000 100.00000 103.00000 113.00000  78.00000\n[127]  68.00000  48.00000 100.00000 120.00000  84.00000  69.00000\n[133]  87.00000  98.00000  31.00000  94.00000  68.81481  42.00000\n[139] 103.00000  62.00000  60.00000  42.00000  84.00000  88.00000\n[145]  68.00000  63.00000  83.00000  54.00000 119.00000 120.00000\n[151]  84.00000  58.00000  67.77778  36.00000  69.00000  72.00000\n[157]  34.00000  58.00000  90.00000  60.00000  28.00000  21.00000\n[163]  74.00000  64.00000  64.00000  58.00000  67.00000  73.00000\n[169]  89.00000  41.00000  39.00000 106.00000 102.00000  91.00000\n[175]  24.00000  89.00000 107.00000  72.00000  89.86364  25.00000\n[181] 112.00000  83.00000  60.00000  74.00000  33.00000 100.00000\n[187]  51.00000  32.00000  37.00000 117.00000  37.00000  42.00000\n[193]  26.00000  70.00000  56.07143  93.00000  65.50000  61.00000\n[199]  80.00000  88.00000  92.00000  83.00000  78.00000  82.00000\n[205]  80.00000  22.00000  67.00000 105.00000  54.00000  21.00000\n[211]  41.00000 118.00000  69.00000  84.00000 115.00000  83.00000\n[217]  33.00000  44.00000  61.00000  79.00000 120.00000  44.00000\n[223] 119.00000  45.00000  82.00000  25.00000  33.00000  64.00000\n[229]  67.50000 104.00000  60.00000  69.00000  80.00000  76.00000\n[235]  62.00000  32.00000  34.00000  28.00000  24.00000 105.00000\n[241]  80.00000  63.00000  46.00000  68.81481  30.00000  43.00000\n[247]  56.00000 114.00000  52.00000  67.00000 105.00000 111.00000\n[253]  97.00000  24.00000 104.00000  55.55556  40.00000  62.00000\n[259]  38.00000  36.00000 117.00000  42.00000  77.00000  26.00000\n[265]  29.00000  35.00000  93.00000  82.00000  57.00000  69.00000\n[271]  26.00000  56.00000  33.00000 106.00000  93.00000 119.00000\n[277]  69.00000  48.00000 113.00000  57.00000  86.00000  69.00000\n[283]  96.00000 110.00000  46.00000  26.00000 118.00000  44.00000\n[289]  40.00000  77.00000 111.00000  70.00000  66.00000  84.00000\n[295]  76.00000  35.00000  44.00000  83.00000  75.34722  40.00000\n[301]  78.00000  93.00000  77.00000  52.00000  98.00000  67.77778\n[307]  32.00000  92.00000  80.00000 111.00000  65.00000  68.00000\n[313] 117.00000  81.00000  33.00000  21.00000  36.00000  30.00000\n[319]  72.00000  45.00000  70.00000  39.00000  50.00000 105.00000\n[325]  65.00000  69.00000  30.00000  41.72727  66.00000  54.00000\n[331]  59.00000  63.00000  33.00000  60.00000 117.00000  70.00000\n[337]  35.00000  38.00000  24.00000  44.00000  29.00000 120.00000\n[343] 102.00000  42.00000  80.00000  68.00000 107.00000  31.85714\n[349] 102.00000  27.00000 101.00000 115.00000 103.00000  67.00000\n[355]  68.81481 100.00000 109.00000  73.00000  96.00000  62.00000\n[361]  86.00000  25.00000  55.00000  75.00000  21.00000  30.00000\n[367]  56.00000 106.00000  22.00000 100.00000  41.00000  81.00000\n[373]  50.00000  83.77778  47.00000  46.00000  60.00000  61.00000\n[379]  88.00000 111.00000  64.00000  65.00000  28.00000 117.00000\n[385]  37.00000  73.00000 116.00000  75.34722  89.00000  42.00000\n[391]  75.00000  63.00000  42.00000  51.00000  58.00000 108.00000\n[397]  23.00000  26.00000  47.50000  37.00000\nattr(,\"var_type\")\n[1] \"numerical\"\nattr(,\"method\")\n[1] \"rpart\"\nattr(,\"na_pos\")\n [1]  14  90  91 118 137 153 179 195 197 229 244 256 299 306 328 348\n[17] 355 374 388 399\nattr(,\"type\")\n[1] \"missing values\"\nattr(,\"message\")\n[1] \"complete imputation\"\nattr(,\"success\")\n[1] TRUE\nattr(,\"class\")\n[1] \"imputation\" \"numeric\"   \n\n# summary of imputate\nsummary(income)\n\n\n* Impute missing values based on Recursive Partitioning and Regression Trees\n - method : rpart\n\n* Information of Imputation (before vs after)\n             Original   Imputation\nn        380.00000000 400.00000000\nna        20.00000000   0.00000000\nmean      69.32105263  69.07811282\nsd        28.06686473  27.53886441\nse_mean    1.43979978   1.37694322\nIQR       48.00000000  45.50000000\nskewness   0.03601821   0.05313579\nkurtosis  -1.10286001  -1.04030028\np00       21.00000000  21.00000000\np01       21.79000000  21.99000000\np05       26.00000000  26.00000000\np10       31.90000000  32.00000000\np20       40.00000000  41.00000000\np25       44.00000000  44.75000000\np30       50.00000000  51.00000000\np40       62.00000000  62.00000000\np50       69.00000000  69.00000000\np60       78.00000000  77.00000000\np70       87.30000000  84.60000000\np75       92.00000000  90.25000000\np80       98.00000000  96.00000000\np90      108.10000000 107.00000000\np95      115.05000000 115.00000000\np99      119.21000000 119.01000000\np100     120.00000000 120.00000000\n\n# viz of imputate\nplot(income)\n\n\n\n\n다음은 범주형 변수인 urban을 “mice” 방법으로 결측치를 대체한다. summary()는 결측치 대체 정보를 요약하고, plot()은 결측정보를 시각화한다.\n\n\nlibrary(mice)\n\nurban <- imputate_na(carseats, Urban, US, method = \"mice\")\n\n\n\n iter imp variable\n  1   1  Income  Urban\n  1   2  Income  Urban\n  1   3  Income  Urban\n  1   4  Income  Urban\n  1   5  Income  Urban\n  2   1  Income  Urban\n  2   2  Income  Urban\n  2   3  Income  Urban\n  2   4  Income  Urban\n  2   5  Income  Urban\n  3   1  Income  Urban\n  3   2  Income  Urban\n  3   3  Income  Urban\n  3   4  Income  Urban\n  3   5  Income  Urban\n  4   1  Income  Urban\n  4   2  Income  Urban\n  4   3  Income  Urban\n  4   4  Income  Urban\n  4   5  Income  Urban\n  5   1  Income  Urban\n  5   2  Income  Urban\n  5   3  Income  Urban\n  5   4  Income  Urban\n  5   5  Income  Urban\n\n# result of imputate\nurban\n\n\n  [1] Yes Yes Yes Yes Yes No  Yes Yes No  No  No  Yes Yes Yes Yes No \n [17] Yes Yes No  Yes Yes No  Yes Yes Yes No  No  Yes Yes Yes Yes Yes\n [33] No  Yes Yes No  No  No  Yes No  No  Yes Yes Yes Yes Yes No  Yes\n [49] Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes No  Yes Yes\n [65] No  No  Yes Yes Yes Yes Yes No  Yes No  No  No  Yes No  Yes Yes\n [81] Yes Yes Yes Yes No  No  Yes No  Yes Yes No  Yes Yes Yes Yes Yes\n [97] No  Yes No  No  No  Yes No  Yes Yes Yes No  Yes Yes No  Yes Yes\n[113] Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes No  Yes No  Yes Yes\n[129] Yes No  Yes Yes Yes Yes Yes No  No  Yes Yes No  Yes Yes Yes Yes\n[145] No  Yes Yes No  No  Yes No  No  No  No  No  Yes Yes No  Yes No \n[161] No  No  Yes No  No  Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes\n[177] No  Yes No  Yes Yes Yes Yes Yes No  Yes No  Yes Yes No  No  Yes\n[193] No  Yes Yes Yes Yes Yes Yes Yes No  Yes No  Yes Yes Yes Yes No \n[209] Yes No  No  Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes\n[225] No  Yes Yes Yes No  No  No  No  Yes No  No  Yes Yes Yes Yes Yes\n[241] Yes Yes No  Yes Yes No  Yes Yes Yes Yes Yes Yes Yes No  Yes Yes\n[257] Yes Yes No  No  Yes Yes Yes Yes Yes Yes No  No  Yes Yes Yes Yes\n[273] Yes Yes Yes Yes Yes Yes No  Yes Yes No  Yes No  No  Yes No  Yes\n[289] No  Yes No  Yes Yes Yes Yes No  Yes Yes Yes No  Yes Yes Yes Yes\n[305] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  No  No \n[321] Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes No  Yes Yes Yes Yes Yes\n[337] Yes Yes Yes Yes Yes No  No  Yes No  Yes No  No  Yes No  No  No \n[353] Yes No  Yes Yes Yes Yes Yes Yes No  No  Yes Yes Yes No  No  Yes\n[369] No  Yes Yes Yes No  Yes Yes Yes Yes No  Yes Yes Yes Yes Yes Yes\n[385] Yes Yes Yes No  Yes Yes Yes Yes Yes No  Yes Yes No  Yes Yes Yes\nattr(,\"var_type\")\n[1] categorical\nattr(,\"method\")\n[1] mice\nattr(,\"na_pos\")\n [1]  38  90 159 206 237 252 281 283 335 378\nattr(,\"seed\")\n[1] 43320\nattr(,\"type\")\n[1] missing values\nattr(,\"message\")\n[1] complete imputation\nattr(,\"success\")\n[1] TRUE\nLevels: No Yes\n\n# summary of imputate\nsummary(urban)\n\n\n* Impute missing values based on Multivariate Imputation by Chained Equations\n - method : mice\n - random seed : 43320\n\n* Information of Imputation (before vs after)\n     original imputation original_percent imputation_percent\nNo        115        117            28.75              29.25\nYes       275        283            68.75              70.75\n<NA>       10          0             2.50               0.00\n\n# viz of imputate\nplot(urban)\n\n\n\n\ndplyr과의 협업\n다음은 dplyr를 이용해서 이상치를 대체한 Income 변수를 US의 수준별로 산술평균을 구하는 예제다.\n\n\n# The mean before and after the imputation of the Income variable\ncarseats %>%\n  mutate(Income_imp = imputate_na(carseats, Income, US, method = \"knn\")) %>%\n  group_by(US) %>%\n  summarise(orig = mean(Income, na.rm = TRUE),\n    imputation = mean(Income_imp))\n\n\n# A tibble: 2 x 3\n  US     orig imputation\n  <fct> <dbl>      <dbl>\n1 No     65.7       65.5\n2 Yes    71.3       71.5\n\n이상치의 대체\nimputate_outlier()을 이용한 이상치의 대체\nimputate_outlier()는 변수에 포함된 이상치를 대체한다. 이상치가 포함된 예측변수(predictor)는 수치형 변수만 지원하며, 다음과 같은 method를 지원한다.\npredictor가 수치형 변수일 경우\n“mean” : 산술평균으로 대체\n“median” : 중위수로 대체\n“mode” : 최빈수로 대체\n“capping” : 상위 이상치를 95/백분위수로 대체하고 하위 이상치를 5/백분위수로 대체\n\n다음처럼 imputate_outlier()는 carseats의 수치형 변수인 Price를 “capping” 방법으로 이상치를 대체한다. summary()는 이상치 대체 정보를 요약하고, plot()은 결측정보를 시각화한다.\n\n\nprice <- imputate_outlier(carseats, Price, method = \"capping\")\n\n# result of imputate\nprice\n\n\n  [1] 120.00  83.00  80.00  97.00 128.00  72.00 108.00 120.00 124.00\n [10] 124.00 100.00  94.00 136.00  86.00 118.00 144.00 110.00 131.00\n [19]  68.00 121.00 131.00 109.00 138.00 109.00 113.00  82.00 131.00\n [28] 107.00  97.00 102.00  89.00 131.00 137.00 128.00 128.00  96.00\n [37] 100.00 110.00 102.00 138.00 126.00 124.00  77.00 134.00  95.00\n [46] 135.00  70.00 108.00  98.00 149.00 108.00 108.00 129.00 119.00\n [55] 144.00 154.00  84.00 117.00 103.00 114.00 123.00 107.00 133.00\n [64] 101.00 104.00 128.00  91.00 115.00 134.00  99.00  99.00 150.00\n [73] 116.00 104.00 136.00  92.00  70.00  89.00 145.00  90.00  79.00\n [82] 128.00 139.00  94.00 121.00 112.00 134.00 126.00 111.00 119.00\n [91] 103.00 107.00 125.00 104.00  84.00 148.00 132.00 129.00 127.00\n[100] 107.00 106.00 118.00  97.00  96.00 138.00  97.00 139.00 108.00\n[109] 103.00  90.00 116.00 151.00 125.00 127.00 106.00 129.00 128.00\n[118] 119.00  99.00 128.00 131.00  87.00 108.00 155.00 120.00  77.00\n[127] 133.00 116.00 126.00 147.00  77.00  94.00 136.00  97.00 131.00\n[136] 120.00 120.00 118.00 109.00  94.00 129.00 131.00 104.00 159.00\n[145] 123.00 117.00 131.00 119.00  97.00  87.00 114.00 103.00 128.00\n[154] 150.00 110.00  69.00 157.00  90.00 112.00  70.00 111.00 160.00\n[163] 149.00 106.00 141.00 155.05 137.00  93.00 117.00  77.00 118.00\n[172]  55.00 110.00 128.00 155.05 122.00 154.00  94.00  81.00 116.00\n[181] 149.00  91.00 140.00 102.00  97.00 107.00  86.00  96.00  90.00\n[190] 104.00 101.00 173.00  93.00  96.00 128.00 112.00 133.00 138.00\n[199] 128.00 126.00 146.00 134.00 130.00 157.00 124.00 132.00 160.00\n[208]  97.00  64.00  90.00 123.00 120.00 105.00 139.00 107.00 144.00\n[217] 144.00 111.00 120.00 116.00 124.00 107.00 145.00 125.00 141.00\n[226]  82.00 122.00 101.00 163.00  72.00 114.00 122.00 105.00 120.00\n[235] 129.00 132.00 108.00 135.00 133.00 118.00 121.00  94.00 135.00\n[244] 110.00 100.00  88.00  90.00 151.00 101.00 117.00 156.00 132.00\n[253] 117.00 122.00 129.00  81.00 144.00 112.00  81.00 100.00 101.00\n[262] 118.00 132.00 115.00 159.00 129.00 112.00 112.00 105.00 166.00\n[271]  89.00 110.00  63.00  86.00 119.00 132.00 130.00 125.00 151.00\n[280] 158.00 145.00 105.00 154.00 117.00  96.00 131.00 113.00  72.00\n[289]  97.00 156.00 103.00  89.00  74.00  89.00  99.00 137.00 123.00\n[298] 104.00 130.00  96.00  99.00  87.00 110.00  99.00 134.00 132.00\n[307] 133.00 120.00 126.00  80.00 166.00 132.00 135.00  54.00 129.00\n[316] 171.00  72.00 136.00 130.00 129.00 152.00  98.00 139.00 103.00\n[325] 150.00 104.00 122.00 104.00 111.00  89.00 112.00 134.00 104.00\n[334] 147.00  83.00 110.00 143.00 102.00 101.00 126.00  91.00  93.00\n[343] 118.00 121.00 126.00 149.00 125.00 112.00 107.00  96.00  91.00\n[352] 105.00 122.00  92.00 145.00 146.00 164.00  72.00 118.00 130.00\n[361] 114.00 104.00 110.00 108.00 131.00 162.00 134.00  77.00  79.00\n[370] 122.00 119.00 126.00  98.00 116.00 118.00 124.00  92.00 125.00\n[379] 119.00 107.00  89.00 151.00 121.00  68.00 112.00 132.00 160.00\n[388] 115.00  78.00 107.00 111.00 124.00 130.00 120.00 139.00 128.00\n[397] 120.00 159.00  95.00 120.00\nattr(,\"method\")\n[1] \"capping\"\nattr(,\"var_type\")\n[1] \"numerical\"\nattr(,\"outlier_pos\")\n[1]  43 126 166 175 368\nattr(,\"outliers\")\n[1]  24  49 191 185  53\nattr(,\"type\")\n[1] \"outliers\"\nattr(,\"message\")\n[1] \"complete imputation\"\nattr(,\"success\")\n[1] TRUE\nattr(,\"class\")\n[1] \"imputation\" \"numeric\"   \n\n# summary of imputate\nsummary(price)\n\n\nImpute outliers with capping\n\n* Information of Imputation (before vs after)\n            Original  Imputation\nn        400.0000000 400.0000000\nna         0.0000000   0.0000000\nmean     115.7950000 115.8927500\nsd        23.6766644  22.6109187\nse_mean    1.1838332   1.1305459\nIQR       31.0000000  31.0000000\nskewness  -0.1252862  -0.0461621\nkurtosis   0.4518850  -0.3030578\np00       24.0000000  54.0000000\np01       54.9900000  67.9600000\np05       77.0000000  77.0000000\np10       87.0000000  87.0000000\np20       96.8000000  96.8000000\np25      100.0000000 100.0000000\np30      104.0000000 104.0000000\np40      110.0000000 110.0000000\np50      117.0000000 117.0000000\np60      122.0000000 122.0000000\np70      128.3000000 128.3000000\np75      131.0000000 131.0000000\np80      134.0000000 134.0000000\np90      146.0000000 146.0000000\np95      155.0500000 155.0025000\np99      166.0500000 164.0200000\np100     191.0000000 173.0000000\n\n# viz of imputate\nplot(price)\n\n\n\n\ndplyr과의 협업\n다음은 dplyr를 이용해서 이상치를 대체한 Price 변수를 US의 수준별로 산술평균을 구하는 예제다.\n\n\n# The mean before and after the imputation of the Price variable\ncarseats %>%\n  mutate(Price_imp = imputate_outlier(carseats, Price, method = \"capping\")) %>%\n  group_by(US) %>%\n  summarise(orig = mean(Price, na.rm = TRUE),\n    imputation = mean(Price_imp, na.rm = TRUE))\n\n\n# A tibble: 2 x 3\n  US     orig imputation\n  <fct> <dbl>      <dbl>\n1 No     114.       114.\n2 Yes    117.       117.\n\n표준화와 치우친 데이터의 보정\ntransform()의 기능\ntransform()는 변수를 변환한다. 수치형 변수만 지원하며, 다음과 같은 method를 제공한다.\n표준화\n“zscore” : z-score 변환. (x - mu) / sigma\n“minmax” : minmax 변환. (x - min) / (max - min)\n\n치우침 보정\n“log” : log 변환. log(x)\n“log+1” : log 변환. log(x + 1). 0을 포함한 값들이 많을 때 유용함.\n“sqrt” : 제곱근 변환\n“1/x” : 1 / x 변환\n“x^2” : 제곱 변환\n“x^3” : 세제곱 변환\n\ntransform()을 이용한 표준화\n표준화를 수행하는 method “zscore”와 “minmax”를 이용한다.\n\n\ncarseats %>% \n  mutate(Income_minmax = transform(carseats$Income, method = \"minmax\"),\n    Sales_minmax = transform(carseats$Sales, method = \"minmax\")) %>% \n  select(Income_minmax, Sales_minmax) %>% \n  boxplot()\n\n\n\n\ntransform()을 이용한 치우친 데이터의 보정\nfind_skewness()는 치우친 데이터를 찾기 위해서 왜도를 구한다.\n\n\n# find index of skewed variables\nfind_skewness(carseats)\n\n\n[1] 4\n\n# find names of skewed variables\nfind_skewness(carseats, index = FALSE)\n\n\n[1] \"Advertising\"\n\n# compute the skewness\nfind_skewness(carseats, value = TRUE)\n\n\n      Sales   CompPrice      Income Advertising  Population \n      0.185      -0.043       0.036       0.637      -0.051 \n      Price         Age   Education \n     -0.125      -0.077       0.044 \n\n# compute the skewness & filtering with threshold\nfind_skewness(carseats, value = TRUE, thres = 0.1)\n\n\n      Sales Advertising       Price \n      0.185       0.637      -0.125 \n\nAdvertising의 왜도가 0.637로 좌측으로 어느정도 기울어져 있어서 다음처럼 transformation()를 이용해서 “log” 방법으로 변환한다. summary()는 변환정보를 요약하고, plot()은 변환정보를 시각화한다.\n\n\nAdvertising_log = transform(carseats$Advertising, method = \"log\")\n\n# result of transformation\nhead(Advertising_log)\n\n\n[1] 2.397895 2.772589 2.302585 1.386294 1.098612 2.564949\n\n# summary of transformation\nsummary(Advertising_log)\n\n\n* Resolving Skewness with log\n\n* Information of Transformation (before vs after)\n            Original Transformation\nn        400.0000000    400.0000000\nna         0.0000000      0.0000000\nmean       6.6350000           -Inf\nsd         6.6503642            NaN\nse_mean    0.3325182            NaN\nIQR       12.0000000            Inf\nskewness   0.6395858            NaN\nkurtosis  -0.5451178            NaN\np00        0.0000000           -Inf\np01        0.0000000           -Inf\np05        0.0000000           -Inf\np10        0.0000000           -Inf\np20        0.0000000           -Inf\np25        0.0000000           -Inf\np30        0.0000000           -Inf\np40        2.0000000      0.6931472\np50        5.0000000      1.6094379\np60        8.4000000      2.1265548\np70       11.0000000      2.3978953\np75       12.0000000      2.4849066\np80       13.0000000      2.5649494\np90       16.0000000      2.7725887\np95       19.0000000      2.9444390\np99       23.0100000      3.1359198\np100      29.0000000      3.3672958\n\n# viz of transformation\nplot(Advertising_log)\n\n\n\n\nlog 변환된 값에 -Inf가 포함되어 있는 것으로 보아 원 데이터에 0이 포함되어 있는 듯하다. 그래서 이번에는 “log+1” 방법으로 변환한다.\n\n\nAdvertising_log = transform(carseats$Advertising, method = \"log+1\")\n\n# result of transformation\nhead(Advertising_log)\n\n\n[1] 2.484907 2.833213 2.397895 1.609438 1.386294 2.639057\n\n# summary of transformation\nsummary(Advertising_log)\n\n\n* Resolving Skewness with log+1\n\n* Information of Transformation (before vs after)\n            Original Transformation\nn        400.0000000   400.00000000\nna         0.0000000     0.00000000\nmean       6.6350000     1.46247709\nsd         6.6503642     1.19436323\nse_mean    0.3325182     0.05971816\nIQR       12.0000000     2.56494936\nskewness   0.6395858    -0.19852549\nkurtosis  -0.5451178    -1.66342876\np00        0.0000000     0.00000000\np01        0.0000000     0.00000000\np05        0.0000000     0.00000000\np10        0.0000000     0.00000000\np20        0.0000000     0.00000000\np25        0.0000000     0.00000000\np30        0.0000000     0.00000000\np40        2.0000000     1.09861229\np50        5.0000000     1.79175947\np60        8.4000000     2.23936878\np70       11.0000000     2.48490665\np75       12.0000000     2.56494936\np80       13.0000000     2.63905733\np90       16.0000000     2.83321334\np95       19.0000000     2.99573227\np99       23.0100000     3.17846205\np100      29.0000000     3.40119738\n\n# viz of transformation\nplot(Advertising_log)\n\n\n\n\nBinning\nbinning()을 이용한 개별 변수의 Binning\nbinning()는 수치형 변수를 비닝하여 범주형 변수로 변환한다. 다음과 같은 type의 비닝을 지원한다.\n“quantile” : 동일한 돗수가 포함되도록 quantile을 이용하여 범주화\n“equal” : 동일한 길이의 구간을 갖도록 범주화\n“pretty” : 적당히 보기 좋은 구간으로 범주화\n“kmeans” : K-means clustering 기법을 이용한 범주화\n“bclust” : Bagged clustering 기법을 이용한 범주화\nbinning()을 이용하여 Income을 비닝하는 몇 가지의 방법을 예시한다.\n\n\n# Binning the carat variable. default type argument is \"quantile\"\nbin <- binning(carseats$Income)\n# Print bins class object\nbin\n\n\nbinned type: quantile\nnumber of bins: 10\nx\n [21,31.36667]  (31.36667,40]        (40,50]        (50,62] \n            38             40             37             40 \n       (62,69]        (69,78]  (78,87.56667]  (87.56667,98] \n            40             34             37             41 \n (98,108.6333] (108.6333,120]           <NA> \n            35             38             20 \n\n# Summarise bins class object\nsummary(bin)\n\n\n           levels freq   rate\n1   [21,31.36667]   38 0.0950\n2   (31.36667,40]   40 0.1000\n3         (40,50]   37 0.0925\n4         (50,62]   40 0.1000\n5         (62,69]   40 0.1000\n6         (69,78]   34 0.0850\n7   (78,87.56667]   37 0.0925\n8   (87.56667,98]   41 0.1025\n9   (98,108.6333]   35 0.0875\n10 (108.6333,120]   38 0.0950\n11           <NA>   20 0.0500\n\n# Plot bins class object\nplot(bin)\n\n\n\n# Using labels argument\nbin <- binning(carseats$Income, nbins = 4,\n              labels = c(\"LQ1\", \"UQ1\", \"LQ3\", \"UQ3\"))\nbin\n\n\nbinned type: quantile\nnumber of bins: 4\nx\n LQ1  UQ1  LQ3  UQ3 <NA> \n  98   97   91   94   20 \n\n# Using another type argument\nbinning(carseats$Income, nbins = 5, type = \"equal\")\n\n\nbinned type: equal\nnumber of bins: 5\nx\n   [21,40.8]  (40.8,60.6]  (60.6,80.4] (80.4,100.2]  (100.2,120] \n          78           68           92           79           63 \n        <NA> \n          20 \n\nbinning(carseats$Income, nbins = 5, type = \"pretty\")\n\n\nbinned type: pretty\nnumber of bins: 5\nx\n  [20,40]   (40,60]   (60,80]  (80,100] (100,120]      <NA> \n       78        68        92        79        63        20 \n\nbinning(carseats$Income, nbins = 5, type = \"kmeans\")\n\n\nbinned type: kmeans\nnumber of bins: 5\nx\n  [21,38.5] (38.5,56.5] (56.5,75.5] (75.5,97.5]  (97.5,120] \n         72          58          87          86          77 \n       <NA> \n         20 \n\nbinning(carseats$Income, nbins = 5, type = \"bclust\")\n\n\nbinned type: bclust\nnumber of bins: 5\nx\n  [21,37.5] (37.5,55.5] (55.5,78.5] (78.5,95.5]  (95.5,120] \n         69          58         102          69          82 \n       <NA> \n         20 \n\n# -------------------------\n# Using pipes & dplyr\n# -------------------------\nlibrary(dplyr)\n\ncarseats %>%\n mutate(Income_bin = binning(carseats$Income)) %>%\n group_by(ShelveLoc, Income_bin) %>%\n summarise(freq = n()) %>%\n arrange(desc(freq)) %>%\n head(10)\n\n\n# A tibble: 10 x 3\n# Groups:   ShelveLoc [1]\n  ShelveLoc Income_bin     freq\n  <fct>     <ord>         <int>\n1 Medium    [21,31.36667]    25\n2 Medium    (62,69]          23\n3 Medium    (50,62]          22\n4 Medium    (31.36667,40]    21\n# … with 6 more rows\n\nbinning_by()을 이용한 Optimal Binning\nbinning_by()는 수치형 변수를 Optimal Binning하여 범주형 변수로 변환한다. 이 방법은 스코어카드 모형을 개발할때 자주 사용하는 방법이다.\n다음의 binning_by() 예제는 US가 binary class를 갖는 target 변수일 경우에 Advertising를 Optimal Binning하는 방법의 예시다.\n\n\n# optimal binning\nbin <- binning_by(carseats, \"US\", \"Advertising\")\n\n\n` US ` ~ ` Advertising `\n<environment: 0x7ff8ef6883d8>\n\nbin\n\n\nbinned type: optimal\nnumber of bins: 3\nx\n[-1,0]  (0,6] (6,29] \n   144     69    187 \n\n# summary optimal_bins class\nsummary(bin)\n\n\n── Binning Table ──────────────────────── Several Metrics ── \n     Bin CntRec CntPos CntNeg RatePos RateNeg    Odds      WoE\n1 [-1,0]    144     19    125 0.07364 0.88028  0.1520 -2.48101\n2  (0,6]     69     54     15 0.20930 0.10563  3.6000  0.68380\n3 (6,29]    187    185      2 0.71705 0.01408 92.5000  3.93008\n4  Total    400    258    142 1.00000 1.00000  1.8169       NA\n       IV     JSD     AUC\n1 2.00128 0.20093 0.03241\n2 0.07089 0.00869 0.01883\n3 2.76272 0.21861 0.00903\n4 4.83489 0.42823 0.06028\n\n── General Metrics ───────────────────────────────────────── \n• Gini index                       :  -0.87944\n• IV (Jeffrey)                     :  4.83489\n• JS (Jensen-Shannon) Divergence   :  0.42823\n• Kolmogorov-Smirnov Statistics    :  0.80664\n• HHI (Herfindahl-Hirschman Index) :  0.37791\n• HHI (normalized)                 :  0.06687\n• Cramer's V                       :  0.81863 \n\n── Significance Tests ──────────────────── Chisquare Test ── \n   Bin A  Bin B statistics      p_value\n1 [-1,0]  (0,6]   87.67064 7.731349e-21\n2  (0,6] (6,29]   34.73349 3.780706e-09\n\n# information value \nattr(bin, \"iv\")\n\n\nNULL\n\n# information value table\nattr(bin, \"ivtable\")\n\n\nNULL\n\n# visualize optimal_bins class\nplot(bin, sub = \"bins of Advertising variable\")\n\n\n\n\ntransformation_report()를 이용한 데이터변환 보고서 작성\ntransformation_report()는 데이터 프레임이나 데이터 프레임을 상속받은 객체(tbl_df, tbl 등)의 모든 변수들에 대해서 데이터변환 보고서를 작성한다.\ntransformation_report()는 데이터변환 보고서를 다음과 같은 두 개의 형태로 작성한다.\nLatex에 기반한 pdf 파일\nhtml 파일\n보고서의 목차는 다음과 같다.\n값의 대체\n결측치\n결측치의 대체 정보\n(해당 변수들)\n\n이상치\n이상치의 대체 정보\n(해당 변수들)\n\n\n치우침의 해결\n치우친 변수들의 정보\n(해당 변수들)\n\n\n비닝\n비닝을 위한 수치형 변수들\n비닝\n(해당 변수들)\n\nOptimal 비닝\n(해당 변수들)\n\n\n다음은 carseats의 데이터변환 보고서를 작성한다. 파일 형식은 pdf이며, 파일이름은 Transformation_Report.pdf다.\n\n\ncarseats %>%\n  transformation_report(target = US)\n\n\n\n다음은 transformation.html라는 이름의 html 형식의 보고서를 생성한다.\n\n\ncarseats %>%\n  transformation_report(target = US, output_format = \"html\", \n    output_file = \"transformation.html\")\n\n\n\n데이터변환 보고서는 데이터 변환 과정에 도움을 주기 위한 자동화 보고서다. 보고서 결과를 참고하여 데이터 변환 시나리오를 설계한다.\n데이터변환 리포트 내용\npdf 파일의 내용\n보고서의 표지는 다음 그림과 같다.\n\n\n\nFigure 1: 데이터변환 보고서 표지\n\n\n\n보고서의 차례는 다음 그림과 같다.\n\n\n\nFigure 2: 데이터변환 보고서 차례\n\n\n\n많은 정보는 보고서에서 표와 시각화 결과로 표현된다. 예시는 다음 그림과 같다.\n\n\n\nFigure 3: 데이터변환 보고서 도표 및 시각화 예시\n\n\n\nhtml 파일의 내용\n보고서의 타이틀과 목차는 다음 그림과 같다.\n\n\n\nFigure 4: 데이터변환 보고서 타이틀과 목차\n\n\n\n많은 정보는 보고서에서 표로 표현된다. html 파일에서 표의 예시는 다음 그림과 같다.\n\n\n\nFigure 5: 데이터변환 보고서 도표 예시 (웹)\n\n\n\n데이터변환 보고서에서 Binning 정보는 시각화 결과를 포함한다. html 파일의 결과는 다음 그림과 같다.\n\n\n\nFigure 6: 데이터변환 보고서 Binning 정보 (웹)\n\n\n\n\n\n\n",
    "preview": "posts/2018-05-07-r-vignette03.ko/2018-05-07-r-vignette03.ko_files/figure-html5/imputate_na-1.png",
    "last_modified": "2021-10-10T23:50:15+09:00",
    "input_file": {},
    "preview_width": 1344,
    "preview_height": 768
  },
  {
    "path": "posts/2018-01-12-google_trends/",
    "title": "Google Trends Analysis_Case Study",
    "description": "R에서 `Google Trends API`를 이용해서 관심 검색어의 traffic과 traffic 추이를 분석하는 방법을 실 사례로 따라가면서 학습할 수 있도로 정리한 학습자료다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2018-01-12",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n들어가기\n“Google Trends Analysis - Case Study”는 R에서 Google Trends API를 이용해서 관심 검색어의 traffic과 traffic 추이를 분석하는 방법을 실 사례로 따라가면서 학습할 수 있도로 정리한 학습자료다.\n배경\n일반 대중들의 관심사와 관심의 트랜드를 설명할 데이터 소스로 검색 포털의 검색 통계를 하나의 대상으로 선정하였다. 그리고 그 가능성을 검증하기 위해서 Google Trends의 통계를 수집하고 표현한 방법을 정리하였고, 그 과정에서의 R 코드를 공유한다.\n학습 목표\nGoogle Trends로부터 관심 키워드의 traffic을 수집하는 방법을 숙지한다.\n단일 키워드와 두개 이상의 키워드로 traffic을 수집하는 방법\n지역별 (geographic regions)로 traffic을 수집하는 방법\n예) 한국, 미국\n\n임의의 기간동안의 traffic을 수집하는 방법\n\nGoogle Trends에서 제공하는 통계 정보를 이해한다.\n일자별 검색 관심도\n단위 기간동안의 지역(region, 광역시도, 주)별 키워드 관심도\n단위 기간동안의 도시별 키워드 관심도\n단위 기간동안의 관련 주제(topics)별 키워드 관심도\n관련 주제 Top Rank\n인기 급상승 관련 주제 Top Rank\n\n단위 기간동안의 관련 검색어(related queries)별 키워드 관심도\n관련 검색어 Top Rank\n인기 급상승 관련 검색어 Top Rank\n\n\nGoogle Trends에서 제공하는 통계 정보를 시각화하는 방법을 이해한다.\ntraffic의 추이를 표현하는 시계열 플롯\ntop traffic을 파악하는 버블(bubble) 플롯\n\nBuzz Trafics Analysis\nNaver, Google 등의 검색 포털에서의 검색 traffic은 그 시대의 일반 대중들의 관심사를 대변해준다. 또한 시간의 추이에 따른 traffic의 증감은 단위 기간 안에서의 관심의 트랜드를 보여 주는 주요한 정보다.\nGoogle Trends\nGoogle Trends는 Google 검색 로그로부터 집계된 검색 통계정보를 제공하는 서비스로 일부 기능들은 Open API로 조회가 가능하다.\n\n\n\nFigure 1: Google Trends Logo\n\n\n\nGoogle Trends 통계정보의 활용\n준비하기\n패키지 로드\nGoogle Trends 정보를 수집하는 패키지는 gtrendsR 패키지다. 분석과 관련된 패키지를 로드한다.\n\n\n###############################################################\n## 필요한 패키지 로드\n###############################################################\nlibrary(gtrendsR)\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\n\n\n\n사용자 함수의 정의\n필요한 패키지를 로드하고, 버블플롯을 그리는 함수를 정의한다. Case Study 후반부에 이 사용자 정의함수로 버블플롯을 그릴 것이므로 미리 생성해 두자.\n\n\n###############################################################\n## 사용자 정의함수 - 버블 플롯을 그리는 함수\n###############################################################\nbbplot <- function(label, value, textColor = \"#333333\",\n                   color = RColorBrewer::brewer.pal(8, \"Dark2\")) {\n  library(bubbles)\n  library(RColorBrewer)\n\n  color <- substr(color, 1, 7)\n  \n  n <- length(label)\n  n.pal <- length(color)\n\n  if (n > n.pal) {\n    color <- c(color, rep(color[n.pal], n - n.pal))\n  }\n\n  if (n < n.pal) {\n    color <- color[1:n]\n  }\n\n  bubbles(value, label, color = color, textColor = textColor)\n}\n\n\n\nBitcoin 거래 시세 데이터\nBitcoin 거래 시세는 별도로 수집해서 CSV 파일에 담아 놓았다. 시세를 담은 CSV 파일인 <a href=“data/market-price.csv”, target = \"_blank\">market-price.csv을 로드한다.\n\n\n###############################################################\n## Bitcoin 거래 시세 정보\n###############################################################\ndpath <- \"data\"\nbitprice <- read.csv(paste(dpath, \"market-price.csv\", sep = \"/\"), header = FALSE)\n\nbitprice <- bitprice %>%\n  transmute(date = as.POSIXct(V1),\n            value = V2)\n\n\n\nGoogle Trends 데이터 수집하기\nGoogle Trends 데이터를 수집하기 위해서는 인터넷에 접속된 온라인 환경이어야만 가능하다. 그러므로 이 절의 예제는 온라인에 접속된 인터넷 환경에서만 사용하기 바란다.\n두 개의 키워드로 traffic 데이터 수집\n다음처럼 gtrends()로 “YOLO”와 “DINK”의 두 검색 키워드로 traffic을 조회한다. 이 경우는 world 기준의 traffic이다. 즉, 전세계 사람들이 Google에서 검색한 통계를 수집하게 된다.\n\n\nyolo_trend <- gtrends(c(\"YOLO\", \"DINK\"))\n\n\n\ntraffic trend plot\n수집한 traffic 데이터는 다음처럼 plot() 함수로 간단하게 시계열 그래프를 그릴 수 있다.\n\n\nplot(yolo_trend)\n\n\n\n\n시계열 그래프를 보면 y-축인 Search hits의 최대값이 100임을 알수 있다. 즉, traffic 데이터는 검색 건수가 아니라 통계 시점에서 가장 큰 규모의 traffic을 100으로 놓고 산정한 상대적인 측도이다. traffic 데이터의 시계열 그래프는 ggplot2 패키지로 생성한다. 그래서 다음의 ggplot2 패키지의 기능을 추가하여 좀 더 팬시하게 그래프를 그릴 수도 있다. 두번 째 플롯이 범례를 아래로 이동해서 가독성을 높인 것이다.\n\n\np <- plot(yolo_trend) \n\n\n\n\n\np +\n  ggtitle(\"Google Trend Traffics (Interest over Time)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n대한민국 traffic 데이터 수집\ngeo 인수값을 “KR”로 지정하면 우리 나라에서 검색한 traffic 통계를 수집한다.\n\n\nyolo_trend2 <- gtrends(c(\"YOLO\", \"DINK\"), geo = \"KR\")\n\n\n\nYOLO 검색 traffic은 전세계적으로 줄어드는 추세이지만, 우리나라에서는 2016년도 말부터 증가하다가 2017년도 하반기부터 줄어드틑 추세임을 알 수 있다.\n\n\np <- plot(yolo_trend2) \n\n\n\n\n\np + \n  ggtitle(\"Google Trend Traffics (Interest over Time)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n한글 키워드 traffic 데이터 수집\n한글 키워드로 비트코인이라는 검색어의 traffic을 수집한다.\n\n\nbit_trend <- gtrends(c(\"비트코인\"), geo = \"KR\")\n\n\n\n2013년도 하반기 반짝 상승 이후로 traffic이 없다가 2017년 초부터 급증함을 알 수 있다.\n\n\np <- plot(bit_trend) \n\n\n\n\n\np +\n  ggtitle(\"Google Trend Traffics (Interest over Time)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n두 나라의 traffic 데이터 수집\n비트코인 검색어의 traffic을 미국과 우리 나라를 비교하기 위해서, 영문으로 검색한다.\n\n\nbit_trend2 <- gtrends(c(\"bitcoin\"), geo = c(\"KR\", \"US\"))\n\n\n\n두 나라의 traffic이 유사하지만 2017년도 말의 급등세는 미국에서의 규모가 더 크다.\n\n\np <- plot(bit_trend2) \n\n\n\n\n\np +\n  ggtitle(\"Google Trend Traffics (Interest over Time)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n참고로 앞에서 수집한 CSV의 비트코인 시세 데이터인 bitprice 데이터 프레임객체를 시계열 플롯으로 출력해보자.\n\n\nhead(bitprice)\n\n\n        date value\n1 2009-01-03     0\n2 2009-01-05     0\n3 2009-01-07     0\n4 2009-01-09     0\n5 2009-01-11     0\n6 2009-01-13     0\n\nbitprice %>%\n  ggplot(aes_string(x = \"date\", y = \"value\")) +\n  geom_line() +\n  xlab(\"Date\") +\n  ylab(\"Price ($)\") +\n  ggtitle(\"Bitcoin Price Trend\") +\n  theme_bw() +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(legend.title = element_blank()) \n\n\n\n\n비트코인 매매 시세가 정확히 traffic 데이터와 유사한 패턴을 보여줌을 알 수 있다.\n특정 기간의 traffic 데이터 수집\n임의의 기간동안의 검색 traffic을 수집하기 위해서는 time 인수에 다음 예제와 같은 포맷으로 시작 일자와 종료 일자를 지정할 수 있다.\n2013, 2014, 2015, 2016, 2017년의 각각의 5년치 traffic을 수집한다.\n\n\nbit_trend_2013 <- gtrends(c(\"bitcoin\"), geo = c(\"KR\", \"US\"),\n                          time = \"2013-01-01 2013-12-31\")\n\nbit_trend_2014 <- gtrends(c(\"bitcoin\"), geo = c(\"KR\", \"US\"),\n                          time = \"2014-01-01 2014-12-31\")\n\nbit_trend_2015 <- gtrends(c(\"bitcoin\"), geo = c(\"KR\", \"US\"),\n                          time = \"2015-01-01 2015-12-31\")\n\nbit_trend_2016 <- gtrends(c(\"bitcoin\"), geo = c(\"KR\", \"US\"),\n                          time = \"2016-01-01 2016-12-31\")\n\nbit_trend_2017 <- gtrends(c(\"bitcoin\"), geo = c(\"KR\", \"US\"),\n                          time = \"2017-01-01 2017-12-31\")\n\nbit_trend_2013_k <- gtrends(c(\"비트코인\"), geo = c(\"KR\"),\n                          time = \"2013-01-01 2013-12-31\")\n\nbit_trend_2014_k <- gtrends(c(\"비트코인\"), geo = c(\"KR\"),\n                          time = \"2014-01-01 2014-12-31\")\n\nbit_trend_2015_k <- gtrends(c(\"비트코인\"), geo = c(\"KR\"),\n                          time = \"2015-01-01 2015-12-31\")\n\nbit_trend_2016_k <- gtrends(c(\"비트코인\"), geo = c(\"KR\"),\n                          time = \"2016-01-01 2016-12-31\")\n\nbit_trend_2017_k <- gtrends(c(\"비트코인\"), geo = c(\"KR\"),\n                          time = \"2017-01-01 2017-12-31\")\n\n\n\n2017년도 말의 급등세가 우리 나라보다 미국이 크다는 것은 4/4분기의 중반 이후부터 두드러짐으로 알 수 있다.\n\n\np <- plot(bit_trend_2017) \n\n\n\n\n\np + \n  ggtitle(\"Google Trend Traffics (Interest over Time)\") +\n  theme(plot.title = element_text(hjust = 0.5)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nGoogle Trends 데이터의 구조\ngtrends 객체\ngtrends 패키지로 수집한 Google Trends 데이터는 gtrends 객체로 저장된다.\n\n\nis(bit_trend2)\n\n\n[1] \"gtrends\"\n\ngtrends 객체는 다음처럼 6개의 속성 데이터를 가지고 있다.\n\n\nnames(bit_trend2)\n\n\n[1] \"interest_over_time\"  \"interest_by_country\" \"interest_by_region\" \n[4] \"interest_by_dma\"     \"interest_by_city\"    \"related_topics\"     \n[7] \"related_queries\"    \n\ninterest_over_time\ninterest_over_time는 일자별로 traffic 정보를 담고 있는 데이터 프레임이다. 일자, 상대 traffic, 키워드, 국가, 그룹, 카테고리 정보를 담고 있다.\n그룹(gprop)은 web, news, images, froogle, youtube 중에서 한가지 값을 갖는데 기본값으로 수집하면 웹검색 traffic인 web으로 지정된다.\n\n\nhead(bit_trend2$interest_over_time)\n\n\n        date hits keyword geo      time gprop category\n1 2016-10-16    1 bitcoin  KR today+5-y   web        0\n2 2016-10-23    1 bitcoin  KR today+5-y   web        0\n3 2016-10-30    1 bitcoin  KR today+5-y   web        0\n4 2016-11-06    1 bitcoin  KR today+5-y   web        0\n5 2016-11-13    1 bitcoin  KR today+5-y   web        0\n6 2016-11-20    2 bitcoin  KR today+5-y   web        0\n\ntail(bit_trend2$interest_over_time)\n\n\n          date hits keyword geo      time gprop category\n515 2021-08-29   15 bitcoin  US today+5-y   web        0\n516 2021-09-05   20 bitcoin  US today+5-y   web        0\n517 2021-09-12   14 bitcoin  US today+5-y   web        0\n518 2021-09-19   18 bitcoin  US today+5-y   web        0\n519 2021-09-26   15 bitcoin  US today+5-y   web        0\n520 2021-10-03   19 bitcoin  US today+5-y   web        0\n\ninterest_by_region\ninterest_by_region은 검색한 사람이 거주하는 지역으로서의 광역시도(우리나라)와 주(States, 미국) 정보를 담고 있다.\n\n\nbit_trend2$interest_by_region\n\n\n               location hits keyword geo gprop\n1                 Seoul  100 bitcoin  KR   web\n2           Gyeonggi-do   78 bitcoin  KR   web\n3               Jeju-do   74 bitcoin  KR   web\n4               Daejeon   73 bitcoin  KR   web\n5               Incheon   60 bitcoin  KR   web\n6      Gyeongsangnam-do   59 bitcoin  KR   web\n7     Chungcheongnam-do   55 bitcoin  KR   web\n8      Gyeongsangbuk-do   55 bitcoin  KR   web\n9                 Busan   49 bitcoin  KR   web\n10                Daegu   46 bitcoin  KR   web\n11           Gangwon-do   44 bitcoin  KR   web\n12                Ulsan   43 bitcoin  KR   web\n13         Jeollabuk-do   40 bitcoin  KR   web\n14         Jeollanam-do   39 bitcoin  KR   web\n15    Chungcheongbuk-do   35 bitcoin  KR   web\n16              Gwangju   33 bitcoin  KR   web\n17               Hawaii  100 bitcoin  US   web\n18               Nevada   92 bitcoin  US   web\n19           California   92 bitcoin  US   web\n20           Washington   88 bitcoin  US   web\n21           New Jersey   86 bitcoin  US   web\n22             New York   85 bitcoin  US   web\n23             Colorado   83 bitcoin  US   web\n24                 Utah   82 bitcoin  US   web\n25              Florida   80 bitcoin  US   web\n26        Massachusetts   75 bitcoin  US   web\n27               Oregon   73 bitcoin  US   web\n28          Connecticut   73 bitcoin  US   web\n29              Arizona   73 bitcoin  US   web\n30             Illinois   72 bitcoin  US   web\n31               Alaska   70 bitcoin  US   web\n32             Maryland   68 bitcoin  US   web\n33              Montana   68 bitcoin  US   web\n34                Idaho   67 bitcoin  US   web\n35             Virginia   65 bitcoin  US   web\n36         Rhode Island   64 bitcoin  US   web\n37        New Hampshire   64 bitcoin  US   web\n38            Minnesota   64 bitcoin  US   web\n39 District of Columbia   64 bitcoin  US   web\n40         Pennsylvania   63 bitcoin  US   web\n41         North Dakota   61 bitcoin  US   web\n42             Michigan   60 bitcoin  US   web\n43              Vermont   60 bitcoin  US   web\n44              Georgia   59 bitcoin  US   web\n45                Texas   59 bitcoin  US   web\n46             Delaware   58 bitcoin  US   web\n47              Wyoming   57 bitcoin  US   web\n48             Nebraska   56 bitcoin  US   web\n49                Maine   56 bitcoin  US   web\n50       North Carolina   55 bitcoin  US   web\n51             Missouri   55 bitcoin  US   web\n52                 Ohio   54 bitcoin  US   web\n53               Kansas   54 bitcoin  US   web\n54         South Dakota   54 bitcoin  US   web\n55                 Iowa   54 bitcoin  US   web\n56            Wisconsin   54 bitcoin  US   web\n57           New Mexico   52 bitcoin  US   web\n58              Indiana   52 bitcoin  US   web\n59            Tennessee   50 bitcoin  US   web\n60       South Carolina   48 bitcoin  US   web\n61            Louisiana   47 bitcoin  US   web\n62             Arkansas   46 bitcoin  US   web\n63             Oklahoma   45 bitcoin  US   web\n64              Alabama   45 bitcoin  US   web\n65             Kentucky   44 bitcoin  US   web\n66          Mississippi   39 bitcoin  US   web\n67        West Virginia   37 bitcoin  US   web\n\ninterest_by_dma\ninterest_by_dma는 DMA(Designated Market Area) 정보를 담고 있다. DMA는 시장분석 전문회사인 Nielsen이 시장 분석을 위해서 지정한 지역 분류체계로 우리나라는 없고, 미국에서만 정의되어 있다.\n\n\ndim(bit_trend2$interest_by_dma)\n\n\n[1] 210   5\n\nhead(bit_trend2$interest_by_dma)\n\n\n                           location hits keyword geo gprop\n1 San Francisco-Oakland-San Jose CA  100 bitcoin  US   web\n2     West Palm Beach-Ft. Pierce FL   90 bitcoin  US   web\n3                      Las Vegas NV   90 bitcoin  US   web\n4                       Honolulu HI   89 bitcoin  US   web\n5           Miami-Ft. Lauderdale FL   88 bitcoin  US   web\n6                 Seattle-Tacoma WA   85 bitcoin  US   web\n\ntail(bit_trend2$interest_by_dma)\n\n\n                                    location hits keyword geo gprop\n205                   Joplin MO-Pittsburg KS   30 bitcoin  US   web\n206 Harlingen-Weslaco-Brownsville-McAllen TX   29 bitcoin  US   web\n207                            Alexandria LA   28 bitcoin  US   web\n208                              Meridian MS   28 bitcoin  US   web\n209                                Laredo TX   27 bitcoin  US   web\n210                     Clarksburg-Weston WV   24 bitcoin  US   web\n\ninterest_by_city\ninterest_by_city는 검색한 사람이 거주하는 지역으로서의 도시 정보를 담고 있다.\n\n\ndim(bit_trend2$interest_by_city)\n\n\n[1] 90  5\n\nhead(bit_trend2$interest_by_city)\n\n\n       location hits keyword geo gprop\n1   Seongnam-si  100 bitcoin  KR   web\n2 Pyeongtaek-si   82 bitcoin  KR   web\n3     Yongin-si   73 bitcoin  KR   web\n4   Hwaseong-si   71 bitcoin  KR   web\n5         Seoul   70 bitcoin  KR   web\n6      Gimpo-si   NA bitcoin  KR   web\n\ntail(bit_trend2$interest_by_city)\n\n\n         location hits keyword geo gprop\n85 Virginia Beach   NA bitcoin  US   web\n86     Fort Worth   NA bitcoin  US   web\n87        Memphis   NA bitcoin  US   web\n88   Indianapolis   39 bitcoin  US   web\n89     Louisville   NA bitcoin  US   web\n90    San Antonio   34 bitcoin  US   web\n\nrelated_topics\nrelated_topics는 검색 키워드와 관련있는 주제(topics)에 대한 통계 정보를 담고 있다.\n이 정보는 2개 이상의 지역에서는 표현되지 않아서 bit_trend로 살펴본다.\nsubject 변수는 상대적 traffics 정보이며, related_topics 변수는 통계의 기준을 의마한다. top은 top frequency를 의미하며, rising는 최근 급상승 관련 주제를 의미한다.\n\n\nhead(bit_trend$related_topics)\n\n\n  subject related_topics    value geo  keyword category\n1     100            top  Bitcoin  KR 비트코인        0\n2      99            top      Bit  KR 비트코인        0\n3      55            top     Coin  KR 비트코인        0\n4       9            top    UPbit  KR 비트코인        0\n5       4            top  Eclipse  KR 비트코인        0\n6       3            top Exchange  KR 비트코인        0\n\ntail(bit_trend$related_topics)\n\n\n   subject related_topics          value geo  keyword category\n20       1            top       Dogecoin  KR 비트코인        0\n21       1            top Cryptocurrency  KR 비트코인        0\n22       1            top        Bittrex  KR 비트코인        0\n23       1         rising           GIMP  KR 비트코인        0\n24       1         rising     Commission  KR 비트코인        0\n25       1         rising        Bittrex  KR 비트코인        0\n\nrelated_queries\nrelated_queries는 검색 키워드와 관련있는 검색어에 대한 통계 정보를 담고 있다.\n이 정보는 2개 이상의 지역에서는 표현되지 않아서 bit_trend로 살펴본다.\nsubject 변수는 상대적 traffics 정보이며, related_queries 변수는 통계의 기준을 의마한다. top은 top frequency를 의미하며, rising는 최근 급상승 관련 검색어를 의미한다.\n\n\nbit_trend$related_queries\n\n\n    subject related_queries                     value geo  keyword\n1       100             top                 비트 코인  KR 비트코인\n2       100             top                      코인  KR 비트코인\n3       100             top                      비트  KR 비트코인\n4        17             top            비트 코인 시세  KR 비트코인\n5        12             top          비트 코인 갤러리  KR 비트코인\n6        11             top               코인 갤러리  KR 비트코인\n7         8             top         업 비트 비트 코인  KR 비트코인\n8         8             top                   업 비트  KR 비트코인\n9         3             top 비트 코인 뽀 개기 rndcoin  KR 비트코인\n10        3             top            비트 코인 가격  KR 비트코인\n11        3             top            빗썸 비트 코인  KR 비트코인\n12        3             top          비트 코인 거래소  KR 비트코인\n13        3             top               코인 거래소  KR 비트코인\n14        3             top                   코인 판  KR 비트코인\n15        3             top                      빗썸  KR 비트코인\n16        2             top            비트 코인 주가  KR 비트코인\n17        2             top            비트 코인 전망  KR 비트코인\n18        2             top                   코인 원  KR 비트코인\n19        2             top            비트 코인 채굴  KR 비트코인\n20        2             top                 바이 비트  KR 비트코인\n21        1             top            비트 코인 캐시  KR 비트코인\n22        1             top       바이 낸스 비트 코인  KR 비트코인\n23        1             top                 이더 리움  KR 비트코인\n24        1             top            비트 코인 차트  KR 비트코인\n25        1             top            비트 코인 주식  KR 비트코인\n26 Breakout          rising          비트 코인 갤러리  KR 비트코인\n27 Breakout          rising               코인 갤러리  KR 비트코인\n28 Breakout          rising         업 비트 비트 코인  KR 비트코인\n29 Breakout          rising                   업 비트  KR 비트코인\n30 Breakout          rising 비트 코인 뽀 개기 rndcoin  KR 비트코인\n31 Breakout          rising                   코인 판  KR 비트코인\n32 Breakout          rising                 바이 비트  KR 비트코인\n33 Breakout          rising       바이 낸스 비트 코인  KR 비트코인\n34 Breakout          rising            비트 코인 차트  KR 비트코인\n35 Breakout          rising                 바이 낸스  KR 비트코인\n36 Breakout          rising              비트 코인 갤  KR 비트코인\n37 Breakout          rising                   코인 갤  KR 비트코인\n38 Breakout          rising       비트 코인 도미 넌스  KR 비트코인\n39 Breakout          rising            미국 비트 코인  KR 비트코인\n40 Breakout          rising            비트 코인 골드  KR 비트코인\n41 Breakout          rising               코인 네스트  KR 비트코인\n42 Breakout          rising            비트 코인 볼트  KR 비트코인\n43 Breakout          rising                 도지 코인  KR 비트코인\n44 Breakout          rising            비트 코인 디시  KR 비트코인\n45 Breakout          rising          테슬라 비트 코인  KR 비트코인\n46 Breakout          rising            비트 코인 김프  KR 비트코인\n47 Breakout          rising                      김프  KR 비트코인\n48 Breakout          rising                 비트 맥스  KR 비트코인\n49 Breakout          rising          비트 코인 실시간  KR 비트코인\n50 Breakout          rising                 비트 렉스  KR 비트코인\n   category\n1         0\n2         0\n3         0\n4         0\n5         0\n6         0\n7         0\n8         0\n9         0\n10        0\n11        0\n12        0\n13        0\n14        0\n15        0\n16        0\n17        0\n18        0\n19        0\n20        0\n21        0\n22        0\n23        0\n24        0\n25        0\n26        0\n27        0\n28        0\n29        0\n30        0\n31        0\n32        0\n33        0\n34        0\n35        0\n36        0\n37        0\n38        0\n39        0\n40        0\n41        0\n42        0\n43        0\n44        0\n45        0\n46        0\n47        0\n48        0\n49        0\n50        0\n\nGoogle Trends traffic 시각화\n시계열 그래프\n일자별 시계열 그래프는 이미 앞에서 다루었으므로 생략한다.\nTop Rank 시각화\n앞서 만들어 놓은 bbplot() 함수를 이용해서 시각화한다.\n관련 주제별 traffic 현황\n관련 주제가 영문으로 출력되지만, 비트코인 검색어의 관련 주제 규모를 버블차트로 출력해 보자.\n\n\n##===========================================================\n## 관련 Topic별 Traffics\n##===========================================================\nbit_trend$related_topics %>%\n  filter(related_topics == \"top\") %>%\n  mutate(relative = as.integer(subject) + 1) %>%\n  mutate(label = gsub(\" \", \"\", value)) %>%\n  filter(relative > 0) %>%\n  select(label, relative) %$%\n  bbplot(label, relative) \n\n\n\n\n\n\ncolor 인수를 사용하면 버블의 색상을 바꿀 수도 있다.\n\n\nbit_trend$related_topics %>%\n  filter(related_topics == \"top\") %>%\n  mutate(relative = as.integer(subject) + 1) %>%\n  mutate(label = gsub(\" \", \"\", value)) %>%\n  filter(relative > 0) %>%\n  select(label, relative) %$%\n  bbplot(label, relative, color = terrain.colors(35, alpha = NULL))\n\n\n\n\n\n\n\n\nbit_trend$related_topics %>%\n  filter(related_topics == \"top\") %>%\n  mutate(relative = as.integer(subject) + 1) %>%\n  mutate(label = gsub(\" \", \"\", value)) %>%\n  filter(relative > 0) %>%\n  select(label, relative) %$%\n  bbplot(label, relative, color = rainbow(20, alpha = NULL))\n\n\n\n\n\n\n관련 검색어별 traffic 현황\n\n\n##===========================================================\n## 관련 검색어별 Traffics\n##===========================================================\nbit_trend$related_queries %>%\n  filter(related_queries == \"top\") %>%\n  mutate(relative = as.integer(subject) + 1) %>%\n  mutate(label = gsub(\" \", \"\", value)) %>%\n  filter(relative > 0) %>%\n  select(label, relative) %$%\n  bbplot(label, relative)\n\n\n\n\n\n\n관련 검색어별 통계를 보면 상대 traffic이 0인 건도 통계로 집계되어 있다. 그래서 시각화 함수에서 traffic이 0인 건의 포함여부를 지정할 수 있도록 gplot.queries() 함수를 만들어 보았다.\n\n\ngplot.queries <- function(x, rm.zero = FALSE, geos = \"KR\", \n                         color = RColorBrewer::brewer.pal(8, \"Dark2\")) {\n  x$related_queries %>%\n    filter(geo %in% geos) %>%\n    filter(related_queries == \"top\") %>%\n    mutate(relative = as.integer(subject) + !rm.zero) %>%\n    mutate(label = gsub(\" \", \"\", value)) %>%\n    filter(relative > 0) %>%\n    select(label, relative, geo) %$%\n    bbplot(label, relative, color = color)\n}\n\n\n\n다음 예제는 traffic이 0인 건을 제거한 후 버블차트를 그리는 예제다.\n\n\ngplot.queries(bit_trend, rm.zero = TRUE)\n\n\n\n\n\n\n2013년부터 2017년까지의 관련 검색어 Top 랭크를 살펴보자.\n2013년도\n\n\ngplot.queries(bit_trend_2013_k, rm.zero = TRUE,\n              color = terrain.colors(20, alpha = NULL))\n\n\n\n\n\n\n2014년도\n\n\ngplot.queries(bit_trend_2014_k, rm.zero = TRUE,\n              color = terrain.colors(20, alpha = NULL))\n\n\n\n\n\n\n2015년도\n\n\nif (!is.null(bit_trend_2015_k$related_queries)) {\n gplot.queries(bit_trend_2015_k, rm.zero = TRUE,\n              color = terrain.colors(20, alpha = NULL)) \n}\n\n\n\n\n\n\n2016년도\n\n\ngplot.queries(bit_trend_2016_k, rm.zero = TRUE,\n              color = terrain.colors(20, alpha = NULL))\n\n\n\n\n\n\n2017년도\n\n\ngplot.queries(bit_trend_2017_k, rm.zero = TRUE,\n              color = terrain.colors(20, alpha = NULL))\n\n\n\n\n\n\n광역시도별 traffic 현황\n광역시도별 traffic 현황을 그리기 위해서 다음의 함수를 만들었다.\n\n\ngplot.region <- function(x, geos = \"KR\", \n                         color = RColorBrewer::brewer.pal(8, \"Dark2\")) {\n  x$interest_by_region %>%\n    filter(geo %in% geos) %>%\n    mutate(region = location) %>%\n    mutate(region = ifelse(region %in% c(\"Daejeon\"), \"대전광역시\", region)) %>% \n    mutate(region = ifelse(region %in% c(\"Incheon\"), \"인천광역시\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Seoul\"), \"서울특별시\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Ulsan\"), \"울산광역시\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Gwangju\"), \"광주광역시\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Daegu\"), \"대구광역시\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Busan\"), \"부산광역시\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Chungcheongnam-do\"), \"충청남도\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Jeollanam-do\"), \"전라남도\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Chungcheongbuk-do\"), \"충청북도\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Gyeongsangbuk-do\"), \"경상북도\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Jeollabuk-do\"), \"전라북도\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Gyeongsangnam-do\"), \"경상남도\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Gangwon-do\"), \"강원도\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Gyeonggi-do\"), \"경기도\", region)) %>%\n    mutate(region = ifelse(region %in% c(\"Jeju-do\"), \"제주도\", region)) %>%    \n    select(region, hits, geo) %$%\n    bbplot(region, hits, color = color)\n}\n\n\n\n광역시도별 traffic 현황은 다음과 같다.\n\n\ngplot.region(bit_trend)\n\n\n\n\n\n\n2017년도 광역시도별 traffic 현황은 다음과 같다.\n\n\ngplot.region(bit_trend_2017)\n\n\n\n\n\n\n2017년도 미국의 주별 traffic 현황은 다음과 같다.\n\n\ngplot.region(bit_trend_2017, geos = \"US\", \n             color = terrain.colors(60, alpha = NULL))\n\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2018-01-12-google_trends/img/google_trends.png",
    "last_modified": "2021-10-10T21:11:05+09:00",
    "input_file": {},
    "preview_width": 649,
    "preview_height": 328
  },
  {
    "path": "posts/2017-11-13-taxonomy/",
    "title": "Documents Taxonomy - Speech",
    "description": "text2vec 패키지로 text를 vector 구조로 변환한 다음, Lasso and Elastic-Net Regularized Generalized Linear Models을 지원하는 glmnet 패키지를 이용해서 모델을 생성한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2017-11-08",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n들어가기\n배경\n\nMachine Learning 기법을 이용해서 Documents Taxonomy를 수행하기 위한 Prototyping을 목적으로 한다.\n\n분석 방법은 text2vec 패키지로 text를 vector 구조로 변환한 다음, Lasso and Elastic-Net Regularized Generalized Linear Models을 지원하는 glmnet 패키지를 이용해서 모델을 생성한다.\n데이터 준비\n그림과 같은 대통령기록연구실 홈페이지http://www.pa.go.kr/research/contents/speech/index.jsp 에는 역대 퇴임 대통령들의 연설문을 제공하고 있다.\n\n\n\nFigure 1: 대통령기록연구실 홈페이지\n\n\n\n역대 대통령별로 연설문의 차이가 있는지 살펴보기 위해서 R로 Crawling 프로그램을 작성하여 이 홈페이지에서 김대중, 노무현, 이명박 전 대통령의 연설문 수집하였다.\n이 leture에서는 김대중, 노무현, 이명박 전 대통령의 연설문만으로 모델을 생성하여, 연설문이 어느 전직 대통령이 연설한 것인지를 예측하는 방법으로 Documents Taxonomy 방법을 제시한다.\n데이터 구조\n수집한 데이터는 R 데이터 프레임 객체로 만들었으며, “data/speech.rda” 파일에 저장하였으며 변수는 다음과 같다.\nid\n연설문(문서) 아이디\n\npresident\n연설 대통령\n\ncategory\n연설문 분야\n\ntype\n연설문 유형\n\ntitle\n연설문 제목\n\ndata\n연설 일자\n\ndoc\n연설문 내용\n\n시작하기\n패키지 로딩\n\n\nlibrary(text2vec)\nlibrary(data.table)\nlibrary(dplyr)\nlibrary(RMeCab)\nlibrary(glmnet)\nlibrary(caret)\nrequire(doMC)\n\n\n\n데이터 로딩\n\n\n\n“data/speech.rda” 파일로부터 데이터를 읽어 speech 데이퍼 프레임을 가져온다.\n\n\nload(\"speech.rda\")\n\n\n\nPreparation data\ndata.table 객체 생성\n데이터의 조작 속도의 개선을 위해서 data.frame 객체를 data.table 객체로 변환해주었다. 적은 양의 데이터는 data.frame 객체로 분석해도 상관 없지만, 일정 규모의 데이터 사이즈는 data.table 객체로 변환하여 분석하는 것이 바람직하다.\n\n\nsetDT(speech)\nsetkey(speech, id)\n\n\n\n모델링을 위한 데이터셋 분리\n연산 속도의 개선을 위해서 데이터 프레임 객체를 데이터 테이블 객체로 변환한다. 그리고 변환한 객체를 traing set과 test ste으로 분리한다.\nrandom simple sampling을 수행하여 원 데이터를 traing : test = 70% : 30%로 분리를 수행한다.\n\n\nids <- speech$id\n\nn <- NROW(speech) * 0.7\n\nset.seed(1234L)\ntrain_ids <- sample(ids, n)\ntest_ids <- setdiff(ids, train_ids)\n\ntrain <- speech[J(train_ids)]\ntest <- speech[J(test_ids)]\n\n\n\nVectorization\n모델링을 위해서 documents 데이터를 vector로 변환해야 한다. 이 경우 엄청난 데이터의 증가가 필연적으로 따라온다. 그래서 연산 속도의 개선을 위해서 vectorization 구조로 연산을 해야하기 때문에 Vectorization 연산을 수행하기 위한 구조로 변환해야 한다.\ndocuments를 vector로 변환하는 tokenizer는 RMeCab 패키지를 이용해서 명사로 정의했다. 그러나 Mac 환경에서는 정상적으로 수행되었으나, 분석서버인 Centos Linux 환경에서는 에러가 발생하였다. 그래서 tokenizer는 명사가 아닌 words로 정의하였다. 이 경우에는 term의 개수가 증가하기 때문에 상대적으로 vector의 원소 개수가 증가하게된다.\n또한 word2vec 패키지의 함수들은 parallel processing을 지원하므로, parallel 처리를 위한 multicores 사용을 지원하는 doMC, doParallel 패키지의 사용이 필요하다. 본 예제에서는 doMC 패키지를 사용한다.\n\n\nnoun_tokenizer <- function(docs, pos = c(\"NNG\", \"NNP\")) {\n  docs %>% lapply(function(x) {\n    morpheme <- RMeCabC(x)\n    morpheme <- unlist(morpheme[\n      which(sapply(morpheme, names) %in% pos)])\n    morpheme[nchar(morpheme) > 0]\n  })  \n}\n\n# not used  \ntoken_fun <- noun_tokenizer\n\ntoken_fun <- tokenizers::tokenize_words\n\n# parallel cores\nnc <- parallel::detectCores()\nregisterDoMC(cores = nc)\n\nit_train <- itoken_parallel(train$doc, \n                   tokenizer = token_fun, \n                   ids = train$id, \n                   progressbar = FALSE)\n\nit_test <- itoken_parallel(test$doc, \n                  tokenizer = token_fun, \n                  ids = test$id, \n                  progressbar = FALSE)\n\n\n\n모델 적합하기\nVocabulary 기반 모델링\nVocabulary 생성\nvocabulary는 documents로부터 생성된 terms의 집합이다. 여기서는 tokenizer를 words로 정의했기 때문에 띄어쓰기 단위의 단어 집합으로 vocabulary가 생성된다.\n몇몇 데이터를 조회해보면 term별로 frequency와 document frequency가 도출되었음을 알 수 있다.\n\n\nvocab <- create_vocabulary(it_train)\n\ntail(vocab, n = 10) %>%\n  knitr::kable(., row.names = FALSE, format.args = list(big.mark = \",\"))\n\n\nterm\nterm_count\ndoc_count\n한\n3,846\n1,129\n그리고\n4,136\n1,226\n이\n4,991\n1,216\n있는\n4,994\n1,240\n합니다\n5,084\n1,098\n여러분\n5,548\n1,386\n우리\n7,395\n1,444\n수\n8,449\n1,381\n것입니다\n8,897\n1,430\n있습니다\n11,950\n1,527\n\nDocument Term Matrix 생성하기\ndocuments taxonomy 분류 모델을 수행하는 데이터셋은 DTM(Document Term Matrix) 구조여야 한다. 그래서 vocabulary를 DTM으로 변환하는 작업을 수행한다.\n\n\nvectorizer <-  vocab_vectorizer(vocab)\n\ndtm_train <- create_dtm(it_train, vectorizer)\ndim(dtm_train)\n\n\n[1]   1685 127049\n\ndtm_test <- create_dtm(it_test, vectorizer)\ndim(dtm_test)\n\n\n[1]    723 127049\n\n모델 생성\n모든 terms을 모델의 독립변수로 사용하려 하기 때문에, terms의 개수가 독립변수의 개수와 같게 된다. 이 경우에는 over-fitting의 이슈가 발생하므로 이를 해경하기 위해서 over-fitting을 방지해주는 LASSO 모델을 사용하기로 한다. 또한 target 변수가 binary가 아닌 3개의 class이기 때문에 family 함수는 “multinomial”을 지정한다. 즉, multinomial logistic regression의 알고리즘에 기반한 LASSO 모델을 만든다.\nLASSO 모델을 생성하기 위해서는 cv.glmnet() 함수에서 penalty값인 alpha의 값을 1로 지정해야 LASSO Generalized Linear Model로 모델이 만들어진다. alpha의 값이 0이면 Ridge Generalized Linear Model, 0.5이면 Elastic Net Regularized Generalized Linear Model이 생성된다.\ntype.measure은 cross-validation을 위한 loss값 계산에 사용하는 측도를 지원한다. 일반적으로 binomial family 함수의 경우에는 type.measure 인수를 AUC(Area Under Curve)인 “auc”를 사용하지만, multinomial family 함수의 경우에는 이를 사용할 수 없기 때문에 여기서는 “deviance”를 사용하였다. 이 값이 기본값이다.\n그리고 k-folds cross-validation의 k의 값은 10으로 지정하여, 10-fold cross-validation을 수행하여 over-fitting 또한 방지하도록 한다.\n\n\nNFOLDS <- 10\n\nclassifier <- cv.glmnet(x = dtm_train, y = train$president, \n                        family = 'multinomial', \n                        alpha = 1,\n                        type.measure = \"deviance\",\n                        nfolds = NFOLDS,\n                        thresh = 0.001,\n                        maxit = 1000,\n                        parallel = TRUE)\n\n\n\n모델의 검증\ntest 데이터로 검증한 결과 Accuracy가 0.9378로 비교적 높게 나타났다.\n\n\npred_voca <- predict(classifier, dtm_test, type = 'response')[, , 1]\npresident_voca <- apply(pred_voca, 1, \n                        function(x) colnames(pred_voca)[which(max(x) == x)])\n\ncmat_voca <- caret::confusionMatrix(factor(president_voca), factor(test$president))\ncmat_voca\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 김대중 노무현 이명박\n    김대중    226      3      3\n    노무현     16    220     10\n    이명박      2      8    235\n\nOverall Statistics\n                                          \n               Accuracy : 0.9419          \n                 95% CI : (0.9223, 0.9578)\n    No Information Rate : 0.343           \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.9129          \n                                          \n Mcnemar's Test P-Value : 0.02536         \n\nStatistics by Class:\n\n                     Class: 김대중 Class: 노무현 Class: 이명박\nSensitivity                 0.9262        0.9524        0.9476\nSpecificity                 0.9875        0.9472        0.9789\nPos Pred Value              0.9741        0.8943        0.9592\nNeg Pred Value              0.9633        0.9769        0.9728\nPrevalence                  0.3375        0.3195        0.3430\nDetection Rate              0.3126        0.3043        0.3250\nDetection Prevalence        0.3209        0.3402        0.3389\nBalanced Accuracy           0.9569        0.9498        0.9633\n\nN-Grams 기반 모델링\nN-grams은 N개의 연속된 terms의 조합을 terms로 간주하여 vocabulary를 생성하고 이 데이터 기반으로 모델을 생성한다. 파편화된 terms이 아니기 때문에 일반적인 vocabulary를 이용한 TA보다는 좀 더 정확하게 문맥을 파악할 수 있는 장점이 있다.\nVocabulary 생성\nN-grams vocabulary는 create_vocabulary() 함수의 ngram 인수를 이용해서 생성한다. 여기서는 N의 값이 2인 2-grams 즉, bigram vocabulary를 생성한다.\n\n\nvocab_bigram <- create_vocabulary(it_train, ngram = c(1L, 2L))\n\ndim(vocab_bigram)\n\n\n[1] 764734      3\n\nhead(vocab_bigram, n = 10) %>%\n  knitr::kable(., row.names = FALSE, format.args = list(big.mark = \",\"))\n\n\nterm\nterm_count\ndoc_count\n0.15\n1\n1\n0.15_수준으로\n1\n1\n0.1_를\n1\n1\n0.1_미만의\n1\n1\n0.230\n1\n1\n0.230_입니다\n1\n1\n0.291\n1\n1\n0.291_에서\n1\n1\n0.301\n1\n1\n0.301_에\n1\n1\n\nPrune Vocabulary\n\nDocuments의 개수가 증가하거나 Documents의 길이가 증가하면, Vocabulary의 규모도 증가한다. 이것은 모델을 생성하는데 많은 컴퓨팅 리소스를 소모해서 속도가 느려지게 된다. 그래서 모델에 영향을 덜 줄 수 있는 terms를 제거하는 작업이 필요하다.\n\nprune_vocabulary() 함수는 Vocabulary에 포함된 terms를 제거하는 작업 수행해 준다. 여기서는 term_count_min 인수를 이용해서 term frequency가 10 미만이고, doc_proportion_max 인수를 이용해서 term을 포함해야하는 문서의 최대 비율이 0.5보다 큰 term을 제거하였다.\n\n\nvocab_bigram <- vocab_bigram %>% \n  prune_vocabulary(term_count_min = 10,\n                   doc_proportion_max = 0.5)\ndim(vocab_bigram)\n\n\n[1] 18781     3\n\nDocuments Term Matrix 생성\n\n\nvectorizer_bigram <- vocab_vectorizer(vocab_bigram)\n\ndtm_train_bigram <- create_dtm(it_train, vectorizer_bigram)\ndim(dtm_train_bigram)\n\n\n[1]  1685 18781\n\ndtm_test_bigram  <- create_dtm(it_test, vectorizer_bigram)\ndim(dtm_test_bigram)\n\n\n[1]   723 18781\n\n모델 생성\n\n\nclassifier <- cv.glmnet(x = dtm_train_bigram, y = train$president, \n                        family = 'multinomial', \n                        type.measure = \"deviance\",\n                        alpha = 1,                        \n                        nfolds = NFOLDS,\n                        parallel = TRUE)\n\n\n\n모델 검증\nvocabulary를 가지지기했음에도 불구하고, 전체 vocabulary를 사용한 모델보다 성능이 좋아졌다.\n\n\npred_bigram <- predict(classifier, dtm_test_bigram, type = 'response')[, , 1]\n\npresident_bigram <- apply(pred_bigram, 1, \n                          function(x) colnames(pred_bigram)[which(max(x) == x)])\n\ncmat_bigram <- confusionMatrix(factor(president_bigram), factor(test$president)) \ncmat_bigram\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 김대중 노무현 이명박\n    김대중    232      2      7\n    노무현     10    223     11\n    이명박      2      6    230\n\nOverall Statistics\n                                          \n               Accuracy : 0.9474          \n                 95% CI : (0.9286, 0.9625)\n    No Information Rate : 0.343           \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.9212          \n                                          \n Mcnemar's Test P-Value : 0.02248         \n\nStatistics by Class:\n\n                     Class: 김대중 Class: 노무현 Class: 이명박\nSensitivity                 0.9508        0.9654        0.9274\nSpecificity                 0.9812        0.9573        0.9832\nPos Pred Value              0.9627        0.9139        0.9664\nNeg Pred Value              0.9751        0.9833        0.9629\nPrevalence                  0.3375        0.3195        0.3430\nDetection Rate              0.3209        0.3084        0.3181\nDetection Prevalence        0.3333        0.3375        0.3292\nBalanced Accuracy           0.9660        0.9613        0.9553\n\nFeature hashing 기반의 모델\nFeature hashing은 Yahoo에서 개발한 기법으로 변수에 해시함수를 적용하고 이를 인덱스로 사용하는 기법이다. 이 방법을 적용하면 vecterization의 수행속도가 빨라지고, 연산 과정에서의 메로리 공간도 절약된다고 한다.\nHash Vectorize 정의\nhash_vectorizer() 함수로 Feature hashing 적용한다.\n\n\nvectorizer_hash <- hash_vectorizer(hash_size = 2 ^ 14, ngram = c(1L, 2L))\n\n\n\nDocument Term Matrix 생성\n\n\ndtm_train_hash <- create_dtm(it_train, vectorizer_hash)\ndtm_test_hash <- create_dtm(it_test, vectorizer_hash)\n\n\n\n모델 생성\n\n\nclassifier <- cv.glmnet(x = dtm_train_hash, y = train$president, \n                        family = 'multinomial', \n                        type.multinomial = \"grouped\", \n                        nfolds = NFOLDS,\n                        thresh = 1e-3,\n                        maxit = 1e3,\n                        parallel = TRUE)\n\n\n\n모델 검증\n모델의 성능은 다른 기법보다는 낮아졌지만, 대용량의 데이터를 분석할 경우에는 속도의 개선을 위해서 사용해봄직 하다.\n\n\npred_hash <- predict(classifier, dtm_test_hash, type = 'response')[, , 1]\n\npresident_hash <- apply(pred_hash, 1, \n                        function(x) colnames(pred_hash)[which(max(x) == x)])\n\ncmat_hash <- confusionMatrix(factor(president_hash), factor(test$president))\ncmat_hash\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 김대중 노무현 이명박\n    김대중    222      5      2\n    노무현     17    215      9\n    이명박      5     11    237\n\nOverall Statistics\n                                          \n               Accuracy : 0.9322          \n                 95% CI : (0.9114, 0.9494)\n    No Information Rate : 0.343           \n    P-Value [Acc > NIR] : < 2e-16         \n                                          \n                  Kappa : 0.8983          \n                                          \n Mcnemar's Test P-Value : 0.04537         \n\nStatistics by Class:\n\n                     Class: 김대중 Class: 노무현 Class: 이명박\nSensitivity                 0.9098        0.9307        0.9556\nSpecificity                 0.9854        0.9472        0.9663\nPos Pred Value              0.9694        0.8921        0.9368\nNeg Pred Value              0.9555        0.9668        0.9766\nPrevalence                  0.3375        0.3195        0.3430\nDetection Rate              0.3071        0.2974        0.3278\nDetection Prevalence        0.3167        0.3333        0.3499\nBalanced Accuracy           0.9476        0.9389        0.9610\n\nTF-IDF 기반의 모델\n대통령 연설문에서는 대부분 “존경하는 국민 여러분”으로 시작할 것이다. 그러므로 “존경하는”이라는 term은 모든 연설문에 포함되기 때문에 term frequency와 document term frequency가 상당히 클 것이다. 그러나 이 term으로 세명의 전직 대통령의 연설문을 구분하기 어렵다. 세명의 전직 대통령이 즐겨 사용하는 단어이기 때문이다. 즉, term frequency와 document term frequency가 상당히 큰 terms은 모델 개발에 의미가 없는 terms인 것이다.\nTF-IDF는 단일문서, 혹은 소수의 문서에서 의미가 있는 terms의 가중치를 높이고 대부분의 문서에서 발현하는 terms의 가중치를 줄이는 용도로 만들어진 측도다. 그러므로 DTM에 TF-IDF 변환을 수행하면 모델의 성능이 개선된다.\nText Anaytics에서는 documents의 길이의 차이가 있으면, 상대적으로 짧거나 긴 documents에서 발현하는 terms들로 인해서 frequency scale에 왜곡이 있을 수 있다. 이 경우에는 표준화를 수행해야 한다. 그런데 TF-IDF 변환은 자동으로 표준화가 되기 때문에 표준화의 잇점이 있다. 만약 표준화를 수행하려면, normalize() 함수를 사용하면 된다.\nDTM의 TF-IDF 변환\nTfIdf class와 fit_transform() 함수를 이용해서 DTM에 TF-IDF 변환을 수행한다.\n\n\ntfidf <- TfIdf$new()\n\ndtm_train_tfidf <- fit_transform(dtm_train, tfidf)\ndtm_test_tfidf <- fit_transform(dtm_test, tfidf) \n\n\n\n모델 생성\n\n\nclassifier <- cv.glmnet(x = dtm_train_tfidf, y = train$president, \n                        family = 'multinomial', \n                        nfolds = NFOLDS,\n                        thresh = 1e-3,\n                        maxit = 1e3,\n                        parallel = TRUE)\n\n\n\n모델의 검증\nTF-IDF 변환된 데이터로 생성한 모델의 성능이 상대적으로 높다.\n\n\npred_tfidf <- predict(classifier, dtm_test_tfidf, type = 'response')[, , 1]\n\npresident_tfidf <- apply(pred_tfidf, 1, \n                         function(x) colnames(pred_tfidf)\n                         [which(max(x) == x)])\n\ncmat_tfidf <- confusionMatrix(factor(president_tfidf), factor(test$president))\ncmat_tfidf\n\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction 김대중 노무현 이명박\n    김대중    236      5      3\n    노무현      7    218      4\n    이명박      1      8    241\n\nOverall Statistics\n                                          \n               Accuracy : 0.9613          \n                 95% CI : (0.9445, 0.9741)\n    No Information Rate : 0.343           \n    P-Value [Acc > NIR] : <2e-16          \n                                          \n                  Kappa : 0.9419          \n                                          \n Mcnemar's Test P-Value : 0.4459          \n\nStatistics by Class:\n\n                     Class: 김대중 Class: 노무현 Class: 이명박\nSensitivity                 0.9672        0.9437        0.9718\nSpecificity                 0.9833        0.9776        0.9811\nPos Pred Value              0.9672        0.9520        0.9640\nNeg Pred Value              0.9833        0.9737        0.9852\nPrevalence                  0.3375        0.3195        0.3430\nDetection Rate              0.3264        0.3015        0.3333\nDetection Prevalence        0.3375        0.3167        0.3458\nBalanced Accuracy           0.9753        0.9607        0.9764\n\n모델 성능의 비교\n\n모델의 성능은 TF-IDF > Bigram > Vocabulary > Feature Hashing의 순서로 나타난다.\n\n그러므로 성능을 높이기 위해서는 TF-IDF 방법을 사용하는 것이 좋으며, 대용량의 데이터 분석에서는 적은 성능 감소와 수행 속도의 개선을 가져오는 Feature Hashing 기법을 사용하면 될 것이다. 이 경우에는 Purne Vocabulary 전처리도 필요할 것이다.\n\n\naccuracy <- rbind(cmat_voca$overall, cmat_bigram$overall, \n                  cmat_hash$overall, cmat_tfidf$overall) %>%\n  round(3)\n\ndata.frame(Method = c(\"Vocabulary\", \"Bigram\", \"FeatueHash\", \"TF-IDF\"),\n           accuracy) %>%\n  arrange(desc(Accuracy)) %>%\n  knitr::kable()\n\n\nMethod\nAccuracy\nKappa\nAccuracyLower\nAccuracyUpper\nAccuracyNull\nAccuracyPValue\nMcnemarPValue\nTF-IDF\n0.961\n0.942\n0.945\n0.974\n0.343\n0\n0.446\nBigram\n0.947\n0.921\n0.929\n0.963\n0.343\n0\n0.022\nVocabulary\n0.942\n0.913\n0.922\n0.958\n0.343\n0\n0.025\nFeatueHash\n0.932\n0.898\n0.911\n0.949\n0.343\n0\n0.045\n\nReference\n본 분석 사례는 Vectorization(‘http://text2vec.org/vectorization.html’)을 참고하였다.\n\n\n\n",
    "preview": "posts/2017-11-13-taxonomy/img/homepage.png",
    "last_modified": "2021-10-10T20:48:08+09:00",
    "input_file": {},
    "preview_width": 1127,
    "preview_height": 792
  },
  {
    "path": "posts/2016-04-08-Stock/",
    "title": "주가 데이터 분석하기",
    "description": "주가 데이터를 수집하는 방법을 알아본다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2016-04-08",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n주가 데이터 수집하기\n국내 주가데이터 수집\n해외 주가데이터 수집\n\n수익률 분석\n한화생명 수익률 분석\n삼성생명 수익률 분석\n\n\n주가 데이터 수집하기\n국내 주가데이터 수집\n종목코드 데이터 수집\n야후\nKOSPI : 종목코드.KS\n코스닥 : 종목코드.KQ\n\n구글\nKOSPI : KRX:종목코드\n코스닥 : KOSDAQ:종목코드\n\n그래서 우선 KRX의 상장회사검색(http://marketdata.krx.co.kr/mdi#document=040601) 메뉴에서 국내 상장 주식회사의 종목코드 정보를 수집하였음. 각각의 코드는 다음의 excel 파일로 저장함.\nKOSPI\nmeta/KRX_KOSPI_CODE.xls\n\nKOSDAQ\nmeta/KRX_KOSDAQ_CODE.xls\n\nKONEX\nmeta/KRX_KONEX_CODE.xls\n\n종목코드 데이터 적재\n\n\nlibrary(xlsx)\n\npath <- \"./meta\"\n\nKOSPI <- \"KRX_KOSPI_CODE.xls\"\nKOSDAQ <- \"KRX_KOSDAQ_CODE.xls\"\nKONEX <- \"KRX_KONEX_CODE.xls\"\n\nfKOSPI <- paste(path, KOSPI, sep=\"/\")\nfKOSDAQ <- paste(path, KOSDAQ, sep=\"/\")\nfKONEX <- paste(path, KONEX, sep=\"/\")\n\nKOSPI <- read.xlsx(fKOSPI, sheetIndex = 1, encoding = \"UTF-8\")\nKOSDAQ <- read.xlsx(fKOSDAQ, sheetIndex = 1, encoding = \"UTF-8\")\nKONEX <- read.xlsx(fKONEX, sheetIndex = 1, encoding = \"UTF-8\")\n\nKOSPI <- KOSPI[, -1]\nnames(KOSPI) <- c(\"symbols\", \"company\" ,\"industry_cd\",\n                  \"industry_nm\", \"stock_cnt\", \"capital\",\n                  \"face_value\", \"currency\", \"phone\", \"address\")\n\nKOSPI$symbols <- as.character(KOSPI$symbols)\nKOSPI$stock_cnt <- as.numeric(gsub(\",\", \"\", KOSPI$stock_cnt))\nKOSPI$capital <- as.numeric(gsub(\",\", \"\", KOSPI$capital))\nKOSPI$face_value <- as.integer(gsub(\",\", \"\", KOSPI$face_value))\n\nKOSDAQ <- KOSDAQ[, -1]\nnames(KOSDAQ) <- c(\"symbols\", \"company\" ,\"industry_cd\",\n                  \"industry_nm\", \"stock_cnt\", \"capital\",\n                  \"face_value\", \"currency\", \"phone\", \"address\")\n\nKOSDAQ$symbols <- as.character(KOSDAQ$symbols)\nKOSDAQ$stock_cnt <- as.numeric(gsub(\",\", \"\", KOSDAQ$stock_cnt))\nKOSDAQ$capital <- as.numeric(gsub(\",\", \"\", KOSDAQ$capital))\nKOSDAQ$face_value <- as.integer(gsub(\",\", \"\", KOSDAQ$face_value))\n\nKONEX <- KONEX[, -1]\nnames(KONEX) <- c(\"symbols\", \"company\" ,\"industry_cd\",\n                  \"industry_nm\", \"stock_cnt\", \"capital\",\n                  \"face_value\", \"currency\", \"phone\", \"address\")\n\nKONEX$symbols <- as.character(KONEX$symbols)\nKONEX$stock_cnt <- as.numeric(gsub(\",\", \"\", KONEX$stock_cnt))\nKONEX$capital <- as.numeric(gsub(\",\", \"\", KONEX$capital))\nKONEX$face_value <- as.integer(gsub(\",\", \"\", KONEX$face_value))\n\nKOSPI[KOSPI$symbols == \"088350\", ]\n\n\n    symbols  company industry_cd industry_nm stock_cnt     capital\n717  088350 한화생명      116501      보험업 868530000 4.34265e+12\n    face_value currency       phone                      address\n717       5000  원(KRW) 02-789-5114 서울특별시 영등포구 63로 50 \n\n종목별 거래정보 가져오기\n종목별 거래정보는 시스템을 구축할 경우에는 적재된 종목코드 데이터의 첫번째 코드부터 마지막 코드까지 반복(iteration) 수행을 통해 개별개별 데이터를 가져와서 쌓아야 함\n본 종목별 거래정보 가져오기 예제에서는 야후에서 가져옴. 실제로 구글에 비해서 야후의 데이터가 더 많은 기간의 데이터를 수용하고 있음. 다음 예제는 KOSPI의 첫 3종목의 주가 정보를 가져오는 예제임.\n\n\nlibrary(\"quantmod\")\n\n# getting stock data which 1st KOSPI code from Yahoo\nsymbols <- paste(KOSPI$symbols[1], \"KS\", sep = \".\")\nKOSPI_STOCKS <- getSymbols(symbols, from = '1990-01-01', auto.assign = FALSE)\nKOSPI_STOCKS <- data.frame(date = index(KOSPI_STOCKS), symbols = KOSPI$symbols[1], \n                Open = as.vector(KOSPI_STOCKS[, 1]), \n                High = as.vector(KOSPI_STOCKS[, 2]), \n                Low = as.vector(KOSPI_STOCKS[, 3]), \n                Close = as.vector(KOSPI_STOCKS[, 4]), \n                Volume = as.vector(KOSPI_STOCKS[, 5]), \n                Adjusted = as.vector(KOSPI_STOCKS[, 6]))\nrow.names(KOSPI_STOCKS) <- NULL\n\nn <- length(KOSPI[1:3])\n  \n# getting stock data which 2nd, 3rd KOSPI code from Yahoo\nfor (i in 2:n) {\n  symbols <- paste(KOSPI$symbols[i], \"KS\", sep = \".\")\n  x <- getSymbols(symbols, from = '1990-01-01', auto.assign = FALSE)\n  x <- data.frame(date = index(x), symbols = KOSPI$symbols[i], \n                  Open = as.vector(x[, 1]), High = as.vector(x[, 2]), \n                  Low = as.vector(x[, 3]), Close = as.vector(x[, 4]), \n                  Volume = as.vector(x[, 5]), Adjusted = as.vector(x[, 6]))\n  row.names(x) <- NULL\n  \n  KOSPI_STOCKS <- rbind(KOSPI_STOCKS, x)\n}\n\nhead(KOSPI_STOCKS)\n\n\n        date symbols Open High  Low Close  Volume Adjusted\n1 2015-08-21  095570 6180 7300 5600  7100 9970110 6097.747\n2 2015-08-24  095570 6700 6940 6240  6480 1964155 5565.268\n3 2015-08-25  095570 6610 6730 6190  6220 1213650 5341.969\n4 2015-08-26  095570 6260 7760 6260  7090 3518605 6089.158\n5 2015-08-27  095570 7240 8060 6960  7570 2499565 6501.400\n6 2015-08-28  095570 7660 7840 7090  7140  956575 6132.101\n\ntail(KOSPI_STOCKS)\n\n\n           date symbols  Open  High   Low Close Volume Adjusted\n7735 2021-09-30  006840 26700 27500 26650 26950  11868    26950\n7736 2021-10-01  006840 27100 27100 25000 26800  27891    26800\n7737 2021-10-05  006840 26900 27300 26300 27250  38909    27250\n7738 2021-10-06  006840 26950 27500 25800 26300  29122    26300\n7739 2021-10-07  006840 26300 26800 26300 26400  12902    26400\n7740 2021-10-08  006840 26350 26850 25950 26300  13620    26300\n\n관심종목 거래정보 가져오기\n삼성전자 거래정보 가져오기\n삼성전자의 종목코드는 0059301이다. 이 종목의 거래정보를 Yahoo로부터 가져오기 위해서 종목코드를 005930.KS로 변환 후 조회한다.\n\n\nsymbols <- \"005930\"\nsymbols <- paste(symbols, \"KS\", sep = \".\")\nsymbols\n\n\n[1] \"005930.KS\"\n\nsamsung.electric <- getSymbols(symbols, from = '1990-01-01', to = '2016-03-31',\n                               auto.assign = FALSE)\n\ntail(samsung.electric)\n\n\n           005930.KS.Open 005930.KS.High 005930.KS.Low\n2016-03-24          25580          25800         25320\n2016-03-25          25660          25800         25560\n2016-03-28          25760          26000         25760\n2016-03-29          25880          26000         25700\n2016-03-30          26200          26420         26040\n2016-03-31          26120          26280         25960\n           005930.KS.Close 005930.KS.Volume 005930.KS.Adjusted\n2016-03-24           25640         10939750           22097.96\n2016-03-25           25760          7172300           22201.38\n2016-03-28           25880          6060900           22304.80\n2016-03-29           25800          8622700           22235.85\n2016-03-30           26160         13380250           22546.12\n2016-03-31           26240         19130750           22615.06\n\n삼성전자 거래정보 시각화\n주가 거래정보를 시각화하기 위해서는 quantmod 패키지의 chartSeries() 함수를 사용한다. 삼성전자의 주가를 주가 그래프로 즐겨보는 candle sticks chart를 그려보면 그림 1과 같다.\n\n\nchartSeries(samsung.electric, subset = \"2016-01-01::2016-03-31\", \n            theme = chartTheme('white', up.col = 'red', dn.col = 'blue'))\n\n\n\n\nFigure 1: 삼성전자 candle sticks chart\n\n\n\n한화생명/삼성생명 거래정보 가져오기\n한화생명과 삼성생명의 종목코드는 각각 088350와 032830이다. 이 종목의 거래정보를 Yahoo로부터 가져오기 위해서 종목코드를 088350.KS, 032830.KS로 변환 후 조회한다.\n\n\nsymbols <- \"088350\"\nsymbols <- paste(symbols, \"KS\", sep = \".\")\nsymbols\n\n\n[1] \"088350.KS\"\n\nhli <- getSymbols(symbols, from = '1990-01-01', to = '2016-03-31',\n                  auto.assign = FALSE)\n\ntail(hli)\n\n\n           088350.KS.Open 088350.KS.High 088350.KS.Low\n2016-03-24           6600           6640          6550\n2016-03-25           6580           6680          6540\n2016-03-28           6680           6680          6580\n2016-03-29           6670           6720          6630\n2016-03-30           6700           6830          6650\n2016-03-31           6820           6830          6700\n           088350.KS.Close 088350.KS.Volume 088350.KS.Adjusted\n2016-03-24            6570           769252           6061.213\n2016-03-25            6650           370720           6135.018\n2016-03-28            6640           230891           6125.792\n2016-03-29            6700           567218           6181.146\n2016-03-30            6800          1192238           6273.402\n2016-03-31            6700          1137339           6181.146\n\nsymbols <- \"032830\"\nsymbols <- paste(symbols, \"KS\", sep = \".\")\nsymbols\n\n\n[1] \"032830.KS\"\n\nsli <- getSymbols(symbols, from = '1990-01-01', to = '2016-03-31',\n                  auto.assign = FALSE)\n\ntail(sli)\n\n\n           032830.KS.Open 032830.KS.High 032830.KS.Low\n2016-03-24         114000         115000        113000\n2016-03-25         114000         116500        113500\n2016-03-28         116000         117500        115000\n2016-03-29         116000         116000        113500\n2016-03-30         115000         116500        114500\n2016-03-31         115500         118500        115000\n           032830.KS.Close 032830.KS.Volume 032830.KS.Adjusted\n2016-03-24          113500           224176           100142.6\n2016-03-25          116000           217698           102348.3\n2016-03-28          116000           249460           102348.3\n2016-03-29          114500           257648           101024.9\n2016-03-30          115000           220132           101466.0\n2016-03-31          117500           478731           103671.8\n\n한화생명/삼성생명 거래정보 시각화\n주가 거래정보를 시각화하기 위해서 chartSeries() 함수로 그린 한화생명과 삼성생명의 candle sticks chart는 각각 그림 2와 그림 3과 같다.\n\n\nchartSeries(hli, subset = \"2016-01-01::2016-03-31\", \n            theme = chartTheme('white', up.col = 'red', dn.col = 'blue'))\n\n\n\n\nFigure 2: 한화생명 candle sticks chart\n\n\n\n\n\nchartSeries(sli, subset = \"2016-01-01::2016-03-31\", \n            theme = chartTheme('white', up.col = 'red', dn.col = 'blue'))\n\n\n\n\nFigure 3: 삼성생명 candle sticks chart\n\n\n\n해외 주가데이터 수집\n미국 종목코드 데이터 수집\nTTR 패키지를 이용해서 미국의 AMEX, NASDAQ, NYSE 시장의 종목코드를 가져온다.\n\n\nstock_symbols <- TTR::stockSymbols()\n\nAMEX <- stock_symbols[stock_symbols$Exchange %in% \"AMEX\", ]\nNASDAQ <- stock_symbols[stock_symbols$Exchange %in% \"NASDAQ\", ]\nNYSE <- stock_symbols[stock_symbols$Exchange %in% \"NYSE\", ]\n\ndim(AMEX)\n\n\n[1] 305  17\n\ndim(NASDAQ)\n\n\n[1] 5101   17\n\ndim(NYSE)\n\n\n[1] 3601   17\n\nhead(NYSE)\n\n\n     Symbol\n7692      A\n7693     AA\n7694    AAC\n7695 AAC-UN\n7696 AAC-WT\n7697   AAIC\n                                                                                                                                               Name\n7692                                                                                                        Agilent Technologies, Inc. Common Stock\n7693                                                                                                                Alcoa Corporation Common Stock \n7694                                                                                           Ares Acquisition Corporation Class A Ordinary Shares\n7695                     Ares Acquisition Corporation Units, each consisting of one Class A ordinary share, and one-fifth of one redeemable warrant\n7696 Ares Acquisition Corporation Redeemable Warrants, each whole warrant exercisable for one Class A ordinary share at an exercise price of $11.50\n7697                                                                                                  Arlington Asset Investment Corp Class A (new)\n     LastSale MarketCap IPOyear Sector Industry Exchange Test.Issue\n7692       NA        NA      NA     NA       NA     NYSE      FALSE\n7693       NA        NA      NA     NA       NA     NYSE      FALSE\n7694       NA        NA      NA     NA       NA     NYSE      FALSE\n7695       NA        NA      NA     NA       NA     NYSE      FALSE\n7696       NA        NA      NA     NA       NA     NYSE      FALSE\n7697       NA        NA      NA     NA       NA     NYSE      FALSE\n     Round.Lot.Size   ETF Market.Category Financial.Status\n7692            100 FALSE            <NA>             <NA>\n7693            100 FALSE            <NA>             <NA>\n7694            100 FALSE            <NA>             <NA>\n7695            100 FALSE            <NA>             <NA>\n7696            100 FALSE            <NA>             <NA>\n7697            100 FALSE            <NA>             <NA>\n     Next.Shares ACT.Symbol CQS.Symbol NASDAQ.Symbol\n7692          NA          A          A             A\n7693          NA         AA         AA            AA\n7694          NA        AAC        AAC           AAC\n7695          NA      AAC.U      AAC.U          AAC=\n7696          NA      AAC.W     AAC.WS          AAC+\n7697          NA       AAIC       AAIC          AAIC\n\n관심종목 거래정보 가져오기\n애플 거래정보 가져오기\n애플의 종목코드는 AAPL이다. 이 종목의 거래정보를 Yahoo로부터 가져오자.\n\n\nNASDAQ[grep(\"Apple\", NASDAQ$Name), ]\n\n\n     Symbol                      Name LastSale MarketCap IPOyear\n2599   AAPL Apple Inc. - Common Stock       NA        NA      NA\n     Sector Industry Exchange Test.Issue Round.Lot.Size   ETF\n2599     NA       NA   NASDAQ      FALSE            100 FALSE\n                   Market.Category Financial.Status Next.Shares\n2599 NASDAQ Global Select MarketSM Normal (Default)          NA\n     ACT.Symbol CQS.Symbol NASDAQ.Symbol\n2599       <NA>       <NA>          AAPL\n\nsymbols <- NASDAQ[grep(\"Apple\", NASDAQ$Name), \"Symbol\"]\n\napple <- getSymbols(symbols, from = '1990-01-01', to = '2016-03-31',\n                    auto.assign = FALSE)\n\ntail(apple)\n\n\n           AAPL.Open AAPL.High AAPL.Low AAPL.Close AAPL.Volume\n2016-03-22   26.3125   26.8225  26.3025    26.6800   129777600\n2016-03-23   26.6200   26.7675  26.4750    26.5325   102814000\n2016-03-24   26.3675   26.5625  26.2225    26.4175   104532000\n2016-03-28   26.5000   26.5475  26.2650    26.2975    77645600\n2016-03-29   26.2225   26.9475  26.2200    26.9200   124760400\n2016-03-30   27.1625   27.6050  27.1500    27.3900   182404400\n           AAPL.Adjusted\n2016-03-22      24.73616\n2016-03-23      24.59941\n2016-03-24      24.49279\n2016-03-28      24.38153\n2016-03-29      24.95867\n2016-03-30      25.39443\n\n애플 거래정보 시각화\n애플의 candle sticks chart는 그림 4와 같다.\n\n\nchartSeries(apple, subset = \"2016-01-01::2016-03-31\", \n            theme = chartTheme('white', up.col = 'red', dn.col = 'blue'))\n\n\n\n\nFigure 4: apple candle sticks chart\n\n\n\n수익률 분석\n한화생명 수익률 분석\n주식 분할 및 배당 정보를 반영한 종가인 Adjusted 변수로 간단한 수익률 분석을 수행한다.\n최고가/최저가\n2010-03-17 이후 2016-03-31까지의 한화생명이 최고가를 친 날은 2015-11-03로 8,090.575원이다. 반대로, 최저가를 친 날은 2011-09-23로 4,688.99원이다.\n\n\nhli1 <- hli[, \"088350.KS.Adjusted\"]\nhli1[which.max(hli1)]\n\n\n           088350.KS.Adjusted\n2015-11-03           7705.852\n\nhli1[which.min(hli1)]\n\n\n           088350.KS.Adjusted\n2011-09-23           4466.019\n\n단순 수익률/복리 수익률\n매일매일의 단순 수익률과 복리 수익률은 각각 다음과 같이 구한다.\n\n\nret.simple <- diff(hli1) / stats::lag(hli1, k = -1) * 100\ntail(ret.simple)\n\n\n           088350.KS.Adjusted\n2016-03-24         -1.3533827\n2016-03-25          1.2048267\n2016-03-28         -0.1492616\n2016-03-29          0.8823524\n2016-03-30          1.4925524\n2016-03-31                 NA\n\nret.cont <- diff(log(hli1)) * 100\ntail(ret.cont)\n\n\n           088350.KS.Adjusted\n2016-03-24         -1.3605645\n2016-03-25          1.2103096\n2016-03-28         -0.1504970\n2016-03-29          0.8995559\n2016-03-30          1.4815234\n2016-03-31         -1.4815234\n\n단순 수익률을 요약하면 다음과 같다. 가장 큰 일일 손실율은 2013-02-26의 약 -10.8%이고, 가장 큰 일일 수익율은 2011-11-01의 약 10.69%이다. 그런데 평균 일일 수익율은 -0.01975%이다.\n\n\nsummary(coredata(ret.simple))\n\n\n 088350.KS.Adjusted \n Min.   :-10.81831  \n 1st Qu.: -1.03488  \n Median :  0.00000  \n Mean   : -0.01976  \n 3rd Qu.:  0.93771  \n Max.   : 10.68815  \n NA's   :2          \n\nret.simple[which.min(ret.simple)]\n\n\n           088350.KS.Adjusted\n2013-02-26          -10.81831\n\nret.simple[which.max(ret.simple)]\n\n\n           088350.KS.Adjusted\n2011-11-01           10.68815\n\n일일 단순 손실율의 분포를 히스토그램으로 그리면 그림 5와 같다. 결과를 보면 0% 근접의 마이너스(-) 수익률의 비율이 가장 높음을 알 수 있다.\n\n\npar(family='NanumGothic')\nhist(ret.simple, breaks=100, freq = FALSE, \n     main = \"한화생명 - Histogram of Simple Returns\", xlab=\"%\")\n\n\n\n\nFigure 5: 한화생명 일일 단순 손실율의 분포를 히스토그램\n\n\n\nValue-at-Risk\n일일 99% Value-at-Risk 값을 과거 자료로 분석하면 일일 손실율이 4.426724% 이상의 확률은 1% 가량 발생한다. 즉, working day 기준 일년에 약 2.5번 정도가 최대 손실율인 4.426724%가 발생한다는 것을 의미한다.\n\n\nquantile(ret.simple, probs = 0.01, na.rm = TRUE)\n\n\n       1% \n-4.426712 \n\n복합 연간 성장률\nC.A.G.R. : 복합 연간 성장률 (CAGR, 연평균성장률)을 구하면 다음과 같다.\n\n\nperiodReturn(hli, period = 'yearly') \n\n\n           yearly.returns\n2010-12-30    -0.08620690\n2011-12-29    -0.06918239\n2012-12-28     0.04864865\n2013-12-30    -0.02190722\n2014-12-30     0.09222661\n2015-12-30    -0.10856454\n2016-03-31    -0.09336942\n\n삼성생명 수익률 분석\n주식 분할 및 배당 정보를 반영한 종가인 `Adjusted’ 변수로 간단한 수익률 분석을 수행한다.\n최고가/최저가\n2010-05-12 이후 2016-03-31까지의 삼성생명이 최고가를 친 날은 2014-12-04로 115,989.5원이다. 반대로, 최저가를 친 날은 2012-01-16로 70,075.63원이다.\n\n\nsli1 <- sli[, \"032830.KS.Adjusted\"]\nsli1[which.max(sli1)]\n\n\n           032830.KS.Adjusted\n2014-12-04           108537.7\n\nsli1[which.min(sli1)]\n\n\n           032830.KS.Adjusted\n2012-01-16           65573.59\n\n단순 수익률/복리 수익률\n매일매일의 단순 수익률과 복리 수익률은 각각 다음과 같이 구한다.\n\n\nret.simple <- diff(sli1) / stats::lag(sli1, k = -1) * 100\ntail(ret.simple)\n\n\n           032830.KS.Adjusted\n2016-03-24         -0.8620682\n2016-03-25          2.1551704\n2016-03-28          0.0000000\n2016-03-29         -1.3043467\n2016-03-30          0.4255240\n2016-03-31                 NA\n\nret.cont <- diff(log(sli1)) * 100\ntail(ret.cont)\n\n\n           032830.KS.Adjusted\n2016-03-24         -0.8771978\n2016-03-25          2.1787334\n2016-03-28          0.0000000\n2016-03-29         -1.3015356\n2016-03-30          0.4357224\n2016-03-31          2.1506263\n\n단순 수익률을 요약하면 다음과 같다. 가장 큰 일일 손실율은 2015-02-13의 -8.415846%이고, 가장 큰 일일 수익율은 2010-05-26의 11.00918%이다. 그런데 평균 일일 수익율은 -0.003446%이다.\n\n\nsummary(coredata(ret.simple))\n\n\n 032830.KS.Adjusted \n Min.   :-8.415844  \n 1st Qu.:-0.930230  \n Median : 0.000000  \n Mean   :-0.003452  \n 3rd Qu.: 0.909096  \n Max.   :11.009166  \n NA's   :2          \n\nret.simple[which.min(ret.simple)]\n\n\n           032830.KS.Adjusted\n2015-02-13          -8.415844\n\nret.simple[which.max(ret.simple)]\n\n\n           032830.KS.Adjusted\n2010-05-26           11.00917\n\n일일 단순 손실율의 분포를 히스토그램으로 그리면 그림 6과 같다. 결과를 보면 0% 근접의 마이너스(-) 수익률의 비율이 가장 높음을 알 수 있다.\n\n\npar(family='NanumGothic')\nhist(ret.simple, breaks=100, freq = FALSE, \n     main = \"삼성생명 - Histogram of Simple Returns\", xlab=\"%\")\n\n\n\n\nFigure 6: 삼성생명 일일 단순 손실율의 분포를 히스토그램\n\n\n\nValue-at-Risk\n일일 99% Value-at-Risk 값을 과거 자료로 분석하면 일일 손실율이 3.886695% 이상의 확률은 1% 가량 발생한다. 즉, working day 기준 일년에 약 2.5번 정도가 최대 손실율인 3.886695%가 발생한다는 것을 의미한다. 한화생명보다는 리스크가 작다.\n\n\nquantile(ret.simple, probs = 0.01, na.rm = TRUE)\n\n\n       1% \n-3.886696 \n\n복합 연간 성장률\n복합 연간 성장률 (CAGR, 연평균성장률)을 구하면 다음과 같다.\n\n\nperiodReturn(sli, period = 'yearly') \n\n\n           yearly.returns\n2010-12-30    -0.14225941\n2011-12-29    -0.21073171\n2012-12-28     0.16563659\n2013-12-30     0.10286320\n2014-12-30     0.12019231\n2015-12-30    -0.05579399\n2016-03-31     0.06818182\n\n\n\n\n",
    "preview": "posts/2016-04-08-Stock/2016-04-08-Stock_files/figure-html5/samsung-1.png",
    "last_modified": "2021-10-10T23:10:00+09:00",
    "input_file": {},
    "preview_width": 1728,
    "preview_height": 1152
  },
  {
    "path": "posts/2016-01-03-distribution/",
    "title": "분포에 대해서",
    "description": "자료의 분포를 설명하는 통계량에 자료의 중심을 설명하는 대표치와 자료의 퍼짐을 설명하는 산포도(분산)가 있다. 대표치로는 평균, 중위수, 최빈수 등이 있고, 퍼진 정도를 표현하는 분산, 분산의 제곱근인 표준편차 등이 있다. 또한 자료가 어느쪽으로 편중되었는지의 기울기를 나타내는 왜도, 대표치 부근에 자료가 밀집한 정도를 나타내는 첨도 등이 있다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2016-01-03",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n들어가기\n최빈수, 왜도, 첨도를 구하는 함수\n데이터 만들기\n분포의 시각화\n\n들어가기\n자료의 분포를 설명하는 통계량에 자료의 중심을 설명하는 대표치와 자료의 퍼짐을 설명하는 산포도(분산)가 있다. 대표치로는 평균, 중위수, 최빈수 등이 있고, 퍼진 정도를 표현하는 분산, 분산의 제곱근인 표준편차 등이 있다. 또한 자료가 어느쪽으로 편중되었는지의 기울기를 나타내는 왜도, 대표치 부근에 자료가 밀집한 정도를 나타내는 첨도 등이 있다.\n이번에는 왜도를 중심으로 이야기를 해 본다. 자료의 대칭성을 설명하는 왜도가 0이면 분포가 좌우 대칭을 이루고 음수이면 왼쪽으로 치우친 분포를, 양수이면 오른쪽으로 치우친 분포를 이룬다. 첨도는 3이면 정규분포 곡선과 유사하며, 3보다 크면 정규분포 곡선보다 정점이 높고 뾰족한 분포며 3보다 작으면 정규분포보다 정점이 낮고 퍼진 모양의 분포다.\n왜도가 1이면 중위수, 평균(산술평균), 최빈수가 같은 값을 가지며 자료가 왼쪽으로 치우친 분포라면 왜도는 음수이며 최빈수 < 중위수 < 평균의 관계가 자료가 오른쪽으로 치우친 분포라면 왜도는 양수이며 평균 < 중위수 < 최빈수의 관계가 성립함을 배운적이 있다. 그러면 이를 보여줄 데이터를 만들고 그림으로 그려보면서 왜도의 값에 따른 평균, 중위수, 최빈수의 관계를 이해해 보자.\n최빈수, 왜도, 첨도를 구하는 함수\n다음처럼 최빈수, 왜도, 첨도를 구하는 함수를 만들어 보았다. 물론 이들 통계량을 구하는 패키지가 있지만, 그 내용을 이해하는 차원에서 구현해 보았다.\n\n\n# 최빈수를 구하는 함수\nget.mode <- function(x) {\n  tbl <- table(x)\n  as.numeric(names(tbl[which(tbl == max(tbl))]))\n}\n\n# 왜도를 구하는 함수\nget.skewness <- function(x) {\n  sum((x - mean(x)) ^ 3)/(length(x) - 1) / sd(x) ^ 3\n}\n\n# 첨도를 구하는 함수\nget.kurtosis<- function(x) {\n  sum((x - mean(x)) ^ 4)/(length(x) - 1) / sd(x) ^ 4  - 3\n}\n\n\n\n데이터 만들기\n\n\n#####################################\n# 왜도가 0에 근사한 데이터를 생성\n#####################################\nset.seed(2)\n# 정규난수 1000개 생성\nprob <- dnorm(1:21, mean=11, sd=3)\nx <- sample(1:21, 1500, replace = TRUE, prob=prob)\n\nx.mean <- mean(x)          # 평균\nx.median <- median(x)      # 중위수\nx.mode <- get.mode(x)      # 최빈수\n\nx.mean\n\n\n[1] 11.06333\n\nsd(x)                      # 표준편차\n\n\n[1] 3.080311\n\nx.median\n\n\n[1] 11\n\nx.mode\n\n\n[1] 11\n\nsummary(x)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2.00    9.00   11.00   11.06   13.00   21.00 \n\nget.skewness(x)\n\n\n[1] 0.01977239\n\nget.kurtosis(x)\n\n\n[1] -0.09515365\n\n#####################################\n# 왜도가 음수인 데이터를 생성\n#####################################\nset.seed(3)\ny <- numeric(1500)   \n\nfor(i in 1:15) {\n  idx <- (i-1)*100+1\n  y[idx:(idx+99)] <- sample(i:20, 100, replace = TRUE)\n}\n\ny.mean <- mean(y)\ny.median <- median(y)\ny.mode <- get.mode(y)\n\ny.mean\n\n\n[1] 13.94733\n\nsd(y)\n\n\n[1] 4.537505\n\ny.median\n\n\n[1] 15\n\ny.mode\n\n\n[1] 16 20\n\nsummary(y)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00   11.00   15.00   13.95   18.00   20.00 \n\nget.skewness(y)\n\n\n[1] -0.6336066\n\nget.kurtosis(y)\n\n\n[1] -0.4125684\n\n#####################################\n# 왜도가 양수인 데이터를 생성\n#####################################\nset.seed(5)\nz <- numeric(1500)\n\nfor(i in 6:20) {\n  idx <- (i-6)*100+1\n  z[idx:(idx+99)] <- sample(1:i, 100, replace = TRUE)\n}\n\nz.mean <- mean(z)\nz.median <- median(z)\nz.mode <- get.mode(z)\n\nz.mean\n\n\n[1] 6.989333\n\nsd(z)\n\n\n[1] 4.419605\n\nz.median\n\n\n[1] 6\n\nz.mode\n\n\n[1] 5\n\nsummary(z)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   3.000   6.000   6.989  10.000  20.000 \n\nget.skewness(z)\n\n\n[1] 0.6470598\n\nget.kurtosis(z)\n\n\n[1] -0.2881661\n\n분포의 시각화\n세 분포를 시각화하면 다음과 같은 그래프를 얻을 수 있다. 왼쪽의 그림들이 분포의 그림이고, 오른쪽은 정규분포에 근사하는지를 검증하기 위한 Q-Q plot이다.\n첫번째 그림은 정규분포에 근사하고, 두번째 그림은 오른쪽으로 치우친 분포(skewed to the left)고, 세번째 그림은 왼쪽으로 치우친 분포(skewed to the right)의 그래프임을 알 수 있다.\n\n\npar(mfrow = c(3, 2))\nplot(density(x), main = \"Like Normal\")\nabline(v = x.mean, col = \"red\", lty = 1)\nabline(v = x.median, col = \"blue\", lty = 2)\nabline(v = x.mode, col = \"black\", lty = 3)\nqqnorm(x)\nqqline(x)\n \nplot(density(y), main = \"Skewed Right\")\nabline(v = y.mean, col = \"red\", lty = 1)\nabline(v = y.median, col = \"blue\", lty = 2)\nabline(v = y.mode, col = \"black\", lty = 3)\nlegend(\"topleft\", legend = c(\"mean\", \"median\", \"mode\"), \n       col=c(\"red\", \"blue\", \"black\"), lty = 1:3)\nqqnorm(y)\nqqline(y)\n \nplot(density(z), main = \"Skewed Left\")\nabline(v = z.mean, col = \"red\", lty = 1)\nabline(v = z.median, col = \"blue\", lty = 2)\nabline(v = z.mode, col = \"black\", lty = 3)\nlegend(\"topright\", legend = c(\"mean\", \"median\", \"mode\"), \n       col=c(\"red\", \"blue\", \"black\"), lty = 1:3)\nqqnorm(z)\nqqline(z)\n\n\n\n\nFigure 1: distribution plot\n\n\n\n\n\n\n",
    "preview": "posts/2016-01-03-distribution/2016-01-03-distribution_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-10T23:07:40+09:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1728
  },
  {
    "path": "posts/2016-01-02-circle/",
    "title": "원에 대하여",
    "description": "가상의 한 점에서 같은 거리 만큼 떨어진 점들의 집합을 원이라 할수 있다. 한 점(원점)에서 1의 거리 만큼 떨어진 점들의 모임인 단위원을 생각해 보자. \\\\( x^2 + y^2 = 1^2 \\\\)인 원의 공식을 기억할지 모르겠다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2016-01-02",
    "categories": [
      "Visualization"
    ],
    "contents": "\n\nContents\n이론적 배경\nR 시각화\n원으로의 수렴\n\n이론적 배경\n가상의 한 점에서 같은 거리 만큼 떨어진 점들의 집합을 원이라 할수 있다. 한 점(원점)에서 1의 거리 만큼 떨어진 점들의 모임인 단위원을 생각해 보자. \\( x^2 + y^2 = 1^2 \\)인 원의 공식을 기억할지 모르겠다. 중학교 수학 시간에 배운 기억이 어렴풋이 나지 않는가?\n그러면 한 점에서 1만큼 떨어진 위치에 꼭지점을 갖는 정n각형을 생각하자. 정삼각형, 정사각형, 정오각형 무수히 많다. 그러면 n의 수가 커질수록 이 정다각형의 모양은 원에 가깝게 된다.\n\n이러한 사실을 다음 과정을 수행하는 R 시각화를 이용해서 증명해 보자.\n\n\\( x^2 + y^2 = 1^2 \\)인 단위원을 그린다. 이 원은 n각형의 외접원이 될 것이다.\n\\( x^2 + y^2 = cos( {n })^2 \\)인 단위원을 그린다. 이 원은 n각형의 내접원이 될 것이다.\n원점에서 1만큼 떨어진 곳에 n개의 점을 찍고 이 점을 연결한 n각형을 그린다.\n내접원의 반지름이 \\( cos( {n}) \\)인 이유는 다음과 같다.\nn각형의 중심에서 각각의 꼭지점을 잇는 선을 그리면 n개의 동일한 면적의 이등변 삼각형이 만들어 진다. 그리고 이 이등변삼각형을 반으로 나누면 두개의 직각 삼각형이 되는데 이 직각삼각형의 원점에서의 각이 \\(  \\)인 직각삼각형이고, 내접원의 반지름은 \\( r cos() \\)이므로 \\( cos() \\)이 된다.\nR 시각화\n내접원과 외접원을 그리는 R 코드와 결과는 다음과 같다.\n\n\npar(family='NanumGothic')\n\nradius <- function () {\n  par(mfrow = c(1, 1), mar=c(1, 1, 1, 1), pty = 's')\n  \n  plot(c(-1, 1), c(-1, 1), type = 'n', \n       axes = FALSE, xlab = '', ylab = '')\n \n  angle <- (0:(10 * 6)) / (10 * 6) * 2 * pi\n  x1 <- cos(angle) / 2\n  y1 <- sin(angle) / 2\n  \n  lines(x1, y1, col = \"blue\")\n  x <- rep(0, 3)\n  y <- rep(0, 3)\n  \n  for (i in 0:3) {\n   x[i+1] <- cos(angle[i * 20 + 1])\n   y[i+1] <- sin(angle[i * 20 + 1])\n   }\n  \n  lines(x, y)\n  points(0,0, pch = 19)\n  lines(c(0, x[2]), c(0, y[2]))\n  lines(c(0, cos(pi) / 2), c(0, 0), col = \"red\", lwd = 2)\n  \n  text(cos(pi) / 4, -0.05, \"반지름\", cex = 0.7)\n  text(-0.15, 0.1, labels = expression(frac(pi, 2 * n)), \n       cex = 0.8)\n}\n\nradius()\n\n\n\n\n원으로의 수렴\n그러면 앞의 방법으로 \\(n\\)각형과 내접원, 외접원을 그려보자. \\(n\\)이 클수록 다각형은 원에 가까워지고, 내접원과 외접원의 크기도 점점 같아지게 된다. 즉, 원에 가까워진다는 반증이다.\n정 3, 6, 9, 12각형을 그려보자.\n\n\ncircle <- function (points) {\n  par(pty = 's')\n  \n  plot(c(-1, 1), c(-1, 1), type = 'n', \n       axes = FALSE, xlab = '', ylab = '')\n  \n  angle <- (0:(10 * points)) / (10 * points) * 2 * pi\n  x1 <- cos(2 * pi / (points * 2)) * cos(angle)\n  y1 <- cos(2 * pi / (points * 2)) * sin(angle)\n  x2 <- cos(angle)\n  y2 <- sin(angle)\n  \n  lines(x1, y1, col = \"blue\")\n  lines(x2, y2)\n  \n  x <- rep(0, points)\n  y <- rep(0, points)\n  \n  for (i in 0:points) {\n    x[i+1] <- cos(angle[i * 10 + 1])\n    y[i+1] <- sin(angle[i * 10 + 1])\n  }\n \n  lines(x, y, col = \"red\", lwd = 1.7)\n}\n\npar(mfrow = c(2, 2), mar = c(1, 1, 1, 1))\ncircle(3)\ncircle(6)\ncircle(12)\ncircle(30)\n\n\n\n\ncircle() 함수로 정30각형만 그려도 거의 원과 구분되지 않는다. 물론 반지름이 1인 경우이다. 여기서 1이라 함은 graphics device에서의 절대좌표 크기다. 실생활에서 그림을 그릴 때, 반지름이 1미터라고 한다면 30각형은 원으로 보기 어려울 것이다. 단지 R의 graphics device 세계에서 원의 성질을 보여주는 예로 이해하기 바란다.\n\n\n\n",
    "preview": "posts/2016-01-02-circle/2016-01-02-circle_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-10T23:05:21+09:00",
    "input_file": {},
    "preview_width": 2600,
    "preview_height": 1600
  },
  {
    "path": "posts/2016-01-02-grid_picture/",
    "title": "grid package로 그림 그리기",
    "description": "R로 야경을 그려보고, 한 낮의 풍경을 그려보겠다. 동심으로 돌아가 하얀 스케치북에 그레파스를 이용해서 산도 그리고, 달고 그리고, 별과 해도 그려보자. 밤의 그림은 Paul Murrell가 그렸고, 필자가  Paul Murrell의 스케치북과 그레파스를 빌려서 모사해보겠다. 해가 중천에 걸린 여름 산은 녹음이 우거져 있다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2016-01-02",
    "categories": [
      "Gallery"
    ],
    "contents": "\n\nContents\nR로 그린 밤 하늘의 풍경\nR로 그린 대낮의 풍경\n\n오클랜드 대학교의 Paul Murrell은 grid라는 이름의 그래픽 라이브러리(package)를 개발했다. 이 라이브러리가 바로 lattice와 ggplot2 package의 토대가 된다.\ngrid package를 이용해여 멋진 풍경을 그려보자. ‘R로도 그림을 그릴 수 있겠구나’하는 흥미차원의 시도다.\n먼저 야경을 그려보고, 한 낮의 풍경을 그려보겠다. 동심으로 돌아가 하얀 스케치북에 그레파스를 이용해서 산도 그리고, 달고 그리고, 별과 해도 그려보자. 밤의 그림은 Paul Murrell가 그렸고, 필자가 Paul Murrell의 스케치북과 그레파스를 빌려서 모사해보겠다. 해가 중천에 걸린 여름 산은 녹음이 우거져 있다.\nR로 그린 밤 하늘의 풍경\n아마 Paul Murrell는 검은색과 회색 크레파스가 다 떨어졌을 것이고, 필자는 하늘색 크레파스가 다 달아 없어졌다. Paul Murrell이 그린 밤 하늘의 풍경이다.\n\n\nlibrary(grid)\n\n# 도화지를 준비했다\npushViewport(viewport(xscale = c(0, 1), yscale = c(0.5, 1),\n                      clip = TRUE))   \n             \n# 검은색과 회색으로 밤하늘을 칠한다\nres <- 50\nfor (i in 1:res)\n  grid.rect(y = 1 - (i-1)/res, just = \"top\",\n            gp = gpar(col = NA, fill = grey(0.5 * i / res)))\n \n# 달을 그리는 함수 정의\nmoon <- function(x, y, size) { \n  t <- seq(-90, 90, length = 50) / 180 * pi\n  x1 <- x + size * cos(t)\n  y1 <- y + size * sin(t)\n  mod <- 0.8\n  x2 <- x + mod * (x1 - x)\n  grid.polygon(c(x1, rev(x2)), c(y1, rev(y1)),\n               default.unit = \"native\",\n               gp = gpar(col = NA, fill = \"white\"))\n}\n \n# 달을 그린다.\nmoon(.1, .9, .03) \n \n# 별을 그리는 함수는 함수 정의\nstar <- function(x, y, size) { \n  x1 <- c(x, x + size * .1, x + size * .5, x + size * .1,\n          x, x - size * .1, x - size * .5, x - size * .1) + .05\n\n  y1 <- c(y - size, y - size * .1, y, y + size * .1,\n          y + size * .7, y + size * .1, y, y - size * .1) + .05\n  grid.polygon(x1, y1, default.unit = \"native\",\n               gp = gpar(col = NA, fill = \"white\"))\n}\n \n# 별을 네개 그린다.\nstar(.5, .7, .02)\nstar(.8, .9, .02)\nstar(.72, .74, .02)\nstar(.62, .88, .02)\n \n# 작은별을 스무 개 그린다.\ngrid.circle(runif(20, .2, 1), runif(20, .6, 1), r = .002,\n            default.unit = \"native\",\n            gp = gpar(col = NA, fill = \"white\"))\n \n# 산을 그리는 함수 정의\nhill <- function(height = 0.1, col = \"black\") {\n  n <- 100\n  x <- seq(0, 1, length = n)\n  y1 <- sin(runif(1) + x * 2 * pi)\n  y2 <- sin(runif(1) + x * 4 * pi)\n  y3 <- sin(runif(1) + x * 8 * pi)\n  y <- 0.6 + height*((y1 + y2 + y3) / 3)\n  grid.polygon(c(x, rev(x)), c(y, rep(0, n)),\n               default.unit = \"native\",\n               gp=gpar(col = NA, fill = col))\n}\n \n# 산을 그린다\nhill()\n\n\n\n\nR로 그린 대낮의 풍경\n이번에는 낮의 풍경을 그려보자. 파란 하늘에 노란 태양이 떠 있는 그림이다. Paul Murrell 그림을 응용해서 그려 보았다. 산을 그리는 로직은 Paul Murrell 그린 밤하늘을 것에 색상만 바꾸었다.\n\n\nlibrary(grid)\n\n# 도화지를 준비했다\npushViewport(viewport(xscale = c(0, 1), yscale = c(0.5, 1),\n                      clip = TRUE))   \n \nsky <- cm.colors(100)\nsky <- sky[-(51:100)] # 하늘색 크레파스\n \n# 하늘색 크레파스로 하늘을 그린다.\nfor (i in 1:res) \n  grid.rect(y = 1 - (i-1) / res, just = \"top\",\n            gp = gpar(col = NA, fill = sky[i]))\n \n# 낮에 보니 푸른 산이다.\nhill(col = \"green4\") \n \n#  해를 그리는 함수 정의\nsun <- function(x, y, r) \n  grid.circle(x, y, r = r, default.unit = \"native\",\n              gp = gpar(col = NA, fill = \"yellow\"))\n \n# 해를 그린다\nsun(0.5, 0.9, 0.04)\n\n\n\n\n구름 한 점 없는 맑은 날이다. 소풍가기 딱 좋은 날인 것 같다.\n\n\n\n",
    "preview": "posts/2016-01-02-grid_picture/2016-01-02-grid_picture_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-10T23:06:09+09:00",
    "input_file": {},
    "preview_width": 2600,
    "preview_height": 1600
  },
  {
    "path": "posts/2016-01-01-outside_plotregion_legend/",
    "title": "plot region 밖에 legend 출력하기",
    "description": "R Graphics Device에 플롯을 출력하는 영역의 구분에 대해서 알아보자.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2016-01-01",
    "categories": [
      "Visualization"
    ],
    "contents": "\n\nContents\nR Graphics Device의 이해\nlegend 표현하기\nplot 영역 밖에 legend 출력하기\n\nExcel처럼 plot 영역의 밖에 legend를 출력할 수 없냐는 혹자의 질문이 있었다. 한번도 생각해보지 않은 주제라 막연히 mtext(), ploygon() 등의 함수로 사용자 정의 함수를 만들어 구현할 수 있을 것 같았다. 그런데, R graphics parameter를 이용하면 쉽게 해결할 수 있다.\nR Graphics Device의 이해\n우선 R Graphics Device에 플롯을 출력하는 영역의 구분에 대해서 알아보자.\n플롯을 그리는 영역에는 figure region과 figure margin이 있다. figure region은 플롯을 표현하는 영역으로 실제로 플롯이 위치하는 plot region을 포함한다. figure margin은 figure region 안에서의 plot region의 여백이다. 그리고 figure region 밖의 여백을 outer margin이라 한다.\nR Graphics Device의 구조는 다음 그림을 통해 쉽게 이해할 수 있다.\n\n\npar(oma = c(2, 2, 2, 2))\nset.seed(1)\nx <- rnorm(100)\nhist(x, col = \"lightblue\")\n\nbox(which = \"plot\"  , lty = 1, cex = 1.5, col = \"red\")\nbox(which = \"figure\", lty = 2, cex = 1.5, col = \"blue\")\n#box(which = \"inner\" , lty = 3, cex = 1.5)\nbox(which = \"outer\" , lty = 4, cex = 1.5, col = \"black\")\n\ntext(-2, 15, \"plot region\")\n\nmtext(\"figure region\", side = 3, adj = 0, at = -3, line = 2)\nmtext(\"margin 1\", side = 1, col = \"red\")\nmtext(\"margin 2\", side = 2, col = \"red\")\nmtext(\"margin 3\", side = 3, col = \"red\")\nmtext(\"margin 4\", side = 4, col = \"red\")\n\nmtext(\"outer margin area\", side = 3, adj = 0, line = 1, outer = TRUE)\nmtext(\"outer margin 1\", side = 1, outer = TRUE, col = \"blue\")\nmtext(\"outer margin 2\", side = 2, outer = TRUE, col = \"blue\")\nmtext(\"outer margin 3\", side = 3, outer = TRUE, col = \"blue\")\nmtext(\"outer margin 4\", side = 4, outer = TRUE, col = \"blue\")\n\n\n\n\nFigure 1: R Graphics Device의 구조\n\n\n\nlegend 표현하기\n일반적인 legend 사용법은 다음과 같으며, legend는 plot region 안에 위치하게 된다. 그러나 plot region이 legend를 표시한 공간이 없이 빼곡하게 플롯으로 채워진다면 legend를 표시할 수 없게 된다. 이 경우는 plot region 밖에 legend를 출력하고 싶어질 것이다.\n\n\nn.row <- 10\nn.col <- 3\n\nmat <- matrix(0, nrow = n.row, ncol = n.col)\n\nset.seed(123)\nfor (i in 1:n.col)\n  mat[, i] <- sample(1:(n.row * n.col), \n                     size = n.row, replace = TRUE)\n \nmatplot(mat, type=\"n\", ylim = c(0, 40))\nmatpoints(mat, type=\"p\", pch = 1:3) \nmatlines(mat)\n\ntitle(\"Inside plot region example\")\nlegend(\"topright\", legend = paste(\"Sample\", 1:n.col), \n       col = 1:n.col, pch = 1:n.col, lty = 1:n.col)\n\n\n\n\nFigure 2: legend의 일반적인 사용 사례\n\n\n\nplot 영역 밖에 legend 출력하기\npar() 함수를 이용해서 R Graphics Device를 두 개의 영역으로 적당히 나눈 후 하나의 figure region에는 플롯을, 나머지 figure region에는 legend를 표시하면 plot region 밖에 legend를 출력할 수 있다.\nplot region 밖에 legend를 출력하는 로직과 예제는 다음과 같다.\npar() 함수의 fig 인수를 이용해서 플롯을 출력할 figure region을 적당히 두 개로 분할한다.\n첫번 째 figure region의 plot region에 플롯을 출력한다.\npar() 함수의 new 인수로 신규 플롯을 현재의 graphic device에 출력하도록 설정한다.\npar() 함수의 fig 인수로 남은 figure region을 legend를 출력할 figure region으로 설정한다.\n두번 째 figure region의 plot region에 빈 플롯을 그린 후, 그 위에 legend를 출력한다.\n\n\npar(fig = c(0, 1, 0.2, 1), mar = c(2, 4, 4, 4))\nmatplot(mat, type=\"n\")\nmatpoints(mat, type=\"p\", pch=1:n.col) \nmatlines(mat)\ntitle(\"Outside plot region example\")\n \npar(new = TRUE)\npar(fig = c(0, 1, 0, 0.2), mar = c(0, 0, 0, 0))\nplot(0, xlim = c(0, 1), ylim = c(0, 5), \n     axes = FALSE, xlab = \"\", ylab = \"\", type = \"n\")\nlegend(0.2, 3.5, legend = paste(\"Sample\", 1:n.col), \n       col = 1:n.col, pch = 1:n.col, lty = 1:n.col, horiz = TRUE)\n\n\n\n\nFigure 3: plot region 밖의 legend\n\n\n\n그런데 이 예제를 엄밀하게 설명하자면 plot region 밖에 legend를 출력하는 것이 아니라, 다른 figure region의 plot region에 legend를 출력하는 것이다. 그러나 시각적으로 plot region 밖에 legend가 출력된 것처럼 착각하게 된다.\n\n\n\n",
    "preview": "posts/2016-01-01-outside_plotregion_legend/2016-01-01-outside_plotregion_legend_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-10T23:04:47+09:00",
    "input_file": {},
    "preview_width": 2600,
    "preview_height": 1600
  },
  {
    "path": "posts/2014-10-24-secret/",
    "title": "메이다이닝, 시크릿 가든",
    "description": "반송과 바오밥 나무, 그리고 가을에 방문하면 잘 익은 몇 개의 모과도 얻어올 수 있는 곳, 메이다이닝",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2014-10-24",
    "categories": [
      "Gallery"
    ],
    "contents": "\n\nContents\n메이 다이닝\n시크릿 가든\n시크릿 가든 전경\n반송\n소나무 군락\n바오밥나무\n모과나무\n근처의 산등성\n\n\n\n\n\n메이 다이닝\n도봉구에 있는 메이다이닝이라는 레스토랑에 두어 번 다녀왔다. 음식보다는 시크릿가든이라는 야외 정원이 아름다운 곳이다. 반송과 바오밥 나무, 그리고 가을에 방문하면 잘 익은 몇 개의 모과도 얻어올 수 있는 곳이다.\n시크릿 가든\n몇 개의 사진을 공유해 본다.\n시크릿 가든 전경\n멀리서 바라본 시크릿 가든. 멀리 도봉산도 보인다.\n\n\n\n반송\n시크릿 가든에는 반송이 유명하다. 마치 반원 모양처럼 생겼다.\n\n\n\n소나무 군락\n소나무가 제법 아름답다.\n\n\n\n\n\n\n바오밥나무\n바오밥나무도 볼 수 있다.\n\n\n\n모과나무\n모과나무도 볼 수 있다.\n\n\n\n근처의 산등성\n근처의 산등성으로 산책을 하다가, 이름 모를 나무의 군락을 보다.\n\n\n\n\n\n\n",
    "preview": "posts/2014-10-24-secret/img/secret.png",
    "last_modified": "2021-10-10T23:04:01+09:00",
    "input_file": {},
    "preview_width": 2000,
    "preview_height": 700
  },
  {
    "path": "posts/2006-12-13-matrix-tip/",
    "title": "행렬의 원소중에 최대값의 위치 알아내기",
    "description": "행렬의 원소중에 최대값의 위치를 알아내는 방법을 알아본다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2006-12-13",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n일러두기\n행렬의 원소중에 최대값의 위치 알아내기\n행렬의 원소중에 최소값의 위치 알아내기\n\n\n일러두기\n2006-12-13 네이버 블로그에 게시된 내용을 옮겨 온 글입니다. 지금의 R 환경과 다소 내용이 다를 수 있음을 밝여둡니다.\n행렬의 원소중에 최대값의 위치 알아내기\n행렬에서 최대 값의 위치를 알아내는 방법입니다.\n\n\nx <- matrix(c(1, 2, 3, 9, 4, 5, 6, 7), nrow = 2, byrow = TRUE)\nx\n\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    2    3    9\n[2,]    4    5    6    7\n\nwhich(x == max(x), arr.ind = TRUE)\n\n\n     row col\n[1,]   1   4\n\n이 예제는 1행 4열에 최대값이 있습니다.\n행렬의 원소중에 최소값의 위치 알아내기\n\n\nwhich(x == min(x), arr.ind = TRUE)\n\n\n     row col\n[1,]   1   1\n\n이 예제는 1행 1열에 최소값이 있습니다.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-10T23:02:42+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2005-07-17-poss/",
    "title": "이항분포의 포아송분포 근사",
    "description": "샘플의 수가 커질수록 이항분포가 포아송분포로 근사하는 것을 시각화를 통해서 확인해 보자.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2005-07-17",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n일러두기\n이항분포의 포아송분포 근사\nR Script\n\n\n일러두기\n2005-07-17 네이버 블로그에 게시된 내용을 옮겨 온 글입니다. 지금의 R 환경과 다소 내용이 다를 수 있음을 밝여둡니다.\n이항분포의 포아송분포 근사\n이항분포 B(n,p)에서 평균인 np값은 고정하고 n값이 충분히 커지면 이 분포는 평균 m이 np인 포아송분포에 근사하게 된다. 이것을 그래프로 그려 포아송분포로 근사하는 것을 추적해 보았다. np=5이고 n=10, 20, 50, 100인 이항분포를 한 좌표에 그려 보았다. 그리고 평균이 5인 포아송 분포도 같이 그려 보았다.\nR Script\n\n\nmean <- 5\n\npar(family = \"NanumSquare\")\nplot(dbinom(0:15, 10, mean / 10), type = \"l\",\n     main = \"이항분포의 포아송분포 근사; B(n,p), np=5\", \n     xlab = \"X\", ylab = \"Probability\")\n\nlines(dbinom(0:15, 20, mean / 20), lty = 2, col = 2)\nlines(dbinom(0:15, 50, mean / 50), lty = 3, col = 3)\nlines(dbinom(0:15, 100, mean / 100), lty = 4, col = 4)\nlines(dpois(0:15, mean), lty = 5, col = 5)\n\nlegend(10.5, 0.2, legend = c(\"Bin(10,0.5)\",\"Bin(20,0.25)\",\"Bin(50,0.1)\",\n                             \"Bin(100,0.05)\",\"Pos(5)\"), \n       lty = 1:5, col = 1:5)\n\n\n\nround(dpois(0:15, 5) - dbinom(0:15, 10, mean / 10), 5)\n\n\n [1]  0.00576  0.02392  0.04028  0.02319 -0.02961 -0.07063 -0.05886\n [8] -0.01274  0.02133  0.02650  0.01716  0.00824  0.00343  0.00132\n[15]  0.00047  0.00016\n\nround(dpois(0:15, 5) - dbinom(0:15, 20, mean / 20), 5)\n\n\n [1]  0.00357  0.01255  0.01728  0.00648 -0.01422 -0.02686 -0.02239\n [8] -0.00796  0.00439  0.00920  0.00821  0.00524  0.00268  0.00117\n[15]  0.00045  0.00015\n\nround(dpois(0:15, 5) - dbinom(0:15, 50, mean / 50), 5)\n\n\n [1]  0.00158  0.00506  0.00628  0.00181 -0.00544 -0.00946 -0.00788\n [8] -0.00318  0.00100  0.00294  0.00295  0.00211  0.00122  0.00060\n[15]  0.00026  0.00010\n\nround(dpois(0:15, 5) - dbinom(0:15, 100, mean / 100), 5)\n\n\n [1]  0.00082  0.00253  0.00304  0.00080 -0.00268 -0.00455 -0.00379\n [8] -0.00158  0.00041  0.00136  0.00142  0.00104  0.00062  0.00032\n[15]  0.00014  0.00006\n\n\n\n\n",
    "preview": "posts/2005-07-17-poss/2005-07-17-poss_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-10T23:31:50+09:00",
    "input_file": {},
    "preview_width": 2600,
    "preview_height": 1600
  },
  {
    "path": "posts/2005-07-06-binomial/",
    "title": "이항분포의 정규분포 근사",
    "description": "샘플의 수가 커질수록 이항분포가 정규분포로 근사하는 것을 시각화를 통해서 확인해 보자.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2005-07-06",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n일러두기\n이항분포의 정규분포 근사\nR Script\n\n\n일러두기\n2005-07-06 네이버 블로그에 게시된 내용을 옮겨 온 글입니다. 지금의 R 환경과 다소 내용이 다를 수 있음을 밝여둡니다.\n이항분포의 정규분포 근사\n이항분포 B(n,p)에서 p값에 관계없이 n값이 충분히 커지면 이 분포는 정규분포에 근사하게 된다. 이것을 그래프로 그려 정규분포로 근사하는 것을 추적해 보았다. p=0.2이고 n=10, 20, 30, 40, 50인 이항분포를 한 좌표에 그려 보았다.\nR Script\n\n\npar(family = \"NanumSquare\")\nplot(dbinom(1:20, 10, 0.2), type = \"l\", \n     main = \"이항분포의 정규분포 근사; B(n, 0.2)\", \n     xlab = \"X\", ylab = \"Probability\")\n\nlines(dbinom(1:20, 20, 0.2), lty = 2, col = 2)\nlines(dbinom(1:20, 30, 0.2), lty = 3, col = 3)\nlines(dbinom(1:20, 40, 0.2), lty = 4, col = 4)\nlines(dbinom(1:20, 50, 0.2), lty = 5, col = 5)\n\nlegend(12, 0.27, legend = c(\"Bin(10, 0.2)\",\"Bin(20, 0.2)\",\"Bin(30, 0.2)\",\n                            \"Bin(40, 0.2)\",\"Bin(50, 0.2)\"),\n       lty = 1:5, col = 1:5)\n\n\n\n\n\n\n\n",
    "preview": "posts/2005-07-06-binomial/2005-07-06-binomial_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-10T23:01:42+09:00",
    "input_file": {},
    "preview_width": 2600,
    "preview_height": 1600
  },
  {
    "path": "posts/2005-06-22-operation-sets/",
    "title": "집합연산",
    "description": "R에서도 집합연산을 수행할 수 있는 여러 유용한 함수를 제공한다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2005-06-22",
    "categories": [
      "R Analytics"
    ],
    "contents": "\n\nContents\n일러두기\n데이터 만들기\n합집합 (x ∪ y)\n교집합 (x ∩ y)\n차집합 (x - y)\n원소의 집합 포함관계 (a ∈ x, a ∈ y)\n집합의 포함관계 (z ⊂ x, z ⊂ y)\nis.element() 함수를 응용한 교집합\nis.element() 함수를 응용한 차집합\n\n집합의 상등(x = y)\n\n일러두기\n2005-06-22 네이버 블로그에 게시된 내용을 옮겨 온 글입니다. 지금의 R 환경과 다소 내용이 다를 수 있음을 밝여둡니다. R에서의 집합연산에 대해서 알아보자.\n데이터 만들기\n먼저 집합 x와 y를 만들어 보자. 두 개의 벡터는 이후에 집한 연산에서 계속 사용될 데이터다.\n\n\nset.seed(3) \nx <- sort(sample(1:10, 5))\nx\n\n\n[1] 2 3 4 5 7\n\nset.seed(5)\ny <- sort(sample(1:10, 5))\ny\n\n\n[1] 1 2 3 7 9\n\n합집합 (x ∪ y)\n합집합은 union() 함수를 사용한다.\n\n\nunion(x, y)\n\n\n[1] 2 3 4 5 7 1 9\n\n그리고 unique() 함수를 응용해서 구할 수도 있다. unique() 함수는 벡터에 대해서 유일(unique)한 값을 구한다.\n\n\nunique(c(x, y))\n\n\n[1] 2 3 4 5 7 1 9\n\n교집합 (x ∩ y)\n교집합은 intersect() 함수를 사용한다.\n\n\nintersect(x, y)\n\n\n[1] 2 3 7\n\n그리고 unique() 함수와 sort() 함수를 응용해서 구할 수도 있다. sort() 함수는 정렬함수다.\n\n\nsort(unique(c(x, y)))[table(c(x, y)) == 2]\n\n\n[1] 2 3 7\n\n또한 다음과 같이 구할 수도 있다.\n\n\nunique(y[match(x, y, 0)])\n\n\n[1] 2 3 7\n\n사실 R에서 intersect() 함수가 이와 같이 정의되어 있다.\n\n\nintersect\n\n\nfunction (x, y) \n{\n    y <- as.vector(y)\n    unique(y[match(as.vector(x), y, 0L)])\n}\n<bytecode: 0x7faa90cda870>\n<environment: namespace:base>\n\n차집합 (x - y)\n차집합은 setdiff() 함수를 사용한다.\n\n\nsetdiff(x, y)  # x-y\n\n\n[1] 4 5\n\nsetdiff(y, x)  # y-x\n\n\n[1] 1 9\n\n그리고 intersect() 함수를 응용해서 구할 수도 있다.\n\n\nintersect(sort(unique(c(x, y)))[table(c(x, y)) == 1], x)  # x-y\n\n\n[1] 4 5\n\nintersect(sort(unique(c(x, y)))[table(c(x, y)) == 1], y)  # y-x\n\n\n[1] 1 9\n\nR의 setdiff() 함수는 다음과 같이 정의되어 있다.\n\n\nsetdiff\n\n\nfunction (x, y) \n{\n    x <- as.vector(x)\n    y <- as.vector(y)\n    unique(if (length(x) || length(y)) \n        x[match(x, y, 0L) == 0L]\n    else x)\n}\n<bytecode: 0x7faa909ce240>\n<environment: namespace:base>\n\n원소의 집합 포함관계 (a ∈ x, a ∈ y)\nis.element() 함수를 이용해서 특정 원소가 집합에 포함되는지를 검증할 수 있다. 단, 이 함수의 반환값은 논리 벡터인데 그 원소의 개수가 함수의 첫번째 인수의 개수와 동일하다.\n\n\na <- 9\nis.element(a,x)  # a ∈ x\n\n\n[1] FALSE\n\nis.element(a,y)  # x ∈ y\n\n\n[1] TRUE\n\n그리고 as.logical() 함수와 sum() 함수를 응용해서 구할 수도 있다.\n\n\nas.logical(sum(x == a))  # a ∈ x\n\n\n[1] FALSE\n\nas.logical(sum(y == a))  # a ∈ y\n\n\n[1] TRUE\n\n다음과 같은 방법을 사용할 수도 있다.\n\n\nall(!is.na(match(a, x)))  # a ∈ x\n\n\n[1] FALSE\n\nall(!is.na(match(a, y)))  # a ∈ y\n\n\n[1] TRUE\n\nall(match(a, x, 0) > 0)   # a ∈ x\n\n\n[1] FALSE\n\nall(match(a, y, 0) > 0)   # a ∈ y\n\n\n[1] TRUE\n\n또한 다음처럼 구할 수도 있다.\n\n\nmatch(a, x, 0) > 0\n\n\n[1] FALSE\n\nmatch(a, y, 0) > 0\n\n\n[1] TRUE\n\nR에서 is.element() 함수가 이와 같이 정의되어 있다.\n\n\nis.element\n\n\nfunction (el, set) \nmatch(el, set, 0L) > 0L\n<bytecode: 0x7faa90e0cc90>\n<environment: namespace:base>\n\n집합의 포함관계 (z ⊂ x, z ⊂ y)\nis.element() 함수를 이용해서 특정 집합이 집합에 포함되는지를 검증할 수 있다. 단, is.element() 함수의 반환값은 논리 벡터인데 그 원소의 개수가 함수의 첫번째 인수의 개수와 동일하기 때문에 prod() 함수를 이용하였다.\n\n\nz <- c(2, 8)\nas.logical(prod(is.element(z, x)))  # z ⊂ x\n\n\n[1] FALSE\n\nas.logical(prod(is.element(z, y)))  # z ⊂ y\n\n\n[1] FALSE\n\nas.logical(prod(is.element(x, z)))  # x ⊂ z\n\n\n[1] FALSE\n\n그리고 as.logical() 함수와 prod() 함수, intersect() 함수를 응용해서 구할 수도 있다.\n\n\nas.logical(prod(intersect(x, z)==z))  # z⊂x\n\n\n[1] FALSE\n\nas.logical(prod(intersect(y, z)==z))  # z⊂y\n\n\n[1] FALSE\n\n# 원소의 개수가 배수가 아니면 경고가 발생한다.\nas.logical(prod(intersect(x, z)==x)) \n\n\n[1] FALSE\n\n다음을 응용할 수도 있다.\n\n\nmatch(z, x, 0) > 0\n\n\n[1]  TRUE FALSE\n\nmatch(z, y, 0) > 0\n\n\n[1]  TRUE FALSE\n\nmatch(x, z, 0) > 0\n\n\n[1]  TRUE FALSE FALSE FALSE FALSE\n\nall(match(z, x, 0) > 0)  # z ⊂ x\n\n\n[1] FALSE\n\nall(match(z, y, 0) > 0)  # z ⊂ y\n\n\n[1] FALSE\n\nall(match(x, z, 0) > 0)  # x ⊂ z\n\n\n[1] FALSE\n\nall(is.element(z, x))    # z ⊂ x\n\n\n[1] FALSE\n\nall(is.element(z, y))    # z ⊂ y\n\n\n[1] FALSE\n\nall(is.element(x, z))    # x ⊂ z\n\n\n[1] FALSE\n\nall() 함수는 인수의 값이 모두 TRUE일 경우에만 TRUE를 반환하고 아니면 FALSE를 반환한다.\nis.element() 함수를 응용한 교집합\nis.element() 함수를 응용하면 sort(unique(c(x, y)))[table(c(x, y)) == 2]을 다음과 같이 간략화 시킬 수 있다.\n\n\nx[is.element(x, y)]  # x ∩ y\n\n\n[1] 2 3 7\n\ny[is.element(y, x)]  # y ∩ x\n\n\n[1] 2 3 7\n\nis.element() 함수를 응용한 차집합\nis.element() 함수를 응용하면 intersect(sort(unique(c(x, y)))[table(c(x, y)) == 1], x)을 다음과 같이 간략화 시킬 수 있다.\n\n\nx[!is.element(x, y)]  # x - y\n\n\n[1] 4 5\n\ny[!is.element(y, x)]  # y - x\n\n\n[1] 1 9\n\n집합의 상등(x = y)\nsetequal() 함수를 이용하여 집합의 상등을 알아볼 수 있다.\n\n\nsetequal(x, y)\n\n\n[1] FALSE\n\nsetequal(x, x)\n\n\n[1] TRUE\n\nsetequal() 함수는 다음과 같이 정의되어 있다.\n\n\nsetequal\n\n\nfunction (x, y) \n{\n    x <- as.vector(x)\n    y <- as.vector(y)\n    !(anyNA(match(x, y)) || anyNA(match(y, x)))\n}\n<bytecode: 0x7faa6632af98>\n<environment: namespace:base>\n\n이 함수를 수식으로 표현하자면 다음과 같다.\n\nif x ⊂ y and y ⊂ x then x = y othwise x ≠ y\n\n다음처럼 구할 수도 있다.\n\n\nall(sort(x) == sort(y))\n\n\n[1] FALSE\n\nall(sort(x) == sort(x))\n\n\n[1] TRUE\n\n# 원소의 개수가 배가 아니면 경고가 발생한다.\nall(sort(x) == sort(z))  \n\n\nWarning in sort(x) == sort(z): longer object length is not a multiple\nof shorter object length\n[1] FALSE\n\nwarning message가 번거롭다면 options() 함수의 warn 인수값을 음수로 바꾸면 출력되지 않는다. 그러나 이 방법은 권장하지 않는다. 차라리 다음과 같이 ifelse() 함수를 이용해서 예외처리를 해주면 된다.\n\n\nifelse(length(x) != length(z), FALSE, all(sort(x) == sort(z)))\n\n\n[1] FALSE\n\n앞서 집합 포함관계에서도 warning message가 출력되었는데 이와 같이 처리하면 된다.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-10T23:01:00+09:00",
    "input_file": {}
  },
  {
    "path": "posts/2005-04-25-color/",
    "title": "color",
    "description": "데이터 시각화에서 색상의 선택은 중요한 작업 중의 하나다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2005-04-25",
    "categories": [
      "Visualization"
    ],
    "contents": "\n일러두기\n2005-04-25 네이버 블로그에 게시된 내용을 옮겨 온 글입니다. 지금의 R 환경과 다소 내용이 다를 수 있음을 밝여둡니다.\ncolors\nChart를 그리기 위해서는 점, 선, 면, 문자 등의 가시적인 요소가 필연적이다. 이들 요소가 모여서, 범례를 만들고, 좌표축을 만들고, 타이틀 등과 그래프를 만든다. 여기에 각각의 객체들에 색상을 부여해서 가독성을 높일 수 있다. 단일 색상이 아니라 여러 색상을 적절하게 조합하면 보다 직관적인 Chart를 생성할 수 있다.\n색채학이라는 학문이 있을 정도로 색에 대한 과학적인 정보는 무궁무진하다. 여기서는 기본적인 색에 대한 부분을 다루고자 한다. 구체적인 색의 원리 등은 다른 문서를 참고하기 바란다.\n그러면 R Graphics에서의 색상에 대해서 알아보자.\ncolors/colours\nR이 인식할 수 있는 내장된 색상 이름을 리턴하는 함수다. 657개의 색상 이름을 반환하며, 두 함수는 이름만 다를 뿐 동일한 함수이다.\n\n\ncolors\n\n\nfunction (distinct = FALSE) \n{\n    c <- .Call(C_colors)\n    if (distinct) \n        c[!duplicated(t(col2rgb(c)))]\n    else c\n}\n<bytecode: 0x7fa0698d5f80>\n<environment: namespace:grDevices>\n\ncolours\n\n\nfunction (distinct = FALSE) \n{\n    c <- .Call(C_colors)\n    if (distinct) \n        c[!duplicated(t(col2rgb(c)))]\n    else c\n}\n<bytecode: 0x7fa0892e3b58>\n<environment: namespace:grDevices>\n\nlength(colors())\n\n\n[1] 657\n\ncolors()[1:10]\n\n\n [1] \"white\"         \"aliceblue\"     \"antiquewhite\"  \"antiquewhite1\"\n [5] \"antiquewhite2\" \"antiquewhite3\" \"antiquewhite4\" \"aquamarine\"   \n [9] \"aquamarine1\"   \"aquamarine2\"  \n\n다음과 같은 함수를 만들었다. 이 함수는 그래프 좌표에 주어진 색깔들을 바둑판처럼 출력한다. 단, 가로와 세로의 동수를 만들기 위해서 몇 개의 색상은 출력이 되지 않을 수 있다. 만약 모든 입력 색상을 다 출력하려면 floor 대신 round 함수를 사용하면 되지만 출력 모양이 사각형이 되지 않을 수 있다.\n\n\ncol.map <- function(cols = colours()) {\n  n = length(cols)\n  cnt = floor(sqrt(n))\n \n  plot.new()\n  plot.window(xlim = c(0,cnt),\n              ylim = c(0,cnt))\n \n  for (i in 1:cnt)\n     for (j in 1:cnt)\n       rect(i - 1, j - 1, i, j, col = cols[(i - 1) * cnt + j], border = NA)\n}\n\n\n\n그러면 colors()를 색상으로 그래픽 디바이스에 출력해 보자.\n\n\ncol.map()\n\n\n\n\n이 색상으로 우리는 좌표를 그릴 수 있고, 문자를 출력하는 등 Graphic Device에 자유롭게 활용 수 있다.\nhsv\n색의 삼속성인 h=색상, s=채도, v=명도를 사용해서 색깔을 지정한다. 이외에 감마나, 알파도 지정할 수 있다. R에서의 많은 색상관련 함수들이 이 함수를 사용해서 구현하였다.\n\n\npar(mfrow = c(2, 2))\ncol.map(c(hsv(.5, .5, .5), hsv(.6, .5, .5), hsv(.7, .5, .5), hsv(.8, .5, .5)))\ncol.map(c(hsv(.5, .5, .5), hsv(.5, .6, .5), hsv(.5, .7, .5), hsv(.5, .8, .5)))\ncol.map(c(hsv(.5, .5, .5), hsv(.5, .5, .6), hsv(.5, .5, .7), hsv(.5, .5, .8)))\ncol.map(c(hsv(.5, .5, .5), hsv(.6, .6, .6), hsv(.7, .7, .7), hsv(.8, .8, .8)))\n\n\n\n\nrainbow Family 함수\nR에서 hsv 함수를 이용해서 근접한 연속선상의 색상들을 n개의 벡터로 반환하는 함수를 다음과 같이 재공한다.\n\nrainbow(n, s = 1, v = 1, start = 0, end = max(1,n - 1)/n, gamma = 1) heat.colors(n) terrain.colors(n) topo.colors(n) cm.colors(n)\n\n\n\n\n그러면 앞서 만든 col.map를 이용해서 몇가지 연속선상의 색깔을 출력해 보자.\n\n\npar(mfrow = c(2, 2)) \ncol.map(rainbow(400))\ncol.map(heat.colors(400))\ncol.map(cm.colors(400))\ncol.map(topo.colors(400))\n\n\n\n\nrgb\n빛의 3원색인 Red, Green, Blue를 이용해서 색상을 나타낸다.\n\nrgb(red, green, blue, alpha, names = NULL, maxColorValue = 1)\n\n\n\nreds <- rgb((0:15) / 15, g = 0, b = 0, names = paste(\"red\", 0:15, sep = \".\"))\nreds\n\n\n    red.0     red.1     red.2     red.3     red.4     red.5     red.6 \n\"#000000\" \"#110000\" \"#220000\" \"#330000\" \"#440000\" \"#550000\" \"#660000\" \n    red.7     red.8     red.9    red.10    red.11    red.12    red.13 \n\"#770000\" \"#880000\" \"#990000\" \"#AA0000\" \"#BB0000\" \"#CC0000\" \"#DD0000\" \n   red.14    red.15 \n\"#EE0000\" \"#FF0000\" \n\ncol.map(reds)\n\n\n\n\ngray\n회색계열의 색상을 만든다. level은 0과 1사이의 숫자이며, 1이면 흰색, 0이면 검정색이다.\n\ngray(level)\n\n\n\ngray(0:8 / 8)\n\n\n[1] \"#000000\" \"#202020\" \"#404040\" \"#606060\" \"#808080\" \"#9F9F9F\"\n[7] \"#BFBFBF\" \"#DFDFDF\" \"#FFFFFF\"\n\ncol.map(gray(0:8 / 8))\n\n\n\n\n\n\n\n",
    "preview": "posts/2005-04-25-color/2005-04-25-color_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-10-10T19:18:50+09:00",
    "input_file": {},
    "preview_width": 2600,
    "preview_height": 1600
  },
  {
    "path": "posts/2005-04-19-radar/",
    "title": "RADAR Plot",
    "description": "삼각함수를 이용해서 원을 그릴 수 있고, 이를 이용하면 RADAR Plot을 그릴 수 있다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2005-04-19",
    "categories": [
      "Visualization"
    ],
    "contents": "\n\nContents\n일러두기\nGraphics Development - RADAR Plot\n예제\n수식의 이해\n원으로의 수렴\nRADAR Plot\n\n\n일러두기\n2005-04-19 네이버 블로그에 게시된 내용을 옮겨 온 글입니다. 지금의 R 환경과 다소 내용이 다를 수 있음을 밝여둡니다.\nGraphics Development - RADAR Plot\n90년대 초 S-PLUS라는 놈을 처음 접했을 때 가장 매료되었던 부분은 미려한 Graphic의 출력물과 다양한 프로그램으로 원하는 Graph를 그릴 수 있다는 점이 었다. 개인적으로 C 언어를 즐겨 사용했던 시절이라 S 언어의 Script 프로그램은 그리 부담이 가지 않았었다. SAS는 Procedure 중심의 절차적인 언어라 정해진 Procedure의 틀안에서만 움직여야 했다. 물론 IML을 이용해서 조금은 융통성이 있는 노력을 시도한 적은 있지만…\n그러나 근 십년이 넘었고, 개인적으로 그 후 SAS를 사용하지 않았기 때문에 십 여년의 전 기준으로 두 언어를 비교할 수는 없다. SAS도 나름대로의 강점이 있다고는 생각하기 때문이다. 하지만 S 언어는 Linux 시스템처럼 생각하는 것은 다 구현할 수있는 자유로움을 선사해 주었으며, 이점은 SAS를 비롯한 다른 통계 언어가 따라 오지 못하는 독보적인 영역이라 생각한다.\nR과 S-PLUS도 비교의 여지는 있다. 공개 프로그램이냐, 상업적인 프로그램이냐의 성격도 중요한 점이다. 개인적으로는 HP-UX나 Sun OS의 UNIX냐 LINUX냐의 차이점과 유사하다고 생각한다.\n상용 vs 공개, Commercial 라이브러리 vs 다양한 3-Party의 라이브러리\n이번에는 이정도까지만 하자. 90년대 초에는 어둠의 S-PLUS를 사용했고 근래에는 1개월 짜리 Trial Version의 S-PLUS를 사용하다보니 짧은 기간 안에 S-PLUS에 대해 섭렵하기는 어렵고 다분히 주관적인 비교만 가능할 뿐이다. 그래서 그냥 고민 없이 R을 쓰고 있기는 하다.\n미안한 점은 여기에 실리는 script가 S-PLUS에서 제대로 돌아가느냐 하는 점이다. 물론 R의 확장 라이브러리는 S-PLUS에서 안돌아 갈 수도 있으며, Porting이 필요한 부분이 있겠다. 그러나 S Language의 Base로 작성된 부분은 동일하게 작동이 되거나 일부의 수정을 전제로 작동이 가능 할 것이다. 그래도 여전히 R & S-PLUS라는 Category에 Blog을 연재하기로 한다.\n그리고 R의 버젼은 2.1.0을 기준으로 작성하였음을 밝혀둔다. 아마 2.0 이전 버전을 사용할 경우 Mosaic Plot을 설명할 때, hcl function이 작동하지 않았을 것이다. hcl은 2.1.0의 grDevices package에 포함된 function인데 2.0 이전에는 이 패키지가 기본 패키지가 아니었기 때문이다. 물론 이 패키지를 library 함수 등을 써서 사용할 수는 있다.\n예제\n각설하고, 이번에는 RADAR Plot을 한번 만들어 보자. 2003년도 모 회사에 근무할 적에 실제 사용했던 Plot이다.\nRADAR Plot은 특정 그룹의 성격을 설명하는 N개의 변수의 점수분포를 비교하는 그림이다. 단위 그룹별로 각각의 변수의 관계를 볼 수도 있고, 여러 개의 그룹의 차이도 비교할 수 있는 그림이다. 다변량 분석에서 사용하는 Star Plot과도 유사한 그림이라 하겠다.\n그러면 예제 그림을 한번 보자.\n2001년, 2002년, 2003년도의 자료를 가지고 ACR, AHT, ASA, MCR, OCR, AAR의 점수를 비교한 그림이다. ACR, AHT, ASA, MCR, OCR, AAR에 대해서는 알 필요가 없을 듯하다. 일단 RADAR Plot이 뭔가 하는 것만 이해 하면 된다.\n다음이 위 RADAR Plot을 그리는 function을 비롯한 R Script이다.\n\n\nmy.radar <- function (score, legends, labels) \n{\n par(mfrow = c(1, 1), pty = 's', mar = c(2, 4, 2, 4))\n plot(c(-2.5, 2.5), c(-2.5, 2.5), type = 'n', axes = FALSE, \n      xlab = '', ylab = '')\n \n angle <- (0:60) / 60 * 2 * pi\n \n x1 <- cos(angle)\n y1 <- sin(angle)\n x2 <- 2 * cos(angle)\n y2 <- 2 * sin(angle)\n \n lines(x1, y1)\n lines(x2, y2)\n \n x <- rep(0,6)\n y <- rep(0,6)\n \n for (i in 1:nrow(score)) {\n  for (j in 0:5) {\n   lines(c(0, x2[j * 10 + 1]), c(0, y2[j * 10 + 1]))\n   x[j + 1] <- 2 * score[i, j + 1]/ 10 * cos(angle[j * 10 + 1])\n   y[j + 1] <- 2 * score[i, j + 1]/ 10 * sin(angle[j * 10 + 1])\n  }\n  points(x, y, pch = 18, col = i + 1)\n  lines(x, y, col = i + 1, lty = i)\n  lines(c(x[1], x[6]), c(y[1], y[6]), col = i + 1, lty = i)\n }\n text(x2[1] + 0.3, y2[1], labels[1])\n text(x2[11] + 0.2, y2[11] + 0.2, labels[2])\n text(x2[21] - 0.2, y2[21] + 0.2, labels[3])\n text(x2[31] - 0.4, y2[31], labels[4])\n text(x2[41] - 0.2, y2[41]- 0.2, labels[5])\n text(x2[51] + 0.2, y2[51]- 0.2, labels[6])\n \n title(\"Radar Diagram for Center Performance\")\n legend(\"topright\", legends, lty = 1:length(legends), bty = \"n\",\n        col = 2:length(legends) + 1, cex = .8)\n}\n\nlabels <- c('ASA', 'AHT', 'ACR', 'AAR', 'OCR', 'MCR')\nscore <- matrix(c(4, 6, 3, 8, 7, 9, 5, 4, 8, 9, 10, 7, 5, 5, 7, 8, 6, 7), \n                nrow = 3, byrow = TRUE)\nlegends <- c('200211', '200212', '200301')\nmy.radar(score, legends, labels)\n\n\n\n\n수식의 이해\n이 Chart에서의 핵심은 원을 그리는 부분이라 할 수 있다. 그러면 R을 이용해서 원을 그리는 방법에 대해서 알아 보자.\n방법의 핵심은 “원이 한 점에서 동일한 거리에 있는 점들의 집합이다.” 라는 것을 이용하는 것이다. 즉, 한 점(원점)에서 거리가 R(반지름)인 집합을 만들어 선으로 이으면 될 것이다. 그리고 무수히 많은 점의 좌표를 구하는 것이 아니라 단지 몇 개의 점의 좌표를 구한 후 선을 잇는 것도 핵심이다. 무수히 많은 좌표가 아니라 어느 정도 많은 좌표를 구한 다음에 선으로 잇는 방법을 택하기로 한다. 어찌보면 점들의 수는 다다익선이 아니라 어느 정도 원의 모양새가 갖추어질 정도의 수가 되겠다.\n그러면 어떻게 점의 좌표를 구할 수 있을까?\n직각 삼각형에서의 밑변의 길이와 높이를 구하는 공식을 고등학교 때 배운 적이 있을 것이다. 원점을 기준으로 밑변의 길가 x-축의 좌표이고, 원점을 기준으로 높이가 y-축의 좌표인 것이다.\n\n\n\n그리고 원의 각도는 \\(2\\times\\pi\\) 라디안이므로 \\(2\\times\\pi\\)를 n개로 나누면 n개의 꼭지점을 갖는 다각형을 만들 수 있으며, 그 점의 갯수가 일정량을 넘으면 원처럼 보이는 것이다. 앞의 예제에서는 n을 60으로 나누었으며, 실제로 그 다각형은 우리의 눈으로 볼 경우 원으로 보여지게 된다. 물론 원으로 보일 정도가 되려면 반지름인 R이 커지면 N의 수도 그 이상으로 커져야 됨은 자명한 일이다.\n원으로의 수렴\n그럼 n의 크기에 따른 다각형이 원에 근접하는 것을 실험해 보자.\n다음과 같은 Function을 만들었다.\n\n\ncircle <- function (n) \n{\n par(pty = 's')\n plot(c(-2.5, 2.5), c(-2.5, 2.5), type = 'n', axes = FALSE, xlab = '', ylab = '')\n \n angle <- (0:n) / n * 2 * pi\n \n x1 <- cos(angle)\n y1 <- sin(angle)\n x2 <- 2 * cos(angle)\n y2 <- 2 * sin(angle)\n \n lines(x1, y1)\n lines(x2, y2)\n title(paste(paste(paste(\"Angle is\", round(2 * pi / n, digit = 1)),\n                   \" Pi Radian\\nPoint is\"), n))\n}\n \n \npar(mfrow = c(2, 2), mar = c(1, 1, 4, 1))\ncircle(10)\ncircle(20)\ncircle(30)\ncircle(40)\n\n\n\n\nN의 갯수가 20을 넘어서도 마치 원처럼 보인다.\nRADAR Plot\n다음은 R의 창시자인 Ross Ihaka의 뉴질랜드 auckland 대학에서 강의한 한 통계학 강의에서 출제된 2003년 8월 기말고사의 모범답안의 예로 RADAR Plot의 이야기를 마치기로 한다.\n\n\n# 빈좌표를 그린다.\nplot.new()\npar(mar = rep(0.1, 4))\nplot.window(xlim = c(-25, 25), ylim = c(-25, 25), asp = 1)\n \n# 회색의 다각형을 만든다.\ntemp = c(4, 4.8, 6.4, 9, 12, 15.4, 17.5, 16.9, 15, 10.4, 6.8, 4.6)\ntheta <- seq(0, length = 12, by = 2 * pi/12)\nx = temp * cos(theta)\ny = temp * sin(theta)\npolygon(x, y, col = \"lightgray\")\n\n# 12개의 기준선(파선)을 그린다.\nsegments(0, 0, 20 * cos(theta), 20 * sin(theta), lty = \"dotted\")\n \n# 원을 그린다.\nsegments(0, 0, 20 * cos(theta), 20 * sin(theta), lty = \"dotted\")\nphi = seq(3, 360 - 3, length = 72) * (pi/180)\nfor (r in c(5, 10, 15)) lines(r * cos(phi), r * sin(phi), lty = \"dotted\")\nlines(20 * cos(phi), 20 * sin(phi))\n \n# 12월도 Text를 출력한다.\ntext(24 * cos(theta), 23 * sin(theta), month.abb)\n \n# 기준점수 Text를 출력한다.\nlabs = seq(5, 20, by = 5)\ntext(labs, rep(0, length(labs)), labs)\n\n\n\n\n\n\n\n",
    "preview": "posts/2005-04-19-radar/2005-04-19-radar_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-10-10T22:57:35+09:00",
    "input_file": {},
    "preview_width": 2600,
    "preview_height": 1600
  },
  {
    "path": "posts/2005-04-17-spine/",
    "title": "Spine Plot",
    "description": "Barchart는 일변량 범주형 자료에서 Class의 돗수의 비율을 막대의 길이로 표현한 것인 반면 Spine Plot은 막대의 길이는 동일하게 하고 Class의 돗수의 비율을 막대의 폭으로 표현한 것이다.",
    "author": [
      {
        "name": "유충현",
        "url": {}
      }
    ],
    "date": "2005-04-17",
    "categories": [
      "Visualization"
    ],
    "contents": "\n\nContents\n일러두기\nGraphics Development - Spine Plot\n함수의 정의\n함수의 이용\n\n\n일러두기\n2005-04-17 네이버 블로그에 게시된 내용을 옮겨 온 글입니다. 지금의 R 환경과 다소 내용이 다를 수 있음을 밝여둡니다.\nGraphics Development - Spine Plot\nSpine Plot은 Barchart의 일종이라 할 수 있다. Barchart는 일변량 범주형 자료에서 Class의 돗수의 비율을 막대의 길이로 표현한 것인 반면 Spine Plot은 막대의 길이는 동일하게 하고 Class의 돗수의 비율을 막대의 폭으로 표현한 것이다.\nMosaic Plot이 Spine Plot을 기본 아이디어로 만들어 졌다고 Mosaic Plot을 이야기할 때 언급한 적이 있었다. Spine Plot은 단지 단변량 Mosaic Plot이라고도 할 수 있다. Spine Plot은 범주들간의 비율차를 이해하는데 장점이 있다.\n함수의 정의\nR의 barplot 함수를 이용해서 다음과 같은 spineplot 함수를 만들어 보았다.\n\n\nspineplot <- function (data, ...) {\n  temp = data\n  \n  if (is.null(ncol(data))) {\n    len = length(data)\n    for (i in 1:len)\n      temp[i] = data[i] * (max(data) / data[i])\n    wid = data\n  } else {\n    len = ncol(data)\n    for (i in 1:len)\n      temp[, i] = data[, i] * (max(apply(data, 2, sum)) / apply(data, 2, sum))[i]\n    wid = apply(data,2,sum)\n  }\n  \n  barplot(temp, width = wid, axes = FALSE, ...)\n}\n\n\n\n함수의 이용\n그러면 Mosaic Plot에서 사용한 클래식 음악에 대한 자료와 타이타닉 자료를 이용해서 Spine Plot을 그려보면서 Barchart및 Mosaic Plot과의 차이점을 알아보자.\n                                       교육수준\n                          --------------------------------\n                               고학력             저학력\n                          --------------------------------\n                                  클래식 음악 듣기\n                나이      예   아니오            예   아니오 \n                          ---------------    --------------\n               고연령       210    190           170   730\n               저연령       194    406           110   290\n                     \n\n\nmusic = c(210, 194, 170, 110, 190, 406, 730, 290)\ndim(music) = c(2, 2, 2)\ndimnames(music) = list(Age = c(\"Old\", \"Young\"), \n                       Education = c(\"High\", \"Low\"), \n                       Listen = c(\"Yes\", \"No\"))\n\npar(mfrow = c(2, 2))\nbarplot(apply(music, 1, sum), col = \"lightblue\",\n        main = \"Bar Chart\")\nmosaicplot(apply(music, 1 , sum), col = \"lightblue\", \n           main = \"Mosaic Plot\")\nspineplot(apply(music, 1, sum), col = \"lightblue\", \n          main = \"Spine Plot\")\n\n\n\n\n그림에서처럼 일변량일 경우에는 Mosaic Plot과 Spine Plot의 차이점은 없다. 단지 Barchart에서는 막대의 폭은 동일한데 길이의 차이가 있고, Spine Plot은 막대의 길이는 동일한데, 폭의 넓이가 차이가 난다.\n타이타닉의 자료를 이용하여 몇 개의 Chart를 그려 보자.\n\n\npar(mfrow = c(2, 2))\nbarplot(apply(Titanic, c(4, 1), sum), col = c(\"lightblue\", \"mistyrose\"),\n        main = \"Survived over Economic status (class)\")\nbarplot(apply(Titanic, c(4, 2), sum), col = c(\"lightblue\", \"mistyrose\"),\n        main = \"Survived over Sex\")  \nbarplot(apply(Titanic, c(4, 3), sum), col = c(\"lightblue\", \"mistyrose\"),\n        main = \"Survived over Age\") \nbarplot(apply(Titanic, 4, sum), col = c(\"lightblue\", \"mistyrose\"),\n        main = \"Survived\")\n\n\n\n\nBarchart보다 Spine Plot이 범주들간의 비율차를 이해하기가 쉽다.\n\n\npar(mfrow = c(2, 2))\nspineplot(apply(Titanic, c(4, 1), sum), col = c(\"lightblue\", \"mistyrose\"),\n          main = \"Survived over Economic status (class)\")\nspineplot(apply(Titanic, c(4, 2), sum), col = c(\"lightblue\", \"mistyrose\"),\n          main = \"Survived over Sex\")\nspineplot(apply(Titanic, c(4, 3), sum), col = c(\"lightblue\", \"mistyrose\"),\n          main=\"Survived over Age\")  \nspineplot(apply(Titanic, 4, sum), col = c(\"lightblue\", \"mistyrose\"),\n          main = \"Survived\")\n\n\n\n\n마지막으로 타이타닉의 승무원과 승객의 생존 비율을 두 가지의 Chart로 그려서 비교해 보았다.\n\n\ntemp = c(apply(Titanic[dimnames(Titanic)[[1]] != \"Crew\", , , ], 2:4, sum),\n                    Titanic[dimnames(Titanic)[[1]] == \"Crew\", , , ])\ndim(temp) = c(2, 2, 2, 2)\ndimnames(temp) = list(Sex = c(\"Male\", \"Female\"),\n                      Age = c(\"Child\", \"Adult\"),\n                      Survived = c(\"No\", \"Yes\"),\n                      Class = c(\"Passenger\", \"Crew\"))\npar(mfrow = c(1, 2))\nbarplot(apply(temp, 3:4, sum), col = c(\"lightblue\", \"mistyrose\"),\n        main = \"Bar Chart\",\n        legend = rownames(apply(temp, 3:4, sum)))\nspineplot(apply(temp, 3:4, sum), col = c(\"lightblue\", \"mistyrose\"),\n          main = \"Spine Plot\")\n\n\n\n\n\n\n\n",
    "preview": "posts/2005-04-17-spine/2005-04-17-spine_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-10-10T23:15:12+09:00",
    "input_file": {},
    "preview_width": 2600,
    "preview_height": 1600
  }
]
